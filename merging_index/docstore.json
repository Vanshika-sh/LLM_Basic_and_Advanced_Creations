{"docstore/data": {"a4213a0c-8fd1-46eb-aafa-d22b7215bb4a": {"__data__": {"id_": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}, {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, {"node_id": "f7399abd-cb1b-48f9-886a-8746f5f1e173", "node_type": "1", "metadata": {}, "hash": "0f63f2d3865eb73bcbe7d9640eb81ce9aaf264d8fc768e2598f9366304350af2", "class_name": "RelatedNodeInfo"}]}, "text": "1\nRetrieval-Augmented Generation for\nAI-Generated Content: A Survey\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\nFangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui\nAbstract \u2014Advancements in model algorithms, the growth of\nfoundational models, and access to high-quality datasets have\npropelled the evolution of Artificial Intelligence Generated Con-\ntent (AIGC). Despite its notable successes, AIGC still faces\nhurdles such as updating knowledge, handling long-tail data,\nmitigating data leakage, and managing high training and infer-\nence costs. Retrieval-Augmented Generation (RAG) has recently\nemerged as a paradigm to address such challenges. In partic-\nular, RAG introduces the information retrieval process, which\nenhances the generation process by retrieving relevant objects\nfrom available data stores, leading to higher accuracy and better\nrobustness. In this paper, we comprehensively review existing\nefforts that integrate RAG technique into AIGC scenarios. We\nfirst classify RAG foundations according to how the retriever\naugments the generator, distilling the fundamental abstrac-\ntions of the augmentation methodologies for various retrievers\nand generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies\nthat help with potential future progress. We also summarize\nadditional enhancements methods for RAG, facilitating effective\nengineering and implementation of RAG systems. Then from\nanother view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for\nresearchers and practitioners. Furthermore, we introduce the\nbenchmarks for RAG, discuss the limitations of current RAG\nsystems, and suggest potential directions for future research.\nGithub: https://github.com/PKU-DAIR/RAG-Survey.\nIndex Terms \u2014Retrieval-augmented generation, AI-generated\ncontent, generative models, information retrieval.\nI.INTRODUCTION\nA.Background\nRECENT years have witnessed the surge in interests\nsurrounding Artificial Intelligence Generated Content\n(AIGC). Various content generation tools have been metic-\nulously crafted to produce diverse outputs across various\nmodalities, such as Large Language Models (LLMs) including\nthe GPT series [1]\u2013[3] and the LLAMA series [4]\u2013[6] for\ntexts and codes, DALL-E [7]\u2013[9] and Stable Diffusion [10]\nfor images, and Sora [11] for videos. The word \u201cAIGC\u201d\nemphasizes that the contents are produced by advanced gen-\nerative models other than human beings or rule-based ap-\nproaches. These generative models have achieved remarkable\nperformance due to the utilization of novel model algorithms,\n\u2022Penghao Zhao and Hailin Zhang contributed equally to this paper.\n\u2022Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng\nGeng, Fangcheng Fu, Ling Yang, Wentao Zhang and Bin Cui are with\nPeking University (e-mail: penghao.zhao@stu.pku.edu.cn, z.hl@pku.edu.cn,\nyuqinhan@stu.pku.edu.cn, wzr@stu.pku.edu.cn, 1800012997@pku.edu.cn,\nccchengff@pku.edu.cn, yangling0818@163.com, wentao.zhang@pku.edu.cn,\nbin.cui@pku.edu.cn).\n\u2022Jie Jiang is with Tencent Inc. (email: zeus@tencent.com)\n\u2022Bin Cui is Corresponding Author.explosive scale of foundation models, and massive high-\nquality datasets. Specifically, sequence-to-sequence tasks have\ntransitioned from utilizing Long Short-Term Memory (LSTM)\nnetworks [12] to Transformer-based models [13], and image-\ngeneration tasks have shifted from Generative Adversarial Net-\nworks (GANs) [14] to Latent Diffusion Models (LDMs) [10]\nas well. Notably, the architecture of foundation models, ini-\ntially constituted by millions of parameters [15], [16], has now\ngrown to billions or even trillions of parameters [1], [4], [17].\nThese advancements are further bolstered by the availability\nof rich, high-quality datasets [1], [18], which provide ample\ntraining samples to fully optimize model parameters.\nInformation retrieval is another pivotal application within\nthe field of computer science. Different from generation,\nretrieval aims to locate relevant existing objects from a vast\npool of resources. The most prevalent application of retrieval\nlies in web search engines, which primarily focus on the task\nof document retrieval [19], [20]. In the present era, efficient\ninformation retrieval systems can handle document collections\non the order of billions [21], [22]. Besides documents, retrieval\nhas also been applied for many other modalities [23]\u2013[26].\nDespite the remarkable progress made by advanced gen-\nerative models, AIGC continues to face a number of well-\nknown challenges, including the struggle to maintain up-to-\ndate knowledge, the inability to incorporate long-tail knowl-\nedge [27], and the risk of leaking private training data [28].\nRetrieval-Augmented Generation (RAG) is proposed to allevi-\nate, if not completely address, the aforementioned challenges\nthrough its adaptable data repository [29]. The knowledge\nstored for retrieval can be conceptualized as non-parametric\nmemory, which is easily modifiable, capable of accommo-\ndating broad long-tail knowledge, and also able to encode\nconfidential data. In addition, retrieval can also be employed\nto reduce the generation costs. For example, RAG can reduce\nthe size of large generative models [30], provide support for\nlong contexts [31], and eliminate certain generation steps [32].\nA typical RAG process is depicted in Fig. 1. Given an\ninput query, the retriever locates and looks up relevant data\nsources, then the retrieved results interact with the generator\nto enhance the overall generation process. There are sev-\neralfoundational paradigms (foundations in short) according\nto how the retrieved results augment the generation: they\ncan serve as augmented input to the generator [33], [34];\nthey can join at the middle stage of generation as latent\nrepresentations [35], [36]; they can contribute to the final\ngeneration results in the form of logits [37], [38]; they can\neven influence or omit certain generation steps [32], [39].arXiv:2402.19473v3  [cs.CV]  14 Apr 20242\nFig. 1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and\nthe generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results\nand ultimately produces outcomes of various modalities.\nMoreover, beyond the foundational RAG process, researchers\nhave proposed numerous enhancements to elevate the overall\nquality. These methods encompass specific optimizations for\nindividual components as well as holistic enhancements aimed\nat the entire pipeline.\nIn addition, while the concept of RAG initially emerged\nin text-to-text generation [34], this technique has also found\napplications across various domains, including codes [40]\u2013\n[42], audios [43], [44], images [45]\u2013[47], videos [48], [49],\n3D [50], [51], knowledge [52]\u2013[54], and AI for science [55],\n[56]. In particular, the essential idea and process of RAG are\nlargely consistent across modalities. However, it necessitates\nminor adjustments in augmentation techniques, and the se-\nlection of retrievers and generators varies depending on the\nspecific modalities and applications.\nDespite the rapid growth in recent research on RAG and\nthe booming applications, a systematic review encompassing\nall foundations, enhancements, and applications is notably\nabsent, hindering the development of this field. For one thing,\nthe absence of discussion on RAG foundations significantly\nundermines the practical value of the research in this do-\nmain, leaving the potential of RAG not fully explored. While\nthe majority of research interest, particularly among LLM\nresearchers, centers on query-based RAG in text-generation\ntasks, it is essential to acknowledge that other RAG foun-\ndations are also effective and with significant potential for\nusage and further development. For another, the lack of an\noverview on RAG applications causes researchers and practi-\ntioners to overlook RAG\u2019s progress across multiple modalities\nand remain unaware of how RAG can be effectively applied.\nAlthough text generation is typically considered as the main\napplication of RAG, we emphasize that the development ofRAG in other modalities has also begun to catch on and\nhas yielded promising advancements. Certain modalities have\na rich historical connection to retrieval techniques, infusing\nRAG with distinctive characteristics. Inspired by this, in this\npaper, our objective is to present a comprehensive survey to\nprovide a systematic overview of RAG.\nB.Contribution\nThis survey offers a comprehensive overview of RAG, cov-\nering foundations, enhancements, applications, benchmarks,\nlimitations, and potential future directions. While retrievers\nand generators exhibit variations across modalities and tasks,\nwe distill the fundamental abstractions of RAG foundations,\nconsidering applications as adaptations stemming from these\nabstractions.", "start_char_idx": 0, "end_char_idx": 8968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78b629a5-d948-4752-953c-92ec8f2abd64": {"__data__": {"id_": "78b629a5-d948-4752-953c-92ec8f2abd64", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}, {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, {"node_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71", "node_type": "1", "metadata": {}, "hash": "0cb925af162397fad1504f1a6f778ebaff0c22db95a3980d89546b4d103ab791", "class_name": "RelatedNodeInfo"}]}, "text": "We aim to offer references and guidelines to\nresearchers and practitioners, providing valuable insights for\nadvancing RAG methodologies and related applications. In\nsummary, we list our contributions as follows:\n\u2022We conduct a comprehensive review of RAG, and distill\nthe abstractions of RAG foundations for various retrievers\nand generators.\n\u2022We investigate the enhancements in the literature of\nRAG, elaborating the techniques leveraged to enable\nmore effective RAG systems.\n\u2022For various modalities and tasks, we survey existing\nAIGC methods that incorporate RAG techniques, exhibit-\ning how RAG contributes to current generative models.\n\u2022We discuss the limitations and promising research di-\nrections of RAG, shedding light on its potential future\ndevelopment.\nC.Related Work\nAs the field of RAG advances, several surveys have\nemerged; yet they address only specific facets of the area. In3\nparticular, they either exclusively focus on a single RAG foun-\ndation or provide only a brief overview of RAG augmentation\nmethodologies for limited scenarios.\nMost of the existing works focus on text-related RAG tasks\nthat are facilitated by LLMs, without in-depth investigation\nin other modalities. The survey by Li et al. [57] offers a\nbasic overview of RAG and discusses specific applications\nwithin the scope of text generation tasks. In a similar vein,\nthe tutorial crafted by Asai et al. [58] centers on retrieval-\nbased language models, detailing their structures and training\nstrategies. Meanwhile, a recent survey by Gao et al. [59]\nexplores RAG in the context of LLMs, with a particular\nemphasis on enhancement approaches for query-based RAG.\nRecognizing that RAG has extended beyond the text domain,\nour work broadens its reach to the entire AIGC landscape,\nfacilitating a more comprehensive coverage of RAG research.\nIn addition, another survey proposed by Zhao et al. [60]\nintroduces RAG applications across multiple modalities, but\nignoring the discussion on RAG foundations. While existing\nresearch has explored various aspects of RAG, there remains\na need for a comprehensive overview that covers RAG foun-\ndations, enhancements, and its applicability across different\ndomains. In this paper, we aim to address the gap by presenting\na systematic survey of RAG.\nD.Roadmap\nThe rest of the paper is organized as follows. Section II elab-\norates on the preliminary of RAG, introducing retrievers and\ngenerators. Section III presents RAG foundations and further\nenhancements on RAG. Section IV reviews existing research\non RAG across various applications. Section V investigates the\nbenchmark frameworks for RAG. Section VI discusses current\nlimitations of RAG and potential future directions. Finally,\nSection VII concludes this paper.\nII. PRELIMINARY\nIn this section, we provide an overview of the general RAG\narchitecture and explore the generators and the retrievers in\ntoday\u2019s RAG-based AIGC.\nA.Overview\nAs shown in Fig. 1, the entire RAG system consists of\ntwo core modules: the retriever and the generator, where the\nretriever searches for relevant information from the data store\nand the generator produces the required contents. The RAG\nprocess unfolds as follows: (i) the retriever initially receives\nthe input query and searches for relevant information; (ii) then,\nthe original query and the retrieval results are fed into the\ngenerator through a specific augmentation methodology; (iii)\nfinally, the generator produces the desired outcomes.\nB.Generator\nThe remarkable performance of generative AI across di-\nverse tasks has ushered in the era of AIGC. The generation\nmodule plays a crucial role within the RAG system. Different\ngenerative models are applied for different scenarios, such\nas transformer models for text-to-text tasks, VisualGPT [61]\nfor image-to-text tasks, Stable Diffusion [10] for text-to-\nimage tasks, Codex [2] for text-to-code tasks, etc. Here we\nintroduce 4 typical generators that are frequently used in RAG:\ntransformer model, LSTM, diffusion model, and GAN.1)Transformer Model :Transformer models are one of the\nbest performing models in the field of Natural Language Pro-\ncessing (NLP), consisting of self-attention mechanisms, feed-\nforward networks, layer normalization modules, and residual\nnetworks [62]. As shown in Fig. 2, the input of the transformer\nis mapped to a tensor xinwith a shape of ( b,s,h) after the\ntokenization process and embedding model, where brepresents\nbatch size, srepresents sequence length and hrepresents\nhidden dimension. Next, the position encoding will be sent\nto the self attention layer along with this tensor. The input\nxinand the output xoutof the self-attention module will be\nconnected by the residual network and the layer normalization\nmodule. Finally, the output of the \u201dAdd & Norm\u201d module xout\nwill be sent to the feed forward network.\nThe entire process can be defined as follows:\nQ=xin\u2217Wq+bq\nK=xin\u2217Wk+bk\nV=xin\u2217Wv+bv\nxout=LayerNorm 1(Softmax (Q\u2217KT\n\u221a\nh)\u2217V\u2217Wo+bo)+xin\ny=LayerNorm 2((xout\u2217W1+b1)\u2217W2+b2) +xout\nIt should be noted that wq, wk, wv, woare learnable tensors\nwith shape ( h,h);bq, bk, bv, boare a learnable tensors with\nshape ( h,).\n2)LSTM :Long Short-Term Memory (LSTM) [63] is a\nspecial Recurrent Neural Network (RNN) model that over-\ncomes the exploding/vanishing gradient problems of RNN in\nprocessing long-term dependency information by introducing\ncell state and gate mechanisms. The LSTM model consists\nof three gates: Input Gate, Forget Gate, and Output Gate.\nThese gates update the cell state by controlling the information\nflow, enabling the model to remember long-term dependent\ninformation. Cell State is the core module of the LSTM model\nwhich can memorize and maintain information. The Input Gate\ndecides which input data should be retained in the cell state.\nForget Gate determines which cell state information should be\ndiscarded to avoid excessive memory. Output Gate determines\nhow the information in the cell state affects the current output.\nThe flow of data and the collaborative work process between\ncomponents are shown in the Fig. 2.\nThe entire process can be defined as follows:\nf=sigmoid (Wf\u2217xt+Uf\u2217yt\u22121+bf)\nz=tanh(Wz\u2217xt+Uz\u2217yt\u22121+bz)\ni=sigmoid (Wi\u2217xt+Ui\u2217yt\u22121+bi)\no=sigmoid (Wo\u2217xt+Uo\u2217yt\u22121+bo)\nct=z\u2299i+f\u2299ct\u22121\nyt=o\u2299tanh(ct)4\n(a) General transformer model architecture.\n (b) General LSTM block architecture.\n(c) General latent diffusion model architecture.\n (d) General GAN architecture.\nFig. 2: General architectures of several generators.\n3)Diffusion Model :Diffusion models [64] are a family\nof deep generative models that can create realistic and di-\nverse samples of data [65]\u2013[72], such as images [73]\u2013[79],\ntexts [80]\u2013[83], videos [84]\u2013[88], and molecules [89]\u2013[93].\nAs shown in Fig. 2, diffusion models work by gradually\nadding noise to data until it becomes random, then reversing\nthe process to generate new data from noise. This process\nis based on probabilistic modeling and neural networks.\nDiffusion models mainly have three equivalent formulations:\ndenoising diffusion probabilistic models [65]\u2013[67], score-\nbased generative models [68], [69], and stochastic differen-\ntial equations [70], [71], with following improvements like\nDDIM [94], Rectified Flow [95], Consistency Model [96] and\nRPG-DiffusionMaster [79].\nEspecially, let x0be a random variable that follows the\ndata distribution q(x0), and let xtbe a random variable that\nfollows the distribution q(xt|x0)after adding noise at time\nstept. Then, DDPM can be formulated as follows:\n\u2022Forward Process The forward process perturbs data with\na sequence of Gaussian noise injections, transforming the\ndata distribution q(x0)into a simple prior distribution\nq(xT)\u2248N(0, I). The transition kernel at each time step\nis given by\nq(xt|xt\u22121) =N(xt;p\n1\u2212\u03b2txt\u22121, \u03b2tI),\nwhere \u03b2t\u2208(0,1)is a hyperparameter. The marginal\ndistribution of xtconditioned on x0is\nq(xt|x0) =N(xt;\u221a\u00af\u03b1tx0,(1\u2212\u00af\u03b1t)I),\nwhere \u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=0\u03b1s.\n\u2022Reverse Process The reverse process generates new\ndata samples by reversing the forward process with alearnable Markov chain.", "start_char_idx": 8969, "end_char_idx": 16986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4961f468-f454-4ff5-8b6c-07d1984842c8": {"__data__": {"id_": "4961f468-f454-4ff5-8b6c-07d1984842c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}, {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, {"node_id": "9d81e32e-a897-46af-85a9-02e278286cd0", "node_type": "1", "metadata": {}, "hash": "1c581ea8f27640b23541bcda4f611dd2bb777d2ed23f9a757478ea4264ea11a1", "class_name": "RelatedNodeInfo"}]}, "text": "The prior distribution is p(xT) =\nN(xT; 0, I)and the transition kernel is\np\u03b8(xt\u22121|xt) =N(xt\u22121;\u00b5\u03b8(xt, t),\u03a3\u03b8(xt, t)),\nwhere \u03b8denotes model parameters, and \u00b5\u03b8(xt, t)and\n\u03a3\u03b8(xt, t)are parameterized by deep neural networks. The\nreverse process starts from sampling xT\u223cp(xT)and\niteratively samples xt\u22121\u223cp\u03b8(xt\u22121|xt)until t= 0.\n\u2022Model Training For each sample x0\u223cq(x0), the model\ntraining objective is to maximizing the variational lower\nbound (VLB) of the log-likelihood of the data x0. The\nsimplified form LVLB(x0)is given by\nEq(x1:T|x0)\"\n\u2212logp(xT)\u2212TX\nt=1logp\u03b8(xt\u22121|xt)\nq(xt|xt\u22121)#\nWith simplication and reparameterization trick, the over-\nall objective Eq(x0)\u0002\nLVLB(x0)\u0003\ncan be simplified into the\nfinal form\nEt\u223cU[1,T],x0\u223cq(x0),\u03f5\u223cN(0,I)[\u03bb(t)\u2225\u03f5\u2212\u03f5\u03b8(xt, t)\u2225]\nwhere \u03bb(t)is a positive weighting function, U[1, T]is\na uniform distribution over the set {1,2, . . . , T }, and \u03f5\u03b8\nis a deep neural network with parameter \u03b8that predicts\nthe noise vector \u03f5given xtandt. Note that, the overall\nobjective is also equivalent to matching the joint distri-\nbution of the reverse process p\u03b8(x0, x1, . . . , x T)to that\nof the forward process q(x0, x1, . . . , x T)by minimizing\nthe KL divergence between them.\n4)GAN :Generative Adversarial Networks (GANs) [14]\nare highly anticipated deep learning models with amazing\ncapabilities which can simulate and generate realistic images,\naudio, and other data. Due to its outstanding performance,\nGANs have achieved significant achievements in various5\nfields [97]. The design inspiration of GANs comes from the\nzero-sum game in game theory.\nAs shown in Fig. 2, a typical GAN consists of two main\ncomponents: a generator and a discriminator. These two parts\ncompete with each other through adversarial learning, allowing\nthe generator to continuously improve its ability to generate re-\nalistic samples, while the discriminator continuously improves\nits ability to distinguish between true and false samples.\nC.Retriever\nRetrieval is to identify and obtain relevant information given\nan information need. Specifically, let\u2019s consider information\nresources that can be conceptualized as a key-value store\n{(ki, vi)}N\ni=1, where each key kicorresponds to a value vi\n(kiandvican be identical). Given a query q, the objective\nis to search for the top- kmost similar keys using a similarity\nfunction s, and obtain the paired values. Based on different\nsimilarity functions, existing retrieval methods can be cate-\ngorized into sparse retrieval, dense retrieval, and others. For\nwidely used sparse and dense retrieval, the whole process can\nbe divided into two distinct phases: in the first phase, each\nobject is encoded into a specific representation; and in the\nsecond phase, an index is constructed to organize the data\nsource for efficient search.\n1)Sparse Retriever :Sparse retrieval methods are widely\nused in document retrieval, where the keys are actually doc-\numents to be searched (values are the same documents in\nthis scenario). These methods leverage term matching metrics\nsuch as TF-IDF [98], query likelihood [99], and BM25 [19],\nwhich analyze word statistics from texts and construct inverted\nindices for efficient searching. Among them, BM25 is a hard-\nto-beat baseline in industrial-scale web search. For a query q\ncontaining keywords {qi}n\ni=1, the BM25 score of a document\nDis:\ns(q, D) =nX\ni=1IDF(qi)\u00b7f(qi, D)\u00b7(a+ 1)\nf(qi, D) +a\u00b7(1\u2212b+b\u00b7|D|\navgdl)\nwhere IDF is the inverse document frequency weight, f(qi, D)\nis the number of times that qioccurs in the document D,|D|\nis the length of D,avgdl is the average document length in\nthe corpus collection, aandbare tunable parameters.\nIDF is computed as:\nIDF(qi) = ln(N\u2212n(qi) + 0.5\nn(qi) + 0.5+ 1)\nwhere Nis the number of documents, and n(qi)is the number\nof documents containing qi.IDF score is also used in TF-IDF.\nTo enable efficient search, sparse retrieval typically lever-\nages an inverted index to organize documents. Concretely, each\nterm from the query performs a lookup to obtain a list of\ncandidate documents, which are subsequently ranked based\non their statistical scores.\n2)Dense Retriever :Unlike sparse retrieval, dense retrieval\nmethods represent queries and keys using dense embedding\nvectors, and build approximate nearest neighbor (ANN) index\nto speed up the search. This can be applied to all modalities.\nFor text data, recent advancements in pre-trained models,including BERT [15] and RoBERTa [100], have been em-\nployed to encode queries and keys individually [20], [101]\u2013\n[104]. Similar to text, models have been proposed to encode\ncode data [25], [105], [106], audio data [26], [107], image\ndata [24], [108], video data [109], [110]. etc. The similarity\nscore between dense representations are usually computed\nwith metrics such as cosine, inner product, L2-distance.\nDuring training, dense retrieval usually follows a contrastive\nlearning paradigm, making positive samples more similar and\nnegative samples less similar. Several hard negative tech-\nniques [101], [111] have been proposed to further improve the\nmodel quality. During inference, approximate nearest neighbor\n(ANN) methods are applied for efficient searching. Various\nindices are developed to serve ANN search, such as tree [112],\n[113], locality sensitive hashing [114], neighbor graph index\n(e.g., HNSW [115], DiskANN [116], HMANN [117]), and\nthe combination of graph index and inverted index (e.g.,\nSPANN [22]).\n3)Others :In addition to sparse retrieval and dense re-\ntrieval, there are alternative methods for retrieving relevant\nobjects [118], [119]. Instead of calculating representations,\nsome research works directly use the edit distance between\nnatural language texts [120] or abstract syntax trees (AST)\nof code snippets [121], [122]. For knowledge graph, entities\nare linked with relations, which can be regarded as a pre-\nbuilt index for retrieval searching. Therefore, RAG methods\nwhich involve knowledge graph can use k-hop neighbor search\nas retrieval process [123], [124]. Named entity recognition\n(NER) [125] is another way of retrieval, where the input is\nthe query and the entites are the keys.\nIII. METHODS\nIn this section, we introduce RAG foundations and outline\nenhancement methods that further improve the effectiveness.\nA.RAG Foundations\nBased on how the retriever augments the generator, we\ncategorize RAG foundations into 4 classes, as shown in Fig. 3.\n1)Query-based RAG :Query-based RAG, originated from\nthe idea of prompt augmentation, integrates the user\u2019s query\nwith insights from information fetched during the retrieval\nprocess, directly into the initial stage of the language model\u2019s\ninput. This paradigm stands as a widely adopted approach\nwithin the applications of RAG. After retrieval, the retrieved\ncontent will be merged with the original user query to create a\ncomposite input sequence which is subsequently fed into the\ngenerator for response generation. Query-based RAG has been\nwidely applied across various modalities.\nFor Text Generation, REALM [33] employs a dual-BERT\nframework to streamline knowledge retrieval and integration,\nmarrying pre-trained models with knowledge extractors. The\ninitial BERT module processes the input question alongside\ndocuments to facilitate retrieval, utilizing MIPS for selecting\nthe top-k documents with the highest probability and periodi-\ncally updating the index. The document snippets obtained are\nthen integrated with the query, feeding into the second BERT\nmodule to produce multiple outputs that are aggregated into\na singular, comprehensive response. Lewis et al. [34] syner-\ngized pre-trained language models with knowledge retrieval6\nFig. 3: Taxonomy of RAG foundations.\nmechanisms, leveraging DPR and BART structures to ac-\ncomplish retrieval-augmented generation tasks. DPR serves as\nthe retrieval component, sourcing pertinent information from\nvast document databases, while BART uses this information\nfor text generation.", "start_char_idx": 16987, "end_char_idx": 24870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "801c3840-c975-4417-9efd-ba90e44414fc": {"__data__": {"id_": "801c3840-c975-4417-9efd-ba90e44414fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}, {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, {"node_id": "d78ca6ea-0bd4-4a06-a4de-9956656051dd", "node_type": "1", "metadata": {}, "hash": "5e1f7e76883b83dfadeac9a72b929a9ede1ad8ef5be9ea2275c5e90a49093aa6", "class_name": "RelatedNodeInfo"}]}, "text": "RAG-Token and RAG-Sequence differ in\ntheir retrieval timings, with the former retrieving information\nat each token generation and the latter conducting a single\nretrieval for the entire sequence. SELF-RAG [126] enhances\nthe accuracy and relevance of responses by integrating a\nretrieval and critique strategy. Initially, the model employs a\nretriever to search for information paragraphs closely related to\nthe input question. Subsequently, the critique model evaluates\nthese paragraphs to determine their relevance and level of\nsupport of the retrieved text, assessing their impact on the\ngeneration of responses. Finally, the generator model con-\nstructs responses based on this information and evaluates the\nquality of these responses through critique marks. In addition\nto being compatible with local generators, Query-based RAG\nis also applicable to scenarios that use LLM through API\ncalls. REPLUG [127] illustrates this methodology by treating\nthe language model as a \u201cblack box\u201d, utilizing Contriever to\nseamlessly incorporate relevant external documents into the\nquery. REPLUG LSR, a variant with LM-Supervised Retrieval,\nfurther refines this process by optimizing retrieval through\nlanguage model-supervised insights, aiming to reduce per-\nplexity scores and improve model performance by enriching\nits contextual understanding. In-Context RALM [128] uses\nBM25 for document retrieval and trains a predictive reranker\nto reorder and integrate the top-ranked documents.\nIn contemporary research on other modalities, augmenting\ninputs with retrieved contents (which are not limited to texts)\nhas proven highly effective in enhancing the performance of\nvarious tasks. This strategy is applicable across several critical\ndomains, including code generation, audio generation, and\nKnowledge Base Question Answering (KBQA).\nFor Text-to-Code task, APICoder [129] and DocPrompt-\ning [42] demonstrate how effectively integrating retrieved\ninformation into language models can improve the accuracy\nand relevance of generated code. In Automatic Program Repair\ntask, CEDAR [130] and InferFix [131] utilize retrieved code\nsnippets to aid the repair process, enhancing the model\u2019s\nunderstanding and application of repair strategies by combin-\ning them with the original input. For Code Completion task,\nReACC [132] employs a prompting mechanism, leveraging\nretrieved code snippets as part of the new input to increase\nthe accuracy and efficiency of code completion.\nRecent researches in Knowledge Base Question Answering(KBQA) has also shown significant effects of combining\nretrieval and language models. For instance, Uni-Parser [133],\nRNG-KBQA [123], and ECBRF [134] effectively improve\nthe performance and accuracy of QA systems by merging\nqueries and retrieved information into prompts. BLLM aug-\nmentation [135] represents an innovative attempt at zero-\nshot KBQA using black-box large language models. This\nmethod, by directly integrating retrieved information into the\nmodel input without the need for additional sample training,\ndemonstrates the great potential of combining retrieval and\nlanguage models to enhance the model\u2019s generalization ability\nin understanding and answering unseen questions.\nIn the AI-for-Science domain, Chat-Orthopedist [136] pro-\nvides support for shared decision-making among adolescents\nwith idiopathic scoliosis. It enhances the application effec-\ntiveness and information accuracy of LLMs by integrating\nretrieved information into the prompts of the model.\nIn the task of Image Generation, RetrieveGAN [45] en-\nhances the relevance and accuracy of generated images by\nintegrating retrieved information, including selected image\npatches and their corresponding bounding boxes, into the\ninput stage of the generator. IC-GAN [137] modulates the\nspecific conditions and details of the generated images by\nconcatenating noise vectors with instance features.\nFor 3D Generation, RetDream [50] initially utilizes\nCLIP [24] to retrieve relevant 3D assets, then merges the\nretrieved contents with the user input during the input phase.\nQuery-based RAG, often paired with LLM generators,\noffers modular flexibility, allowing swift integration of pre-\ntrained components for quick deployment. Prompt design is\ncrucial for utilizing retrieved data within this setup.\n2)Latent Representation-based RAG :In Latent\nRepresentation-based RAG framework, retrieved objects are\nincorporated into generative models as latent representations.\nThis enhances the model\u2019s comprehension abilities and\nimproves the quality of the generated content.\nThe Fusion-in-Decoder (FiD) [35] technique leverages both\nBM25 and DPR for sourcing supportive paragraphs. It con-\ncatenates each retrieved paragraph and its title with the query,\nprocessing them individually through the encoder. FiD re-\nduces computational complexity and efficiently utilizes rel-\nevant information to generate answers by fusing information\nfrom multiple retrieved paragraphs in the decoder, rather than\nprocessing each paragraph in the encoder. The application\nof Fusion-in-Decoder methodologies transcends the realm of\ntextual content processing, demonstrating substantial potential\nand adaptability in processing code, structured knowledge,7\nand diverse multimodal datasets. Specifically within the code-\nrelated domain, technologies such as EDITSUM [138], BASH-\nEXPLAINER [139], and RetrieveNEdit [140] adopt the FiD\napproach, facilitating integration through encoder-processed\nfusion. Re2Com [141], and RACE [142] , among other meth-\nods, also feature the design of multiple encoders for different\ntypes of inputs.\nIn the field of Science, RetMol [55] also employs the\nFusion-in-Decoder strategy, integrating information at the\ndecoder stage to enhance the relevance and quality of the\ngenerated molecular structures.\nIn the field of Knowledge Base Question Answer-\ning (KBQA), the FiD method has been widely adopted,\ndemonstrating significant effectiveness. UniK-QA [143], DE-\nCAF [144], SKP [145], KD-CoT [146], and ReSKGC [147]\nhave effectively enhanced the performance of QA systems\nthrough the application of Fusion-in-Decoder technology. This\nillustrates that by integrating RAG for KBQA, the\nRetro [36] pioneers the integration of retrieved text via\n\u201cChunked Cross-Attention\u201d a novel mechanism that segments\nthe input sequence into discrete chunks. Each chunk indepen-\ndently executes cross-attention operations, thereby mitigating\ncomputational burdens. This technique enables the model to\nselectively retrieve and assimilate distinct documents for var-\nied sequence segments, fostering dynamic retrieval throughout\nthe generation process. This enhances the model\u2019s adaptability\nand enriches the contextual backdrop of generated content. In\nthe domain of image generation, cross-attention mechanisms\nhave been widely adopted within RAG frameworks. Methods\nsuch as Re-imagen [148], KNN-Diffusion [149], RDM [150]\nand LAION-RDM & ImageNet-RDM [151] utilize cross-\nattention to integrate multiple retrieval results, effectively\nenhancing the overall performance of the models.\nIn addition, there are also some other novel structures worth\nour attention, Li [152] introduced the ACM, a text-image affine\ncombination module, which notably does not employ any form\nof attention mechanism. Memorizing Transformers [31] revo-\nlutionize long document processing through the integration of\na kNN-augmented attention mechanism within a Transformer\nlayer. This innovation triggers a kNN search amidst input se-\nquence processing, fetching data based on similarities between\nthe sequence and stored key-value pairs, thereby elevating\nperformance without necessitating complete retraining. This\napproach not only bolsters processing efficiency but also\nbroadens the model\u2019s memory span, enabling self-retrieval\nfrom its generated outputs and fine-tuning for extensive\nknowledge bases or code repositories. Unlimiformer [153],\nby embedding a k-nearest neighbors (kNN) index within a\npre-trained encoder-decoder transformer framework, pioneers\nhandling inputs of indefinite length. Storing hidden states\nof input tokens in the kNN index allows for the efficient\nretrieval of highly relevant tokens during decoding. This\ninnovation extends the model\u2019s capacity to manage prolonged\nsequences. Kuratov et al. [154] integrated Transformer with\nRNN, utilizing the model\u2019s intermediate output as the content\nfor retrieval. This process was executed at each layer of the\nTransformer, thereby significantly extending the text window\u2019s\nlength and effectively mitigating the issue of \u201cLost in theMiddle\u201d [155]. Diverging from prior methods for knowledge,\nEaE [156] empowers language models to internalize explicit\nentity knowledge. EaE introduces an entity-specific param-\neterization, optimizing inference efficacy through an entity\nmemory layer embedded within the transformer architecture.\nThis layer directly acquires entity representations from textual\ndata, utilizing a sparse retrieval strategy to fetch the nearest\nentities based on their embeddings, thus refining the model\u2019s\ncomprehension through a calculated aggregation of entity-\nspecific information. on this basis , TOME [157] shifts the\nfocus towards comprehensive mention encodings, prioritizing\nthe granularity of mention over mere entity representations.\nIt meticulously creates a database that stores key and value\nencodings along with entity IDs, enabling the retrieval of much\nmore fine-grained information. TOME integrates an initial\ntransformer block to process input texts, followed by TOME\nblocks with memory attention layers, facilitating the synthesis\nof multifaceted information sources and enhancing inferential\nreasoning capabilities, even for unencountered entities.", "start_char_idx": 24871, "end_char_idx": 34579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c96feaa-3fa4-4003-80fa-069f0af17fb7": {"__data__": {"id_": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}, {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, {"node_id": "638f46aa-98ea-47c6-9083-3d5c75cc98b4", "node_type": "1", "metadata": {}, "hash": "47aa8341805eb8373995fbe9e89f3cf4f7b160423e5f87be8c6e8eee1b2e8464", "class_name": "RelatedNodeInfo"}]}, "text": "In the field of 3D generation, ReMoDiffuse [51] introduces\na semantics-modulated attention mechanism which enhances\nthe accuracy of generating corresponding 3D motions based on\ntextual descriptions. AMD [158] achieves efficient conversion\nfrom text to 3D motion by fusing the original diffusion process\nwith the reference diffusion process.\nIn the Audio domain, Koizumi et al. [43] utilized an\nLLM, incorporating encoded dense features in the atten-\ntion module to guide the generation of audio captions. Re-\nAudioLDM [159] utilizes distinct encoders to extract deep\nfeatures from text and audio, which are then integrated into the\nattention mechanism of its Latent Diffusion Model (LDM).\nFor video captioning, R-ConvED [48] uses a convolutional\nencoder-decoder network to process retrieved video-sentence\npairs with an attention mechanism, generating hidden states to\nproduce captions. CARE [160] introduces a concept detector\nto produce concept probabilities, and incorporates concept\nrepresentations into a hybrid attention mechanism. EgoIn-\nstructor [49] employs gated-cross attention to integrate textual\ninputs with encoded video features, enhancing the relevance\nand coherence of the generated captions for egocentric video\ncontent.\nFinally, Latent Representation-based RAG is adaptable to\nvarious modalities and tasks. It obtains the hidden states of\nretrieved data, enabling seamless integration between retrievers\nand generators. However, additional training is required to\nalign the latent space. Within this paradigm, we have the flex-\nibility to design intricate and novel algorithms that effectively\ncombine the information from retrieved contents.\n3)Logit-based RAG :In logit-based RAG, generative mod-\nels integrate retrieval information through logits during the\ndecoding process. Typically, the logits are combined through\nsimple summation or models to compute the probabilities for\nstep-wise generation.\nThe kNN-LM [37] model integrates a pre-trained neural\nlanguage model with the k-nearest neighbor search. It employs\nthe pre-trained model to generate a list of candidate words and\ntheir probability distribution, while simultaneously performing\nretrieval from a data repository to find the k most relevant8\nneighbors based on the current context, thus enhancing the\noutput of the original language model. The innovation at the\ncore of this model lies in its ability for dynamic retrieval of\ninformation from a broad text corpus, significantly improving\nthe accuracy and relevance of its predictions, particularly in\naddressing rare patterns and adapting to various fields. He et\nal. [38] introduced a new framework that is predicated on\nperforming retrieval operations only when necessary, aimed\nat enhancing the inference efficiency of the kNN-LM model\nthrough an adaptive retrieval. This framework accelerates the\nmodel\u2019s inference speed by training a retrieval adapter, which\nautomatically identifies and eliminates unnecessary retrieval\nactions in certain scenarios. This method allows the model to\ndynamically decide on the necessity of retrieval based on the\ncurrent context, thereby balancing the trade-off between per-\nformance and efficiency, and substantially increasing inference\nspeed while maintaining model performance.\nUnlike previous methods that only merge memories during\nthe testing time, TRIME [161] achieves memory merging\nduring both training and testing phases, treating in-batch\nexamples as accessible memory. It leverages new data batching\nand memory construction techniques to effectively utilize\nexternal memory. It employs BM25 scores to pack paragraphs\nwith high lexical overlap into the same batch, constructing\nthe training memory to further optimize model performance.\nNPM [162] is a non-parametric masked language model com-\nprised of an encoder and a reference corpus. Unlike traditional\nmodels that apply a softmax over a finite vocabulary, NPM\nmodels a non-parametric distribution over the corpus. The\nencoder\u2019s role is to map phrases from the corpus into fixed-\nsize vectors, filling in [MASK] by retrieving the phrase most\nsimilar to the masked position.\nBeyond Text, other modalities, such as code and Image,\nalso leverage logit-based RAG. For code-to-text conversion\ntask, Rencos [121] generates multiple summary candidates in\nparallel from the retrieved code. It then normalizes these can-\ndidates using edit distance and calculates the final probability\nto select the summary output that best matches the original\ncode. In code summarization task, EDITSUM [138] enhances\nthe quality of summary generation by integrating prototype\nsummaries at the probability level. For text-to-code tasks,\nthe kNN-TRANX [163] model employs a combination of a\nconfidence network and meta-knowledge to merge retrieved\ncode fragments. It utilizes a seq2tree structure to generate\ntarget code that closely matches the input query, thereby\nincreasing the accuracy and relevance of code generation. In\nimage captioning tasks, MA [164] combines an attention-based\nencoder-decoder, using the image encoder to extract visual\nfeatures to construct the semantic part, and decodes it word by\nword with the information retrieved. MA interpolates between\ntwo distributions generated by the caption decoder and the\nmemory-augmented module to determine the distribution of\nthe next word.\nIn conclusion, Logit-based RAG effectively leverages histor-\nical states (or other data sources) to infer the current state and\ncombines information at the logit level. This approach is well-\nsuited for sequence generation tasks. It couples retrieval and\ngeneration, primarily requiring training of the generator. Novelapproaches can be designed to effectively utilize the obtained\nprobability distributions and adapt to subsequent tasks.\n4)Speculative RAG :Speculative RAG seeks opportunities\nto use retrieval instead of pure generation, aiming to save\nresources and accelerate response speed. REST [32] replaces\nthe small models in speculative decoding [165] with retrieval,\nenabling the generation of drafts. GPTCache [39] addresses the\nissue of high latency when using the LLM APIs by building\na semantic cache for storing LLM responses. COG [166]\ndecomposes the text generation process into a series of copy-\nand-paste operations, retrieving words or phrases from the\ndocuments instead of generation. Cao et al. [167] proposed a\nnew paradigm to eliminate the dependence of the final result\non the quality of the first-stage retrieved content, replacing\ngeneration with directly retrieved phrase level content.\nIn conclusion, Speculative RAG is currently primarily ap-\nplicable to sequential data. It decouples the generator and\nthe retriever, enabling the direct use of pre-trained models as\ncomponents. Within this paradigm, we can explore a wider\nrange of strategies to effectively utilize the retrieved content.\nB.RAG Enhancements\nIn this section, we introduce methods which enhance the\nperformance of a constructed RAG system. We categorize\nexisting methods into 5 groups based on their enhancement tar-\ngets: input, retriever, generator, result, and the entire pipeline.\n1)Input Enhancement :The input, initially fed into the re-\ntriever, significantly impacts the final outcome of the retrieval\nstage. In this section, we introduce two methods for input\nenhancement: query transformation and data augmentation.\na)Query Transformation :Query transformation can\nenhance the result of retrieval by modifying the input query.\nQuery2doc [168] and HyDE [169] use the original query to\ngenerate a pseudo document, which is later used as the query\nfor retrieval. The pseudo document contains richer relevant\ninformation, which helps to retrieve more accurate results.\nTOC [170] employs recursive retrieval augmented clarification\non ambiguous questions to construct a tree of disambiguated\nquestions, thereby generating comprehensive answers to these\nambiguous questions. During the construction process of this\ntree structure, TOC utilizes a self-verification pruning method\nto ensure the factual relevance of each node.\nb)Data Augmentation :Data augmentation improves\ndata before retrieval, including techniques such as removing ir-\nrelevant information, eliminating ambiguity, updating outdated\ndocuments, synthesize new data, etc.\nMake-An-Audio [44] uses captioning and audio-text re-\ntrieval to generate captions for language-free audio to mitigate\ndata sparsity, and adds random concept audio to improve the\noriginal audio. LESS [171] strategically selects an optimal\ndataset for downstream tasks by analyzing gradient informa-\ntion. It aims to maximize the dataset\u2019s impact on fine-tuning\nthe model\u2019s performance in response to instructional prompts.\nReACC [132] employs data augmentation (including renaming\nand dead code insertion) to pre-train the code retrieval model.\n2)Retriever Enhancement :The retrieval process is crucial\nin RAG systems. Generally, the better the retrieved content\nquality, the easier it is to stimulate the ability of LLMs in-\ncontext learning as well as other generators and paradigms.9\nFig. 4: Taxonomy of RAG Enhancements.\nThe worse the content quality, the more likely it is to cause\nmodel hallucinations. Therefore, in this section, we will\ndiscuss how to efficiently improve the effectiveness of the\nretrieval process.\na)Recursive Retrieval :Recursive retrieval is to perform\nmultiple searches to retrieve richer and higher-quality contents.\nReACT [172] uses Chain-of-Thought (CoT) [173] to break\nqueries down for recursive retrieval and provide richer in-\nformation. RATP [174] uses the Monte-Carlo Tree Search\n(MCTS) to perform multiple simulations, identifying optimal\nretrieval content. This content is then integrated into a template\nand sent to the generator for final production.", "start_char_idx": 34580, "end_char_idx": 44329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0eb84439-aaea-4a8a-b8d2-504a38ad5eef": {"__data__": {"id_": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}, {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, {"node_id": "b833d9b9-cff4-4e06-9dba-20efb87d02d1", "node_type": "1", "metadata": {}, "hash": "86acf41015531bda8785a43dc2b5ced0fd4a0692d6ac5e7721c155949d42e2d9", "class_name": "RelatedNodeInfo"}]}, "text": "This content is then integrated into a template\nand sent to the generator for final production.\nb)Chunk Optimization :Chunk optimization refers to\nadjusting chunk size for improved retrieval results. Sentence-\nwindow retrieval [175] is an efficient approach that enhances\nretrieval by fetching small chunks of text and returning\na window of relevant sentences surrounding the retrieved\nsegment. This method ensures that the context before and\nafter the targeted sentence is included, providing a more\ncomprehensive understanding of the retrieved information.\nAuto-merge retrieval is another advanced RAG method of\nLlamaIndex [175] which organizes the document in a tree-\nlike structure, with the parent node containing the content of\nall children nodes. For example, articles and paragraphs, as\nwell as paragraphs and sentences, all follow a parent-child\nrelationship. In the retrieve process, fine-grained search for\nchildren nodes ultimately returns the parent node, effectively\nproviding richer information. To address the lack of contextual\ninformation, RAPTOR [176] employs recursive embedding,\nclustering, and summarization of text chunks until further\nclustering becomes infeasible, thereby constructing a multi-\nlevel tree structure.\nc)Retriever Finetuning :As a core component in the\nRAG system, the retriever plays a crucial role in the entire\nsystem operation process. An effective embedding model can\ncluster semantically similar content in vector space, enhancing\nthe retriever\u2019s capability to provide valuable information for\nthe subsequent generator, thus boosting the RAG system\u2019s effi-\nciency. Hence, the proficiency of the embedding model [177]\u2013\n[180] is vital for the RAG system\u2019s overall effectiveness.\nIn addition, for embedding models that already have good\nexpression power, we can still finetune them using high-quality\ndomain data or task related data to improve their performance\nin specific domains or tasks. REPLUG [127] treats LM as ablack box and update the retriever model based on the final\nresults. APICoder [129] finetunes the retriever with python\nfiles and api names, signature, description. EDITSUM [138]\nfinetunes the retriever to decrease the jaccard distance between\nsummaries after retrieval. SYNCHROMESH [122] adds tree\ndistance os ASTs in the loss and uses Target Similarity\nTuning to finetune the Retriever. R-ConvED [48] finetunes the\nRetriever with the same data as generator. Kulkarni et al. [181]\napplied infoNCE loss to finetune the Retriever.\nd)Hybrid Retrieval :Hybrid retrieve denotes the concur-\nrent employment of a diverse array of retrieval methodologies\nor the extraction of information from multiple distinct sources.\nRAP-Gen [182] and ReACC [132] use both dense retriever\nand sparse retriever to improve the quality of retrieval. Ren-\ncos [121] uses sparse retriever to retrieve similar code snippets\non syntactic-level and usse dense retriever to retrieve similar\ncode snippets on semantic-level. BASHEXPLAINER [139]\nfirst uses dense retriever to capture semantic information\nand then uses sparse retriever to acquire lexical information.\nRetDream [50] first retrieves with text and then retrieves with\nthe image embedding. CRAG [183] has designed a retrieval\nevaluator to assess the relevance of retrieved documents to the\ninput query, triggering three types of retrieval actions based on\nvarying confidence levels: if deemed correct, it directly uses\nthe retrieval results for Knowledge Refinement; if incorrect,\nit resorts to Web Search; and if ambiguous, it combines both\napproaches. To enhance performance in question-and-answer\ntasks, Huang et al. [184] introduced two metrics, DKS(Dense\nKnowledge Similarity) and RAC(Retriever as Answer Clas-\nsifier), during the retrieval phase. These metrics account for\nboth the pertinence of the answers and the applicability of the\nunderlying knowledge. UniMS-RAG [185] introduces a novel\nkind of token, termed as the \u201cacting token\u201d, which determines\nthe source from which to retrieve information.\ne)Re-ranking :The Rerank technique refers to reorder-\ning the retrieved content in order to achieve greater diversity\nand better results. Re2G [186] applies a re-ranker [187]\nmodel after the traditional retriever. The effect of the re-\nranker model is to re-rank retrieved documents, the purpose\nof which is to reduce the impact of information loss caused\nby compressing text into vectors on the quality of retrieval.\nAceCoder [188] reranks the retrieved programs with a selector\nto reduce redundant programs and obtain diverse retrieved\nprograms. XRICL [189] uses a distillation-based exemplar10\nreranker after retrieval. Rangan [190] employs the Quantized\nInfluence Measure, assessing statistical biases between a query\nand a reference to evaluate the similarity of data subsets\nand rerank retrieval results. UDAPDR [191] uses LLMs to\ncost-effectively generate synthetic queries that train domain-\nspecific rerankers, which then apply multi-teacher knowledge\ndistillation to develop a cohesive retriever. LLM-R [192]\niteratively trains the retriever, using feedback from a frozen\nLLM to rank candidate documents and train the reward model.\nThe retriever is further trained based on knowledge distillation.\nEach iteration of training builds upon the retriever trained\nin the previous cycle, facilitating iterative optimization in\nsubsequent rounds.\nf)Retrieval Transformation :Retrieval Transformation\ninvolves rephrasing retrieved content to better activate the\ngenerator\u2019s potential, resulting in improved output.\nFILCO [193] effectively filters out irrelevant content from\nthe retrieved text chunk, leaving only the precise supporting\ncontent. This process simplifies the task for the generator,\nmaking it easier to predict the correct answer. FiD-Light [194]\ninitially employs an encoder to convert the retrieved content\ninto a vector, which it then compresses, resulting in a sub-\nstantial reduction of latency time. RRR [195] integrates the\ncurrent query with the top-k document in each round through\na template, and subsequently restructures it via a pre-trained\nLLMs(GPT-3.5-Turbo etc.).\ng)Others :In addition to the above optimization meth-\nods, there are also some other optimization methods for the\nretrieve process. For example, Meta-data filtering [196] is\na method to help processing retrieved documents which uses\nmetadata (such as time, purpose, etc.) to filter the retrieved\ndocuments for better results. GENREAD [197] and GRG [198]\nintroduce a novel approach where the retrieval process is\nsupplanted or improved by prompting a LLM to generate\ndocuments in response to a given question.\n3)Generator Enhancement :In RAG systems, the quality\nof the generator often determines the quality of the final output\nresults. Therefore, the ability of the generator determines the\nupper limit of the entire RAG system\u2019s effectiveness.\na)Prompt Engineering :Technologies in prompt engi-\nneering [199] that focus on improving the quality of LLMs\u2019\noutput, such as Prompt compression, Stepback Prompt [200],\nActive Prompt [201], Chain of Thought Prompt [173], etc.,\nare all applicable to LLM generators in RAG systems. LLM-\nLingua [202] applies a small model to compresses the overall\nlength of the query to accelerate model inference, relieving\nthe negative impact of irrelevant information on the model\nand alleviating the phenomenon of \u201cLost in the Middle\u201d\n[155]. ReMoDiffuse [51] decomposes complex descriptions\ninto anatomical text scripts by using ChatGPT. ASAP [203]\nadd exemplar tuples to the prompt for better results. An\nexemplar tuple is composed of the input code, a function\ndefinition, the results of analyzing that definition and its\nassociated comment. CEDAR [130] uses a designed prompt\ntemplate to organize code demonstration, query, and natural\nlanguage instructions into a prompt. XRICL [189] utilizes\nCOT technology to add translation pairs as an intermediate\nstep in cross linguistic semantic parsing and inference. AC-TIVERAG [204] employs the Cognition Nexus mechanism to\ncalibrate the intrinsic cognition of LLMs and applies COT\nprompt in answer generation. Make-An-Audio [44] is able to\nuse other modalities as input which can provide much richer\ninformation for the following process.\nb)Decoding Tuning :Decoding tuning refers to adding\nadditional controls during the generator processing, which can\nbe achieved by adjusting hyperparameters to achieve greater\ndiversity, limiting the output vocabulary in some form, and so\non.\nInferFix [131] balances the diversity and quality of\nresults by adjusting the temperature in decoder. SYN-\nCHROMESH [122] limits the output vocabulary of the de-\ncoder by implementing a completion engine to eliminate\nimplementation errors.\nc)Generator Finetuning :The finetuning of the gener-\nator can enhance the model\u2019s ability to have more precise\ndomain knowledge or better fit with the retriever.\nRETRO [36] fixes the parameters of the retriever and\nuses the chunked cross attention mechanism in the gen-\nerator to combine the content of the query and retriever.", "start_char_idx": 44234, "end_char_idx": 53262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f170389f-7074-4cd1-a5c2-dd67b644a58f": {"__data__": {"id_": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}, {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}]}, "text": "APICoder [129] finetunes the generator CODEGEN-MONO\n350M [205] with a shuffled new file combined with API\ninformation and code blocks. CARE [160] first uses image\ndata, audio data and vedio-text pairs to train encoders and then\nfinetune the decoder (generator) with the target of decreas-\ning caption loss and concept detection loss together, during\nwhich the encoders and the retriever are frozen. Animate-A-\nStory [206] optimizes the video generator with image data, and\nthen finetunes a LoRA [207] adapter to capture the appearance\ndetails of the given character. RetDream [50] finetunes a LoRA\nadapter [207] with the rendered images.\n4)Result Enhancement :In many scenarios, the result of\nRAG may not achieve the expected effect, and some tech-\nniques of Result Enhancement can help alleviate this problem.\na)Output Rewrite :Output Rewrite refers to rewriting\nthe content generated by the generator in certain scenarios to\nmeet the needs of downstream tasks.\nSARGAM [208] refines outputs in code-related tasks by em-\nploying a special Transformer alongside Deletion, Placeholder,\nand Insertion Classifiers to better align with the real-world\ncode context. Ring [209] obtains diversity results by reranking\ncandidates based on the average of per token log probabilities\nproduced by the generator. CBR-KBQA [54] revises the result\nby aligning generated relations with those presented in the\nlocal neighborhood of the query entity in knowledge graph.\n5)RAG Pipeline Enhancement :RAG Pipeline Enhance-\nment refers to optimizing the processes of RAG at the system\nlevel in order to achieve better performance results.\na)Adaptive Retrieval :Some studies and practices on\nRAG have shown that retrieval is not always beneficial for\nthe final generated results When the parameterized knowledge\nof the model itself is sufficient to answer relevant questions,\nexcessive retrieval will cause resource waste and may increase\nthe model\u2019s confusion. Therefore, in this chapter, we will\ndiscuss two types of methods for determining whether to\nretrieve, named rule-based and model-based methods.11\nRule-based: FLARE [210] actively decides whether and\nwhen to search through the probability in the generation pro-\ncess. Efficient-KNNLM [38] combines the generation proba-\nbility of KNN-LM [37] and NPM [162] with a hyperparameter\n\u03bbto determine the proportion of generation and retrieval.\nMallen et al. [211] used statistical analysis on questions to\nenable direct answers for high-frequency ones and applied\nRAG for low-frequency ones. Jiang et al. [212] studied Model\nUncertainty, Input Uncertainty, and Input Statistics to compre-\nhensively assess the confidence level of the model. Ultimately,\nbased on the confidence level of the model, a decision is made\nwhether to retrieve. Kandpal et al. [213] studied the correlation\nbetween the number of relevant documents and the model\u2019s\nknowledge mastery to assess the need for retrieval.\nModel-based: Self-RAG [126] uses a trained generator to\ndetermine whether to perform a retrieval based on the retrieve\ntoken under different instructions, and evaluates the relevance\nand level of support of the retrieved text through the Self-\nReflection token. Finally, the quality of the final output result\nis evaluated based on the Critique token. Ren et al. [214]\nused \u201dJudgment Prompting\u201d to determine whether LLMs can\nanswer relevant questions and whether their answers are\ncorrect or not, thereby assisting in determining the necessity of\na retrieval. SKR [215] uses the ability of LLMs themselves to\njudge in advance whether they can answer the question, and\nif they can answer, no retrieval is performed. Rowen [216]\nemploys a model as a sophisticated multilingual detection\nsystem to evaluate the semantic coherence of answers to\nidentical questions posed across various languages. In the\nevent of detected inconsistencies, it decides to retrieve external\ninformation, thereby enhancing the reasoning process and\nrectifying inaccuracies. Conversely, when responses exhibit\nconsistency, the system upholds the initially generated answer,\nwhich is derived from internal reasoning. AdaptiveRAG [217]\ndynamically decides whether to retrieve based on the query\ncomplexity by a classifier, which is a smaller LM.\nb)Iterative RAG :Iterative RAG progressively refines\nresults by repeatedly cycling through retrieval and generation\nphases, rather than a single round.\nRepoCoder [218] employs an iterative retrieval-generation\npipeline for code completion tasks, enhancing each retrieval\nquery with code generated in prior iterations to more ef-\nfectively leverage information dispersed across various files,\nthereby achieving superior results. ITER-RETGEN [219] syn-\nergizes retrieval and generation in an iterative manner. The\ncurrent output of the generator can to some extent reflect\nthe knowledge it still lacks, and the retrieve can retrieve the\nmissing information as contextual information for the next\nround, which helps to improve the quality of the generated\ncontent in the next round. SelfMemory [220] employs a\nretrieval-augmented generator in an iterative manner to create\nan unlimited memory pool. Following this, a memory selector\nis used to choose one output, which then serves as the\nmemory for the subsequent generation round. RAT [221] initial\ngenerates content by an LLM with a zero-shot CoT prompt,\nthen revises each thought step by retrieving knowledge from\nexternal knowledge base.IV.APPLICATIONS\nIn this section, we focus on RAG applications spanning\nvarious modalities. To echo with the taxonomy of RAG\nfoundations and enhancements, we also demonstrate their\nutilization across different tasks in Table I.\nA.RAG for Text\nTo begin with, text generation is among the most important\nand widely deployed applications for RAG. Here we introduce\npopular works for seven tasks, respectively.\n1)Question Answering :Question Answering involves the\nprocess of providing responses to posed questions by drawing\nfrom a vast and comprehensive collection of textual sources.\nFiD [35] and REALM [33] identify the top-k most pertinent\narticle snippets based on the query and forward each snippet\nalong with the question to LLMs to generate k responses.\nThese responses are then synthesized into a final answer.\nToutanova et al. [222] substituted the text corpus in REALM\nwith subgraphs from a knowledge graph, yielding impressive\nresults. As shown in Fig. 5, RETRO [36] employs attention\nmechanisms to integrate the question with relevant retrieved\ndocuments within the model to produce the final answer.\nSKR [215] observes that using RAG does not invariably\nbenefit Question Answering and thus explored guiding the\nmodel to evaluate its grasp of pertinent knowledge, subse-\nquently adapting its use of external resources for retrieval\nenhancement. TOG [223] introduces an innovative knowledge\ngraph-augmented LLM framework, which excels by fostering\ninteractions between LLMs and the Knowledge Graph and\nby expanding the inference path space with beam search.\nNPM [162] pioneers the use of nonparametric data distribu-\ntions in lieu of the softmax layer, enabling models with fewer\nparameters to perform effectively. Self-RAG [126] improves\nanswer quality by learning to discern when to retrieve, as-\nsess the retrieved content\u2019s relevance, and evaluate the final\ngenerated results using four types of reflective tokens. CL-\nReLKT [224] employs a language-generalized encoder to\nbridge the gap between question-document pairs across lan-\nguages, thus better leveraging multilingual data. CORE [225]\nmitigates language resource disparities by introducing a novel\ndense passage retrieval algorithm and a multilingual autore-\ngressive generation model. Lastly, EAE [156] enhances answer\nquality by retrieving entity embeddings for query entities and\nintegrating these with hidden states for further processing. UR-\nC C A F FW T r a ns f ormer \nEnc o der \nR etrie v al \nd ataset \nFrozen kNN Retriever \nKV\nRETR O b lo c k ( x L ) N eig hb o ur s \nIn p ut \nt o k ens Ch unk ed cr oss-att en tion ( C C A ) \nB ER T B ER T \nCondition \nA tt ending c h unk s Enc o ded neig hb o ur s \nC A \nC A \nA T T N Q \nEMB REA D A tt end Enc o ded neig hb o ur s \nC1 \nC2 \nC3 H1 \nH2 \nH3 \nHH1+ \nH2+ E1E2E1\nE2\nCA (H1+, E1) \nCA (H2+, E2) \nCCA (H, E) \nX\nFig. 5: Architecture of RETRO [36] model.\nQA [226] found that when encountering unseen problems,\nretrieving QA pairs has a better final effect; When encoun-\ntering problems that have not been seen before, the retrieve12\nTABLE I: Taxonomy of RAG applications across various modalities.", "start_char_idx": 53263, "end_char_idx": 61806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2915bcf-d475-4bab-9409-6cf973c97c17": {"__data__": {"id_": "d2915bcf-d475-4bab-9409-6cf973c97c17", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, {"node_id": "c4884c63-ab65-499b-a999-c681de0dfcc4", "node_type": "1", "metadata": {}, "hash": "46bd8a4fc3ed4d3290e7003f3407af8e4d2db97749eb64131085bf710c8ab18a", "class_name": "RelatedNodeInfo"}]}, "text": "RAG for Text\nQuestion Answering Human-Machine Conversation Neural Machine Translation Summarization Others\nREALM\u2021\u00a7TKEGEN\u00a7TOG\u2021\nSKR\u00a7\u00b6Self-RAG\u00a7\u00b6RIAG\u2021\nFiD\u2021\u00a7RETRO\u00a7NPM\u2021\u00a7CREA-ICL\u2020\u2021BlenderBot3\u2021\u00a7\nCEG\u2021\u2225Internet-Augmented-DG\u2021\u00a7\nConceptFlow\u2021\u00a7Skeleton-to-Response\u2021\u00a7NMT-with-Monolingual-TM\u2020\u2021\u00a7\nTRIME\u2021\u00a7KNN-MT\u2021\u00a7COG\u2021RAMKG\u2021\u00a7RPRR\u2021RIGHT\u2021\u00a7\nUnlimiformer\u00a7CONCRETE\u2021\u00a7Atlas\u2021\u00a7\nKG-BART\u2021\u00a7R-GQA\u2021\u00a7\nRAG for Code\nCode Generation Code Summarization Code CompletionAutomatic\nProgram RepairText-to-SQL and Code\n-based Semantic ParsingOthers\nSKCODER\u00a7RRGCode\u2021\nARKS\u2020\u00b6RECODE\nKNN-TRANX\u2225Toolcoder\u00a7\u2225RACE\u2020BASHEXPLAINER\u2021\nREADSUM\u2225Rencos\u2021\nCoRec\u2021Tram\u00a7\nEDITSUM\u2021ReACC\u2020\u2021RepoCoder\u2020\u00a7\u00b6\nDe-Hallucinator\u00b6REPOFUSE\u00a7\nRepoFusion\u00a7EDITAS\u00a7RING\u2225CEDAR\u00a7\nRAP-Gen\u2021\u00a7InferFix\u00a7\nSARGAM\u00a7RTLFixer\u2021\u00a7XRICL\u2021\u00a7SYNCHROMESH\u2021\u00a7\nRESDSQL\u00a7REFSQL\u2021\u00a7\nCodeICL\u00a7MURRE\u2225\u00b6De-fine\u2021\u2225Code4UIE\u00a7\nE&V StackSpotAI\u2021\u00a7\nImputBlaster\u00b6\nRAG for Knowledge RAG for 3D\nKnowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others Text-to-3D\nCBR-KBQA\u2021\u00a7\u2225TIARA\u2020\u2021\u00a7Keqing\u2020\u2021\u00a7\nRNG-KBQA\u2021\u2225ReTraCk\u00a7SKP\u2020\u2021\u00a7UniK-QA\u2020\u2021KG-FiD\u2021GRAPE\u2021\nSKURG\u2020\u2021KnowledGPT\u2021EFSUM\u00a7EfficientQA\u2021CORE\u00a7Convinse\u2020\u2021\nRINK\u2021\u00a7T-RAG\u2021\u00a7StructGPT\u2021GRetriever\u00a7SURGE\u00a7\nK-LaMP RHO\u2225ReMoDiffuse\u2020\u2021\nAMD\u2020\nRAG for Image RAG for Video\nImage Generation Image Captioning Others Video Captioning Video QA & Dialogue Others\nRetrieveGAN\u2021IC-GAN\u00a7Re-imagen\u00a7\nRDM Retrieve&Fuse\u00a7KNN-DiffusionMA\u2225REVEAL\u2021SMALLCAP\u2020\nCRSR\u2020RA-TransformerPICa\u2225Maira\u2021\nKIF\u2021RA-VQA\u2021KaVD\u2021\u00a7R-ConvED\u2021\u00a7\nCARE\u00a7\nEgoInstructor\u2020\u2021\u00a7MA-DRNN\u2020\u2021R2A\u2021\nTvqa+\u00a7VGNMN\u2021VidIL\u2020\u2021RAG-Driver\u2021\nAnimate-A-Story\u2020\u00a7\nRAG for Science RAG for Audio\nDrug Discovery Biomedical Informatics Enhancement Math Applications Audio Generation Audio Captioning\nRetMol\u2020\u00a7PromptDiff\u2020\u2021PoET\u2021Chat-Orthopedist\u2020\u00a7BIOREADER\u2020MedWriter\u2021QARAG\u2020\u2021LeanDojo\u2021RAG-for-math-QA\u2020\u2021Re-AudioLDM\u00a7Make-An-Audio\u2020\u00a7RECAP\u2021\u00a7\nQuery-based Latent-based Logit-based Speculative\n Query+Latent\n Latent+Logit \u2020Input \u2021Retriever \u00a7Generator \u2225Output \u00b6Pipeline\ntext chunk performs better. Therefore, it is proposed to simul-\ntaneously retrieve QA pairs and text chunks, and select the\nfinal answer by comparing the calibrated confidences. DISC-\nLawLLM [227] constructs a supervised fine-tuning dataset\nthrough a legal syllogism prompting strategy, enabling the\nmodel to receive support from the latest legal information.\nRAG-end2end [228] conducts simultaneous training of the\nretriever (DPR) and the generator (BART) to optimize per-\nformance for the end-to-end question-answering task and to\nfacilitate domain adaptation. MultiHop-RAG [229] is designed\nto extract pertinent information from a variety of distinct\ndocuments, aggregating this knowledge to equip the generator\nwith the necessary context for producing the definitive answer\nto the query.\n2)Fact Verification :Fact Verification involves assessing\nthe veracity of information, a critical function in disciplines\nsuch as Natural Language Processing (NLP), Information Re-\ntrieval, and Data Mining. In today\u2019s digital era, characterized\nby an exponential increase in data, particularly across social\nmedia and online news platforms, there is a rapid proliferation\nof unchecked information. Fact verification plays an essential\nrole in countering the spread of fake news, deceptive content,\nand rumors, thereby preserving the integrity of the information\nlandscape and ensuring the public\u2019s access to accurate knowl-\nedge. Consequently, automated fact verification systems are of\nimmense importance, with broad applications and significant\npractical value. CONCRETE [230] leverages cross-lingual\nretrieval mechanisms to tap into a wealth of multilingual evi-\ndence, effectively bridging the gap in resources for languages\nthat are underrepresented in fact-checking datasets. Hagstr \u00a8om\net al. [231] proved on LLaMA [4] and Atlas [30] that search\naugmentation is more beneficial for solving inconsistencyproblems than increasing model size. Atlas [30] shows that\nusing RAG to support LLMs in knowledge-intensive tasks\nmarkedly improves their few-shot learning performance.\n3)Commonsense Reasoning :Commonsense Reasoning\nentails the capability of machines to infer or make decisions on\nproblems or tasks in a human-like manner, drawing upon their\nacquired external knowledge and its application. However, the\nvast scope of common sense knowledge and the intricacies of\nreasoning processes make Commonsense Reasoning a peren-\nnially challenging and prominent area of research within the\nfield of NLP. KG-BART [232] expands the conceptual land-\nscape by incorporating intricate interrelations among diverse\nconcepts within a knowledge graph. It employs graph attention\nmechanisms to aid LLMs in crafting more nuanced and\nlogically coherent sentences. This approach not only improves\nthe models\u2019 generalization capabilities but also significantly\nbolsters their Commonsense Reasoning proficiency. Wan et\nal. [233] constructed the CONFLICTINGQA dataset, compris-\ning contentious questions and conflicting answer documents,\nto examine which textual features significantly influence LMs\u2019\nability to independently navigate controversial issues. The\nfindings reveal that LMs often neglect the stylistic aspects of\ntext that are typically valued by people.\n4)Human-Machine Conversation :Human-Machine Con-\nversation encompasses the ability of machines to comprehend\nnatural language and adeptly employ this skill to engage with\nhumans seamlessly. This capability represents a significant\nchallenge within the realms of Artificial Intelligence and Natu-\nral Language Processing and offers a broad spectrum of practi-\ncal applications. As such, Human-Machine Conversation con-\ntinues to be a focal point of research for many scholars. Con-\nceptFlow [234] leverages a commonsense knowledge graph to13\nstructure conversations, directing the flow of dialogue based on\nattention scores, and propelling the conversation forward. This\nmethod achieves commendable results even with a substantial\nreduction in model parameters. Cai et al. [235] reimagined the\ntext generation task as a cloze test by retrieving and distilling\nthe essence of past conversational history, leading to notable\noutcomes. Komeili et al. [236] augmented dialogue generation\nquality by harnessing advanced search engine technologies to\nsource pertinent content from the internet. BlenderBot3 [237]\nbroadens its search horizon, not only mining relevant internet\ncontent but also local dialogue history, and employs entity\nextraction among other techniques to refine the quality of the\nresulting dialogue. Kim et al. [238], PARC [239], and CREA-\nICL [240] improve the caliber of non-English conversations by\nincorporating cross-lingual knowledge, effectively addressing\nthe scarcity of non-English datasets and enhancing the quality\nof the generated dialogue. CEG [241] addresses hallucination\nissues through a post-processing mechanism, verifying LLM-\ngenerated answers through retrieval. If the answer is correct,\nthe retrieved document is added to the original answer as a\nreference; if the answer lacks reliable references, it guides the\nLLM to respond to the question anew.\n5)Neural Machine Translation :Neural Machine Transla-\ntion (NMT) is the automated process of translating text from\na source language to a target language [161], [242], [243].\nIt is a pivotal task in the domain of NLP and represents a\nsignificant objective in the pursuit of AI, boasting considerable\nscientific and practical significance. Cai et al.", "start_char_idx": 61807, "end_char_idx": 69149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d55f2bc4-1980-41a0-929f-f90452cd71ca": {"__data__": {"id_": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}, {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, {"node_id": "ad848f9e-6e82-4e71-92f7-1a534a8a184b", "node_type": "1", "metadata": {}, "hash": "58b053e1b7137d45e355adb640ff6ae383be6bfb4865c6fe8d3ada623fe85b69", "class_name": "RelatedNodeInfo"}]}, "text": "Cai et al. [242] proposed\nan innovative approach that utilizes monolingual corpora\nalongside multilingual learning techniques, challenging the\ntraditional dependency on bilingual corpora in Neural Machine\nTranslation. This approach ensures that the retrieval system\nprovides ample information while simultaneously optimizing\nboth the retrieval mechanism and the translation model, cul-\nminating in impressive performance. KNN-MT [243] executes\ntranslation tasks at the token level by computing vector space\ndistances. TRIME [161] effectively minimizes the discrepancy\nbetween training and inference phases by jointly training the\nretrieval system and the generation model, thereby enhancing\nthe precision of translations.\n6)Event Extraction :Event Extraction is a specialized\ntask within Natural Language Processing (NLP) that focuses\non pinpointing and extracting instances of particular event\ntypes from unstructured textual data. An event is generally\ncharacterized by a central action or predicate and the related\nentities, which can include participants, temporal indicators,\nlocations, and other relevant attributes. The objective of event\nextraction is to convert the nuanced details embedded within\ntext into a structured format, thereby facilitating advanced\nanalysis, efficient information retrieval, and practical down-\nstream applications. R-GQA [244] employs a retrieval-based\napproach to enhance the context of a given issue by identifying\nand utilizing the most closely aligned Question-Answer pair\nfrom a repository, thereby enriching the information available\nfor processing the current query.\n7)Summarization :In the realm of NLP, Summarization\nis a task aimed at distilling the essential information from\nlengthy texts and producing a concise, coherent summary thatencapsulates the primary themes. Summarization enables users\nto quickly grasp the essence of a text, thereby conserving\ntime that would otherwise be spent on reading extensive\nmaterial. There are two main approaches to Summarization:\nExtractive and Abstractive. Extractive Summarization involves\nthe automatic selection and compilation of key phrases directly\nfrom the source text. A key phrase succinctly captures the\nmain themes, content, or perspectives of the text and is\ntypically composed of one or several words. The generation\nof key phrases is instrumental for understanding, categorizing,\nretrieving, and organizing textual information. It is extensively\napplied in fields such as search engine optimization, aca-\ndemic research, text summarization, and more. This technique\nrefrains from creating new sentences, instead repurposing\nsegments from the original text. Abstractive Summarization,\non the other hand, entails comprehending the original text\u2019s\nmeaning and reformulating it into new sentences [153], [245]\u2013\n[247]. This approach can convey the source\u2019s intent more\nfluidly but poses greater challenges in terms of implementation\ndue to its complexity. RAMKG [245] effectively leverages a\ncomprehensive English corpus to bolster the performance of\nKeyphrase Generation in non-English contexts. It does so by\nenhancing the alignment of keywords extracted from texts in\ndifferent languages that share similar subject matter. Unlimi-\nformer [153] addresses the issue of input length constraints in\ntransformer-based models by retrieving and utilizing the top-\nk most relevant hidden states, thereby extending the model\u2019s\ncapacity to handle longer inputs. RPRR [246] employs a\nRetrieve-Plan-Retrieve-Read approach to overcome the limited\ncontext window constraints faced by LLMs, utilizing retrieved\ninformation to generate high-quality Wikipedia documents for\nemerging events. RIGHT [247] chooses to use different types\nof retrievers in different datasets to enhance the generator,\nwhich can effectively improve the quality of automatically\ngenerated labels in simple deployment.\nB.RAG for Code\nSeparate retrieval and generation approaches have histor-\nically been employed for code-related tasks. For retrieval,\nsimilar code snippets can be identified using Abstract Syntax\nTrees (AST) or text edit distance. For generation, sequence-\nto-sequence models are employed to generate code or natural\nlanguage. Recent RAG research combines both retrieval and\ngeneration techniques to enhance the overall performance.\n1)Code Generation :The goal of code generation is to\ntransform natural language (NL) descriptions into code im-\nplementation, which can be seen a process of text-to-code.\nTherefore, LSTM and transformer models are widely used for\ngenerator. Whether to use code-specific retrieval or text-based\nretrieval depends on the contents to be searched.\nRetrieval-based prompt engineering is one of the most\nprevalent scenarios of RAG in code generation. In-context\nlearning includes training samples in prompts as the input for\nsequence-to-sequence generative models. Retrieval techniques\nare adopted to find similar training samples to the test input,\nso that the prompt can be more informative and related.\nREDCODER [40] retrieves similar NL descriptions using\ndense retriever CodeBert [25], then concatenates the NL texts,\ntheir paired codes, for downstream generator PLBART [41].14\nCodeT5Mix [248] adopts the paradigm of RECODER, propos-\ning a model that functions dually as both the retriever and\ngenerator. APICoder [129] first train a Bert-based deep re-\ntriever to align the embeddings of NL descriptions and API\ndocumentation; then, it retrieves relevant API information to\nbuild prompt for the generator CODEGEN-MONO [249]. The\nfollowing work [250] uses the same pipline, renaming the\nretriever module as APIFinder. COCOGEN [251] aims at com-\nmonsense reasoning, which generates code-based reasoning-\ngraphs with Codex [2] given NL inputs; it adds an evaluation\nsetting of dynamic prompt selection, which actually retrieves\nrelevant examples for prompts. In DocPrompting [42] given\nan NL intent, the retriver retrieves relevant documentations,\nthen the generator generates codes based on the NL and\nretrieved documents. It evaluates both sparse retrievers and\ndense retrievers, and also tries different generators in ex-\nperiments. CodeT5+ [252] adopts the CodeT5 [106] model\nfor both the retriever and generator, leveraging only the\nencoder part in the retrieval process. AceCoder [188] fixes the\nretriever to BM25 [19], and tests several LLM generators for\ncode generation. A3CodGen [253] extracts local information,\nretrieves relevant global functions using embeddings, and\nincorporates third-party library information to construct the\nprompt for LLM-based code generation. SKCODER [254]\nemploys BM25 to retrieve relevant code snippets, which are\nfurther processed to produce sketch template. The template and\nthe original description are concatenated for final generation.\nCodeGen4Libs [255] uses RAG for both import statements and\ncodes, employing BM25 as retriever and finetuned CodeT5\nas generator. CODEAGENT [256] design agents to search\non web, retrieve relevant documentation, generate programs,\nand test correctness. RRGCode [257] retrieves relevant code\nsnippets using both sparse and dense retrieval, employs a\ncross-encoder to re-rank the retrieval results, then generates\ncode using the concatenation of the query and the retrieved\ncodes. A recent study [258] shows that retrieval-augmented\nframework for code suggestions, including code generation\nand code completion, can improve the performance by a\nlarge margin. ARKS [259] steps further upon prompt-based\nRAG, incorporating iterative RAG to re-formulate queries and\nupdate knowledge soup (containing documentation, execution\nfeedback, generated code, etc.) for dense retrieval, improving\nfinal generation accuracy.\nRetrieval results can be applied during the generation pro-\ncess as well. RECODE [120] retrieves NL descriptions and\npaired codes using edit distance, then extracts n-gram action\nsubtrees from codes\u2019 ASTs. During LSTM-based generation,\nthe patterns of processed subtrees are leveraged to increase the\ncorresponding word probability at each decoding step. kNN-\nTRANX [163] uses seq2tree model BertranX [260] to convert\nNL to code AST. It constructs a datastore for each code AST\nprefix and NL pair; i.e., for each NL-code pair, the context\nrepresentation of the i-th context is obtained by encoding\nNL and the i-th prefix of code\u2019s AST through the seq2tree\nmodel. At each decoding step of the generation, the hidden\nrepresentations are searched within the datastore to form a\nnew probability, which is later combined with the seq2tree\nmodel\u2019s output through a confidence network.ToolCoder [261] performs normal code generation, and\nconducts online search or offline retrieval when encountering\nspecial < API > token. This paradigm makes the model learn\nto leverage API tools.\n2)Code Summarization :The goal of code summarization\nis to transform code into natural language (NL) descriptions,\nwhich is a process of code-to-text. Same to code generation,\nmany sequence-to-sequence models are applied as generator.\nIn many research works, the retrieval results are processed\nby additional encoders. Re2Com [141] and EditSum [138]\nSource Code Repository\nExtract\nJava MethodsComments\nCommentCode\nCommentCode\nCommentCodeTraining Set\nTest SetValidation SetDivided by projectRetrieval Corpus / Training SetRetrieve Module\nRefine ModuleAttention MechanismInput Code RepresentationSimilar Code RepresentationExemplar RepresentationData PreprocessingTraining and Test\nEncodersEncodersEncodersDecoder\nFig.", "start_char_idx": 69139, "end_char_idx": 78626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cd5886b-518b-42b6-80e5-8fcaf57365aa": {"__data__": {"id_": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}, {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, {"node_id": "b7b4af96-3f92-422b-a14e-63fbead4b498", "node_type": "1", "metadata": {}, "hash": "c94d5758c800353256e28de210bee124682beb04df926d46fc0e7f952804b1ee", "class_name": "RelatedNodeInfo"}]}, "text": "6: Architecture of Re2Com [141] model.\nboth retrieve similar code using sparse retrieval BM25 and\ngenerate summary using LSTM. As shown in Fig. 6, they\nseparately encode the input, the retrieved code, and the corre-\nsponding summary, then combine the middle representations\nor probabilities in the decoder. HGNN [262] instead uses\ncode edit distance for retrieval, and substitutes the encoders\nfor codes with hybrid GNN on their Code Property Graphs\n(CPG) [263]. RACE [142] aims at generating commit message\nfor code difference. It leverages dense retrieval for similar\nexamples and transformer model for generation. It also uses\nthree encoders for the input, the retrieved code difference,\nand corresponding commit message, then combines the results\nbefore feeding into the decoder. BASHEXPLAINER [139]\nshares the similar idea. Its dense retrieval module is based on\nCodeBert [25], and for generation, the output representations\nof the input and the similar code from CodeBert are directly\nfused for transformer-based decoder. READSUM [264] re-\ntrieves similar code using Levenshtein distance, applies code\nencoder and summary encoder to retrieved pairs, and generates\nsummary using a decoder where a fusion network combines\nthe information.\nRAG for in-context learning, which retrieves similar ex-\namples and build prompt for generation, also works in code\nsummary. REDCODER [40] works for both code generation\nand summary, and it replaces the retriever with GraphCode-\nBert [105] for code summary task. ASAP [203] retrieves\nsimilar code with BM25, and generates summary with GPT\nmodels. Similar to the paradigm described above, research\non pseudocode generation [265] employs the retrieved code\nas input for generation and subsequently replaces the results\nwith the original input. SCCLLM [266] retrieves relevant code\nsnippets by semantic, syntactic, and lexical-based retrieval,\nthen forms in-context prompts for smart contract comment\ngeneration via LLM. UniLog [267] retrieves similar code\nsnippets paired with their log statements to conduct in-context\nlearning for log statement generation.\nLogit-based RAG also prevails in code summarization.\nRencos [121] utilizes two different methods to retrieve similar15\ncode snippets, syntactic similarity between abstract syntax\ntrees (AST) and cosine similarity between dense representa-\ntions. For generator, it adopts attention-based LSTM. There are\nthree encoder-decoder models for the code input and two re-\ntrieval results respectively, and the probabilities are combined\nfor final generation. CoRec [268] generates commit message\ngiven code diff and retrieved similar code diff. Multiple LSTM\ngenerators handle the input code diff and the retrieved code\ndiff, then adds the probabilities for final generation. kNN-\nTransformer [269] obtains context vector by feeding code into\nan encoder-decoder generator, then gets three parts of logits:\nthe first is from searching the vector, the second is from normal\nTransformer, and the third is the copy mechanism that reserve\nrare tokens from the input. Tram [270] involves retrieval in\nboth token-level and sentence-level. It encodes source code\nand corresponding AST into hidden states representations; for\ntoken-level, it retrieves similar representations in the database\nto form next-token prediction logits; for sentence-level, it uses\nsimilar code for autoregressive generation logits; it also add the\noriginal autoregressive generation logits. CMR-Sum [271] uses\nencoder-decoder model to generate summaries. It conducts\ncross attention between representations of retrieved summary\nand generated summary, and add the logits to the original\ntransformer logits for final distribution.\n3)Code Completion :Code completion can be thought of\nas the coding equivalent of the \u201cnext sentence prediction\u201d task.\nEarly attempts on function completion [272] employs DPR to\nretrieve relevant template functions using function docstring,\nthen concatenate the information as the input to the code\ngenerator BART. ReACC [132] retrieves similar codes to build\nprompts for generation. For retrieval, it uses hybrid retriever,\nwhich combines sparse and dense retrieval; for generation, it\nuses CodeGPT-adapted [273]. RepoCoder [218] steps further\nto perform iterative retrieval and generation to bridge the gap\nbetween the retrieval context and the intended completion tar-\nget. In each iteration, for retrieval, the code query is augmented\nwith previously generated code; for generation, the prompt\nis formed by combining the newest retrieved codes with the\ncode query. Other than combining retrieval results in prompts,\nthe retrieval-and-edit framework [140] first retrieves similar\ntraining examples using dense retrieval, then encodes the\ninput and the retrieved result separately, finally combine them\nthrough attention for later LSTM decoder. CoCoMic [274]\nbuilds a project context graph for the whole code project, and\nretrieves top-k neighbors of the input source code. It generates\nrepresentations of both source code and retrieved contexts,\nthen jointly processes the embeddings to complete the code.\nRepoFusion [275] follows the idea of Fusion-in-Decoder; it\nemploys multiple encoder to input the concatenation of the\nretrieved repo contexts and the unfinished code, them fuses\nthem and generates the results through a decoder. KNM-\nLM [276] uses the same model for retrieval encoding and\ngeneration, then combines the logits using bayes inference.\nEDITAS [277] aims at assertion generation. It retrieves similar\nqueries and their assertions, encodes the edit sequence (from\nthe retrieved query to the original query) and the retrieved\nassertion, then fuses the information for decoder generation.\nDe-Hallucinator [278] first generates code snippets withoutretrieval, then retrieves relevant API references given gener-\nated contents. In the second pass, retrieved API references are\ncombined in prompt for better generation. REPOFUSE [279]\nuses both rationale context and retrieved similar codes to\nconstruct prompt. To fit in the input length limit, it reserves\nthe contexts that are most relevant to the query.\n4)Automatic Program Repair :Buggy code can take a lot\nof effort to fix. Automatic program repair leverages generative\nmodels to output the correct version. Query-based RAG tech-\ninque is widely used in automatic program repair [130], [131],\n[182], [208], [209]. Among them, RING [209] retrieves similar\nerror messages based on both sparse and dense vectors, then\nbuilds prompt for the generator Codex [2]. CEDAR [130] ap-\nplies for both assertion generation and program repairs tasks; it\nuses sparse and dense retrieval to search for similar codes, then\nforms prompt for Codex to generate results. InferFix [131]\ncrafts a prompt carrying the bug type, location, relevant syntax\nhierarchies, and similar fixes through dense retrieval. Then it\nalso uses Codex for generation. RAP-Gen [182] also retrieves\nsimilar buggy codes and corresponding fixes through hybrid\nretriever (including both sparse and dense retriever). It fine-\ntunes CodeT5 [106] with this RAG paradigm. SARGAM [208]\nsearches similar buggy code using dense retrieval, generates\npatches using augmented input, then applies another model to\nmodify the final result. These research works also involve error\nlocalization, which is not our focus. RTLFixer [280] leverages\nReAct and RAG to implement an agent fixing errors in Verilog\ncodes. It iteratively retrieves relevant errors and corresponding\nsolutions, and combines reasoning and action planning to form\nprompts for LLMs.\n5)Text-to-SQL and Code-based Semantic Parsing :Se-\nmantic parsing is the task of translating natural language\nutterances to unambiguous structured meaning representations,\nwhere code language is often leveraged to augment this pro-\ncess. SQL is not only a programming language but can also be\nconsidered as a structured representation, so we place text-to-\nSQL (a special case of code generation) in this subsection. Re-\nlated research works all apply query-based RAG. XRICL [189]\nfocuses on the problem of translating non-English utterances\ninto SQL queries. It searches and reranks English utterance\nusing non-English ones by dense retrieval, then builds prompt\nfor Codex to generate SQL queries. SYNCHROMESH [122]\nproposes constrained semantic decoding to enforce rich syn-\ntactic and semantic constraints during generation of SQL or\nother languages. It uses the similarity between abstract syntax\ntrees (AST) to finetune the dense retriever S-Bert. During\ninference, similar NL and SQL are retrived to form the prompt\nof GPT-3. CodeICL [281] uses Python for semantic parsing,\nand augments prompts with a structured domain description\nfor GPT generation. In few-shot learning setting, it leverages\nBM25 sparse retrieval to find similar training examples. RES-\nDSQL [282] ranks schemas using cross-encoder, then includes\nranked schemas into prompts to generate SQL skeleton and\nSQL query. ReFSQL [283] retrieves relevant questions and\ncorresponding SQL to augment text-to-SQL generation. It\ninvolves structure-enhanced retriever with schema linking, and\nMahalanobis contrastive learning to improve the retrieval per-\nformance.", "start_char_idx": 78627, "end_char_idx": 87804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e": {"__data__": {"id_": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}, {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}]}, "text": "ODIS [284] retrieves in-domain and out-of-domain16\ndemonstrations using BM25, then includes them for in-context\nlearning to generate SQL. Another work [285] retrieves both\nsimilar and diverse demonstrations, and then builds prompt for\nin-context learning to generate SQL. MURRE [286] conducts\nmulti-hop retrieve-rewrite, where relevant tables are retrieved\nthrough dense retrieval and then re-writed to generate new\ntabularized question. A rank module at last integrate the\nretrieval results and select the top tables for the text-to-SQL\nmodule. CodeS [287] retrieves relevant information from table\ndatabases in a coarse-to-fine manner, then includes retrieved\nvalues to build prompts for both finetuning and inference.\n6)Others :There are several other code-related tasks that\nadopt RAG paradigm. In [288] for numerical reasoning task,\nthe Chain-of-Thought is replaced with the programs as the\nintermediate reasoning step, and dense retrieval-based similar\nexamples are augmented in prompt for downstream LLMs.\nDe-fine [289] tries to resolve intricate tasks using programs.\nIt follows the paradigm in SKCODER, retrieves relevant\npairs of query and code, then produces sketch template for\nreal program generation. After generation, it combines the\nfeedback to refine the answer with the same generator. The\nrefined programs, regarded as optimal results, are added back\nto the codebase for subsequent serving. E&V [290] leverages\nan LLM-based agent for program static analysis. The agent\nuses source code retrieval, pseudo-code execution, execution\nspecifications verification, and other tools to form interme-\ndiate results. The retrieval is implemented by AST pattern\nmatching. Code4UIE [291] leverages code representation for\ninformation extraction tasks. It retrieves relevant examples\nthrough dense retrieval, and constructs in-context learning\nprompt using the retrieved queries and their corresponding\ncodes. StackSpotAI [292] builds an AI coding assistant,\nwhich incorporates many code-related tasks. It implements\nan RAG component, identifying the most relevant pieces of\ninformation which serve as the context for GPT generation.\nInputBlaster [293] aims to generate unusual text input that\ncould cause mobile app crash. It combines retrieved relevant\nbuggy text with valid input to form the prompt for generation.\nC.RAG for Knowledge\nStructured knowledge, including KGs (knowledge graph)\nand tables, is widely used in language-related tasks. It usually\nserves as the retrieval source to augment generation. In addi-\ntion to regular sparse and dense retrieval, NER (named-entity\nrecognition) technique and graph-aware neighbor retrieval are\napplied to identify and extract relevant entities and relations.\n1)Knowledge Base Question Answering :KBQA (knowl-\nedge base question answering) typically utilizes a knowledge\nbase to determine the correct answer to a question. Many\nsemantic parsing methods have been proposed, generating\nlogical forms (e.g. SPARQL) based on the question.\nQuery-based RAG is the mainstream approach. For a given\nquery, Unseen Entity Handling [53] retrieves topic entities\nthrough FreeBase [294], and concatenates the query with the\nentity for an encoder-decoder to generate SPARQL output.\nCBR-KBQA [54] retrieves relevant questions and correspond-\ning logical form answers with roberta-based deep retrieval,\nthen concatenates the question and the retrieved pairs for\nencoder-decoder transformer model. It also revises the finalgeneration result to align the generated relations with relations\npresent in the local neighborhood of the query entity in the\nknowledge graph. GMT-KBQA [52] first retrieves relevant\nentities and relations through bert-based deep retrieval, then\nconcatenates the information for T5 generation. To improve\nthe retrieval result, it leverages cross-encoder to rerank the\ncandidates, and uses the same T5 structure to conduct relation\nclassification and entity dismbiguation. RNG-KBQA [123]\nenumerates candidate logical forms in the knowledge graph,\nthen ranks the candidates and concatenates them with query to\nform the prompt input to generate final logical form through\na T5 model. Based on this idea, TIARA [124] also retrieve\nentity and schema besides logical forms, while a follow-\ning work [295] retrieves top-k questions with BM25. Uni-\nParser [133] retrieves relevant entities from knowledge graph\nusing mention detection, cross-encoder ranker, and 2-hop paths\nextraction. It also considers enumerating tables and columns\nfrom databases. On obtaining the relevant information, it\nconcatenates the top-k primitives with the query and generates\nlogical forms through T5. BLLM augmentation [135] uses\nTIARA as the retrieval for relevant knowledge base elements,\nthen performs in-context learning through black-box LLM\nsuch as GPT-4 for generating logical forms. ECBRF [134]\nfollows the case-based reasoning paradigm [296], retrieving\nsimilar triplet with dense retriever and constructing prompt in-\nput for BART or GPT-2 in-context learning. FC-KBQA [297]\nextracts relevant class, relation, and entity given a question.\nFor class and relation, it uses BM25 as retriever and a\nBert-based cross-encoder as re-ranker. For entity, it follows\nthe mention detection paradigm. To compose all the com-\nponent candidates, it applies T5 model to generate logical\nexpression. StructGPT [298] extracts relevant information,\nincluding triplets and nearest entities, to form prompt for\nsubsequent LLM. KAPING [299] builds prompt including the\nuser query and the retrieved relevant facts (through entity\nmatching), then generates the answer through LLM. Another\nwork [300] also follows the retrieve-then-generate paradigm,\nreplacing the retrieval with a relation distribution generation\nmodel for weighted triplets. Retrieve-Rewrite-Answer [301]\nfirst retrieves subgraph using hop prediction, relation path\nprediction, and triplet sampling. It then performs KG-to-text\nand zero-shot generation with retrieved subgraphs as prompt.\nKeqing [302] first decomposes a complext question into simple\nsub-questions through finetuned LLM, then retrieves similar\nsub-question template by dense retriever RoBERTa to extract\ncandidate entities from knowledge graph, and finally generates\nanswer through ChatGPT given relevant entities as context\ninput. To probe the deep understanding of natural language in\nLLMs with formal languages, a research work [303] explores\nthe capability of formal language understanding and formal\nlanguage generation. It leverages retrieved pairs to perform in-\ncontext learning. For understanding, it uses tree edit distance\nto retrieve similar logical forms, while for generation, it uses\nBM25 to retrieve similar natural language queries. Interactive-\nKBQA [304] treats LLM as agent and KG as environment.\nIn each step, the LLM conducts entity-linking and one-hop\nretrieval on KG, then generates current thought and action,\nuntil obtaining the final answer.17\nLatent representation-based RAG is also employed in\nKBQA. ReTraCk [305] links entities using mention detection,\nand retrieves schema items using dense retriever Bert. It\nthen generates logical forms by LSTM, incorporating re-\ntrieved items through knowledge-specific rules. SKP [145]\nconcatenates triplets with the query and uses fusion-in-decoder\ntechnique in inference. It adds a pretraining stage with a\nknowledge-aware MLM loss on triplets and knowledge con-\nstrastive loss with respect to the retrieved items. DECAF [144]\nforms Resource Description Format knowledge base triplets\ninto sentences, then concatenates sentences with the same\nhead entity into documents. It retrieves relevant documents\nusing BM25 sparse retrieval or Bert dense retrieval, then\nleverages Fusion-in-Decoder technique to generate logical\nform and direct answer given each (query, document) pair as\ninput. It combines the output to obtain the final answer. KD-\nCoT [146] uses the same dense retriever and fusion-in-decoder\ngenerator as DECAF. It follows a Chain-of-Thought paradigm,\niteratively performing retrieval, generation, and verification\nuntil the CoT is finished.\n2)Knowledge-augmented Open-domain Question An-\nswering :Structured knowledge is often leveraged to augment\nODQA (open-domain question answering).\nThe use of latent representation-based RAG, particularly\nthe fusion-in-decoder technique, is prevalent for knowledge-\naugmented open-domain question answering. UniK-QA [143]\nconcatenates the text forms of the components in a triplet\nand build document pool for retrieval. For a given question,\nit leverages dense retriever for relevant documents, then per-\nforms fusion-in-decoder technique to incorporate the infor-\nmation for answer generation.", "start_char_idx": 87805, "end_char_idx": 96477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "041dcbc3-b10c-426b-995b-38234f2d3a38": {"__data__": {"id_": "041dcbc3-b10c-426b-995b-38234f2d3a38", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}, {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, {"node_id": "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca", "node_type": "1", "metadata": {}, "hash": "c60ae4ed8fdcfe51dafce5c4af7675cbd46ee8f0bb30e601660346acd0633957", "class_name": "RelatedNodeInfo"}]}, "text": "KG-FiD [306] conducts the\nEncoder\nL\n1\n \nLayers \nText \nKnowledge \nSource\nDPR \nRetriever\nP1\nP3\nP5\nP6\nP2\nP4\nP7\nP8\nKG\nDecoder\nInput \nQuestion\nEncoder\nL\n1\n \nLayers \nEncoder\nL-L\n1\n \nLayers \nEncoder\nL\n1\n \nLayers \nEncoder\nL\n1\n \nLayers \nConcatenation\nOutput\nAnswer\nP1\nP3\nP5\nP2\nP7\nQuestion \n+ \nP1\nQuestion \n+ \nP2\nQuestion \n+ \nP3\nQuestion \n+ \nP5\nEncoder\nL\n1\n \nLayers \nQuestion \n+ \nP7\nEncoder\nL-L\n1\n \nLayers \nRetrieved\nPassages \n& \nEmbeddings\nStage-1 \nReranking\nReading \nModule\nWhen \ndid \nthe \nYankees \nmove \nto \nNew \nYork?\n1903\nNew \nYork \nYankees\nYankee \nStadium\nStaten \nIsland \nYankees\nNew \nYork \nYankees\nOperator\nYankee \nStadium\nNew \nYork \nYankees\nParent\nClub\nStaten \nIsland \nYankees\n......\nN\n0\n \nPassages\nN\n1\n \nPassages\nN\n2\n \nPassages\nStage-2 \nReranking\nFig. 7: Architecture of KG-FiD [306] model.\nretrieval and generation as normal FiD. It add re-ranking in\ntwo ways: the first is to use a graph attention network on the\ngraph formed by retrieved documents; the second is to use\nthe hidden states in the generator. OREOLM [307] empowers\nLLM with knowledge reasoning paths. Concretely, it uses\nentity linking to determine the initial state, then conducts\ncontextualized random walk on KG to get reasoning paths,\nwhose entity value memory are combined into the hidden\nstates of LLM for better generation. GRAPE [308] constructs\nbipartite graph for each pair of question and retrieved passage,\nthen builds bipartite graph on entity for fusing knowledge.It leverages FiD as backbone model and generate answers.\nSKURG [309] forms a knowledge graph using text and image\ndata, then updates each source\u2019s representation with their\nrelevant sources. It then uses a specially designed decoder\nto iteratively retrieve and generate. It conducts cross-attention\nwith all the sources, then retrieves the source with highest\nscore and concatenates to the original input embedding; if a\ngate score does not exceed the threshold, it starts to generate\nthe real answer, otherwise the retrieval stage re-starts.\nWith the rapid development of LLMs, query-based RAG is\nemerging as a new standard. DIVKNOWQA [310] develops\na retrieval-augmentation tool, including sparse retrieval on\nstructured knowledge, dense retrieval on texts, and sparql\ngeneration on KB. Through CoT-based LLM, it retrieves\nand re-ranks in each step, and generates the final answer.\nKnowledGPT [311] uses generated code to retrieve from\nboth public and personal knowledge bases, so as to build\nprompt for LLM question answering. EFSUM [312] generates\nevidence-focused summary with retrieved relevant facts, then\noptimizes the summary to align the QA-specific preference\nwith another generator and the filters for helpfulness and\nfaithfulness. GenTKGQA [313] conducts subgraph retrieval\nthrough relation ranking and time mining, then employs GNN\nto incorporate structural and temporal information into virtual\ntoken representations for subsequent LLM generation. Knowl-\nedgeNavigator [314] analyzes complex questions and performs\nretrieval on knowledge graph through iterative filtering of\nrelations with respect to core entities, so as to obtain relevant\ntriplets. It then includes the triplets into prompt for generation.\n3)Table for Question Answering :Tables, as another form\nof structured knowledge, also facilitates question answering.\nFusion-in-decoder [35] style RAG is often used for table\nQA. EfficientQA [315], a competition held in NeurIPS 2020,\nwitnessed the proposal of numerous retrieval-reader systems\nthat rely on textual and tabular data. Dual Reader-Parser [316]\nre-ranks the retrieved textual and tabular data for generation.\nConvinse [317] retrieves information from heterogeneous re-\nsources (including knowledge bases, tables, and texts) after\nquestion understanding. CORE [318] retrieves relevant tables\nand passages through dense representation, then re-ranks the\nretrieved results using query-generation score from a T0 linker.\nIt uses FiD for final generation. RINK [319] follows the\nretriever-reranker-reader paradigm for table-augmented ques-\ntion answering. It designs a set-level reader-inherited reranker\nto get the relevance score of blocks (table segments). TAG-\nQA [320] retrieves from both tables and texts: for tables, it\nconverts tables to graphs then performs GNN to select relevant\ncontents; for texts, it uses BM25 for retrieval. It then leverages\nFiD for answer generation.\nTables can be incorporated into prompts for query-based\nRAG. T-RAG [321] retrieves relevant tables given a user\nquery, then concatenates the query with the tables as a prompt\nto generate the answer through BART. OmniTab [322] con-\nducts multi-task training to improve the performance of table\nquestion answering model. It retrieves relevant tables given a\nquery, then concatenates them for masked-token pre-training.\nCARP [323] retrieves relevant tables and passages using\nentity linking, then extracts hybrid chain of retrieved knowl-18\nedge, which is later used to construct prompt for generation.\nStructGPT [298] extracts relevant information from knowledge\ngraph, table, or database, to form prompt and generate answers\nthrough LLMs. cTBLS [324] retrieves relevant tables through\ndense retrieval, then for each query, it forms prompt with\nranked tables for answer generation. A recent work [325]\nfirst uses table-to-text techniques to integrate tabular data into\ncorpora, then conducts experiments on both finetuning and\nRAG for question answering.\n4)Others :Prototype-KRG [326] retrieves knowledge facts\nand dialogue prototypes, then integrates them into the\nGRU-based generator by both hidden states and logits.\nSURGE [327] retrieves relevant subgraphs using dense re-\ntrieval, then adds them into the input of the generator for\ndialogue generation. RHO [328] fuses KG embedding of\nrelevant entities and relations into textual embeddings during\nthe generation of open-domain dialogue. K-LaMP [329] re-\ntrieves entity in history queries to construct prompt for query\nsuggestion. ReSKGC [147] linearizes all training triplets into\ntext by concatenation, then retrieves relevant triplets using\nBM25, and generates completed triplet using T5 with fusion-\nin-decoder. G-Retriever [330] retrieves relevant nodes and\nedges from graph-based data, then constructs subgraph and\nperforms graph prompt tuning for question answering based\non textual graphs.\nD.RAG for Image\n1)Image Generation :Image Generation refers to the\nprocess of creating new images, typically using algorithms in\nthe field of artificial intelligence and machine learning.\nThe retrieval process can not only help yield high-quality\nimages even for rare or unseen subjects, but also reduces\nthe parameter count and computational expense [45], [137],\n[148]\u2013[152], [331]. For GAN-based model, RetrieveGAN [45]\nemploys a differentiable retrieval process to select compatible\nimage patches for generation, with Gumbel-softmax trick and\nend-to-end training. IC-GAN [137] models data as a mix\nof conditional distributions around each training instance,\nconditioning both the generator and discriminator on these\ninstances, and can control the semantics and style by swap-\nping class labels or conditional instances. Recently, diffusion\nmodels beat GANs on image generation [332]. KNN-Diffusion\n[149] trains a text-to-image diffusion model without text data,\nby conditions the model on CLIP joint embedding of the\ninstance and k-nearest neighbors from image database. These\nk-NN embeddings bridge the text-image distribution gap and\nallow image generation from different domains by swapping\nthe database. Similarly, RDM [150] conditions diffusion or\nautoregressive models on CLIP embeddings of external image\ndatabases. It enables post-hoc conditioning on class labels,\ntext prompts and zero-shot stylization [151]. Beyond retrieving\nonly images, Re-imagen [148] conditions on both text prompts\nand retrieved image-text pairs for text-to-image generation.\nInterleaved classifier-free guidance is also proposed to balance\nthe alignment between text prompts and retrieval conditions.\nTo avoid information loss of CLIP embeddings and access\nto all visual condition, Retrieve&Fuse [331] concatenates\nthe retrieved conditional image and the noised image before\neach attention block in U-Net, and allowing interaction viaself-attention. RPG [79] retrieves representative images to\nconstruct informative in-context examples (i.e., image-region\npairs), and utilizes multi-modal chain-of-thought reasoning\n[333] to plan out complementary subregions for compositional\ntext-to-image diffusion.", "start_char_idx": 96478, "end_char_idx": 104977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4de4eb18-c462-40ee-ba75-65aad1ab0a9e": {"__data__": {"id_": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}, {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, {"node_id": "c98e98ee-03b5-4bf9-a652-999317a0b671", "node_type": "1", "metadata": {}, "hash": "70828131a4e089a5fd70194bf07328bef70c911b67ea89eae6f21470b4185c0c", "class_name": "RelatedNodeInfo"}]}, "text": "2)Image Captioning :Image Captioning is the process of\ngenerating a textual description of an image.\nRetrieval-augmented image captioning typically synthesises\ndescription with a collection of retrieved captions, instead\ndepending only on the input image. MA [164] augments via\na memory bank, built with historical context and target word\nof image-text training set, and queried during inference with\nthe current context. V ocabulary distribution is computed based\non retrieved entries, and interpolated with the original predic-\ntion. In adversarial training, RAMP [334] employs retrieved\ncaptions as reference for discriminator training, prompting\nthe generator to make full use of retrieved captions. The\nmemory-augmented attention and copying mechanism are also\nexploited to better use. The RA-Transformer [46] and EXTRA\n[335], both retrieval-augmented transformer-based captioning\nmodels, utilize cross-attention over encoded retrieved captions.\nEXTRA, as depicted in Fig. 8, jointly process the image and\naman slopeSEP...\n... aa\nBOSEOS\nskierskierheads\nmountainsmountains\nthe......\n......\nv1 w1wM v2 vN-1vN CLS...\n\"a man riding skis\ndown a snow covered slope\"Autoregressive\nLanguage DecoderVision-and-Language\nEncoder\nCross-Attention\nCurrent Image Retrieved Caption\n\"a man riding skis\ndown a snow covered slope\"... ......\"a couple of people with\nski 's standing in the snow\"DISTANCESINPUT\nIMAGE CAPTIONDatastore\n215...\nhv1hv2 hvN-1hvNhw1hw2hwM-1hwM...\nFig. 8: Architecture of EXTRA [335] model.\nretrieved captions with V&L encoder, such that the decoder\nattends to both visual and linguistic contexts. Beyond retrieved\ncaptions, REVEAL [336] uniformly encodes and retrieves\nmulti-modal world knowledge, including image-text pairs,\nquestion answering pairs, and knowledge graph triplets, which\nis then integrated with image features by retrieval score-\naware attention module. Straightforwardly, SMALLCAP [47]\nemploys a CLIP vision encoder and a LLM decoder, linked\nby trainable cross-attention layers, where retrieved captions\nserve as input-specific in-context examples for prompt for en-\nhancement. For remote sensing image, CRSR [337] enhances\nretrieved captions with semantic refinement, i.e. filters out\nmisleading details and emphasizes visually salient content.\nBesides, the visual features is also enriched by transformer\nnetwork with learnable queries, capturing more intricate details\nwithin the images19\n3)Others :There also exist many retrieval augmented\nworks for other image-related tasks. For visual question\nanswering (VQA), PICa [338] leverages GPT-3\u2019s implicit\nknowledge retrieval and reasoning capabilities, which con-\nverts images into textual descriptions, then prompts GPT-3\nto predict answers based on these descriptions and the ques-\ntion, finally ensembles multi-query results. RA-VQA [339]\nidentifies a limitation that the retrieval in previous work is\ntrained separately from answer generation, and propose a joint\ntraining scheme that integrates differentiable retrieval with\nanswer generation, enabling end-to-end training. For visually\ngrounded dialogue, KNN-based Information Fetching (KIF)\n[340] enhances generative Transformer for dialog modeling.\nEach KIF module learns a read operation to access fixed\nexternal knowledge. Maria [341], a neural conversation agent,\nenhances dialog generation by leveraging visual experiences\nretrieved from a large-scale image index as extra context.\nFor multi-modal machine translation, which aims to improve\nNMT with multi-modal information, [342] incorporates visual\ninformation at the phrase level to address the sparsity of paired\nsentence-image, employing a conditional V AE to filters out\nredundant visual information from sentence-image datasets.\nE.RAG for Video\n1)Video Captioning :Video captioning is to describe the\nvisual content with a descriptive utterance. KaVD [343]\ngenerates news video caption with background knowledge\nmined from topically related documents such as named entities\nand events. R-ConvED [48] introduces retrieval-augmented\nmechanism to facilitate the word prediction. It uses Dual\nEncoding [109] for video-text retrieval, and proposes a convo-\nlutional encoder-decoder network for generation. For a given\ninput video, R-ConvED first retrieves top-k relevant sentences\nand their corresponding video from training set, then feeds\nthese pairs and the input video into the generator separately.\nThe obtained decoder hidden states are combined through\nattention-like read operation, so that the target word can be\npredicted using the final representation. CARE [160] utilizes\nvisual encoder, audio encoder, and text encoder for frame,\naudio, and retrieved texts, respectively. It uses CLIP as re-\ntriever, and transformer decoder as generator. The embeddings\nof the three modalities are combined to augment the decoder,\nproducing global semantic guidance which attends the input\nembedding, and local semantic guidance which attends the\nattention layer. EgoInstructor [49] generates captions for first-\nperson view videos. It retrieves relevant exocentric videos\nand corresponding texts via dense retrieval, then encodes the\ninput egocentric video and the retrieved exocentric videos\nthrough a CLIP-based visual encoder and a transformer-\ndecoder-based bottleneck module. Then it generates captions\nthrough decoder-based LLM which takes the retrieved texts\nas input and interacts with encoded videos in gated cross-\nattention layer.\n2)Video QA&Dialogue :Video QA&Dialogue generates\nsingle or multiple-round responses in alignment with video\ncontent. For video question answering (VideoQA), MA-DRNN\n[344] leverages the differentiable neural computer (DNC) with\nan external memory, for storing and retrieving useful infor-\nmation in questions and videos, and modeling the long-termvisual-textual dependence. Given the video input, R2A [345]\nretrieves semantically similar texts by multi-modal model, e.g.\nCLIP, and query LLM with both the question and the retrieved\ntexts. For video-grounded dialogue, [346] proposes TVQA+\ndataset which enables to retrieve relevant moments and visual\nconcepts to answer questions about videos, and also proposes\nSpatio-Temporal Answerer with Grounded Evidence (STAGE)\nto exploit it. VGNMN [347] also extracts visual cues from\nvideos, while the retrieval is carried out using neural module\nnetworks parameterized by entities and actions in previous\ndialogues.\n3)Others :There also exist many retrieval augmented\nworks for other video-related tasks. VidIL [348] exploits\nimage-language models to translate video content into\ntemporal-aware prompts with few-shot examples, for vari-\nous video-language tasks including video captioning, video\nquestion answering, video caption retrieval, and video fu-\nture event prediction. Notably, for trustworthy autonomous\ndriving, RAG-Driver [349] grounds the MLLM in relevant\nexpert demonstrations from a memory database, to produce\ndriving action explanations, justifications, and control signal\nprediction. Text-to-video generation is to generate video given\nnatural language descriptions. As shown in Fig. 9, Animate-\nPlot 1Motion structure retrieval\nStoryboard descriptionStructure-guidedtext-to-video synthesisText promptsVideo databaseText queriesPlot iPlot n\u22ef\u22ef\n\u22ef\u22efStory script\nFig. 9: Architecture of Animate-A-Story [206] model.\nA-Story [206] develops a framework which can generate high-\nquality storytelling videos based on texts. It first separates the\ntext into individual plots, and decorates the description using\nLLM. It then retrieves relevant videos for each plot through\na dense retriever [110]. It generates videos through a latent\ndiffusion model, consisting of two branches: a text encoder20\nCLIP, and a structure encoder which takes the estimated depth\nof the retrieved videos as structure control.\nF .RAG for Audio\n1)Audio Generation :The goal of audio generation is to\ngenerate audio with natural language input.\n\u201cA bottle of champagne is popped and then poured into a glass\u201dInputprompt\nOutputWaveform\nCLAPEncoder\nDatabaseAudio FeatureLanguageFeature\u201cSome water pure into the glass\u201d\n\u201cWater pure into the glass\u201d\n\u201cA champagne is popped while a man talks\u201d\nVAEDecoderHiFi-GANRetrievalAudioMAET5Audio & Language FeatureLDMCrossAttention\nFig. 10: Architecture of Re-AudioLDM [159] model.\nRe-AudioLDM [159] adopts dense retriever CLAP [26] to\nretrieve similar caption-audio pairs given input prompt. As\nshown in Fig. 10, the generator, latent diffusion model and\nV AE-based decoder, take the representations of input text and\nretrieved pairs as input and generate output audio. Make-An-\nAudio [44] uses dense retriever CLAP [26] to augment data,\nretrieving related audio given natural language text. It then\nconstructs pseudo prompts for diffusion-based text-to-audio\nmodel training.\n2)Audio Captioning :The goal of audio captioning is\nto generate natural language data with audio data, which is\nbasically a sequence-to-sequence task. RECAP [350] leverages\nCLAP [26] to retrieve related captions given audio data.", "start_char_idx": 104978, "end_char_idx": 113975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3090aed2-bf72-47d4-8ba2-f82f70cef882": {"__data__": {"id_": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}, {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, {"node_id": "c26f6c18-927a-4e11-b542-6fa85387b872", "node_type": "1", "metadata": {}, "hash": "d4de0904dc60501c95e3fd2e514611431f3b100c9259692bc819ded096879298", "class_name": "RelatedNodeInfo"}]}, "text": "The\nretrieved captions are then included into the prompt input for\nGPT-2 model, which interacts with audio embeddings through\ncross attention. In [43], dense retriver VGGish [107] is adopted\nto produce dense embedding of audio data, and GPT-2 is\nadopted to generate representations of the retrieved captions\nwhich are paired with similar audios. After obtaining the\nrepresentations of audios and captions, an extra multi-head\nattention block and a linear layer fuses all the information\nand generates the output. Some research studies transform\naudio modality to text, in order to leverage advancements\nin LLMs [351]\u2013[353]. They take advantage of deep retrieval\nmodels, aligning the modalities into the same latent space for\ndownstream text generation.\nG.RAG for 3D\n1)Text-to-3D :Retrieval can be applied to augment the\ngeneration of 3D contents. ReMoDiffuse [51] aims at gener-\nating motions using diffusion models. It first retrieves relevant\nmotion entites through CLIP given text input, then leverages\nthe information of the text and the retrieved entities through\na semantic-modulated attention layer.\nAMD [158] designs two branches of motion diffusion for\nfidelity and diversity. As shown in Fig. 11, the first branch\ninputs the original prompt text for diffusion; the second branch\ndecomposes the input text into anatomical scripts and retrieve\nsimilar reference motions for diffusion. A transformer-based\nfusion module is further applied to adaptively balance the\nresult from two branches. RetDream [50] targets general 3D\ngeneration, using retrieved 3D assets to augment the process\n...A man is pretending to \nbe a chicken , constantly \npecking at the ground \nand waving his arms like a \nchicken.\n1)Amanlowers his head     \ntowards the ground.\n2) ... opens and closes \nhis mouth rapidly.\n3) ... moves his head up and \ndown, mimicking a \npecking motion .\n4) ... flaps his arms up \nand down ,imitating a \nchicken's wings .\ud835\udc61 MLPCross\nAttentionLinearFusion Block\nTransformer  EncoderCLIP\nText\nCLIP\nText\nLinear\ud835\udc65\ud835\udc611\ud835\udc65\ud835\udc612\ud835\udc65\ud835\udc613\ud835\udc65\ud835\udc61\ud835\udc41\n\ud835\udc5a1\ud835\udc5a2\ud835\udc67\ud835\udc61\ud835\udc58\ud835\udc60\ud835\udc67\ud835\udc61\ud835\udc58\ud835\udc59\n\ud835\udc5a3\ud835\udc5a\ud835\udc45 \u2295\u0ddc\ud835\udc6501\n\u0ddc\ud835\udc6502\n\u0ddc\ud835\udc6503\n\u0ddc\ud835\udc650\ud835\udc41\u22ef\u22ef\n\u22ef\n\u22ef\n\u22ef\u22efLinear\nLinearTransformer  EncoderFusion Block\u2295Origin  Motion Diffusion\nFeature FusionText Decomposition\nfine -tuned\nSelf \nAttention\nDropout\nLayer Norm\nLinear\nGELU\nLayer NormDropout\nLinear\nFusion Block optional\u2131\ud835\udc592\n\u2131\ud835\udc60\u2131\ud835\udc61\ud835\udc9e\ud835\udc59\n\ud835\udc9e\ud835\udc60\nHybrid Retrieval\u2130\ud835\udc60\u2130\ud835\udc59\u2131\ud835\udc591\ud835\udc5d\ud835\udf031\nSearch with Anatomical TextBest Match\nDataset\ud835\udc5a1:\ud835\udc45Reference Action \nTokensRandom Select\nComplex and Decomposed\nText Features\nDiffused Motion and \nReference Action Features \ud835\udc65\ud835\udc611:\ud835\udc41Diffused Motion Data\ud835\udc58,\ud835\udc63\n\ud835\udc5e\nReference Motion Diffusion\ud835\udc5d\ud835\udf032Fig. 11: Architecture of AMD [158] model.\nof variational score distillation from 2D diffusion models.\nGiven an input query, it retrieves relevant 3D assets through\nCLIP, then utilizes the retrieved assets to provide geometric\nprior and adapted 2D prior. Concretely, retrieved assets not\nonly impose an additional velocity on particles for distribution\ninitialization, but also help optimize the 2D diffusion model\nthrough low-rank adaptation.\nH.RAG for Science\nRAG has also emerged as a promising research direction\nfor many interdisciplinary applications, such as molecular\ngeneration, medical tasks and computational research.\n1)Drug Discovery :The goal of drug discovery is to\ngenerate molecules that concurrently fulfill diverse properties.\nRetMol [55] integrates a lightweight retrieval mechanism and\n-4.9 kcal/molRetrieval databaseRetrieverInformation fusionDecoderEncoder\n-8.4 kcal/mol-10.3 kcal/mol-10.9 kcal/molEncoderShared  weightsInput molecule\nRetrieved exemplar moleculesInput embedding\nRetrieved embeddingsFused embedding-8.4 kcal/mol\nOutput molecule\nRetrieval modulePre-trained module\nFig. 12: Architecture of RetMol [55] model.\nmolecular strings into a pre-trained encoder-decoder gener-\native model to retrieve and fuse exemplar molecules with\nthe input. PromptDiff [354] introduces an interaction-based,\nretrieval-augmented 3D molecular diffusion model that re-\ntrieves a curated set of ligand references to guide the synthesis\nof ligands meeting specific design criteria.\n2)Biomedical Informatics Enhancement :Several re-\ncent studies have improved the expressiveness of LLM\nby retrieving information from biomedical domain-specific\ndatabases, thereby augmenting the model\u2019s capabilities to\nprovide valuable guidance for tasks in the medical field.\nPoET [355] is an autoregressive generative model based on\na variant of Transformer that integrates a retrieval mech-\nanism to enable prompt augmentation, thereby expediting\nthe prediction of fitness properties for protein variants.\nChat-Orthopedist [136] enhances ChatGPT with a retrieval-\naugmented mechanism focused on adolescent idiopathic sco-\nliosis (AIS), utilizing an external knowledge base for precise\nresponses. BIOREADER [356] is the first retrieval-enhanced\ntext-to-text transformer-based model for biomedical natural21\nlanguage processing, incorporating the retrieved literature\nevidence into the model using a chunked-cross attention\nmechanism. MedWriter [357] employs a hierarchical retrieval-\naugmented generation method that combines report-level and\nsentence-level templates to produce coherent and clinically\naccurate medical reports from images. QA-RAG [358] em-\nploys a dual-track RAG strategy to enhance pharmaceutical\ncompliance by effectively retrieving and integrating regula-\ntory guidelines based on language model responses and user\nqueries.\n3)Math Applications :Retrieval-augmented generation\ntechnology in mathematics streamlines problem-solving,\nboosts research innovation, and refines educational strategies.\nLeanDojo [359] boosts theorem proving by using retrieval-\naugmented methods to choose relevant premises from exten-\nsive mathematical libraries, improving automation and theo-\nrem generalization. RAG-for-math-QA [360] improves math\nquestion-answering by integrating a high-quality math text-\nbook with retrieval-augmented generation, enhancing LLM-\ngenerated responses for middle-school algebra and geometry.\nV. B ENCHMARK\nGiven the increasing research interests and applications of\nRAG, there have also been several benchmarks assessing RAG\nfrom certain aspects.\nChen et al. [361] proposed an RAG benchmark, which\nevaluates RAG from four aspects: Noise Robustness, Neg-\native Rejection, Information Integration, and Counterfactual\nRobustness, respectively. Noise Robustness evaluates whether\nLLMs could extract the necessary information from documents\ncontaining noisy information. The noisy information is rele-\nvant to the input query but useless for answering it. Negative\nRejection measures whether LLMs would reject to respond the\nquery when the retrieved content is not enough. Information\nIntegration assesses whether LLMs could acquire knowledge\nand make responses by integrating multiple retrieved contents.\nCounterfactual Robustness refers to the ability of LLMs to\nidentify counterfactual errors in the retrieved content.\nAnother three benchmarks, RAGAS [362], ARES [363] and\nTruLens [364], consider three different aspects: Faithfulness,\nAnswer Relevance, and Context Relevance, respectively. Faith-\nfulness focuses on the factual errors in the results when the\ncorrect answers can be inferred from the retrieved contents.\nAnswer Relevance measures whether the generated results\nactually address the problems (i.e., queries) or not. Context\nRelevance judges whether the retrieved contents contain as\nmuch knowledge as possible to answer the queries, and as\nlittle irrelevant information as possible.\nCRUD-RAG [365] divides all RAG tasks into four cate-\ngories, which are Create, Read, Update, and Delete, respec-\ntively, and also evaluates each category using text continuation,\nquestion answering (with single- and multi-document ques-\ntions), hallucination modification, and open-domain multi-\ndocument summary. MIRAGE [366] is a benchmark designed\nfor assessing the application of RAG in the medical domain,\nfocusing on the comparison and optimization of medical\nquestion-answering systems\u2019 performance. KILT [367] is an-\nother benchmark focuses on ensuring information accuracyand reliability by aligning Wikipedia pages with specific snap-\nshots and pinpointing the most pertinent text ranges through\nBLEU score evaluations.", "start_char_idx": 113976, "end_char_idx": 122183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "143faccb-70a4-4f32-99ba-66824f74c219": {"__data__": {"id_": "143faccb-70a4-4f32-99ba-66824f74c219", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}, {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}, {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, {"node_id": "45593217-9f9a-4741-a8d5-1545ac7e429e", "node_type": "1", "metadata": {}, "hash": "c97aee1c760250dd596a785c4de320feadd0c4f892ad3cd7d5518a1cd165e1d0", "class_name": "RelatedNodeInfo"}]}, "text": "It filters out lower-quality data to\nmaintain a high standard of information mapping, offering a\nvariety of retrieval system options like TF-IDF, DPR, RAG,\nand BLINK + flair to support evidence-based predictions or\ncitations according to task requirements.\nVI. D ISCUSSION\nA.Limitations\nDespite the widespread adoption of RAG, it suffers from\nseveral limitations by nature. In this paper, we provide a sum-\nmary of the limitations and engage in an in-depth discussion.\n1)Noises in Retrieval Results :Information retrieval can-\nnot yield perfect results because information loss appears in\nrepresentations generated by encoder models. Additionally,\nANN search can also provide approximate results rather than\nexact ones. Consequently, certain degree of noise is inevitable\nin retrieval results, manifesting as irrelevant objects or mis-\nleading information, which may cause failure points in RAG\nsystems [368]. Though the common sense is that increasing\nthe accuracy of retrieval will contribute to the effectiveness of\nRAG, a recent study surprisingly shows that noisy retrieval\nresults may conversely help improve the generation qual-\nity [369]. A possible explanation is that diversity in retrieval\nresults may also be necessary for prompt construction [370].\nAs a result, the impact of noise in retrieval results remains\nuncertain, leading to confusion in practical uses regarding\nwhich metric to employ for retrieval and how to facilitate the\ninteraction between the retriever and the generator.\n2)Extra Overhead :While retrieval can help mitigate the\ncosts of generation in certain cases [30]\u2013[32], the incorporation\nof retrieval sometimes introduces non-negligible overhead.\nConsidering that RAG is primarily employed to improve the\nperformance of existing generative models, the inclusion of\nadditional retrieval and interaction processes leads to increased\nlatency. Worse still, when combined with complex enhance-\nment methods, such as recursive retrieval [371] and iterative\nRAG [218], the extra overhead will become even more sig-\nnificant. Furthermore, as the scale of retrieval expands, the\nstorage and access complexity associated with data sources\nwill also increase. In presence, RAG systems exhibit a trade-\noff between costs and benefits. Looking ahead, we anticipate\nfurther optimization to alleviate the associated overhead [372].\n3)The Gap between Retrievers and Generators :Seam-\nlessly integrating retrieval and generation components requires\nmeticulous design and optimization. Since the objectives of\nretrievers and generators may not align, and their latent spaces\nmight differ, designing their interaction poses challenges. As\nintroduced in Section III, numerous approaches have been\nproposed to enable effective RAG, and these approaches\neither disentangle the retrieval and generation processes or\nintegrate them at an intermediate stage. While the former is\nmore modularized, the latter could potentially benefit from\njoint training. Till not, there lacks a sufficient comparison of\ndifferent ways of interaction across various scenarios.22\n4)Increased System Complexity :The introduction of re-\ntrieval unavoidably increases the system complexity and the\nnumber of hyper-parameters to tune. For instance, a recent\nstudy on the trade-off between attribution and fluency in\nprompt-augmentation-style RAG demonstrates that using top-\nk retrieval for generation improves attribution, but hurts flu-\nency in turns [373]. The counter effects of different aspects\nin RAG, such as metric selection, are still under explored.\nTherefore, further refinement of RAG systems, both in terms\nof algorithms, and deployment, is necessary to fully unlock\ntheir potentials.\n5)Lengthy Context :One of the primary shortcomings of\nRAG, in particular the query-based RAG, is that it lengthens\nthe context tremendously, making it infeasible for generators\nwith limited context length. In addition, the lengthened context\nalso slows down the generation process generally. The research\nadvancements in prompt compression [202] and long-context\nsupport [374] have partially mitigated these challenges, albeit\nwith a slight trade-off in accuracy or costs.\nB.Potential Future Directions\nLastly, we wish to outline several potential directions for\nfuture RAG research and applications.\n1)More Advanced Research on RAG Methodologies, En-\nhancements, and Applications :A straight-forward research\ndirection is to develop more advanced methodologies, en-\nhancements, and applications of RAG.\nAs introduced in Section III-A, existing works have ex-\nplored various interaction patterns between retrievers and\ngenerators. However, since the optimization target of these two\ncomponents are distinct, the practical augmentation process\nhas a large impact on the final generation results. Investigation\nof more advanced foundations for augmentation holds promise\nfor fully unleashing the potential of RAG.\nBased on a constructed RAG system, enhancements are\nhelpful to improve the effectiveness of certain components\nor the entire pipeline. Given the inherent complexity of the\nsystem, there exists significant potential for RAG to improve,\nnecessitating proper tuning and careful engineering. We look\nforward to further experimental analysis and in-depth explo-\nration that will contribute to the development of more effective\nand more robust RAG systems.\nAs introduced in Section IV, RAG is a general technique\nthat has been applied across diverse modalities and tasks. Yet\nmost of existing works straightforwardly integrate external\nknowledge with the specific generation tasks, without thor-\noughly taking into account the key characteristics of the target\ndomains. Therefore, for generation tasks that do not fully\nleverage the power of RAG, we are confident that designing\nproper RAG system will be beneficial.\n2)Efficient Deployment and Processing :Currently, sev-\neral deployment solutions of query-based RAG for LLMs\nhave been proposed, such as LangChain [375], LLAMA-\nIndex [175], and PipeRAG [376]. However, for other founda-\ntions of RAG and/or generation tasks, there lacks a plug-and-\nplay solution. In addition, given the extra overhead introduced\nby retrieval, and considering that the complexities of both\nthe retriever and generator will continue to grow, achievingefficient processing in RAG remains a challenge, necessitating\ntargeted system optimization.\n3)Incorporating Long-tail and Real-time Knowledge :\nWhile a key motivation of RAG is to harness real-time and\nlong-tail knowledge, few studies have explored the pipeline\nfor knowledge updating and expansion. Many existing works\nmake up the retrieval sources with merely the training data\nof generators, thereby neglecting the dynamic and flexible\ninformation advantages that could have been offered by re-\ntrieval. As a consequence, designing a useful RAG system with\ncontinuously updated knowledge and/or flexible knowledge\nsources, along with corresponding system-level optimizations,\nis a growing research direction. With the capability of utilizing\nlong-tail knowledge, we also expect RAG to leverage person-\nalized information and features, so as to adapt to today\u2019s web\nservice.\n4)Combined with Other Techniques :In essential, RAG\nis orthogonal to other techniques that share the goal of\nimproving AIGC effectiveness, including fine tuning, rein-\nforcement learning, chain-of-thought, agent-based generation,\nand other potential optimizations. However, the exploration of\nsimultaneously applying these techniques is still in its early\nstages, calling for further research to delve into algorithm\ndesign and fully leverage their potential. It is worthy to note\nthat a recent notion appears \u201clong-context models like Gemini\n1.5 will replace RAG\u201d. Nevertheless, this assertion does not\nhold true \u2014 RAG exhibits greater flexibility in managing\ndynamic information, encompassing both up-to-date and long-\ntail knowledge [377]. We believe that RAG in the future will\ntake advantage of long context generation to achieve even\nbetter performance, rather than simply being weeded out by\nit.\nVII. C ONCLUSION\nIn this paper, we conducted a thorough and comprehensive\nsurvey on RAG within the context of AIGC, with a particular\nfocus on augmentation foundations, enhancements, and ap-\nplications. We first systematically organized and summarize\nthe foundation paradigms in RAG, providing insights into\nthe interaction between retrievers and generators. Then, we\nreviewed the enhancements that further improve the effective-\nness of RAG, including the enhancements on each component\nor the entire pipeline. To facilitate researchers across diverse\ndomains, we showcased practical applications of RAG in a\nrange of modalities and tasks. Finally, we also presented\nexisting benchmarks for RAG, discussed current limitations\nof RAG, and shed light on promising future directions.\nREFERENCES\n[1] T. B. Brown, B. Mann etal., \u201cLanguage models are few-shot learners,\u201d\ninNeurIPS, 2020.\n[2] M. Chen, J. Tworek etal., \u201cEvaluating large language models trained\non code,\u201d arXiv:2107.03374, 2021.\n[3] OpenAI, \u201cGPT-4 technical report,\u201d arXiv:2303.08774, 2023.\n[4] H. Touvron, T. Lavril etal., \u201cLlama: Open and efficient foundation\nlanguage models,\u201d arXiv:2302.13971, 2023.\n[5] H. Touvron, L. Martin etal., \u201cLlama 2: Open foundation and fine-tuned\nchat models,\u201d arXiv:2307.09288, 2023.", "start_char_idx": 122184, "end_char_idx": 131515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc72ac09-e3b2-4896-9bf4-18aedb7738da": {"__data__": {"id_": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}, {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, {"node_id": "e209caf1-ead6-467f-9555-1a2e27ae1c9f", "node_type": "1", "metadata": {}, "hash": "e5f3c064f008e09ee6687542919075f8b54044aab99ed4a359a26a7af4ea6f39", "class_name": "RelatedNodeInfo"}]}, "text": "[6] B. Rozi `ere, J. Gehring etal., \u201cCode llama: Open foundation models\nfor code,\u201d arXiv:2308.12950, 2023.\n[7] A. Ramesh, M. Pavlov, G. Goh etal., \u201cZero-shot text-to-image gener-\nation,\u201d in ICML, 2021.23\n[8] A. Ramesh, P. Dhariwal, A. Nichol etal., \u201cHierarchical text-conditional\nimage generation with CLIP latents,\u201d arXiv:2204.06125, 2022.\n[9] J. Betker, G. Goh, L. Jing etal., \u201cImproving image generation with\nbetter captions,\u201d Computer Science, vol. 2, no. 3, p. 8, 2023.\n[10] R. Rombach, A. Blattmann, D. Lorenz etal., \u201cHigh-resolution image\nsynthesis with latent diffusion models,\u201d in IEEE/CVF, 2022.\n[11] OpenAI, \u201cVideo generation models as world simulators,\u201d https://openai.\ncom/research/video-generation-models-as-world-simulators, 2024.\n[12] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\nComput., vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[13] A. Vaswani, N. Shazeer, N. Parmar etal., \u201cAttention is all you need,\u201d\ninNeurIPS, 2017.\n[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza etal., \u201cGenerative adver-\nsarial networks,\u201d CACM, vol. 63, no. 11, pp. 139\u2013144, 2020.\n[15] J. Devlin, M. Chang etal., \u201cBERT: pre-training of deep bidirectional\ntransformers for language understanding,\u201d in NAACL-HLT, 2019.\n[16] C. Raffel, N. Shazeer, A. Roberts etal., \u201cExploring the limits of transfer\nlearning with a unified text-to-text transformer,\u201d JMLR, vol. 21, pp.\n140:1\u2013140:67, 2020.\n[17] W. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling to\ntrillion parameter models with simple and efficient sparsity,\u201d JMLR,\nvol. 23, no. 120, pp. 1\u201339, 2022.\n[18] J. Kaplan, S. McCandlish, T. Henighan etal., \u201cScaling laws for neural\nlanguage models,\u201d 2020.\n[19] S. E. Robertson and H. Zaragoza, \u201cThe probabilistic relevance frame-\nwork: BM25 and beyond,\u201d FTIR, vol. 3, no. 4, pp. 333\u2013389, 2009.\n[20] V . Karpukhin, B. Oguz, S. Min etal., \u201cDense passage retrieval for\nopen-domain question answering,\u201d in EMNLP, 2020.\n[21] J. Johnson, M. Douze, and H. J \u00b4egou, \u201cBillion-scale similarity search\nwith gpus,\u201d IEEE Trans. BigData, vol. 7, no. 3, pp. 535\u2013547, 2021.\n[22] Q. Chen, B. Zhao, H. Wang etal., \u201cSPANN: highly-efficient billion-\nscale approximate nearest neighborhood search,\u201d in NeurIPS, 2021.\n[23] R. Datta, D. Joshi, J. Li etal., \u201cImage retrieval: Ideas, influences, and\ntrends of the new age,\u201d CSUR, vol. 40, no. 2, pp. 5:1\u20135:60, 2008.\n[24] A. Radford, J. W. Kim, C. Hallacy etal., \u201cLearning transferable visual\nmodels from natural language supervision,\u201d in ICML, 2021.\n[25] Z. Feng, D. Guo etal., \u201cCodebert: A pre-trained model for program-\nming and natural languages,\u201d in EMNLP Findings, 2020.\n[26] Y . Wu, K. Chen, T. Zhang etal., \u201cLarge-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-caption augmen-\ntation,\u201d in ICASSP, 2023.\n[27] A. Mallen, A. Asai, V . Zhong etal., \u201cWhen not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric\nmemories,\u201d in ACL, 2023.\n[28] N. Carlini, F. Tram `eretal., \u201cExtracting training data from large\nlanguage models,\u201d in USENIX, 2021.\n[29] M. Kang, N. M. G \u00a8urel etal., \u201cC-RAG: certified generation risks for\nretrieval-augmented language models,\u201d arXiv:2402.03181, 2024.\n[30] G. Izacard, P. Lewis, M. Lomeli etal., \u201cAtlas: Few-shot learning with\nretrieval augmented language models,\u201d arXiv:2208.03299, 2022.\n[31] Y . Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, \u201cMemorizing\ntransformers,\u201d in ICLR, 2022.\n[32] Z. He, Z. Zhong, T. Cai etal., \u201cREST: retrieval-based speculative\ndecoding,\u201d arxiv:2311.08252, 2023.\n[33] K. Guu, K. Lee, Z. Tung etal., \u201cREALM: retrieval-augmented language\nmodel pre-training,\u201d ICML, 2020.\n[34] P. S. H. Lewis, E. Perez, A. Piktus etal., \u201cRetrieval-augmented\ngeneration for knowledge-intensive NLP tasks,\u201d in NeurIPS, 2020.\n[35] G. Izacard and E. Grave, \u201cLeveraging passage retrieval with generative\nmodels for open domain question answering,\u201d in EACL, 2021.\n[36] S. Borgeaud, A. Mensch etal., \u201cImproving language models by\nretrieving from trillions of tokens,\u201d in ICML, 2022.\n[37] U. Khandelwal, O. Levy, D. Jurafsky etal., \u201cGeneralization through\nmemorization: Nearest neighbor language models,\u201d in ICLR, 2020.\n[38] J. He, G. Neubig, and T. Berg-Kirkpatrick, \u201cEfficient nearest neighbor\nlanguage models,\u201d in EMNLP, 2021.\n[39] zilliztech. (2023) Gptcache. [Online]. Available: https://github.com/\nzilliztech/GPTCache\n[40] M. R. Parvez, W. U. Ahmad etal., \u201cRetrieval augmented code gener-\nation and summarization,\u201d in EMNLP Findings, 2021.\n[41] W. U. Ahmad, S. Chakraborty, B. Ray etal., \u201cUnified pre-training for\nprogram understanding and generation,\u201d in NAACL-HLT, 2021.\n[42] S. Zhou, U. Alon, F. F. Xu etal., \u201cDocprompting: Generating code by\nretrieving the docs,\u201d in ICLR, 2023.\n[43] Y . Koizumi, Y . Ohishi etal., \u201cAudio captioning using pre-trained large-\nscale language model guided by audio-based similar caption retrieval,\u201d\narXiv:2012.07331, 2020.[44] R. Huang, J. Huang, D. Yang etal., \u201cMake-an-audio: Text-to-audio\ngeneration with prompt-enhanced diffusion models,\u201d in ICML, 2023.\n[45] H.-Y . Tseng, H.-Y . Lee etal., \u201cRetrievegan: Image synthesis via\ndifferentiable patch retrieval,\u201d in ECCV, 2020.\n[46] S. Sarto, M. Cornia, L. Baraldi, and R. Cucchiara, \u201cRetrieval-\naugmented transformer for image captioning,\u201d in CBMI, 2022.\n[47] R. Ramos, B. Martins etal., \u201cSmallcap: lightweight image captioning\nprompted with retrieval augmentation,\u201d in CVPR, 2023.\n[48] J. Chen, Y . Pan, Y . Li etal., \u201cRetrieval augmented convolutional\nencoder-decoder networks for video captioning,\u201d TOMCCAP, vol. 19,\nno. 1s, pp. 48:1\u201348:24, 2023.\n[49] J. Xu, Y . Huang, J. Hou etal., \u201cRetrieval-augmented egocentric video\ncaptioning,\u201d arXiv:2401.00789, 2024.", "start_char_idx": 131516, "end_char_idx": 137205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd": {"__data__": {"id_": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}, {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, {"node_id": "4d4afe67-524b-4449-a53a-5b1d6ae6c70f", "node_type": "1", "metadata": {}, "hash": "8edc8e65283541f8c15f5ecce593f0b9e2b2228dce4d420f53679e94054d3b16", "class_name": "RelatedNodeInfo"}]}, "text": "[50] J. Seo, S. Hong etal., \u201cRetrieval-augmented score distillation for text-\nto-3d generation,\u201d arXiv:2402.02972, 2024.\n[51] M. Zhang, X. Guo, L. Pan etal., \u201cRemodiffuse: Retrieval-augmented\nmotion diffusion model,\u201d in ICCV, 2023.\n[52] X. Hu, X. Wu, Y . Shu, and Y . Qu, \u201cLogical form generation via multi-\ntask learning for complex question answering over knowledge bases,\u201d\ninCOLING, 2022.\n[53] X. Huang, J. Kim, and B. Zou, \u201cUnseen entity handling in complex\nquestion answering over knowledge base via language generation,\u201d in\nEMNLP Findings, 2021.\n[54] R. Das, M. Zaheer, D. Thai etal., \u201cCase-based reasoning for natural\nlanguage queries over knowledge bases,\u201d in EMNLP, 2021.\n[55] Z. Wang, W. Nie, Z. Qiao etal., \u201cRetrieval-based controllable molecule\ngeneration,\u201d in ICLR, 2022.\n[56] Q. Jin, Y . Yang, Q. Chen, and Z. Lu, \u201cGenegpt: Augmenting large\nlanguage models with domain tools for improved access to biomedical\ninformation,\u201d Bioinformatics, vol. 40, no. 2, p. btae075, 2024.\n[57] H. Li, Y . Su, D. Cai etal., \u201cA survey on retrieval-augmented text\ngeneration,\u201d arxiv:2202.01110, 2022.\n[58] A. Asai, S. Min, Z. Zhong, and D. Chen, \u201cAcl 2023 tutorial: Retrieval-\nbased language models and applications,\u201d ACL 2023, 2023.\n[59] Y . Gao, Y . Xiong etal., \u201cRetrieval-augmented generation for large\nlanguage models: A survey,\u201d arxiv:2312.10997, 2023.\n[60] R. Zhao, H. Chen etal., \u201cRetrieving multimodal information for\naugmented generation: A survey,\u201d in EMNLP, 2023.\n[61] J. Chen, H. Guo, K. Yi etal., \u201cVisualgpt: Data-efficient adaptation of\npretrained language models for image captioning,\u201d in CVPR, 2022.\n[62] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, \u201cEfficient transformers:\nA survey,\u201d CSUR, vol. 55, no. 6, pp. 109:1\u2013109:28, 2023.\n[63] G. V . Houdt etal., \u201cA review on the long short-term memory model,\u201d\nArtif. Intell. Rev., vol. 53, no. 8, pp. 5929\u20135955, 2020.\n[64] L. Yang, Z. Zhang etal., \u201cDiffusion models: A comprehensive survey\nof methods and applications,\u201d CSUR, vol. 56, no. 4, pp. 1\u201339, 2023.\n[65] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d\ninICML, 2015.\n[66] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d in NeurIPS, 2020.\n[67] A. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffusion proba-\nbilistic models,\u201d in ICML, 2021.\n[68] Y . Song and S. Ermon, \u201cGenerative modeling by estimating gradients\nof the data distribution,\u201d in NeurIPS, 2019.\n[69] \u2014\u2014, \u201cImproved techniques for training score-based generative mod-\nels,\u201d in NeurIPS, 2020.\n[70] Y . Song, J. Sohl-Dickstein, D. P. Kingma etal., \u201cScore-based generative\nmodeling through stochastic differential equations,\u201d in ICLR, 2021.\n[71] Y . Song, C. Durkan, I. Murray, and S. Ermon, \u201cMaximum likelihood\ntraining of score-based diffusion models,\u201d in NeurIPS, 2021.\n[72] L. Yang, H. Qian, Z. Zhang etal., \u201cStructure-guided adversarial training\nof diffusion models,\u201d in CVPR, 2024.\n[73] X. Zhang, L. Yang, Y . Cai etal., \u201cRealcompo: Dynamic equilibrium\nbetween realism and compositionality improves text-to-image diffusion\nmodels,\u201d arXiv:2402.12908, 2024.\n[74] R. Rombach, A. Blattmann, D. Lorenz etal., \u201cHigh-resolution image\nsynthesis with latent diffusion models,\u201d in CVPR, 2022.\n[75] A. Ramesh, P. Dhariwal, A. Nichol etal., \u201cHierarchical text-conditional\nimage generation with clip latents,\u201d arXiv:2204.06125, 2022.\n[76] H. Li, Y . Yang, M. Chang etal., \u201cSrdiff: Single image super-resolution\nwith diffusion probabilistic models,\u201d Neurocomputing, vol. 479, pp.\n47\u201359, 2022.\n[77] J. Ho, C. Saharia, W. Chan etal., \u201cCascaded diffusion models for high\nfidelity image generation,\u201d JMLR, vol. 23, no. 1, pp. 2249\u20132281, 2022.\n[78] L. Yang, J. Liu, S. Hong etal., \u201cImproving diffusion-based image\nsynthesis with context prediction,\u201d in NeurIPS, 2024.24\n[79] L. Yang, Z. Yu, C. Meng etal., \u201cMastering text-to-image diffu-\nsion: Recaptioning, planning, and generating with multimodal llms,\u201d\narXiv:2401.11708, 2024.\n[80] S. Gong, M. Li, J. Feng etal., \u201cDiffuseq: Sequence to sequence text\ngeneration with diffusion models,\u201d in ICLR, 2023.\n[81] X. Li, J. Thickstun, I. Gulrajani etal., \u201cDiffusion-lm improves control-\nlable text generation,\u201d in NeurIPS, 2022.\n[82] J. Austin, D. D. Johnson, J. Ho etal., \u201cStructured denoising diffusion\nmodels in discrete state-spaces,\u201d in NeurIPS, 2021.\n[83] T. Chen, R. Zhang, and G. Hinton, \u201cAnalog bits: Generating discrete\ndata using diffusion models with self-conditioning,\u201d in ICLR, 2023.\n[84] J. Ho, W. Chan, C. Saharia etal., \u201cImagen video: High definition video\ngeneration with diffusion models,\u201d arXiv:2210.02303, 2022.\n[85] W. Harvey, S. Naderiparizi, V . Masrani etal., \u201cFlexible diffusion\nmodeling of long videos,\u201d in NeurIPS, 2022.\n[86] R. Yang, P. Srivastava, and S. Mandt, \u201cDiffusion probabilistic modeling\nfor video generation,\u201d Entropy, vol. 25, no. 10, p. 1469, 2023.\n[87] M. Zhang, Z. Cai, L. Pan etal., \u201cMotiondiffuse: Text-driven human\nmotion generation with diffusion model,\u201d TPAMI, 2024.\n[88] L. Yang, Z. Zhang, Z. Yu etal., \u201cCross-modal contextualized diffusion\nmodels for text-guided visual generation and editing,\u201d in ICLR, 2024.\n[89] N. Anand and T. Achim, \u201cProtein structure and sequence gen-\neration with equivariant denoising diffusion probabilistic models,\u201d\narXiv:2205.15019, 2022.\n[90] M. Xu, L. Yu, Y . Song etal., \u201cGeodiff: A geometric diffusion model\nfor molecular conformation generation,\u201d in ICLR, 2022.\n[91] E. Hoogeboom, V . G. Satorras, C. Vignac, and M. Welling, \u201cEquivari-\nant diffusion for molecule generation in 3d,\u201d in ICML, 2022.\n[92] B. Jing, G. Corso, J. Chang etal., \u201cTorsional diffusion for molecular\nconformer generation,\u201d in NeurIPS, 2022.\n[93] Z. Huang, L. Yang, X. Zhou etal., \u201cProtein-ligand interaction prior for\nbinding-aware 3d molecule diffusion models,\u201d in ICLR, 2024.", "start_char_idx": 137206, "end_char_idx": 143058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26864623-e72d-4b91-b583-8d12ace4d8e1": {"__data__": {"id_": "26864623-e72d-4b91-b583-8d12ace4d8e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, {"node_id": "be6792a4-da55-47a2-b289-7b1b5869811d", "node_type": "1", "metadata": {}, "hash": "f96435e0fc6aced74c8216eab27599c4dced159add03bf04dae32d2430393afc", "class_name": "RelatedNodeInfo"}]}, "text": "[94] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit mod-\nels,\u201d in ICLR, 2021.\n[95] X. Liu, C. Gong, and Q. Liu, \u201cFlow straight and fast: Learning to\ngenerate and transfer data with rectified flow,\u201d 2023.\n[96] Y . Song, P. Dhariwal, M. Chen, and I. Sutskever, \u201cConsistency models,\u201d\ninICML, 2023.\n[97] J. Gui, Z. Sun, Y . Wen etal., \u201cA review on generative adversarial\nnetworks: Algorithms, theory, and applications,\u201d TKDE, vol. 35, no. 4,\npp. 3313\u20133332, 2023.\n[98] S. E. Robertson and S. Walker, \u201cOn relevance weights with little\nrelevance information,\u201d in SIGIR, 1997.\n[99] J. D. Lafferty and C. Zhai, \u201cDocument language models, query models,\nand risk minimization for information retrieval,\u201d in SIGIR, 2001.\n[100] Y . Liu, M. Ott etal., \u201cRoberta: A robustly optimized BERT pretraining\napproach,\u201d arxiv:1907.11692, 2019.\n[101] L. Xiong, C. Xiong, Y . Li etal., \u201cApproximate nearest neighbor\nnegative contrastive learning for dense text retrieval,\u201d in ICLR, 2021.\n[102] H. Zhang, Y . Gong, Y . Shen etal., \u201cAdversarial retriever-ranker for\ndense text retrieval,\u201d in ICLR, 2022.\n[103] Y . Qu, Y . Ding, J. Liu etal., \u201cRocketqa: An optimized training approach\nto dense passage retrieval for open-domain question answering,\u201d in\nNAACL-HLT, 2021.\n[104] L. Gao and J. Callan, \u201cCondenser: a pre-training architecture for dense\nretrieval,\u201d in EMNLP, 2021.\n[105] D. Guo, S. Ren etal., \u201cGraphcodebert: Pre-training code representa-\ntions with data flow,\u201d in ICLR, 2021.\n[106] Y . Wang, W. Wang, S. R. Joty, and S. C. H. Hoi, \u201cCodet5: Identifier-\naware unified pre-trained encoder-decoder models for code understand-\ning and generation,\u201d in EMNLP, 2021.\n[107] S. Hershey, S. Chaudhuri etal., \u201cCNN architectures for large-scale\naudio classification,\u201d in ICASSP, 2017.\n[108] X. Yuan, Z. Lin, J. Kuen etal., \u201cMultimodal contrastive training for\nvisual representation learning,\u201d in CVPR, 2021.\n[109] J. Dong, X. Li, C. Xu etal., \u201cDual encoding for zero-example video\nretrieval,\u201d in CVPR, 2019.\n[110] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, \u201cFrozen in time:\nA joint video and image encoder for end-to-end retrieval,\u201d in ICCV,\n2021.\n[111] J. Zhan, J. Mao, Y . Liu etal., \u201cOptimizing dense retrieval model\ntraining with hard negatives,\u201d in SIGIR, 2021.\n[112] J. L. Bentley, \u201cMultidimensional binary search trees used for associa-\ntive searching,\u201d CACM, vol. 18, no. 9, pp. 509\u2013517, 1975.\n[113] W. Li, C. Feng, D. Lian etal., \u201cLearning balanced tree indexes for\nlarge-scale vector retrieval,\u201d in SIGKDDg, 2023.[114] M. Datar, N. Immorlica, P. Indyk etal., \u201cLocality-sensitive hashing\nscheme based on p-stable distributions,\u201d in SCG, 2004.\n[115] Y . A. Malkov and D. A. Yashunin, \u201cEfficient and robust approxi-\nmate nearest neighbor search using hierarchical navigable small world\ngraphs,\u201d TPAMI, vol. 42, no. 4, pp. 824\u2013836, 2018.\n[116] S. Jayaram Subramanya, F. Devvrit etal., \u201cDiskann: Fast accurate\nbillion-point nearest neighbor search on a single node,\u201d NeurIPS, 2019.\n[117] J. Ren, M. Zhang, and D. Li, \u201cHm-ann: Efficient billion-point nearest\nneighbor search on heterogeneous memory,\u201d NeurIPS, 2020.\n[118] Y . Wang, Y . Hou, H. Wang etal., \u201cA neural corpus indexer for\ndocument retrieval,\u201d in NeurIPS, 2022.\n[119] H. Zhang, Y . Wang, Q. Chen etal., \u201cModel-enhanced vector index,\u201d\ninNeurIPS, 2023.\n[120] S. A. Hayati, R. Olivier, P. Avvaru etal., \u201cRetrieval-based neural code\ngeneration,\u201d in EMNLP, 2018.\n[121] J. Zhang, X. Wang, H. Zhang etal., \u201cRetrieval-based neural source\ncode summarization,\u201d in ICSE, 2020.\n[122] G. Poesia, A. Polozov, V . Le etal., \u201cSynchromesh: Reliable code\ngeneration from pre-trained language models,\u201d in ICLR, 2022.\n[123] X. Ye, S. Yavuz etal., \u201cRNG-KBQA: generation augmented iterative\nranking for knowledge base question answering,\u201d in ACL, 2022.\n[124] Y . Shu etal., \u201cTIARA: multi-grained retrieval for robust question\nanswering over large knowledge bases,\u201d arXiv:2210.12925, 2022.\n[125] X. V . Lin, R. Socher etal., \u201cBridging textual and tabular data for\ncross-domain text-to-sql semantic parsing,\u201d arXiv:2012.12627, 2020.\n[126] A. Asai, Z. Wu, Y . Wang etal., \u201cSelf-rag: Learning to retrieve, generate,\nand critique through self-reflection,\u201d arxiv:2310.11511, 2023.\n[127] W. Shi, S. Min, M. Yasunaga etal., \u201cReplug: Retrieval-augmented\nblack-box language models,\u201d arXiv:2301.12652, 2023.\n[128] O. Ram, Y . Levine, I. Dalmedigos etal., \u201cIn-context retrieval-\naugmented language models,\u201d arXiv:2302.00083, 2023.\n[129] D. Zan, B. Chen, Z. Lin etal., \u201cWhen language model meets private\nlibrary,\u201d in EMNLP Findings, 2022.\n[130] N. Nashid, M. Sintaha, and A. Mesbah, \u201cRetrieval-based prompt\nselection for code-related few-shot learning,\u201d in ICSE, 2023.\n[131] M. Jin, S. Shahriar, M. Tufano etal., \u201cInferfix: End-to-end program\nrepair with llms,\u201d in ESEC/FSE, 2023.\n[132] S. Lu, N. Duan, H. Han etal., \u201cReacc: A retrieval-augmented code\ncompletion framework,\u201d in ACL, 2022.\n[133] Y . Liu etal., \u201cUni-parser: Unified semantic parser for question answer-\ning on knowledge base and database,\u201d in EMNLP, 2022.\n[134] Z. Yang, X. Du, E. Cambria etal., \u201cEnd-to-end case-based reasoning\nfor commonsense knowledge base completion,\u201d in EACL, 2023.\n[135] M. Patidar, A. K. Singh, R. Sawhney etal., \u201cCombining transfer\nlearning with in-context learning using blackbox llms for zero-shot\nknowledge base question answering,\u201d arXiv:2311.08894, 2023.\n[136] W. Shi, Y . Zhuang, Y . Zhu etal., \u201cRetrieval-augmented large language\nmodels for adolescent idiopathic scoliosis patients in shared decision-\nmaking,\u201d in ACM-BCB, 2023.\n[137] A. Casanova, M. Careil, J. Verbeek etal., \u201cInstance-conditioned gan,\u201d\ninNeurIPS, 2021.\n[138] J. Li, Y . Li, G. Li etal., \u201cEditsum: A retrieve-and-edit framework for\nsource code summarization,\u201d in ASE, 2021.\n[139] C. Yu, G. Yang, X. Chen etal., \u201cBashexplainer: Retrieval-augmented\nbash code comment generation based on fine-tuned codebert,\u201d in\nICSME, 2022.\n[140] T. B. Hashimoto, K. Guu, Y .", "start_char_idx": 143059, "end_char_idx": 149003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "436bed26-cd7a-4c41-ab9b-76184bb157fd": {"__data__": {"id_": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, {"node_id": "3592c0a4-8fb2-4091-8a28-67a42ea67b94", "node_type": "1", "metadata": {}, "hash": "d363c37c0208a96b75cdf2c8c5083629ba0e987c7fb6517541f69fbf03d4867c", "class_name": "RelatedNodeInfo"}]}, "text": "[140] T. B. Hashimoto, K. Guu, Y . Oren, and P. Liang, \u201cA retrieve-and-edit\nframework for predicting structured outputs,\u201d in NeurIPS, 2018.\n[141] B. Wei, Y . Li, G. Li etal., \u201cRetrieve and refine: Exemplar-based neural\ncomment generation,\u201d in ASE, 2020.\n[142] E. Shi, Y . Wang, W. Tao etal., \u201cRACE: retrieval-augmented commit\nmessage generation,\u201d in EMNLP, 2022.\n[143] B. Oguz, X. Chen, V . Karpukhin etal., \u201cUnik-qa: Unified repre-\nsentations of structured and unstructured knowledge for open-domain\nquestion answering,\u201d in NAACL Findings, 2022.\n[144] D. Yu, S. Zhang etal., \u201cDecaf: Joint decoding of answers and logical\nforms for question answering over knowledge bases,\u201d in ICLR, 2023.\n[145] G. Dong, R. Li, S. Wang etal., \u201cBridging the kb-text gap: Leveraging\nstructured knowledge-aware pre-training for KBQA,\u201d in CIKM, 2023.\n[146] K. Wang, F. Duan, S. Wang etal., \u201cKnowledge-driven cot: Exploring\nfaithful reasoning in llms for knowledge-intensive question answering,\u201d\narXiv:2308.13259, 2023.\n[147] D. Yu and Y . Yang, \u201cRetrieval-enhanced generative model for large-\nscale knowledge graph completion,\u201d in SIGIR, 2023.\n[148] W. Chen, H. Hu, C. Saharia, and W. W. Cohen, \u201cRe-imagen: Retrieval-\naugmented text-to-image generator,\u201d in ICLR, 2023.25\n[149] S. Sheynin, O. Ashual, A. Polyak etal., \u201cKnn-diffusion: Image gener-\nation via large-scale retrieval,\u201d in ICLR, 2023.\n[150] A. Blattmann, R. Rombach, K. Oktay etal., \u201cRetrieval-augmented\ndiffusion models,\u201d in NeurIPS, 2022.\n[151] R. Rombach, A. Blattmann, and B. Ommer, \u201cText-guided synthe-\nsis of artistic images with retrieval-augmented diffusion models,\u201d\narXiv:2207.13038, 2022.\n[152] B. Li, P. H. Torr, and T. Lukasiewicz, \u201cMemory-driven text-to-image\ngeneration,\u201d arXiv:2208.07022, 2022.\n[153] A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley, \u201cUnlimiformer:\nLong-range transformers with unlimited length input,\u201d 2023.\n[154] Y . Kuratov, A. Bulatov etal., \u201cIn search of needles in a 10m haystack:\nRecurrent memory finds what llms miss,\u201d arXiv:2402.10790, 2024.\n[155] N. F. Liu, K. Lin, J. Hewitt etal., \u201cLost in the middle: How language\nmodels use long contexts,\u201d arxiv:2307.03172, 2023.\n[156] T. F \u00b4evry, L. B. Soares etal., \u201cEntities as experts: Sparse memory access\nwith entity supervision,\u201d in EMNLP, 2020.\n[157] M. de Jong, Y . Zemlyanskiy, N. FitzGerald etal., \u201cMention memory:\nincorporating textual knowledge into transformers through entity men-\ntion attention,\u201d in ICLR, 2021.\n[158] B. Jing, Y . Zhang, Z. Song etal., \u201cAmd: Anatomical motion diffusion\nwith interpretable motion decomposition and fusion,\u201d in AAAI, 2024.\n[159] Y . Yuan, H. Liu, X. Liu etal., \u201cRetrieval-augmented text-to-audio\ngeneration,\u201d in ICASSP, 2024.\n[160] B. Yang, M. Cao, and Y . Zou, \u201cConcept-aware video captioning:\nDescribing videos with effective prior information,\u201d TIP, vol. 32, pp.\n5366\u20135378, 2023.\n[161] Z. Zhong, T. Lei, and D. Chen, \u201cTraining language models with\nmemory augmentation,\u201d in EMNLP, 2022.\n[162] S. Min, W. Shi, M. Lewis etal., \u201cNonparametric masked language\nmodeling,\u201d in ACL Findings, 2023.\n[163] X. Zhang, Y . Zhou, G. Yang, and T. Chen, \u201cSyntax-aware retrieval\naugmented code generation,\u201d in EMNLP Findings, 2023.\n[164] Z. Fei, \u201cMemory-augmented image captioning,\u201d in AAAI, 2021.\n[165] Y . Leviathan, M. Kalman, and Y . Matias, \u201cFast inference from trans-\nformers via speculative decoding,\u201d in ICML, 2023.\n[166] T. Lan, D. Cai, Y . Wang etal., \u201cCopy is all you need,\u201d in ICLR, 2023.\n[167] B. Cao, D. Cai, L. Cui etal., \u201cRetrieval is accurate generation,\u201d\narXiv:2402.17532, 2024.\n[168] L. Wang, N. Yang, and F. Wei, \u201cQuery2doc: Query expansion with\nlarge language models,\u201d in EMNLP, 2023.\n[169] L. Gao, X. Ma, J. Lin, and J. Callan, \u201cPrecise zero-shot dense retrieval\nwithout relevance labels,\u201d in ACL, 2023.\n[170] G. Kim, S. Kim, B. Jeon etal., \u201cTree of clarifications: Answering\nambiguous questions with retrieval-augmented large language models,\u201d\ninEMNLP, 2023.\n[171] M. Xia, S. Malladi, S. Gururangan etal., \u201cLESS: selecting influential\ndata for targeted instruction tuning,\u201d arXiv:2402.04333, 2024.\n[172] S. Yao, J. Zhao, D. Yu etal., \u201cReact: Synergizing reasoning and acting\nin language models,\u201d in ICLR, 2023.\n[173] J. Wei, X. Wang, D. Schuurmans etal., \u201cChain-of-thought prompting\nelicits reasoning in large language models,\u201d in NeurIPS, 2022.\n[174] T. Pouplin, H. Sun, S. Holt, and M. Van der Schaar, \u201cRetrieval-\naugmented thought process as sequential decision making,\u201d\narXiv:2402.07812, 2024.\n[175] J. Liu, \u201cLlamaIndex,\u201d 11 2022. [Online]. Available: https://github.\ncom/jerryjliu/llama index\n[176] P. Sarthi, S. Abdullah, A. Tuli etal., \u201cRaptor: Recursive abstractive\nprocessing for tree-organized retrieval,\u201d in ICLR, 2023.\n[177] S. Xiao, Z. Liu, P. Zhang etal., \u201cC-pack: Packaged resources to advance\ngeneral chinese embedding,\u201d arxiv:2309.07597, 2023.\n[178] J. Chen, S. Xiao, P. Zhang etal., \u201cBge m3-embedding: Multi-lingual,\nmulti-functionality, multi-granularity text embeddings through self-\nknowledge distillation,\u201d arxiv:2309.07597, 2023.\n[179] S. Xiao, Z. Liu, P. Zhang, and X. Xing, \u201cLm-cocktail: Resilient tuning\nof language models via model merging,\u201d arxiv:2311.13534, 2023.\n[180] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, \u201cRetrieve anything\nto augment large language models,\u201d arxiv:2310.07554, 2023.\n[181] M. Kulkarni, P. Tangarajan, K. Kim etal., \u201cReinforcement learning for\noptimizing RAG for domain chatbots,\u201d arXiv:2401.06800, 2024.\n[182] W. Wang, Y . Wang etal., \u201cRap-gen: Retrieval-augmented patch gener-\nation with codet5 for automatic program repair,\u201d in ESEC/FSE, 2023.\n[183] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, \u201cCorrective retrieval\naugmented generation,\u201d arXiv:2401.15884, 2024.\n[184] W. Huang, M. Lapata, P. V ougiouklis etal., \u201cRetrieval augmented\ngeneration with rich answer encoding,\u201d in IJCNLP-AACL, 2023.", "start_char_idx": 148969, "end_char_idx": 154804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c4ac0a6-1a71-4196-9877-d2964b0f3744": {"__data__": {"id_": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}, {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, {"node_id": "87de77de-8c9a-4dc5-987b-0dc007f852a7", "node_type": "1", "metadata": {}, "hash": "1cea245af87ea4c2c0f46e0709808b42ed0bfb92ef38349f3939fecce04411e0", "class_name": "RelatedNodeInfo"}]}, "text": "[185] H. Wang, W. Huang, Y . Deng etal., \u201cUnims-rag: A unified multi-source\nretrieval-augmented generation for personalized dialogue systems,\u201d\narXiv:2401.13256, 2024.\n[186] M. R. Glass, G. Rossiello, M. F. M. Chowdhury etal., \u201cRe2g: Retrieve,\nrerank, generate,\u201d in NAACL, 2022.\n[187] R. F. Nogueira and K. Cho, \u201cPassage re-ranking with BERT,\u201d\narxiv:1901.04085, 2019.\n[188] J. Li, Y . Zhao, Y . Li etal., \u201cAcecoder: Utilizing existing code to\nenhance code generation,\u201d arXiv:2303.17780, 2023.\n[189] P. Shi, R. Zhang, H. Bai, and J. Lin, \u201cXRICL: cross-lingual retrieval-\naugmented in-context learning for cross-lingual text-to-sql semantic\nparsing,\u201d in EMNLP Findings, 2022.\n[190] K. Rangan and Y . Yin, \u201cA fine-tuning enhanced rag system with\nquantized influence measure as ai judge,\u201d arXiv:2402.17081, 2024.\n[191] J. Saad-Falcon, O. Khattab, K. Santhanam etal., \u201cUdapdr: Unsu-\npervised domain adaptation via llm prompting and distillation of\nrerankers,\u201d in EMNLP, 2023.\n[192] L. Wang, N. Yang, and F. Wei, \u201cLearning to retrieve in-context\nexamples for large language models,\u201d arXiv:2307.07164, 2023.\n[193] Z. Wang, J. Araki, Z. Jiang etal., \u201cLearning to filter context for\nretrieval-augmented generation,\u201d arxiv:2311.08377, 2023.\n[194] S. Hofst \u00a8atter, J. Chen, K. Raman, and H. Zamani, \u201cFid-light: Efficient\nand effective retrieval-augmented text generation,\u201d in SIGIR, 2023.\n[195] D. Arora, A. Kini, S. R. Chowdhury etal., \u201cGar-meets-rag paradigm\nfor zero-shot information retrieval,\u201d arXiv:2310.20158, 2023.\n[196] https://www.pinecone.io.\n[197] W. Yu, D. Iter etal., \u201cGenerate rather than retrieve: Large language\nmodels are strong context generators,\u201d arXiv:2209.10063, 2022.\n[198] A. Abdallah and A. Jatowt, \u201cGenerator-retriever-generator: A novel ap-\nproach to open-domain question answering,\u201d arXiv:2307.11278, 2023.\n[199] E. Saravia, \u201cPrompt Engineering Guide,\u201d\nhttps://github.com/dair-ai/Prompt-Engineering-Guide, 12 2022.\n[200] H. S. Zheng, S. Mishra etal., \u201cTake a step back: Evoking reasoning\nvia abstraction in large language models,\u201d arxiv:2310.06117, 2023.\n[201] S. Diao, P. Wang, Y . Lin, and T. Zhang, \u201cActive prompting with chain-\nof-thought for large language models,\u201d arxiv:2302.12246, 2023.\n[202] H. Jiang, Q. Wu, C. Lin etal., \u201cLlmlingua: Compressing prompts for\naccelerated inference of large language models,\u201d in EMNLP, 2023.\n[203] T. Ahmed, K. S. Pai, P. Devanbu, and E. T. Barr, \u201cAutomatic semantic\naugmentation of language model prompts (for code summarization),\u201d\narXiv:2304.06815, 2024.\n[204] Z. Xu, Z. Liu, Y . Liu etal., \u201cActiverag: Revealing the treasures of\nknowledge via active learning,\u201d arXiv:2402.13547, 2024.\n[205] E. Nijkamp, B. Pang, H. Hayashi etal., \u201cA conversational paradigm\nfor program synthesis,\u201d arxiv:2203.13474, 2022.\n[206] Y . He, M. Xia, H. Chen etal., \u201cAnimate-a-story: Storytelling with\nretrieval-augmented video generation,\u201d arXiv:2307.06940, 2023.\n[207] E. J. Hu, Y . Shen, P. Wallis etal., \u201cLora: Low-rank adaptation of large\nlanguage models,\u201d in ICLR, 2022.\n[208] C. Liu, P. C \u00b8 etin, Y . Patodia etal., \u201cAutomated code editing with search-\ngenerate-modify,\u201d arXiv:2306.06490, 2023.\n[209] H. Joshi, J. P. C. S \u00b4anchez, S. Gulwani etal., \u201cRepair is nearly\ngeneration: Multilingual program repair with llms,\u201d in AAAI, 2023.\n[210] Z. Jiang, F. F. Xu, L. Gao etal., \u201cActive retrieval augmented genera-\ntion,\u201d arXiv:2305.06983, 2023.\n[211] A. Mallen, A. Asai, V . Zhong etal., \u201cWhen not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric\nmemories,\u201d in ACL, 2023.\n[212] Z. Jiang, J. Araki, H. Ding, and G. Neubig, \u201cHow can we know When\nlanguage models know? on the calibration of language models for\nquestion answering,\u201d TACL, 2021.\n[213] N. Kandpal, H. Deng, A. Roberts etal., \u201cLarge language models\nstruggle to learn long-tail knowledge,\u201d in ICML, 2023.\n[214] R. Ren, Y . Wang, Y . Qu etal., \u201cInvestigating the factual knowledge\nboundary of large language models with retrieval augmentation,\u201d\narxiv:2307.11019, 2023.\n[215] Y . Wang, P. Li, M. Sun, and Y . Liu, \u201cSelf-knowledge guided retrieval\naugmentation for large language models,\u201d in EMNLP Findings, 2023.\n[216] H. Ding, L. Pang, Z. Wei etal., \u201cRetrieve only when it needs: Adaptive\nretrieval augmentation for hallucination mitigation in large language\nmodels,\u201d arXiv:2402.10612, 2024.\n[217] S. Jeong, J. Baek, S. Cho etal., \u201cAdaptive-rag: Learning to adapt\nretrieval-augmented large language models through question complex-\nity,\u201d arXiv:2403.14403, 2024.\n[218] F. Zhang, B. Chen etal., \u201cRepocoder: Repository-level code completion\nthrough iterative retrieval and generation,\u201d in EMNLP, 2023.26\n[219] Z. Shao, Y . Gong, Y . Shen etal., \u201cEnhancing retrieval-augmented\nlarge language models with iterative retrieval-generation synergy,\u201d in\nEMNLP Findings, 2023.\n[220] X. Cheng, D. Luo, X. Chen etal., \u201cLift yourself up: Retrieval-\naugmented text generation with self-memory,\u201d in NeurIPS, 2023.\n[221] Z. Wang, A. Liu, H. Lin etal., \u201cRat: Retrieval augmented\nthoughts elicit context-aware reasoning in long-horizon generation,\u201d\narXiv:2403.05313, 2024.\n[222] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, \u201cKnowledge graph\nbased synthetic corpus generation for knowledge-enhanced language\nmodel pre-training,\u201d in NAACL-HLT, 2021.\n[223] J. Sun, C. Xu, L. Tang etal., \u201cThink-on-graph: Deep and respon-\nsible reasoning of large language model with knowledge graph,\u201d\narXiv:2307.07697, 2023.\n[224] P. Limkonchotiwat, W. Ponwitayarat, C. Udomcharoenchaikit etal.,\n\u201cCl-relkt: Cross-lingual language knowledge transfer for multilingual\nretrieval question answering,\u201d in NAACL Findings, 2022.\n[225] A. Asai, X. Yu, J. Kasai, and H. Hajishirzi, \u201cOne question answering\nmodel for many languages with cross-lingual dense passage retrieval,\u201d\ninNeurIPS, 2021.\n[226] K. Lee, S. Han etal., \u201cWhen to read documents or QA history: On\nunified and selective open-domain QA,\u201d in ACL Findings, 2023.", "start_char_idx": 154804, "end_char_idx": 160718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22825eff-8ad1-4d03-823a-8e232176aa19": {"__data__": {"id_": "22825eff-8ad1-4d03-823a-8e232176aa19", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}, {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}]}, "text": "[227] S. Yue, W. Chen etal., \u201cDisc-lawllm: Fine-tuning large language\nmodels for intelligent legal services,\u201d arXiv:2309.11325, 2023.\n[228] S. Siriwardhana, R. Weerasekera, T. Kaluarachchi etal., \u201cImproving\nthe domain adaptation of retrieval augmented generation (RAG) models\nfor open domain question answering,\u201d TACL, vol. 11, pp. 1\u201317, 2023.\n[229] Y . Tang and Y . Yang, \u201cMultihop-rag: Benchmarking retrieval-\naugmented generation for multi-hop queries,\u201d arXiv:2401.15391, 2024.\n[230] K. Huang, C. Zhai, and H. Ji, \u201cCONCRETE: improving cross-lingual\nfact-checking with cross-lingual retrieval,\u201d in COLING, 2022.\n[231] L. Hagstr \u00a8om, D. Saynova, T. Norlund etal., \u201cThe effect of scaling,\nretrieval augmentation and form on the factual consistency of language\nmodels,\u201d arXiv:2311.01307, 2023.\n[232] Y . Liu, Y . Wan etal., \u201cKG-BART: knowledge graph-augmented BART\nfor generative commonsense reasoning,\u201d in AAAI, 2021.\n[233] A. Wan, E. Wallace, and D. Klein, \u201cWhat evidence do language models\nfind convincing?\u201d arXiv:2402.11782, 2024.\n[234] H. Zhang, Z. Liu etal., \u201cGrounded conversation generation as guided\ntraverses in commonsense knowledge graphs,\u201d in ACL, 2020.\n[235] D. Cai, Y . Wang, W. Bi etal., \u201cSkeleton-to-response: Dialogue gener-\nation guided by retrieval memory,\u201d in NAACL-HLT, 2019.\n[236] M. Komeili, K. Shuster, and J. Weston, \u201cInternet-augmented dialogue\ngeneration,\u201d in ACL, 2022.\n[237] K. Shuster, J. Xu etal., \u201cBlenderbot 3: a deployed conversational agent\nthat continually learns to responsibly engage,\u201d arXiv:2208.03188, 2022.\n[238] S. Kim, J. Y . Jang, M. Jung, and S. Shin, \u201cA model of cross-lingual\nknowledge-grounded response generation for open-domain dialogue\nsystems,\u201d in EMNLP Findings, 2021.\n[239] E. Nie, S. Liang, H. Schmid, and H. Sch \u00a8utze, \u201cCross-lingual retrieval\naugmented prompt for low-resource languages,\u201d in ACL, 2023.\n[240] X. Li, E. Nie, and S. Liang, \u201cFrom classification to generation: Insights\ninto crosslingual retrieval augmented icl,\u201d in NeurIPS, 2023.\n[241] W. Li, J. Li, W. Ma, and Y . Liu, \u201cCitation-enhanced generation for\nllm-based chatbot,\u201d arXiv:2402.16063, 2024.\n[242] D. Cai, Y . Wang, H. Li etal., \u201cNeural machine translation with\nmonolingual translation memory,\u201d in ACL/IJCNLP, 2021.\n[243] U. Khandelwal, A. Fan, D. Jurafsky etal., \u201cNearest neighbor machine\ntranslation,\u201d in ICLR, 2021.\n[244] X. Du and H. Ji, \u201cRetrieval-augmented generative question answering\nfor event argument extraction,\u201d in EMNLP, 2022.\n[245] Y . Gao, Q. Yin, Z. Li etal., \u201cRetrieval-augmented multilingual\nkeyphrase generation with retriever-generator iterative training,\u201d in\nNAACL Findings, 2022.\n[246] J. Zhang, E. J. Yu, Q. Chen etal., \u201cRetrieval-based full-length wikipedia\ngeneration for emergent events,\u201d arXiv:2402.18264, 2024.\n[247] R. Fan, Y . Fan, J. Chen etal., \u201cRIGHT: retrieval-augmented generation\nfor mainstream hashtag recommendation,\u201d arxiv:2312.10466, 2023.\n[248] Y . Wang, H. Le, A. D. Gotmare etal., \u201cCodet5mix: A pretrained\nmixture of encoder-decoder transformers for code understanding and\ngeneration,\u201d 2022.\n[249] E. Nijkamp, B. Pang, H. Hayashi etal., \u201cA conversational paradigm\nfor program synthesis,\u201d arXiv:2203.13474, 2022.\n[250] D. Zan, B. Chen, Y . Gong etal., \u201cPrivate-library-oriented code gener-\nation with large language models,\u201d arXiv:2307.15370, 2023.\n[251] A. Madaan, S. Zhou, U. Alon etal., \u201cLanguage models of code are\nfew-shot commonsense learners,\u201d in EMNLP, 2022.[252] Y . Wang, H. Le, A. Gotmare etal., \u201cCodet5+: Open code large language\nmodels for code understanding and generation,\u201d in EMNLP, 2023.\n[253] D. Liao, S. Pan, Q. Huang etal., \u201cContext-aware code generation\nframework for code repositories: Local, global, and third-party library\nawareness,\u201d arXiv:2312.05772, 2023.\n[254] J. Li, Y . Li, G. Li etal., \u201cSkcoder: A sketch-based approach for\nautomatic code generation,\u201d in ICSE, 2023.\n[255] M. Liu, T. Yang, Y . Lou etal., \u201cCodegen4libs: A two-stage approach\nfor library-oriented code generation,\u201d in ASE, 2023.\n[256] K. Zhang, J. Li, G. Li etal., \u201cCodeagent: Enhancing code generation\nwith tool-integrated agent systems for real-world repo-level coding\nchallenges,\u201d arXiv:2401.07339, 2024.\n[257] Q. Gou, Y . Dong, Y . Wu, and Q. Ke, \u201cRrgcode: Deep hierarchical\nsearch-based code generation,\u201d Journal ofSystems andSoftware, vol.\n211, p. 111982, 2024.\n[258] J. Chen, X. Hu, Z. Li etal., \u201cCode search is all you need? improving\ncode suggestions with code search,\u201d in ICSE, 2024.\n[259] H. Su, S. Jiang, Y . Lai etal., \u201cArks: Active retrieval in knowledge soup\nfor code generation,\u201d arXiv:2402.12317, 2024.\n[260] N. Beau and B. Crabb \u00b4e, \u201cThe impact of lexical and grammatical pro-\ncessing on generating code from natural language,\u201d in ACL Findings,\n2022.\n[261] K. Zhang, G. Li, J. Li etal., \u201cToolcoder: Teach code generation models\nto use API search tools,\u201d arXiv:2305.04032, 2023.\n[262] S. Liu, Y . Chen, X. Xie etal., \u201cRetrieval-augmented generation for\ncode summarization via hybrid GNN,\u201d in ICLR, 2021.\n[263] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck, \u201cModeling and\ndiscovering vulnerabilities with code property graphs,\u201d in S&P, 2014.\n[264] Y . Choi, C. Na etal., \u201cReadsum: Retrieval-augmented adaptive trans-\nformer for source code summarization,\u201d IEEE Access, 2023.\n[265] A. Alokla, W. Gad, W. Nazih etal., \u201cRetrieval-based transformer\npseudocode generation,\u201d Mathematics, vol. 10, no. 4, p. 604, 2022.\n[266] J. Zhao, X. Chen, G. Yang, and Y . Shen, \u201cAutomatic smart contract\ncomment generation via large language models and in-context learn-\ning,\u201d IST, vol. 168, p. 107405, 2024.\n[267] J. Xu, Z. Cui etal., \u201cUnilog: Automatic logging via LLM and in-\ncontext learning,\u201d in ICSE, 2024.\n[268] H. Wang, X. Xia etal., \u201cContext-aware retrieval-based deep commit\nmessage generation,\u201d TOSEM, vol. 30, no. 4, pp. 56:1\u201356:30, 2021.\n[269] X. Zhu, C. Sha, and J. Niu, \u201cA simple retrieval-based method for code\ncomment generation,\u201d in SANER, 2022.", "start_char_idx": 160719, "end_char_idx": 166645, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf": {"__data__": {"id_": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}, {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, {"node_id": "b2b79e9f-0df5-46f7-b817-a49c288a6411", "node_type": "1", "metadata": {}, "hash": "1c7c217317296557c05da625f77cc0c9529a58c4230c59a18a99f88ff08fe221", "class_name": "RelatedNodeInfo"}]}, "text": "[270] T. Ye, L. Wu, T. Ma etal., \u201cTram: A token-level retrieval-augmented\nmechanism for source code summarization,\u201d arXiv:2305.11074, 2023.\n[271] L. Li, B. Liang, L. Chen, and X. Zhang, \u201cCross-modal retrieval-\nenhanced code summarization based on joint learning for retrieval and\ngeneration,\u201d Available atSSRN 4724884.\n[272] D. Drain, C. Hu, C. Wu etal., \u201cGenerating code with the\nhelp of retrieved template functions and stack overflow answers,\u201d\narXiv:2104.05310, 2021.\n[273] S. Lu, D. Guo etal., \u201cCodexglue: A machine learning benchmark\ndataset for code understanding and generation,\u201d in NeurIPS Datasets\nandBenchmarks, 2021.\n[274] Y . Ding, Z. Wang etal., \u201cCocomic: Code completion by jointly\nmodeling in-file and cross-file context,\u201d arXiv:2212.10007, 2022.\n[275] D. Shrivastava, D. Kocetkov etal., \u201cRepofusion: Training code models\nto understand your repository,\u201d arXiv:2306.10998, 2023.\n[276] Z. Tang, J. Ge, S. Liu etal., \u201cDomain adaptive code completion via\nlanguage models and decoupled domain databases,\u201d in ASE, 2023.\n[277] W. Sun, H. Li, M. Yan etal., \u201cRevisiting and improving retrieval-\naugmented deep assertion generation,\u201d in ASE, 2023.\n[278] A. Eghbali and M. Pradel, \u201cDe-hallucinator: Iterative grounding for\nllm-based code completion,\u201d arXiv:2401.01701, 2024.\n[279] M. Liang, X. Xie, G. Zhang etal., \u201cRepofuse: Repository-level code\ncompletion with fused dual context,\u201d arXiv:2402.14323, 2024.\n[280] Y . Tsai, M. Liu, and H. Ren, \u201cRtlfixer: Automatically fixing RTL syntax\nerrors with large language models,\u201d arXiv:2311.16543, 2023.\n[281] B. Bogin, S. Gupta, P. Clark etal., \u201cLeveraging code to improve in-\ncontext learning for semantic parsing,\u201d arXiv:2311.09519, 2023.\n[282] H. Li, J. Zhang, C. Li, and H. Chen, \u201cResdsql: Decoupling schema\nlinking and skeleton parsing for text-to-sql,\u201d in AAAI, 2023.\n[283] K. Zhang, X. Lin, Y . Wang etal., \u201cRefsql: A retrieval-augmentation\nframework for text-to-sql generation,\u201d in EMNLP Findings, 2023.\n[284] S. Chang and E. Fosler-Lussier, \u201cSelective demonstrations for cross-\ndomain text-to-sql,\u201d arXiv:2310.06302, 2023.\n[285] L. Nan, Y . Zhao, W. Zou etal., \u201cEnhancing text-to-sql capabilities\nof large language models: A study on prompt design strategies,\u201d in\nEMNLP Findings, 2023.27\n[286] X. Zhang, D. Wang, L. Dou etal., \u201cMulti-hop table retrieval for open-\ndomain text-to-sql,\u201d arXiv:2402.10666, 2024.\n[287] H. Li, J. Zhang, H. Liu etal., \u201cCodes: Towards building open-source\nlanguage models for text-to-sql,\u201d arXiv:2402.16347, 2024.\n[288] Z. Jie and W. Lu, \u201cLeveraging training data in few-shot prompting for\nnumerical reasoning,\u201d arXiv:2305.18170, 2023.\n[289] M. Gao, J. Li, H. Fei etal., \u201cDe-fine: Decomposing and refining visual\nprograms with auto-feedback,\u201d arXiv:2311.12890, 2023.\n[290] Y . Hao, W. Chen, Z. Zhou, and W. Cui, \u201cE&v: Prompting large\nlanguage models to perform static analysis by pseudo-code execution\nand verification,\u201d arXiv:2312.08477, 2023.\n[291] Y . Guo, Z. Li etal., \u201cRetrieval-augmented code generation for universal\ninformation extraction,\u201d arXiv:2311.02962, 2023.\n[292] G. Pinto, C. de Souza etal., \u201cLessons from building stackspot ai: A\ncontextualized ai coding assistant,\u201d arXiv:2311.18450, 2024.\n[293] Z. Liu, C. Chen, J. Wang etal., \u201cTesting the limits: Unusual text inputs\ngeneration for mobile app crash detection with large language model,\u201d\narXiv:2310.15657, 2023.\n[294] K. D. Bollacker, C. Evans etal., \u201cFreebase: a collaboratively created\ngraph database for structuring human knowledge,\u201d in SIGMOD, 2008.\n[295] Y . Shu and Z. Yu, \u201cData distribution bottlenecks in grounding language\nmodels to knowledge bases,\u201d arXiv:2309.08345, 2023.\n[296] D. Leake and D. J. Crandall, \u201cOn bringing case-based reasoning\nmethodology to deep learning,\u201d in ICCBR, 2020.\n[297] L. Zhang, J. Zhang etal., \u201cFC-KBQA: A fine-to-coarse composition\nframework for knowledge base question answering,\u201d in ACL, 2023.\n[298] J. Jiang, K. Zhou etal., \u201cStructgpt: A general framework for large\nlanguage model to reason over structured data,\u201d in EMNLP, 2023.\n[299] J. Baek, A. F. Aji, and A. Saffari, \u201cKnowledge-augmented language\nmodel prompting for zero-shot knowledge graph question answering,\u201d\narXiv:2306.04136, 2023.\n[300] P. Sen, S. Mavadia, and A. Saffari, \u201cKnowledge graph-augmented\nlanguage models for complex question answering,\u201d in NLRSE, 2023.\n[301] Y . Wu, N. Hu, S. Bi etal., \u201cRetrieve-rewrite-answer: A kg-to-text\nenhanced llms framework for knowledge graph question answering,\u201d\narXiv:2309.11206, 2023.\n[302] C. Wang, Y . Xu, Z. Peng etal., \u201ckeqing: knowledge-based ques-\ntion answering is a nature chain-of-thought mentor of LLM,\u201d\narXiv:2401.00426, 2024.\n[303] J. Liu, S. Cao, J. Shi etal., \u201cProbing structured semantics under-\nstanding and generation of language models via question answering,\u201d\narXiv:2401.05777, 2024.\n[304] G. Xiong, J. Bao, and W. Zhao, \u201cInteractive-kbqa: Multi-turn inter-\nactions for knowledge base question answering with large language\nmodels,\u201d arXiv:2402.15131, 2024.\n[305] S. Chen, Q. Liu, Z. Yu etal., \u201cRetrack: A flexible and efficient\nframework for knowledge base question answering,\u201d in ACL, 2021.\n[306] D. Yu, C. Zhu, Y . Fang etal., \u201cKg-fid: Infusing knowledge graph in\nfusion-in-decoder for open-domain question answering,\u201d in ACL, 2022.\n[307] Z. Hu, Y . Xu, W. Yu etal., \u201cEmpowering language models with\nknowledge graph reasoning for open-domain question answering,\u201d in\nEMNLP, 2022.\n[308] M. Ju, W. Yu, T. Zhao etal., \u201cGrape: Knowledge graph enhanced\npassage reader for open-domain question answering,\u201d in EMNLP\nFindings, 2022.\n[309] Q. Yang, Q. Chen, W. Wang etal., \u201cEnhancing multi-modal multi-\nhop question answering via structured knowledge and unified retrieval-\ngeneration,\u201d in MM, 2023.\n[310] W. Zhao, Y . Liu, T. Niu etal., \u201cDIVKNOWQA: assessing the reasoning\nability of llms via open-domain question answering over knowledge\nbase and text,\u201d arXiv:2310.20170, 2023.\n[311] X. Wang, Q. Yang, Y . Qiu etal., \u201cKnowledgpt: Enhancing large\nlanguage models with retrieval and storage access on knowledge bases,\u201d\narXiv:2308.11761, 2023.", "start_char_idx": 166646, "end_char_idx": 172699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d9297b7-b236-40fe-8cef-bb5cfd5f196f": {"__data__": {"id_": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "node_type": "1", "metadata": {}, "hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}, {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, {"node_id": "9278baa9-0cba-447f-b50c-77148e46b013", "node_type": "1", "metadata": {}, "hash": "29be3e881568d726fbf2459b3f5c8063aaf55426eea9a659531cae58f1f57c27", "class_name": "RelatedNodeInfo"}]}, "text": "[312] S. Ko, H. Cho, H. Chae etal., \u201cEvidence-focused fact summa-\nrization for knowledge-augmented zero-shot question answering,\u201d\narXiv:2403.02966, 2024.\n[313] Y . Gao, L. Qiao, Z. Kan etal., \u201cTwo-stage generative question\nanswering on temporal knowledge graph using large language models,\u201d\narXiv:2402.16568, 2024.\n[314] T. Guo, Q. Yang, C. Wang etal., \u201cKnowledgenavigator: Leveraging\nlarge language models for enhanced reasoning over knowledge graph,\u201d\narXiv:2312.15880, 2023.\n[315] S. Min, J. Boyd-Graber, C. Alberti etal., \u201cNeurips 2020 efficientqa\ncompetition: Systems, analyses and lessons learned,\u201d in NeurIPS 2020\nCompetition andDemonstration Track, 2021.[316] A. H. Li, P. Ng, P. Xu etal., \u201cDual reader-parser on hybrid tex-\ntual and tabular evidence for open domain question answering,\u201d in\nACL/IJCNLP, 2021.\n[317] P. Christmann, R. S. Roy, and G. Weikum, \u201cConversational question\nanswering on heterogeneous sources,\u201d in SIGIR, 2022.\n[318] K. Ma, H. Cheng, X. Liu etal., \u201cOpen-domain question answering\nvia chain of reasoning over heterogeneous knowledge,\u201d in EMNLP\nFindings, 2022.\n[319] E. Park, S.-M. Lee etal., \u201cRink: reader-inherited evidence reranker for\ntable-and-text open domain question answering,\u201d in AAAI, 2023.\n[320] W. Zhao, Y . Liu, Y . Wan etal., \u201cLocalize, retrieve and fuse: A\ngeneralized framework for free-form question answering over tables,\u201d\narXiv:2309.11049, 2023.\n[321] F. Pan, M. Canim etal., \u201cEnd-to-end table question answering via\nretrieval-augmented generation,\u201d arXiv:2203.16714, 2022.\n[322] Z. Jiang, Y . Mao, P. He etal., \u201cOmnitab: Pretraining with natural\nand synthetic data for few-shot table-based question answering,\u201d in\nNAACL, 2022.\n[323] W. Zhong, J. Huang, Q. Liu etal., \u201cReasoning over hybrid chain for\ntable-and-text open domain question answering,\u201d in IJCAI, 2022.\n[324] A. S. Sundar and L. Heck, \u201cctbl: Augmenting large language models\nfor conversational tables,\u201d arXiv:2303.12024, 2023.\n[325] D. Min, N. Hu, R. Jin etal., \u201cExploring the impact of table-to-text\nmethods on augmenting llm-based question answering with domain\nhybrid data,\u201d arXiv:2402.12869, 2024.\n[326] S. Wu, Y . Li, D. Zhang, and Z. Wu, \u201cImproving knowledge-aware\ndialogue response generation by using human-written prototype dia-\nlogues,\u201d in EMNLP Findings, 2020.\n[327] M. Kang, J. M. Kwak etal., \u201cKnowledge-consistent dialogue genera-\ntion with knowledge graphs,\u201d in ICML Workshop, 2022.\n[328] Z. Ji, Z. Liu, N. Lee etal., \u201cRHO: reducing hallucination in open-\ndomain dialogues with knowledge grounding,\u201d in ACL Findings, 2023.\n[329] J. Baek, N. Chandrasekaran, S. Cucerzan etal., \u201cKnowledge-\naugmented large language models for personalized contextual query\nsuggestion,\u201d arXiv:2311.06318, 2023.\n[330] X. He, Y . Tian, Y . Sun etal., \u201cG-retriever: Retrieval-augmented\ngeneration for textual graph understanding and question answering,\u201d\narXiv:2402.07630, 2024.\n[331] Y . Kirstain, O. Levy, and A. Polyak, \u201cX&fuse: Fusing visual informa-\ntion in text-to-image generation,\u201d arXiv:2303.01000, 2023.\n[332] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d NeurIPS, 2021.\n[333] Z. Zhang, A. Zhang, M. Li etal., \u201cMultimodal chain-of-thought\nreasoning in language models,\u201d arXiv:2302.00923, 2023.\n[334] C. Xu, M. Yang, X. Ao etal., \u201cRetrieval-enhanced adversarial train-\ning with dynamic memory-augmented attention for image paragraph\ncaptioning,\u201d Knowledge-Based Systems, vol. 214, p. 106730, 2021.\n[335] R. Ramos, D. Elliott, and B. Martins, \u201cRetrieval-augmented image\ncaptioning,\u201d in EACL, 2023.\n[336] Z. Hu, A. Iscen, C. Sun etal., \u201cReveal: Retrieval-augmented visual-\nlanguage pre-training with multi-source multimodal knowledge mem-\nory,\u201d in CVPR, 2023.\n[337] Z. Li, W. Zhao, X. Du etal., \u201cCross-modal retrieval and semantic\nrefinement for remote sensing image captioning,\u201d Remote Sensing,\nvol. 16, no. 1, p. 196, 2024.\n[338] Z. Yang, Z. Gan, J. Wang etal., \u201cAn empirical study of gpt-3 for\nfew-shot knowledge-based vqa,\u201d in AAAI, 2022.\n[339] W. Lin and B. Byrne, \u201cRetrieval augmented visual question answering\nwith outside knowledge,\u201d in EMNLP, 2022.\n[340] A. Fan, C. Gardent, C. Braud, and A. Bordes, \u201cAugmenting transform-\ners with knn-based composite memory for dialog,\u201d TACL, vol. 9, pp.\n82\u201399, 2021.\n[341] Z. Liang, H. Hu, C. Xu etal., \u201cMaria: A visual experience powered\nconversational agent,\u201d in ACL-IJCNLP, 2021.\n[342] Q. Fang and Y . Feng, \u201cNeural machine translation with phrase-level\nuniversal visual representations,\u201d in ACL, 2022.\n[343] S. Whitehead, H. Ji, M. Bansal etal., \u201cIncorporating background\nknowledge into video description generation,\u201d in EMNLP, 2018.\n[344] C. Yin, J. Tang, Z. Xu, and Y . Wang, \u201cMemory augmented deep\nrecurrent neural network for video question answering,\u201d TNNLS,\nvol. 31, no. 9, pp. 3159\u20133167, 2019.\n[345] J. Pan, Z. Lin, Y . Ge etal., \u201cRetrieving-to-answer: Zero-shot video\nquestion answering with frozen large language models,\u201d in ICCV, 2023.\n[346] J. Lei, L. Yu, T. L. Berg, and M. Bansal, \u201cTvqa+: Spatio-temporal\ngrounding for video question answering,\u201d in ACL, 2020.\n[347] H. Le, N. Chen, and S. Hoi, \u201cVgnmn: Video-grounded neural module\nnetworks for video-grounded dialogue systems,\u201d in NAACL, 2022.28\n[348] Z. Wang, M. Li, R. Xu etal., \u201cLanguage models with image descriptors\nare strong few-shot video-language learners,\u201d in NeurIPS, 2022.\n[349] J. Yuan, S. Sun, D. Omeiza etal., \u201cRag-driver: Generalisable driving\nexplanations with retrieval-augmented in-context learning in multi-\nmodal large language model,\u201d arXiv:2402.10828, 2024.\n[350] S. Ghosh, S. Kumar, C. K. R. Evuru etal., \u201cRecap: retrieval-augmented\naudio captioning,\u201d in ICASSP, 2024.\n[351] B. Elizalde, S. Deshmukh, and H. Wang, \u201cNatural language supervision\nfor general-purpose audio representations,\u201d in ICASSP, 2024.\n[352] T. Kouzelis and V . Katsouros, \u201cWeakly-supervised automated audio\ncaptioning via text only training,\u201d in DCASE Workshop, 2023.\n[353] S. Deshmukh, B. Elizalde, D. Emmanouilidou etal., \u201cTraining audio\ncaptioning models without audio,\u201d in ICASSP, 2024.\n[354] L. Yang, Z. Huang, X. Zhou etal., \u201cPrompt-based 3d molecular\ndiffusion models for structure-based drug design,\u201d 2023.", "start_char_idx": 172700, "end_char_idx": 178857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c03adbde-2467-4504-9b19-72d0e86c8cd5": {"__data__": {"id_": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59", "node_type": "4", "metadata": {}, "hash": "fa3c086f3d76d63deb327b67264f44e973ade78846bab687d724a690deeee674", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}, {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, {"node_id": "ea3280e2-9061-4d36-86ab-2bc4d70fce08", "node_type": "1", "metadata": {}, "hash": "d84a4fa0aa5d5dfaf3467309165bb22c19101c2ed6dec85cb948cdc96c82bf34", "class_name": "RelatedNodeInfo"}]}, "text": "[355] T. Truong Jr and T. Bepler, \u201cPoet: A generative model of protein\nfamilies as sequences-of-sequences,\u201d NeurIPS, 2024.\n[356] G. Frisoni, M. Mizutani, G. Moro, and L. Valgimigli, \u201cBioreader: a\nretrieval-enhanced text-to-text transformer for biomedical literature,\u201d\ninEMNLP, 2022.\n[357] X. Yang, M. Ye, Q. You etal., \u201cWriting by memorizing: Hierarchical\nretrieval-based medical report generation,\u201d arXiv:2106.06471, 2021.\n[358] J. Kim and M. Min, \u201cFrom rag to qa-rag: Integrating generative ai\nfor pharmaceutical regulatory compliance process,\u201d arXiv:2402.01717,\n2024.\n[359] K. Yang, A. Swope etal., \u201cLeandojo: Theorem proving with retrieval-\naugmented language models,\u201d in NeurIPS, 2024.\n[360] Z. Levonian, C. Li, W. Zhu etal., \u201cRetrieval-augmented generation to\nimprove math question-answering: Trade-offs between groundedness\nand human preference,\u201d arXiv:2310.03184, 2023.\n[361] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large language\nmodels in retrieval-augmented generation,\u201d arxiv:2309.01431, 2023.\n[362] S. ES, J. James, L. E. Anke, and S. Schockaert, \u201cRAGAS: automated\nevaluation of retrieval augmented generation,\u201d arxiv:2309.15217, 2023.\n[363] J. Saad-Falcon, O. Khattab, C. Potts etal., \u201cARES: an automated\nevaluation framework for retrieval-augmented generation systems,\u201d\narxiv:2311.09476, 2023.\n[364] https://github.com/truera/trulens.\n[365] Y . Lyu, Z. Li, S. Niu etal., \u201cCRUD-RAG: A comprehensive chinese\nbenchmark for retrieval-augmented generation of large language mod-\nels,\u201d arxiv:2401.17043, 2024.\n[366] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, \u201cBenchmarking retrieval-\naugmented generation for medicine,\u201d arXiv:2402.13178, 2024.\n[367] F. Petroni, A. Piktus etal., \u201cKilt: a benchmark for knowledge intensive\nlanguage tasks,\u201d in NAACL-HLT, 2021.\n[368] S. Barnett, S. Kurniawan, S. Thudumu etal., \u201cSeven failure\npoints when engineering a retrieval augmented generation system,\u201d\narXiv:2401.05856, 2024.\n[369] F. Cuconasu, G. Trappolini, F. Siciliano etal., \u201cThe power of noise:\nRedefining retrieval for RAG systems,\u201d arXiv:2401.14887, 2024.\n[370] L. Qiu, P. Shaw, P. Pasupat etal., \u201cEvaluating the impact of\nmodel scale for compositional generalization in semantic parsing,\u201d\narXiv:2205.12253, 2022.\n[371] R. Jagerman, H. Zhuang, Z. Qin etal., \u201cQuery expansion by prompting\nlarge language models,\u201d arxiv:2305.03653, 2023.\n[372] H. Zhang, P. Zhao, X. Miao etal., \u201cExperimental analysis of large-scale\nlearnable vector storage compression,\u201d VLDB, 2023.\n[373] R. Aksitov, C. Chang, D. Reitter etal., \u201cCharacterizing attribution\nand fluency tradeoffs for retrieval-augmented large language models,\u201d\narXiv:2302.05578, 2023.\n[374] C. Han, Q. Wang, W. Xiong etal., \u201cLm-infinite: Simple on-the-fly\nlength generalization for large language models,\u201d arXiv:2308.16137,\n2023.\n[375] H. Chase, \u201cLangchain,\u201d https://github.com/langchain-ai/langchain,\n2022.\n[376] W. Jiang, S. Zhang, B. Han etal., \u201cPiperag: Fast retrieval-augmented\ngeneration via algorithm-system co-design,\u201d arXiv:2403.05676, 2024.\n[377] S. Jindal, \u201cDid google gemini 1.5 really kill rag?\u201d https://\nanalyticsindiamag.com/did-google-gemini-1-5-really-kill-rag/, 2024.", "start_char_idx": 178858, "end_char_idx": 182003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2587bb96-4640-4265-9076-9e1d531a2381": {"__data__": {"id_": "2587bb96-4640-4265-9076-9e1d531a2381", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "af53148f-7cee-4843-ad16-bf3bb0321242", "node_type": "1", "metadata": {}, "hash": "866f7c66be8396524be9584d90ae3a7613dce39989d2655a236c128d947807bf", "class_name": "RelatedNodeInfo"}, {"node_id": "8d19f54d-598e-4495-8f5b-08c18d8e9da1", "node_type": "1", "metadata": {}, "hash": "347331191612ed06eab5a9c310a913709ba4d9575417ddcf11d6b55441e9ac26", "class_name": "RelatedNodeInfo"}, {"node_id": "e33ebd69-c0b6-48d0-a472-cc8f6eeba54b", "node_type": "1", "metadata": {}, "hash": "90a8a0c5263affd2d6674b130a9634e7f2b7716d1e334f4f70270efee4b263f6", "class_name": "RelatedNodeInfo"}, {"node_id": "ea666fa4-20be-459b-881c-fb0a89ce9d54", "node_type": "1", "metadata": {}, "hash": "0f634c2ab09acf1f889da7c595adf8f12a4f7a1b247c26988828da1984de4d11", "class_name": "RelatedNodeInfo"}, {"node_id": "e04f5524-c742-40cc-a345-c5fec7d32d50", "node_type": "1", "metadata": {}, "hash": "379cd72cb5a5165ce5f6728716016e1b26e865949f5fe62c125577aee89ab89a", "class_name": "RelatedNodeInfo"}]}, "text": "1\nRetrieval-Augmented Generation for\nAI-Generated Content: A Survey\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\nFangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui\nAbstract \u2014Advancements in model algorithms, the growth of\nfoundational models, and access to high-quality datasets have\npropelled the evolution of Artificial Intelligence Generated Con-\ntent (AIGC). Despite its notable successes, AIGC still faces\nhurdles such as updating knowledge, handling long-tail data,\nmitigating data leakage, and managing high training and infer-\nence costs. Retrieval-Augmented Generation (RAG) has recently\nemerged as a paradigm to address such challenges. In partic-\nular, RAG introduces the information retrieval process, which\nenhances the generation process by retrieving relevant objects\nfrom available data stores, leading to higher accuracy and better\nrobustness. In this paper, we comprehensively review existing\nefforts that integrate RAG technique into AIGC scenarios. We\nfirst classify RAG foundations according to how the retriever\naugments the generator, distilling the fundamental abstrac-\ntions of the augmentation methodologies for various retrievers\nand generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies\nthat help with potential future progress. We also summarize\nadditional enhancements methods for RAG, facilitating effective\nengineering and implementation of RAG systems. Then from\nanother view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for\nresearchers and practitioners. Furthermore, we introduce the\nbenchmarks for RAG, discuss the limitations of current RAG\nsystems, and suggest potential directions for future research.\nGithub: https://github.com/PKU-DAIR/RAG-Survey.\nIndex Terms \u2014Retrieval-augmented generation, AI-generated\ncontent, generative models, information retrieval.\nI.INTRODUCTION\nA.Background\nRECENT years have witnessed the surge in interests\nsurrounding Artificial Intelligence Generated Content\n(AIGC).", "start_char_idx": 0, "end_char_idx": 2100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45966987-f98a-421f-9271-5c503d67344d": {"__data__": {"id_": "45966987-f98a-421f-9271-5c503d67344d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "af6b7a8d-77d9-4a37-8955-6eb0c86e7669", "node_type": "1", "metadata": {}, "hash": "bf0ae4fab0c3be5444915814833542f3ed98f2b32f86b28f76ebf583d1e26c65", "class_name": "RelatedNodeInfo"}, {"node_id": "dd22f499-9d78-4f2d-ad79-a53ad1f46361", "node_type": "1", "metadata": {}, "hash": "da3d7e3d3284975ddf529101a852570f368f18f6e85446545f1e3e7188dd6f02", "class_name": "RelatedNodeInfo"}, {"node_id": "2fbcab7a-aeb1-4153-ab79-2b54badcec49", "node_type": "1", "metadata": {}, "hash": "cfc8a6aa8e58551d569f4405bd319a5080a341da2c636413b7818cc8228c0428", "class_name": "RelatedNodeInfo"}, {"node_id": "33c2d5cf-ea46-493f-a3e7-c702f5cc33af", "node_type": "1", "metadata": {}, "hash": "08a1bde488e1f091cdc30800f1ed032404445623c5e324821944130ca8ebe2dd", "class_name": "RelatedNodeInfo"}, {"node_id": "e89ba5c4-3f8d-45e0-b6cc-e385486d7eff", "node_type": "1", "metadata": {}, "hash": "e16c5fa766882a4cebdb6bd5466a6834e7a19828da5649095b05c17b1ebca04e", "class_name": "RelatedNodeInfo"}]}, "text": "Various content generation tools have been metic-\nulously crafted to produce diverse outputs across various\nmodalities, such as Large Language Models (LLMs) including\nthe GPT series [1]\u2013[3] and the LLAMA series [4]\u2013[6] for\ntexts and codes, DALL-E [7]\u2013[9] and Stable Diffusion [10]\nfor images, and Sora [11] for videos. The word \u201cAIGC\u201d\nemphasizes that the contents are produced by advanced gen-\nerative models other than human beings or rule-based ap-\nproaches. These generative models have achieved remarkable\nperformance due to the utilization of novel model algorithms,\n\u2022Penghao Zhao and Hailin Zhang contributed equally to this paper.\n\u2022Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng\nGeng, Fangcheng Fu, Ling Yang, Wentao Zhang and Bin Cui are with\nPeking University (e-mail: penghao.zhao@stu.pku.edu.cn, z.hl@pku.edu.cn,\nyuqinhan@stu.pku.edu.cn, wzr@stu.pku.edu.cn, 1800012997@pku.edu.cn,\nccchengff@pku.edu.cn, yangling0818@163.com, wentao.zhang@pku.edu.cn,\nbin.cui@pku.edu.cn).\n\u2022Jie Jiang is with Tencent Inc. (email: zeus@tencent.com)\n\u2022Bin Cui is Corresponding Author.explosive scale of foundation models, and massive high-\nquality datasets. Specifically, sequence-to-sequence tasks have\ntransitioned from utilizing Long Short-Term Memory (LSTM)\nnetworks [12] to Transformer-based models [13], and image-\ngeneration tasks have shifted from Generative Adversarial Net-\nworks (GANs) [14] to Latent Diffusion Models (LDMs) [10]\nas well. Notably, the architecture of foundation models, ini-\ntially constituted by millions of parameters [15], [16], has now\ngrown to billions or even trillions of parameters [1], [4], [17].\nThese advancements are further bolstered by the availability\nof rich, high-quality datasets [1], [18], which provide ample\ntraining samples to fully optimize model parameters.\nInformation retrieval is another pivotal application within\nthe field of computer science.", "start_char_idx": 2101, "end_char_idx": 4005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08040b3d-2224-4c75-a034-cfb9aa9f9b61": {"__data__": {"id_": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2ac6f711-0a4c-4e61-907b-5adb6944be21", "node_type": "1", "metadata": {}, "hash": "3464011a2cb4059692c14d8527315f1dba17774d87c447fc33b773f28cd3c392", "class_name": "RelatedNodeInfo"}, {"node_id": "5639ad72-ec31-424b-982e-a75e3975ca22", "node_type": "1", "metadata": {}, "hash": "b353c1d2f59611a400cebe1c774a7e9d5cb5339cae16b0b8a24d953a59d50008", "class_name": "RelatedNodeInfo"}, {"node_id": "988d88de-c8fb-44a0-bfcc-d9d9ebb6f2ea", "node_type": "1", "metadata": {}, "hash": "a328839604cca5ee1243766b1505cfdc95b7fa143f41a7f80698ba25c8198164", "class_name": "RelatedNodeInfo"}, {"node_id": "3db498c9-6714-4117-8544-f0cbc7c7ba24", "node_type": "1", "metadata": {}, "hash": "59d8d14f952e64cd64764abf93c5e33633a4ffb20c4b4dec0bdb45a6fb0d6491", "class_name": "RelatedNodeInfo"}, {"node_id": "a9c019c1-3346-44e1-89aa-8b9ce920f21c", "node_type": "1", "metadata": {}, "hash": "be7e5b2089d0ff6c9291ceab1e3fd6412df95bea66674a6d6630e8d40af875ea", "class_name": "RelatedNodeInfo"}, {"node_id": "2b2e91e1-6e16-4f33-9934-a2862d8270d6", "node_type": "1", "metadata": {}, "hash": "ddca79e02d5b90a77e6f8aa43a7fe26217b9450e1678ea7ec37299f667dc2219", "class_name": "RelatedNodeInfo"}]}, "text": "Information retrieval is another pivotal application within\nthe field of computer science. Different from generation,\nretrieval aims to locate relevant existing objects from a vast\npool of resources. The most prevalent application of retrieval\nlies in web search engines, which primarily focus on the task\nof document retrieval [19], [20]. In the present era, efficient\ninformation retrieval systems can handle document collections\non the order of billions [21], [22]. Besides documents, retrieval\nhas also been applied for many other modalities [23]\u2013[26].\nDespite the remarkable progress made by advanced gen-\nerative models, AIGC continues to face a number of well-\nknown challenges, including the struggle to maintain up-to-\ndate knowledge, the inability to incorporate long-tail knowl-\nedge [27], and the risk of leaking private training data [28].\nRetrieval-Augmented Generation (RAG) is proposed to allevi-\nate, if not completely address, the aforementioned challenges\nthrough its adaptable data repository [29]. The knowledge\nstored for retrieval can be conceptualized as non-parametric\nmemory, which is easily modifiable, capable of accommo-\ndating broad long-tail knowledge, and also able to encode\nconfidential data. In addition, retrieval can also be employed\nto reduce the generation costs. For example, RAG can reduce\nthe size of large generative models [30], provide support for\nlong contexts [31], and eliminate certain generation steps [32].\nA typical RAG process is depicted in Fig. 1. Given an\ninput query, the retriever locates and looks up relevant data\nsources, then the retrieved results interact with the generator\nto enhance the overall generation process. There are sev-\neralfoundational paradigms (foundations in short) according\nto how the retrieved results augment the generation: they\ncan serve as augmented input to the generator [33], [34];\nthey can join at the middle stage of generation as latent\nrepresentations [35], [36]; they can contribute to the final\ngeneration results in the form of logits [37], [38]; they can\neven influence or omit certain generation steps [32], [39].arXiv:2402.19473v3  [cs.CV]  14 Apr 20242\nFig. 1: A generic RAG architecture.", "start_char_idx": 3915, "end_char_idx": 6104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e438b00-2436-4718-b18e-6835511b6962": {"__data__": {"id_": "6e438b00-2436-4718-b18e-6835511b6962", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7399abd-cb1b-48f9-886a-8746f5f1e173", "node_type": "1", "metadata": {}, "hash": "0f63f2d3865eb73bcbe7d9640eb81ce9aaf264d8fc768e2598f9366304350af2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "461b359e-1bb5-4912-bc7d-3a73a3c00b25", "node_type": "1", "metadata": {}, "hash": "8945a6cd35fd64a02d7491742402b0fd31272bc02f9c239e754cb33f9ad4f567", "class_name": "RelatedNodeInfo"}, {"node_id": "2fde5e6b-04c9-4ac1-bf71-09a1c76b804b", "node_type": "1", "metadata": {}, "hash": "4603ff28f1c0e33013749ce1b15466d652003b6b49ecf2cc15839cf8bd130de7", "class_name": "RelatedNodeInfo"}, {"node_id": "32a36560-4b02-4f0a-a52e-a1ee6003e14b", "node_type": "1", "metadata": {}, "hash": "a148dc182352bbfd29ea06de2d176c5da69665699191df60b55dddc94330b211", "class_name": "RelatedNodeInfo"}, {"node_id": "f3be1b9c-cdfc-424a-b473-b1897295dd57", "node_type": "1", "metadata": {}, "hash": "176a10f956a1fb498b8fc006f5f597d844c43bc9e2c56c9736c6b7352de7e847", "class_name": "RelatedNodeInfo"}, {"node_id": "30557630-9b86-470e-a715-5603ee609c82", "node_type": "1", "metadata": {}, "hash": "73f3d26a92a23a10a934a1ae6f3813f014cafffe9d0e40b91745e5fab450a52c", "class_name": "RelatedNodeInfo"}]}, "text": "1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and\nthe generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results\nand ultimately produces outcomes of various modalities.\nMoreover, beyond the foundational RAG process, researchers\nhave proposed numerous enhancements to elevate the overall\nquality. These methods encompass specific optimizations for\nindividual components as well as holistic enhancements aimed\nat the entire pipeline.\nIn addition, while the concept of RAG initially emerged\nin text-to-text generation [34], this technique has also found\napplications across various domains, including codes [40]\u2013\n[42], audios [43], [44], images [45]\u2013[47], videos [48], [49],\n3D [50], [51], knowledge [52]\u2013[54], and AI for science [55],\n[56]. In particular, the essential idea and process of RAG are\nlargely consistent across modalities. However, it necessitates\nminor adjustments in augmentation techniques, and the se-\nlection of retrievers and generators varies depending on the\nspecific modalities and applications.\nDespite the rapid growth in recent research on RAG and\nthe booming applications, a systematic review encompassing\nall foundations, enhancements, and applications is notably\nabsent, hindering the development of this field. For one thing,\nthe absence of discussion on RAG foundations significantly\nundermines the practical value of the research in this do-\nmain, leaving the potential of RAG not fully explored. While\nthe majority of research interest, particularly among LLM\nresearchers, centers on query-based RAG in text-generation\ntasks, it is essential to acknowledge that other RAG foun-\ndations are also effective and with significant potential for\nusage and further development. For another, the lack of an\noverview on RAG applications causes researchers and practi-\ntioners to overlook RAG\u2019s progress across multiple modalities\nand remain unaware of how RAG can be effectively applied.\nAlthough text generation is typically considered as the main\napplication of RAG, we emphasize that the development ofRAG in other modalities has also begun to catch on and\nhas yielded promising advancements. Certain modalities have\na rich historical connection to retrieval techniques, infusing\nRAG with distinctive characteristics.", "start_char_idx": 6074, "end_char_idx": 8452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7399abd-cb1b-48f9-886a-8746f5f1e173": {"__data__": {"id_": "f7399abd-cb1b-48f9-886a-8746f5f1e173", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "node_type": "1", "metadata": {}, "hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "09fba7a0-ea1b-4ddf-991c-663c646563c6", "node_type": "1", "metadata": {}, "hash": "0f63f2d3865eb73bcbe7d9640eb81ce9aaf264d8fc768e2598f9366304350af2", "class_name": "RelatedNodeInfo"}]}, "text": "Inspired by this, in this\npaper, our objective is to present a comprehensive survey to\nprovide a systematic overview of RAG.\nB.Contribution\nThis survey offers a comprehensive overview of RAG, cov-\nering foundations, enhancements, applications, benchmarks,\nlimitations, and potential future directions. While retrievers\nand generators exhibit variations across modalities and tasks,\nwe distill the fundamental abstractions of RAG foundations,\nconsidering applications as adaptations stemming from these\nabstractions.", "start_char_idx": 8453, "end_char_idx": 8968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc987eab-006b-480a-8593-97ad22ce1f35": {"__data__": {"id_": "cc987eab-006b-480a-8593-97ad22ce1f35", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9b298a12-d1e7-402e-ac06-abf397573cc9", "node_type": "1", "metadata": {}, "hash": "28500d7bb796994b26ac0e5962b36a6456ee7a2cfc38b53e38f9b53634161352", "class_name": "RelatedNodeInfo"}, {"node_id": "4b602e68-392e-4e7a-9923-1045a4070a95", "node_type": "1", "metadata": {}, "hash": "97823f7b067ccf61c9f5cdf75c41ff33bf257b5f41220aa95cbe18e4c2449ae5", "class_name": "RelatedNodeInfo"}, {"node_id": "11ccb175-7cb7-493c-9654-5ff59910f24d", "node_type": "1", "metadata": {}, "hash": "2fda599b9d1537e2d126793d666912a1dd6324a10d5df6c6c9ca1d2536546990", "class_name": "RelatedNodeInfo"}, {"node_id": "f750e47e-2259-45fc-a520-500bf6a0d0dc", "node_type": "1", "metadata": {}, "hash": "5e1886df3a599483dc5a2cce1e016a486444b6cae97ce671fe6d01f1fca65c92", "class_name": "RelatedNodeInfo"}, {"node_id": "8a702c1b-9836-4918-beb5-5562d850aa84", "node_type": "1", "metadata": {}, "hash": "748db795c9935cd715683a62c64256119c87f6332e6c9f65237935c1c9fd8f63", "class_name": "RelatedNodeInfo"}]}, "text": "We aim to offer references and guidelines to\nresearchers and practitioners, providing valuable insights for\nadvancing RAG methodologies and related applications. In\nsummary, we list our contributions as follows:\n\u2022We conduct a comprehensive review of RAG, and distill\nthe abstractions of RAG foundations for various retrievers\nand generators.\n\u2022We investigate the enhancements in the literature of\nRAG, elaborating the techniques leveraged to enable\nmore effective RAG systems.\n\u2022For various modalities and tasks, we survey existing\nAIGC methods that incorporate RAG techniques, exhibit-\ning how RAG contributes to current generative models.\n\u2022We discuss the limitations and promising research di-\nrections of RAG, shedding light on its potential future\ndevelopment.\nC.Related Work\nAs the field of RAG advances, several surveys have\nemerged; yet they address only specific facets of the area. In3\nparticular, they either exclusively focus on a single RAG foun-\ndation or provide only a brief overview of RAG augmentation\nmethodologies for limited scenarios.\nMost of the existing works focus on text-related RAG tasks\nthat are facilitated by LLMs, without in-depth investigation\nin other modalities. The survey by Li et al. [57] offers a\nbasic overview of RAG and discusses specific applications\nwithin the scope of text generation tasks. In a similar vein,\nthe tutorial crafted by Asai et al. [58] centers on retrieval-\nbased language models, detailing their structures and training\nstrategies. Meanwhile, a recent survey by Gao et al. [59]\nexplores RAG in the context of LLMs, with a particular\nemphasis on enhancement approaches for query-based RAG.\nRecognizing that RAG has extended beyond the text domain,\nour work broadens its reach to the entire AIGC landscape,\nfacilitating a more comprehensive coverage of RAG research.\nIn addition, another survey proposed by Zhao et al. [60]\nintroduces RAG applications across multiple modalities, but\nignoring the discussion on RAG foundations. While existing\nresearch has explored various aspects of RAG, there remains\na need for a comprehensive overview that covers RAG foun-\ndations, enhancements, and its applicability across different\ndomains. In this paper, we aim to address the gap by presenting\na systematic survey of RAG.", "start_char_idx": 0, "end_char_idx": 2271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1743ce2-002e-4688-b43d-d0294a9d9f23": {"__data__": {"id_": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "873991ab-7ccf-4af5-bb28-c4b73edd709a", "node_type": "1", "metadata": {}, "hash": "79deaa4bb2eb8ae20ce2a74ac4175e0643b01c33b404c18c04bf704ec62372fb", "class_name": "RelatedNodeInfo"}, {"node_id": "65a5d11a-33fc-43e0-9aa2-49305de58cfd", "node_type": "1", "metadata": {}, "hash": "d7c01fceb9f999eb6749330aace0566051bb4dea21bfeb60262e964fb0671e7a", "class_name": "RelatedNodeInfo"}, {"node_id": "566ee55e-c714-4b11-a525-4fca02f448fd", "node_type": "1", "metadata": {}, "hash": "cc6746ec2074e0f9c722d9fef2fa36b7a4e52d08bf176da8573157e39a5abdc6", "class_name": "RelatedNodeInfo"}, {"node_id": "7ff83fd5-cee9-4712-8271-478f9ddb56cc", "node_type": "1", "metadata": {}, "hash": "ea67e182e6425e4c8675a79ab0a108a3d47f0c3f5a7c90b389a77768fa147b0b", "class_name": "RelatedNodeInfo"}, {"node_id": "e2ad5fef-179a-446f-a02a-c82560506f03", "node_type": "1", "metadata": {}, "hash": "b215b851556d71c698c41fae930e384481cd27d2cdff11de5c1ed17f62fcc398", "class_name": "RelatedNodeInfo"}]}, "text": "In this paper, we aim to address the gap by presenting\na systematic survey of RAG.\nD.Roadmap\nThe rest of the paper is organized as follows. Section II elab-\norates on the preliminary of RAG, introducing retrievers and\ngenerators. Section III presents RAG foundations and further\nenhancements on RAG. Section IV reviews existing research\non RAG across various applications. Section V investigates the\nbenchmark frameworks for RAG. Section VI discusses current\nlimitations of RAG and potential future directions. Finally,\nSection VII concludes this paper.\nII. PRELIMINARY\nIn this section, we provide an overview of the general RAG\narchitecture and explore the generators and the retrievers in\ntoday\u2019s RAG-based AIGC.\nA.Overview\nAs shown in Fig. 1, the entire RAG system consists of\ntwo core modules: the retriever and the generator, where the\nretriever searches for relevant information from the data store\nand the generator produces the required contents. The RAG\nprocess unfolds as follows: (i) the retriever initially receives\nthe input query and searches for relevant information; (ii) then,\nthe original query and the retrieval results are fed into the\ngenerator through a specific augmentation methodology; (iii)\nfinally, the generator produces the desired outcomes.\nB.Generator\nThe remarkable performance of generative AI across di-\nverse tasks has ushered in the era of AIGC. The generation\nmodule plays a crucial role within the RAG system. Different\ngenerative models are applied for different scenarios, such\nas transformer models for text-to-text tasks, VisualGPT [61]\nfor image-to-text tasks, Stable Diffusion [10] for text-to-\nimage tasks, Codex [2] for text-to-code tasks, etc. Here we\nintroduce 4 typical generators that are frequently used in RAG:\ntransformer model, LSTM, diffusion model, and GAN.1)Transformer Model :Transformer models are one of the\nbest performing models in the field of Natural Language Pro-\ncessing (NLP), consisting of self-attention mechanisms, feed-\nforward networks, layer normalization modules, and residual\nnetworks [62]. As shown in Fig.", "start_char_idx": 2189, "end_char_idx": 4271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1": {"__data__": {"id_": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2f7fd9ca-0cf7-4fb7-9873-c9b592738bdc", "node_type": "1", "metadata": {}, "hash": "d09340f2882e335df5e06604099dbd05404f2ee214e76d4e22a4d1eda04b58d3", "class_name": "RelatedNodeInfo"}, {"node_id": "843191dc-4cee-4491-b706-4214fcde7797", "node_type": "1", "metadata": {}, "hash": "15c631742fc154993f28ddd810e0d9c996fe80de582c88fa6d3bb3356d7ab941", "class_name": "RelatedNodeInfo"}, {"node_id": "195f3a73-cdcf-4c70-a5b2-1c2cf4b85211", "node_type": "1", "metadata": {}, "hash": "ede1e45f85f765d5f9be10c8cabb9afc2a6cd703028f6fe71201a313e51a5516", "class_name": "RelatedNodeInfo"}, {"node_id": "5596ba26-93d2-4d1f-8f45-2a37cacbbd60", "node_type": "1", "metadata": {}, "hash": "4a9c2e8b557cdb642dd6c1f722d256d6b893715893101fc743604efe255c3498", "class_name": "RelatedNodeInfo"}, {"node_id": "e4d4f399-d234-4f87-a944-243f12a6b60b", "node_type": "1", "metadata": {}, "hash": "772cf8d71c580420436a1510accd393345c93d7c0ff6357695d358e5f2ebd70b", "class_name": "RelatedNodeInfo"}, {"node_id": "b3b0c525-5be2-4ddd-a29f-c9a87792531b", "node_type": "1", "metadata": {}, "hash": "df8dbbb4c5dae6a952eaef0e3264696c3ecd2174061bd7bd1a48941b57f620bb", "class_name": "RelatedNodeInfo"}]}, "text": "As shown in Fig. 2, the input of the transformer\nis mapped to a tensor xinwith a shape of ( b,s,h) after the\ntokenization process and embedding model, where brepresents\nbatch size, srepresents sequence length and hrepresents\nhidden dimension. Next, the position encoding will be sent\nto the self attention layer along with this tensor. The input\nxinand the output xoutof the self-attention module will be\nconnected by the residual network and the layer normalization\nmodule. Finally, the output of the \u201dAdd & Norm\u201d module xout\nwill be sent to the feed forward network.\nThe entire process can be defined as follows:\nQ=xin\u2217Wq+bq\nK=xin\u2217Wk+bk\nV=xin\u2217Wv+bv\nxout=LayerNorm 1(Softmax (Q\u2217KT\n\u221a\nh)\u2217V\u2217Wo+bo)+xin\ny=LayerNorm 2((xout\u2217W1+b1)\u2217W2+b2) +xout\nIt should be noted that wq, wk, wv, woare learnable tensors\nwith shape ( h,h);bq, bk, bv, boare a learnable tensors with\nshape ( h,).\n2)LSTM :Long Short-Term Memory (LSTM) [63] is a\nspecial Recurrent Neural Network (RNN) model that over-\ncomes the exploding/vanishing gradient problems of RNN in\nprocessing long-term dependency information by introducing\ncell state and gate mechanisms. The LSTM model consists\nof three gates: Input Gate, Forget Gate, and Output Gate.\nThese gates update the cell state by controlling the information\nflow, enabling the model to remember long-term dependent\ninformation. Cell State is the core module of the LSTM model\nwhich can memorize and maintain information. The Input Gate\ndecides which input data should be retained in the cell state.\nForget Gate determines which cell state information should be\ndiscarded to avoid excessive memory. Output Gate determines\nhow the information in the cell state affects the current output.\nThe flow of data and the collaborative work process between\ncomponents are shown in the Fig. 2.", "start_char_idx": 4255, "end_char_idx": 6053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43": {"__data__": {"id_": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71", "node_type": "1", "metadata": {}, "hash": "0cb925af162397fad1504f1a6f778ebaff0c22db95a3980d89546b4d103ab791", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "52c28400-6e09-4062-b489-6291b73575ec", "node_type": "1", "metadata": {}, "hash": "31463060176385aa624f33fc503b9991212c4cdc6288d626b9d44a6fc4861c11", "class_name": "RelatedNodeInfo"}, {"node_id": "e9b29251-dbf2-403d-9baf-1f9dbcca521e", "node_type": "1", "metadata": {}, "hash": "27e00ad0fb86e64f8660b771e78be1a457d3d99e23257d8799ff20a08739a7f9", "class_name": "RelatedNodeInfo"}, {"node_id": "56eb2784-6b7f-4a78-8646-50d89033c3fa", "node_type": "1", "metadata": {}, "hash": "6fda48f7eb64f054bc5f51e56354a7dc48d0caca7dbdb79b66f1634bb01ab283", "class_name": "RelatedNodeInfo"}, {"node_id": "9395cc04-3804-4a99-b757-9923150dc466", "node_type": "1", "metadata": {}, "hash": "03d485375ae0b899ea6f66194472dfc66985b777ce69e233e348e5f88d9bba5e", "class_name": "RelatedNodeInfo"}, {"node_id": "86fc9790-90b1-4c30-a828-0b1d798b9b7c", "node_type": "1", "metadata": {}, "hash": "59f1fa94b6a045018deef756912ca8cf75564b2112e2f85c8531952cfd102210", "class_name": "RelatedNodeInfo"}]}, "text": "2.\nThe entire process can be defined as follows:\nf=sigmoid (Wf\u2217xt+Uf\u2217yt\u22121+bf)\nz=tanh(Wz\u2217xt+Uz\u2217yt\u22121+bz)\ni=sigmoid (Wi\u2217xt+Ui\u2217yt\u22121+bi)\no=sigmoid (Wo\u2217xt+Uo\u2217yt\u22121+bo)\nct=z\u2299i+f\u2299ct\u22121\nyt=o\u2299tanh(ct)4\n(a) General transformer model architecture.\n (b) General LSTM block architecture.\n(c) General latent diffusion model architecture.\n (d) General GAN architecture.\nFig. 2: General architectures of several generators.\n3)Diffusion Model :Diffusion models [64] are a family\nof deep generative models that can create realistic and di-\nverse samples of data [65]\u2013[72], such as images [73]\u2013[79],\ntexts [80]\u2013[83], videos [84]\u2013[88], and molecules [89]\u2013[93].\nAs shown in Fig. 2, diffusion models work by gradually\nadding noise to data until it becomes random, then reversing\nthe process to generate new data from noise. This process\nis based on probabilistic modeling and neural networks.\nDiffusion models mainly have three equivalent formulations:\ndenoising diffusion probabilistic models [65]\u2013[67], score-\nbased generative models [68], [69], and stochastic differen-\ntial equations [70], [71], with following improvements like\nDDIM [94], Rectified Flow [95], Consistency Model [96] and\nRPG-DiffusionMaster [79].\nEspecially, let x0be a random variable that follows the\ndata distribution q(x0), and let xtbe a random variable that\nfollows the distribution q(xt|x0)after adding noise at time\nstept. Then, DDPM can be formulated as follows:\n\u2022Forward Process The forward process perturbs data with\na sequence of Gaussian noise injections, transforming the\ndata distribution q(x0)into a simple prior distribution\nq(xT)\u2248N(0, I).", "start_char_idx": 6051, "end_char_idx": 7653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f5a5cde-c754-4888-a5ed-2a21ee98be71": {"__data__": {"id_": "4f5a5cde-c754-4888-a5ed-2a21ee98be71", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78b629a5-d948-4752-953c-92ec8f2abd64", "node_type": "1", "metadata": {}, "hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1c73d00c-a783-4a14-ae37-a8248bcc62cd", "node_type": "1", "metadata": {}, "hash": "f35556f35ab7cd771b0f8860bcb865c52d138dab0eb7290d1207f7ffcbb24221", "class_name": "RelatedNodeInfo"}, {"node_id": "9147c0b4-957d-4bab-b394-3aec4a36c6f2", "node_type": "1", "metadata": {}, "hash": "7627320f42e1ff59feeb8974a33ce815410340af46f69417dff0f36a9c018276", "class_name": "RelatedNodeInfo"}]}, "text": "The transition kernel at each time step\nis given by\nq(xt|xt\u22121) =N(xt;p\n1\u2212\u03b2txt\u22121, \u03b2tI),\nwhere \u03b2t\u2208(0,1)is a hyperparameter. The marginal\ndistribution of xtconditioned on x0is\nq(xt|x0) =N(xt;\u221a\u00af\u03b1tx0,(1\u2212\u00af\u03b1t)I),\nwhere \u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=0\u03b1s.\n\u2022Reverse Process The reverse process generates new\ndata samples by reversing the forward process with alearnable Markov chain.", "start_char_idx": 7654, "end_char_idx": 8017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37fb7373-7684-4059-9b8a-f119e12a9483": {"__data__": {"id_": "37fb7373-7684-4059-9b8a-f119e12a9483", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ab9b36f9-5822-4d55-8dc2-a0c2334f7256", "node_type": "1", "metadata": {}, "hash": "ef4fa561d6aed9c408bdc9c4bf8c8ceba6886bfb415a196a2df45845dca3dab3", "class_name": "RelatedNodeInfo"}, {"node_id": "4a547e43-731c-454a-9b43-aa6128e5d71c", "node_type": "1", "metadata": {}, "hash": "524dd1964c06f3d7675bb36972b3b339af91fcbb234d8c81408462b17be67142", "class_name": "RelatedNodeInfo"}, {"node_id": "02f8004b-01f4-49f6-a3a6-adc81e2de14b", "node_type": "1", "metadata": {}, "hash": "6f377e222014d759c6dfaea880d0140c54a26d7c0b72f4c74d8d732f221b70dd", "class_name": "RelatedNodeInfo"}, {"node_id": "d0a2ad27-0fd6-4daa-b109-1ccbbad1a3a4", "node_type": "1", "metadata": {}, "hash": "c2351f2fec996d1e208ac30d9a16876479790b03e069383a295f5ee867a243d0", "class_name": "RelatedNodeInfo"}, {"node_id": "b600cf7c-debb-4303-a317-d75ff0849894", "node_type": "1", "metadata": {}, "hash": "f2b60bcf8228604996d8bf4bfe72713723d26f70e7b57031a9577fd7042e851e", "class_name": "RelatedNodeInfo"}, {"node_id": "ee606a3f-6129-4795-9c3f-d7c743aaa714", "node_type": "1", "metadata": {}, "hash": "4ce7d5b1795c44c295eec329b71d2b2afc46556d4d31eea300ba77a808410574", "class_name": "RelatedNodeInfo"}]}, "text": "The prior distribution is p(xT) =\nN(xT; 0, I)and the transition kernel is\np\u03b8(xt\u22121|xt) =N(xt\u22121;\u00b5\u03b8(xt, t),\u03a3\u03b8(xt, t)),\nwhere \u03b8denotes model parameters, and \u00b5\u03b8(xt, t)and\n\u03a3\u03b8(xt, t)are parameterized by deep neural networks. The\nreverse process starts from sampling xT\u223cp(xT)and\niteratively samples xt\u22121\u223cp\u03b8(xt\u22121|xt)until t= 0.\n\u2022Model Training For each sample x0\u223cq(x0), the model\ntraining objective is to maximizing the variational lower\nbound (VLB) of the log-likelihood of the data x0. The\nsimplified form LVLB(x0)is given by\nEq(x1:T|x0)\"\n\u2212logp(xT)\u2212TX\nt=1logp\u03b8(xt\u22121|xt)\nq(xt|xt\u22121)#\nWith simplication and reparameterization trick, the over-\nall objective Eq(x0)\u0002\nLVLB(x0)\u0003\ncan be simplified into the\nfinal form\nEt\u223cU[1,T],x0\u223cq(x0),\u03f5\u223cN(0,I)[\u03bb(t)\u2225\u03f5\u2212\u03f5\u03b8(xt, t)\u2225]\nwhere \u03bb(t)is a positive weighting function, U[1, T]is\na uniform distribution over the set {1,2, . . . , T }, and \u03f5\u03b8\nis a deep neural network with parameter \u03b8that predicts\nthe noise vector \u03f5given xtandt. Note that, the overall\nobjective is also equivalent to matching the joint distri-\nbution of the reverse process p\u03b8(x0, x1, . . . , x T)to that\nof the forward process q(x0, x1, . . . , x T)by minimizing\nthe KL divergence between them.\n4)GAN :Generative Adversarial Networks (GANs) [14]\nare highly anticipated deep learning models with amazing\ncapabilities which can simulate and generate realistic images,\naudio, and other data. Due to its outstanding performance,\nGANs have achieved significant achievements in various5\nfields [97].", "start_char_idx": 0, "end_char_idx": 1485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "240d64f4-947b-4a30-8a40-4c32f3f3fb31": {"__data__": {"id_": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "edf2a2b8-ee8c-4db5-95c2-41153e5155e3", "node_type": "1", "metadata": {}, "hash": "0cb7d262d972da8edd2f256f796aff89e8e4002ab357ab90135fe7557797d35a", "class_name": "RelatedNodeInfo"}, {"node_id": "ad84ebeb-7552-4a56-a95c-9bd66c84eb7b", "node_type": "1", "metadata": {}, "hash": "93cf02f3b171215472aa74e19b6dc943807c49aadfae97b599ccd02db18afe50", "class_name": "RelatedNodeInfo"}, {"node_id": "7adf1354-3259-464f-bed2-aeeb85ded3a7", "node_type": "1", "metadata": {}, "hash": "d3bb99f609dfd449331b064089a29027712f890adf3fa4c6a87a49dfe2883a71", "class_name": "RelatedNodeInfo"}, {"node_id": "d2228bbc-835c-4746-9b2a-fa52a330de44", "node_type": "1", "metadata": {}, "hash": "2f366efbd57daa81af8bf8b34a96ddba48f07feeb03c6bfe59214b00f36cd44b", "class_name": "RelatedNodeInfo"}]}, "text": "The design inspiration of GANs comes from the\nzero-sum game in game theory.\nAs shown in Fig. 2, a typical GAN consists of two main\ncomponents: a generator and a discriminator. These two parts\ncompete with each other through adversarial learning, allowing\nthe generator to continuously improve its ability to generate re-\nalistic samples, while the discriminator continuously improves\nits ability to distinguish between true and false samples.\nC.Retriever\nRetrieval is to identify and obtain relevant information given\nan information need. Specifically, let\u2019s consider information\nresources that can be conceptualized as a key-value store\n{(ki, vi)}N\ni=1, where each key kicorresponds to a value vi\n(kiandvican be identical). Given a query q, the objective\nis to search for the top- kmost similar keys using a similarity\nfunction s, and obtain the paired values. Based on different\nsimilarity functions, existing retrieval methods can be cate-\ngorized into sparse retrieval, dense retrieval, and others. For\nwidely used sparse and dense retrieval, the whole process can\nbe divided into two distinct phases: in the first phase, each\nobject is encoded into a specific representation; and in the\nsecond phase, an index is constructed to organize the data\nsource for efficient search.\n1)Sparse Retriever :Sparse retrieval methods are widely\nused in document retrieval, where the keys are actually doc-\numents to be searched (values are the same documents in\nthis scenario). These methods leverage term matching metrics\nsuch as TF-IDF [98], query likelihood [99], and BM25 [19],\nwhich analyze word statistics from texts and construct inverted\nindices for efficient searching. Among them, BM25 is a hard-\nto-beat baseline in industrial-scale web search.", "start_char_idx": 1486, "end_char_idx": 3232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2415d406-648d-488e-adee-8bd0fb74a5f5": {"__data__": {"id_": "2415d406-648d-488e-adee-8bd0fb74a5f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c5823942-5724-49f0-bdcc-35b8eea56ee0", "node_type": "1", "metadata": {}, "hash": "5750a8ed7dc03f2dbad8f4a9134ee3069242bbd4cf24f7d26c9daab8814f5cc2", "class_name": "RelatedNodeInfo"}, {"node_id": "b66452e2-8ce6-4fa9-9050-d4e0f3728888", "node_type": "1", "metadata": {}, "hash": "b3f08b94deeb1151858ca99daf69914479f6932e2fdae49240db89a369ea3276", "class_name": "RelatedNodeInfo"}, {"node_id": "d95c5819-7658-4df8-8296-49d7e5cf845c", "node_type": "1", "metadata": {}, "hash": "4be66e81cfcdb363195b3275e58c2cadb5e648ce4c0c640771753ff2d90a6f21", "class_name": "RelatedNodeInfo"}, {"node_id": "0200ef30-c427-45b4-a9d9-64b6cbc20ca6", "node_type": "1", "metadata": {}, "hash": "867fd0d62c4e70740f0a74c5fccb102ffec38f067da618a9940a1d4cc4863a39", "class_name": "RelatedNodeInfo"}, {"node_id": "98cbfd2c-a158-4fba-9f6c-1a9279934bae", "node_type": "1", "metadata": {}, "hash": "3eeb808bd67c8491ab0ed7e46319290a43e2dc8b98e9b68eea007eca637d02e7", "class_name": "RelatedNodeInfo"}]}, "text": "Among them, BM25 is a hard-\nto-beat baseline in industrial-scale web search. For a query q\ncontaining keywords {qi}n\ni=1, the BM25 score of a document\nDis:\ns(q, D) =nX\ni=1IDF(qi)\u00b7f(qi, D)\u00b7(a+ 1)\nf(qi, D) +a\u00b7(1\u2212b+b\u00b7|D|\navgdl)\nwhere IDF is the inverse document frequency weight, f(qi, D)\nis the number of times that qioccurs in the document D,|D|\nis the length of D,avgdl is the average document length in\nthe corpus collection, aandbare tunable parameters.\nIDF is computed as:\nIDF(qi) = ln(N\u2212n(qi) + 0.5\nn(qi) + 0.5+ 1)\nwhere Nis the number of documents, and n(qi)is the number\nof documents containing qi.IDF score is also used in TF-IDF.\nTo enable efficient search, sparse retrieval typically lever-\nages an inverted index to organize documents. Concretely, each\nterm from the query performs a lookup to obtain a list of\ncandidate documents, which are subsequently ranked based\non their statistical scores.\n2)Dense Retriever :Unlike sparse retrieval, dense retrieval\nmethods represent queries and keys using dense embedding\nvectors, and build approximate nearest neighbor (ANN) index\nto speed up the search. This can be applied to all modalities.\nFor text data, recent advancements in pre-trained models,including BERT [15] and RoBERTa [100], have been em-\nployed to encode queries and keys individually [20], [101]\u2013\n[104]. Similar to text, models have been proposed to encode\ncode data [25], [105], [106], audio data [26], [107], image\ndata [24], [108], video data [109], [110]. etc. The similarity\nscore between dense representations are usually computed\nwith metrics such as cosine, inner product, L2-distance.\nDuring training, dense retrieval usually follows a contrastive\nlearning paradigm, making positive samples more similar and\nnegative samples less similar. Several hard negative tech-\nniques [101], [111] have been proposed to further improve the\nmodel quality.", "start_char_idx": 3156, "end_char_idx": 5028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9": {"__data__": {"id_": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d81e32e-a897-46af-85a9-02e278286cd0", "node_type": "1", "metadata": {}, "hash": "1c581ea8f27640b23541bcda4f611dd2bb777d2ed23f9a757478ea4264ea11a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d96d7a26-cf40-465c-a85b-e3e272e7b8b2", "node_type": "1", "metadata": {}, "hash": "af39deba15a30236adc1be3bc1713da6e884d3929e95c0dfab3857df1c947d8e", "class_name": "RelatedNodeInfo"}, {"node_id": "725f8aab-d383-442b-a039-3f59785fa12d", "node_type": "1", "metadata": {}, "hash": "c5f5eb951d0813ca20637583c09c4e3723f826e8e42418599ac200f5b64cea0b", "class_name": "RelatedNodeInfo"}, {"node_id": "b270e70d-0842-46d4-aec3-b2325f853782", "node_type": "1", "metadata": {}, "hash": "9f82351a0c19c16dd19ba00171a7fae601df89158aa923f206da092993c290a7", "class_name": "RelatedNodeInfo"}, {"node_id": "1fdd4dd2-7a43-4965-ae4c-ef91448aece7", "node_type": "1", "metadata": {}, "hash": "4c80eb3165b3c06c748289ad7aa84c5eb49fb4fa284c1cbb799b13f2a2938867", "class_name": "RelatedNodeInfo"}, {"node_id": "0a35fa84-92c0-4504-bc57-83425e0b900b", "node_type": "1", "metadata": {}, "hash": "b34420caa937cadacf76dd62cad12261d129a4457c4940bf6df16e2ab3a62c6d", "class_name": "RelatedNodeInfo"}]}, "text": "During inference, approximate nearest neighbor\n(ANN) methods are applied for efficient searching. Various\nindices are developed to serve ANN search, such as tree [112],\n[113], locality sensitive hashing [114], neighbor graph index\n(e.g., HNSW [115], DiskANN [116], HMANN [117]), and\nthe combination of graph index and inverted index (e.g.,\nSPANN [22]).\n3)Others :In addition to sparse retrieval and dense re-\ntrieval, there are alternative methods for retrieving relevant\nobjects [118], [119]. Instead of calculating representations,\nsome research works directly use the edit distance between\nnatural language texts [120] or abstract syntax trees (AST)\nof code snippets [121], [122]. For knowledge graph, entities\nare linked with relations, which can be regarded as a pre-\nbuilt index for retrieval searching. Therefore, RAG methods\nwhich involve knowledge graph can use k-hop neighbor search\nas retrieval process [123], [124]. Named entity recognition\n(NER) [125] is another way of retrieval, where the input is\nthe query and the entites are the keys.\nIII. METHODS\nIn this section, we introduce RAG foundations and outline\nenhancement methods that further improve the effectiveness.\nA.RAG Foundations\nBased on how the retriever augments the generator, we\ncategorize RAG foundations into 4 classes, as shown in Fig. 3.\n1)Query-based RAG :Query-based RAG, originated from\nthe idea of prompt augmentation, integrates the user\u2019s query\nwith insights from information fetched during the retrieval\nprocess, directly into the initial stage of the language model\u2019s\ninput. This paradigm stands as a widely adopted approach\nwithin the applications of RAG. After retrieval, the retrieved\ncontent will be merged with the original user query to create a\ncomposite input sequence which is subsequently fed into the\ngenerator for response generation. Query-based RAG has been\nwidely applied across various modalities.\nFor Text Generation, REALM [33] employs a dual-BERT\nframework to streamline knowledge retrieval and integration,\nmarrying pre-trained models with knowledge extractors. The\ninitial BERT module processes the input question alongside\ndocuments to facilitate retrieval, utilizing MIPS for selecting\nthe top-k documents with the highest probability and periodi-\ncally updating the index.", "start_char_idx": 5029, "end_char_idx": 7314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d81e32e-a897-46af-85a9-02e278286cd0": {"__data__": {"id_": "9d81e32e-a897-46af-85a9-02e278286cd0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4961f468-f454-4ff5-8b6c-07d1984842c8", "node_type": "1", "metadata": {}, "hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "75700ca3-a4eb-4c65-a733-d621db10c1a3", "node_type": "1", "metadata": {}, "hash": "1c581ea8f27640b23541bcda4f611dd2bb777d2ed23f9a757478ea4264ea11a1", "class_name": "RelatedNodeInfo"}]}, "text": "The document snippets obtained are\nthen integrated with the query, feeding into the second BERT\nmodule to produce multiple outputs that are aggregated into\na singular, comprehensive response. Lewis et al. [34] syner-\ngized pre-trained language models with knowledge retrieval6\nFig. 3: Taxonomy of RAG foundations.\nmechanisms, leveraging DPR and BART structures to ac-\ncomplish retrieval-augmented generation tasks. DPR serves as\nthe retrieval component, sourcing pertinent information from\nvast document databases, while BART uses this information\nfor text generation.", "start_char_idx": 7315, "end_char_idx": 7883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59f340a8-342e-491c-94b2-43642ff312f4": {"__data__": {"id_": "59f340a8-342e-491c-94b2-43642ff312f4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c450e73d-15e3-4bdb-a091-1ec7e6dca295", "node_type": "1", "metadata": {}, "hash": "e3acc618300ff01dbd232052cfe306eccb2854ebc332a79567274eef6e95a625", "class_name": "RelatedNodeInfo"}, {"node_id": "07038048-4e61-4bd9-8db8-e5de3caa80bb", "node_type": "1", "metadata": {}, "hash": "eedaed2218a3cd897167bc1608e4df7dd1e3b2f72b1a07d7563cd1fa21a941e7", "class_name": "RelatedNodeInfo"}, {"node_id": "70e6d4c2-9123-48b5-9496-517b95606d04", "node_type": "1", "metadata": {}, "hash": "38b4d78d9bc27300083d3bde2e5c1eb8b334564fc52d284947505999b74310f4", "class_name": "RelatedNodeInfo"}, {"node_id": "fab5a902-7a88-4285-acd8-0558cd7a3974", "node_type": "1", "metadata": {}, "hash": "ba4ca6bbf960bacc69853c2d66ebc687e3f3421111d91b7e160683b0d651aeef", "class_name": "RelatedNodeInfo"}, {"node_id": "647c2fe0-ab0c-45db-82b0-9af9eaf4feac", "node_type": "1", "metadata": {}, "hash": "4919cb79ee556e0ae4d86eefb39a90956910dbf1366a7b3d81de2c4e66d3cf27", "class_name": "RelatedNodeInfo"}]}, "text": "RAG-Token and RAG-Sequence differ in\ntheir retrieval timings, with the former retrieving information\nat each token generation and the latter conducting a single\nretrieval for the entire sequence. SELF-RAG [126] enhances\nthe accuracy and relevance of responses by integrating a\nretrieval and critique strategy. Initially, the model employs a\nretriever to search for information paragraphs closely related to\nthe input question. Subsequently, the critique model evaluates\nthese paragraphs to determine their relevance and level of\nsupport of the retrieved text, assessing their impact on the\ngeneration of responses. Finally, the generator model con-\nstructs responses based on this information and evaluates the\nquality of these responses through critique marks. In addition\nto being compatible with local generators, Query-based RAG\nis also applicable to scenarios that use LLM through API\ncalls. REPLUG [127] illustrates this methodology by treating\nthe language model as a \u201cblack box\u201d, utilizing Contriever to\nseamlessly incorporate relevant external documents into the\nquery. REPLUG LSR, a variant with LM-Supervised Retrieval,\nfurther refines this process by optimizing retrieval through\nlanguage model-supervised insights, aiming to reduce per-\nplexity scores and improve model performance by enriching\nits contextual understanding. In-Context RALM [128] uses\nBM25 for document retrieval and trains a predictive reranker\nto reorder and integrate the top-ranked documents.\nIn contemporary research on other modalities, augmenting\ninputs with retrieved contents (which are not limited to texts)\nhas proven highly effective in enhancing the performance of\nvarious tasks. This strategy is applicable across several critical\ndomains, including code generation, audio generation, and\nKnowledge Base Question Answering (KBQA).\nFor Text-to-Code task, APICoder [129] and DocPrompt-\ning [42] demonstrate how effectively integrating retrieved\ninformation into language models can improve the accuracy\nand relevance of generated code. In Automatic Program Repair\ntask, CEDAR [130] and InferFix [131] utilize retrieved code\nsnippets to aid the repair process, enhancing the model\u2019s\nunderstanding and application of repair strategies by combin-\ning them with the original input. For Code Completion task,\nReACC [132] employs a prompting mechanism, leveraging\nretrieved code snippets as part of the new input to increase\nthe accuracy and efficiency of code completion.", "start_char_idx": 0, "end_char_idx": 2458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70f446ab-c0c8-4500-a291-c9562ad75f60": {"__data__": {"id_": "70f446ab-c0c8-4500-a291-c9562ad75f60", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "cfeff0d1-aadf-4d52-8c3b-0e986b4cd6ce", "node_type": "1", "metadata": {}, "hash": "bb303f85a73ee0e2a6e9b5a3f85514eed6fc574efb2e5fa7040e62656d90b57e", "class_name": "RelatedNodeInfo"}, {"node_id": "20581bbc-4949-4403-b086-da6b3edb5099", "node_type": "1", "metadata": {}, "hash": "c49587207e9182413b4e17af9ab7d8e81b11c4569c388e439b0f0b728cd667b3", "class_name": "RelatedNodeInfo"}, {"node_id": "9d6adc2c-4482-41e2-8db4-1d7c490f0a90", "node_type": "1", "metadata": {}, "hash": "258c9adf36230883a49a612423d7da47361fc5d8d8da64202310bb0d37713a5f", "class_name": "RelatedNodeInfo"}, {"node_id": "b70cee44-ce5a-4788-8ff4-c52b0d7696f2", "node_type": "1", "metadata": {}, "hash": "f1548a6b1ad6a89c9ee9bab6feadf0fe3f2cec50d8a73e615c203f7111fd8ac3", "class_name": "RelatedNodeInfo"}, {"node_id": "f447e72d-4b10-4ac6-92c8-2e00aed7fe48", "node_type": "1", "metadata": {}, "hash": "8ec63b70c55853ccdad77674647c54e4852946db45bfda073a76591a38bf13a2", "class_name": "RelatedNodeInfo"}]}, "text": "Recent researches in Knowledge Base Question Answering(KBQA) has also shown significant effects of combining\nretrieval and language models. For instance, Uni-Parser [133],\nRNG-KBQA [123], and ECBRF [134] effectively improve\nthe performance and accuracy of QA systems by merging\nqueries and retrieved information into prompts. BLLM aug-\nmentation [135] represents an innovative attempt at zero-\nshot KBQA using black-box large language models. This\nmethod, by directly integrating retrieved information into the\nmodel input without the need for additional sample training,\ndemonstrates the great potential of combining retrieval and\nlanguage models to enhance the model\u2019s generalization ability\nin understanding and answering unseen questions.\nIn the AI-for-Science domain, Chat-Orthopedist [136] pro-\nvides support for shared decision-making among adolescents\nwith idiopathic scoliosis. It enhances the application effec-\ntiveness and information accuracy of LLMs by integrating\nretrieved information into the prompts of the model.\nIn the task of Image Generation, RetrieveGAN [45] en-\nhances the relevance and accuracy of generated images by\nintegrating retrieved information, including selected image\npatches and their corresponding bounding boxes, into the\ninput stage of the generator. IC-GAN [137] modulates the\nspecific conditions and details of the generated images by\nconcatenating noise vectors with instance features.\nFor 3D Generation, RetDream [50] initially utilizes\nCLIP [24] to retrieve relevant 3D assets, then merges the\nretrieved contents with the user input during the input phase.\nQuery-based RAG, often paired with LLM generators,\noffers modular flexibility, allowing swift integration of pre-\ntrained components for quick deployment. Prompt design is\ncrucial for utilizing retrieved data within this setup.\n2)Latent Representation-based RAG :In Latent\nRepresentation-based RAG framework, retrieved objects are\nincorporated into generative models as latent representations.\nThis enhances the model\u2019s comprehension abilities and\nimproves the quality of the generated content.\nThe Fusion-in-Decoder (FiD) [35] technique leverages both\nBM25 and DPR for sourcing supportive paragraphs. It con-\ncatenates each retrieved paragraph and its title with the query,\nprocessing them individually through the encoder.", "start_char_idx": 2459, "end_char_idx": 4784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7706ee9-3add-42ac-8e9c-799cc02542a3": {"__data__": {"id_": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "446508f1-2908-4bc4-a220-f7638b3b0714", "node_type": "1", "metadata": {}, "hash": "898c8a93a0eb4e100148740afe8e4f05259180e4e65e5e5066b93294abb89988", "class_name": "RelatedNodeInfo"}, {"node_id": "c7bcbfc6-a287-44da-824a-07ea4bcb9eba", "node_type": "1", "metadata": {}, "hash": "d80e6780de80cc16cf05ef1dca2f8639b01b6bba7b5891ecab3041e1458e91a5", "class_name": "RelatedNodeInfo"}, {"node_id": "bf28dde5-d99f-4207-a394-11b22ff17b04", "node_type": "1", "metadata": {}, "hash": "844ad0c835ce614b7d7395bc902ccb91c97ed1430419c0b6001a67aee95de240", "class_name": "RelatedNodeInfo"}, {"node_id": "d2fae188-76b6-4255-862b-8c0d22114254", "node_type": "1", "metadata": {}, "hash": "fb60aed20fb772e19ba17d8488314d0091d5167c8a07165a5194ff1c51421afb", "class_name": "RelatedNodeInfo"}, {"node_id": "5157b639-edab-4c47-a9c6-771b6464b8a3", "node_type": "1", "metadata": {}, "hash": "9ab7544abb879d39377f58d84bd644ea3306f7e8b15210b7c8c3fe89ef36d91f", "class_name": "RelatedNodeInfo"}]}, "text": "FiD re-\nduces computational complexity and efficiently utilizes rel-\nevant information to generate answers by fusing information\nfrom multiple retrieved paragraphs in the decoder, rather than\nprocessing each paragraph in the encoder. The application\nof Fusion-in-Decoder methodologies transcends the realm of\ntextual content processing, demonstrating substantial potential\nand adaptability in processing code, structured knowledge,7\nand diverse multimodal datasets. Specifically within the code-\nrelated domain, technologies such as EDITSUM [138], BASH-\nEXPLAINER [139], and RetrieveNEdit [140] adopt the FiD\napproach, facilitating integration through encoder-processed\nfusion. Re2Com [141], and RACE [142] , among other meth-\nods, also feature the design of multiple encoders for different\ntypes of inputs.\nIn the field of Science, RetMol [55] also employs the\nFusion-in-Decoder strategy, integrating information at the\ndecoder stage to enhance the relevance and quality of the\ngenerated molecular structures.\nIn the field of Knowledge Base Question Answer-\ning (KBQA), the FiD method has been widely adopted,\ndemonstrating significant effectiveness. UniK-QA [143], DE-\nCAF [144], SKP [145], KD-CoT [146], and ReSKGC [147]\nhave effectively enhanced the performance of QA systems\nthrough the application of Fusion-in-Decoder technology. This\nillustrates that by integrating RAG for KBQA, the\nRetro [36] pioneers the integration of retrieved text via\n\u201cChunked Cross-Attention\u201d a novel mechanism that segments\nthe input sequence into discrete chunks. Each chunk indepen-\ndently executes cross-attention operations, thereby mitigating\ncomputational burdens. This technique enables the model to\nselectively retrieve and assimilate distinct documents for var-\nied sequence segments, fostering dynamic retrieval throughout\nthe generation process. This enhances the model\u2019s adaptability\nand enriches the contextual backdrop of generated content. In\nthe domain of image generation, cross-attention mechanisms\nhave been widely adopted within RAG frameworks. Methods\nsuch as Re-imagen [148], KNN-Diffusion [149], RDM [150]\nand LAION-RDM & ImageNet-RDM [151] utilize cross-\nattention to integrate multiple retrieval results, effectively\nenhancing the overall performance of the models.", "start_char_idx": 4785, "end_char_idx": 7059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "331dde64-1f43-487b-a9f4-dd1be4acf4a7": {"__data__": {"id_": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d78ca6ea-0bd4-4a06-a4de-9956656051dd", "node_type": "1", "metadata": {}, "hash": "5e1f7e76883b83dfadeac9a72b929a9ede1ad8ef5be9ea2275c5e90a49093aa6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e910ae6a-e737-4702-b02a-6eb60c5e82fa", "node_type": "1", "metadata": {}, "hash": "b248567956e50d66624a62467f52786021bb02342a7b0bb21da3b937bf6b11d8", "class_name": "RelatedNodeInfo"}, {"node_id": "f4617e78-1587-42e9-baff-320cf53f0c30", "node_type": "1", "metadata": {}, "hash": "e77c39de403e8a735b83488b744ffa6721f4e9096ebf3a923eb6c6f4977b2b0b", "class_name": "RelatedNodeInfo"}, {"node_id": "7182afb7-8ad9-4544-96da-88dd70391ad6", "node_type": "1", "metadata": {}, "hash": "443b17deeaab2e6bb7b31d5f23b69420e9579246d367a7d5c59baf4bb717b4ee", "class_name": "RelatedNodeInfo"}, {"node_id": "d11a8593-e158-4491-a23f-387fa20a2949", "node_type": "1", "metadata": {}, "hash": "9ad9fe17ae4654d2a4442d753dde3e5e34e59a46712df8c5dbe9e4d1eb663936", "class_name": "RelatedNodeInfo"}, {"node_id": "67354aca-f02d-489b-8076-768319861122", "node_type": "1", "metadata": {}, "hash": "944c0a0b19482c05747ab66de688411e786894d90558f161ef81ee4457836e2e", "class_name": "RelatedNodeInfo"}]}, "text": "In addition, there are also some other novel structures worth\nour attention, Li [152] introduced the ACM, a text-image affine\ncombination module, which notably does not employ any form\nof attention mechanism. Memorizing Transformers [31] revo-\nlutionize long document processing through the integration of\na kNN-augmented attention mechanism within a Transformer\nlayer. This innovation triggers a kNN search amidst input se-\nquence processing, fetching data based on similarities between\nthe sequence and stored key-value pairs, thereby elevating\nperformance without necessitating complete retraining. This\napproach not only bolsters processing efficiency but also\nbroadens the model\u2019s memory span, enabling self-retrieval\nfrom its generated outputs and fine-tuning for extensive\nknowledge bases or code repositories. Unlimiformer [153],\nby embedding a k-nearest neighbors (kNN) index within a\npre-trained encoder-decoder transformer framework, pioneers\nhandling inputs of indefinite length. Storing hidden states\nof input tokens in the kNN index allows for the efficient\nretrieval of highly relevant tokens during decoding. This\ninnovation extends the model\u2019s capacity to manage prolonged\nsequences. Kuratov et al. [154] integrated Transformer with\nRNN, utilizing the model\u2019s intermediate output as the content\nfor retrieval. This process was executed at each layer of the\nTransformer, thereby significantly extending the text window\u2019s\nlength and effectively mitigating the issue of \u201cLost in theMiddle\u201d [155]. Diverging from prior methods for knowledge,\nEaE [156] empowers language models to internalize explicit\nentity knowledge. EaE introduces an entity-specific param-\neterization, optimizing inference efficacy through an entity\nmemory layer embedded within the transformer architecture.\nThis layer directly acquires entity representations from textual\ndata, utilizing a sparse retrieval strategy to fetch the nearest\nentities based on their embeddings, thus refining the model\u2019s\ncomprehension through a calculated aggregation of entity-\nspecific information. on this basis , TOME [157] shifts the\nfocus towards comprehensive mention encodings, prioritizing\nthe granularity of mention over mere entity representations.\nIt meticulously creates a database that stores key and value\nencodings along with entity IDs, enabling the retrieval of much\nmore fine-grained information.", "start_char_idx": 7060, "end_char_idx": 9439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d78ca6ea-0bd4-4a06-a4de-9956656051dd": {"__data__": {"id_": "d78ca6ea-0bd4-4a06-a4de-9956656051dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "801c3840-c975-4417-9efd-ba90e44414fc", "node_type": "1", "metadata": {}, "hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "521bc758-b6b8-4767-ba22-0b5c7695f7e6", "node_type": "1", "metadata": {}, "hash": "5e1f7e76883b83dfadeac9a72b929a9ede1ad8ef5be9ea2275c5e90a49093aa6", "class_name": "RelatedNodeInfo"}]}, "text": "TOME integrates an initial\ntransformer block to process input texts, followed by TOME\nblocks with memory attention layers, facilitating the synthesis\nof multifaceted information sources and enhancing inferential\nreasoning capabilities, even for unencountered entities.", "start_char_idx": 9440, "end_char_idx": 9708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9dc57153-6fd5-499b-a634-fc574002380e": {"__data__": {"id_": "9dc57153-6fd5-499b-a634-fc574002380e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0fce0132-01b6-44ae-a579-03dddea37351", "node_type": "1", "metadata": {}, "hash": "df5df7a3d7632afe85b819f047ab0cd93d28f96e1ffb607b3508128f16e7a23f", "class_name": "RelatedNodeInfo"}, {"node_id": "ff9fbb87-c761-46f6-a6ab-81f552c06371", "node_type": "1", "metadata": {}, "hash": "9e297e1e6d0ad11041908374be148a28c4936d88e235a043f9a94e16b3095e60", "class_name": "RelatedNodeInfo"}, {"node_id": "833e21b8-65c9-44c2-980d-9da78f2499ec", "node_type": "1", "metadata": {}, "hash": "c6a57722d21e1eb0004da56067ae071eebd079f844d4f65db34fed921c337800", "class_name": "RelatedNodeInfo"}, {"node_id": "06fba012-1729-4a78-b242-4b7f23225683", "node_type": "1", "metadata": {}, "hash": "443c5d8a8fba2f3bb196aa65a1f98dcd476d2414ec911f1113b77f8cbf481034", "class_name": "RelatedNodeInfo"}, {"node_id": "4947b6b4-0407-4482-a4c2-6ede5a612a61", "node_type": "1", "metadata": {}, "hash": "a83c5c00f76247cc6deb0a886bb04e96740a53be43fc33841cd3c613d01ce650", "class_name": "RelatedNodeInfo"}]}, "text": "In the field of 3D generation, ReMoDiffuse [51] introduces\na semantics-modulated attention mechanism which enhances\nthe accuracy of generating corresponding 3D motions based on\ntextual descriptions. AMD [158] achieves efficient conversion\nfrom text to 3D motion by fusing the original diffusion process\nwith the reference diffusion process.\nIn the Audio domain, Koizumi et al. [43] utilized an\nLLM, incorporating encoded dense features in the atten-\ntion module to guide the generation of audio captions. Re-\nAudioLDM [159] utilizes distinct encoders to extract deep\nfeatures from text and audio, which are then integrated into the\nattention mechanism of its Latent Diffusion Model (LDM).\nFor video captioning, R-ConvED [48] uses a convolutional\nencoder-decoder network to process retrieved video-sentence\npairs with an attention mechanism, generating hidden states to\nproduce captions. CARE [160] introduces a concept detector\nto produce concept probabilities, and incorporates concept\nrepresentations into a hybrid attention mechanism. EgoIn-\nstructor [49] employs gated-cross attention to integrate textual\ninputs with encoded video features, enhancing the relevance\nand coherence of the generated captions for egocentric video\ncontent.\nFinally, Latent Representation-based RAG is adaptable to\nvarious modalities and tasks. It obtains the hidden states of\nretrieved data, enabling seamless integration between retrievers\nand generators. However, additional training is required to\nalign the latent space. Within this paradigm, we have the flex-\nibility to design intricate and novel algorithms that effectively\ncombine the information from retrieved contents.\n3)Logit-based RAG :In logit-based RAG, generative mod-\nels integrate retrieval information through logits during the\ndecoding process. Typically, the logits are combined through\nsimple summation or models to compute the probabilities for\nstep-wise generation.\nThe kNN-LM [37] model integrates a pre-trained neural\nlanguage model with the k-nearest neighbor search. It employs\nthe pre-trained model to generate a list of candidate words and\ntheir probability distribution, while simultaneously performing\nretrieval from a data repository to find the k most relevant8\nneighbors based on the current context, thus enhancing the\noutput of the original language model.", "start_char_idx": 0, "end_char_idx": 2326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "633f7b61-b5f8-4142-8666-0a0ba4b804a8": {"__data__": {"id_": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "93d495a1-fa6d-4e6b-b58e-d71d21fc6dcc", "node_type": "1", "metadata": {}, "hash": "ef24e4c88670f09703fc785f860654a130018a2cf3ff4ffb7db3596545470e27", "class_name": "RelatedNodeInfo"}, {"node_id": "cf11fe80-d25f-479b-ad30-4a45fd1b4ee4", "node_type": "1", "metadata": {}, "hash": "7650665160ddcadabdaf6ed5f29669f2ad40fafd4b0aa01982e41d56c408ece2", "class_name": "RelatedNodeInfo"}, {"node_id": "916c7dd8-b06b-4893-8ee9-4802ad8ba187", "node_type": "1", "metadata": {}, "hash": "bb48b4a67eb4de213d26ba81df501946bb33c5681a9a0994f4e038d7eba69bd2", "class_name": "RelatedNodeInfo"}, {"node_id": "c3c637f9-57fc-4689-b0ab-401bf0f103cd", "node_type": "1", "metadata": {}, "hash": "fdcbf9f039f932d129a769237f162be470154d20738827ae577b560820dd7396", "class_name": "RelatedNodeInfo"}, {"node_id": "b5c1bab0-af3f-4195-9a99-494366bdfaeb", "node_type": "1", "metadata": {}, "hash": "70d586b0b3b6757f09540c5ff6b3369e4cf4f456f9bc1d14e9bbdd4710dc19fb", "class_name": "RelatedNodeInfo"}]}, "text": "The innovation at the\ncore of this model lies in its ability for dynamic retrieval of\ninformation from a broad text corpus, significantly improving\nthe accuracy and relevance of its predictions, particularly in\naddressing rare patterns and adapting to various fields. He et\nal. [38] introduced a new framework that is predicated on\nperforming retrieval operations only when necessary, aimed\nat enhancing the inference efficiency of the kNN-LM model\nthrough an adaptive retrieval. This framework accelerates the\nmodel\u2019s inference speed by training a retrieval adapter, which\nautomatically identifies and eliminates unnecessary retrieval\nactions in certain scenarios. This method allows the model to\ndynamically decide on the necessity of retrieval based on the\ncurrent context, thereby balancing the trade-off between per-\nformance and efficiency, and substantially increasing inference\nspeed while maintaining model performance.\nUnlike previous methods that only merge memories during\nthe testing time, TRIME [161] achieves memory merging\nduring both training and testing phases, treating in-batch\nexamples as accessible memory. It leverages new data batching\nand memory construction techniques to effectively utilize\nexternal memory. It employs BM25 scores to pack paragraphs\nwith high lexical overlap into the same batch, constructing\nthe training memory to further optimize model performance.\nNPM [162] is a non-parametric masked language model com-\nprised of an encoder and a reference corpus. Unlike traditional\nmodels that apply a softmax over a finite vocabulary, NPM\nmodels a non-parametric distribution over the corpus. The\nencoder\u2019s role is to map phrases from the corpus into fixed-\nsize vectors, filling in [MASK] by retrieving the phrase most\nsimilar to the masked position.\nBeyond Text, other modalities, such as code and Image,\nalso leverage logit-based RAG. For code-to-text conversion\ntask, Rencos [121] generates multiple summary candidates in\nparallel from the retrieved code. It then normalizes these can-\ndidates using edit distance and calculates the final probability\nto select the summary output that best matches the original\ncode. In code summarization task, EDITSUM [138] enhances\nthe quality of summary generation by integrating prototype\nsummaries at the probability level. For text-to-code tasks,\nthe kNN-TRANX [163] model employs a combination of a\nconfidence network and meta-knowledge to merge retrieved\ncode fragments.", "start_char_idx": 2327, "end_char_idx": 4779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b": {"__data__": {"id_": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "5291ac61-aa0c-4268-aa6c-4431e2115530", "node_type": "1", "metadata": {}, "hash": "00c625443826b23f67f5255c081d9ed10a712cc6dc61dc66af70dbdf3a9b45f5", "class_name": "RelatedNodeInfo"}, {"node_id": "db893a9b-7adf-4d69-a247-e0187299a6a5", "node_type": "1", "metadata": {}, "hash": "09159869813772405435517b8005b4db6f8d3d3285d16ca1afa985eec6600bc5", "class_name": "RelatedNodeInfo"}, {"node_id": "b6cb972f-9382-4be4-8251-374170f5deaf", "node_type": "1", "metadata": {}, "hash": "eac9829ece5ce364de5719e2f67a1c72c733e9ce951d5d3e858aabe6c1433caa", "class_name": "RelatedNodeInfo"}, {"node_id": "e8bbd7aa-500f-4460-b15c-c8959ea97ed7", "node_type": "1", "metadata": {}, "hash": "db71fc1a77cb8df69ca9fd2ad78c71ecf08144091bcb67a8a1acd3e455bd856d", "class_name": "RelatedNodeInfo"}, {"node_id": "bc6d7f07-411a-40b5-b47f-5332e2253a43", "node_type": "1", "metadata": {}, "hash": "db6346e86c2a29df736cb6f745d44a7b111b1a149dfe97522bcf7cab30133aab", "class_name": "RelatedNodeInfo"}]}, "text": "It utilizes a seq2tree structure to generate\ntarget code that closely matches the input query, thereby\nincreasing the accuracy and relevance of code generation. In\nimage captioning tasks, MA [164] combines an attention-based\nencoder-decoder, using the image encoder to extract visual\nfeatures to construct the semantic part, and decodes it word by\nword with the information retrieved. MA interpolates between\ntwo distributions generated by the caption decoder and the\nmemory-augmented module to determine the distribution of\nthe next word.\nIn conclusion, Logit-based RAG effectively leverages histor-\nical states (or other data sources) to infer the current state and\ncombines information at the logit level. This approach is well-\nsuited for sequence generation tasks. It couples retrieval and\ngeneration, primarily requiring training of the generator. Novelapproaches can be designed to effectively utilize the obtained\nprobability distributions and adapt to subsequent tasks.\n4)Speculative RAG :Speculative RAG seeks opportunities\nto use retrieval instead of pure generation, aiming to save\nresources and accelerate response speed. REST [32] replaces\nthe small models in speculative decoding [165] with retrieval,\nenabling the generation of drafts. GPTCache [39] addresses the\nissue of high latency when using the LLM APIs by building\na semantic cache for storing LLM responses. COG [166]\ndecomposes the text generation process into a series of copy-\nand-paste operations, retrieving words or phrases from the\ndocuments instead of generation. Cao et al. [167] proposed a\nnew paradigm to eliminate the dependence of the final result\non the quality of the first-stage retrieved content, replacing\ngeneration with directly retrieved phrase level content.\nIn conclusion, Speculative RAG is currently primarily ap-\nplicable to sequential data. It decouples the generator and\nthe retriever, enabling the direct use of pre-trained models as\ncomponents. Within this paradigm, we can explore a wider\nrange of strategies to effectively utilize the retrieved content.\nB.RAG Enhancements\nIn this section, we introduce methods which enhance the\nperformance of a constructed RAG system. We categorize\nexisting methods into 5 groups based on their enhancement tar-\ngets: input, retriever, generator, result, and the entire pipeline.", "start_char_idx": 4780, "end_char_idx": 7100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fac41315-4d32-4359-b216-55c151e8b5e1": {"__data__": {"id_": "fac41315-4d32-4359-b216-55c151e8b5e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "638f46aa-98ea-47c6-9083-3d5c75cc98b4", "node_type": "1", "metadata": {}, "hash": "47aa8341805eb8373995fbe9e89f3cf4f7b160423e5f87be8c6e8eee1b2e8464", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "236dd4eb-04a5-45f2-84ca-1d849255c593", "node_type": "1", "metadata": {}, "hash": "88b8e3bdcfc87b9878aa403bc13c7999170b44f9a32c7180d2a27852c8f06d99", "class_name": "RelatedNodeInfo"}, {"node_id": "c1887afa-cd93-4a27-b62f-d7f1d01e26cb", "node_type": "1", "metadata": {}, "hash": "c69585fb915b622cc5e4cb4273bc2131e5e30cc2a978be334bff95c847346264", "class_name": "RelatedNodeInfo"}, {"node_id": "037972d6-86da-41d6-85fd-6f03cf5d8c9a", "node_type": "1", "metadata": {}, "hash": "d830658810f3425ba1435f16d727e21d94d20d4957acd423a0b36ebc0b685672", "class_name": "RelatedNodeInfo"}, {"node_id": "071b41de-61a2-4b47-8be3-d14fd05f5538", "node_type": "1", "metadata": {}, "hash": "46ffbf322df316c22fec8e4897f85b727381ef659acb96dbb7dd4ab3824be140", "class_name": "RelatedNodeInfo"}, {"node_id": "e656c762-fee1-489b-8bbf-5f3172ddcef9", "node_type": "1", "metadata": {}, "hash": "701b642bf94593756a8b62edfddb7cbad92bd19637fc9973dbc83231747cc9b6", "class_name": "RelatedNodeInfo"}]}, "text": "1)Input Enhancement :The input, initially fed into the re-\ntriever, significantly impacts the final outcome of the retrieval\nstage. In this section, we introduce two methods for input\nenhancement: query transformation and data augmentation.\na)Query Transformation :Query transformation can\nenhance the result of retrieval by modifying the input query.\nQuery2doc [168] and HyDE [169] use the original query to\ngenerate a pseudo document, which is later used as the query\nfor retrieval. The pseudo document contains richer relevant\ninformation, which helps to retrieve more accurate results.\nTOC [170] employs recursive retrieval augmented clarification\non ambiguous questions to construct a tree of disambiguated\nquestions, thereby generating comprehensive answers to these\nambiguous questions. During the construction process of this\ntree structure, TOC utilizes a self-verification pruning method\nto ensure the factual relevance of each node.\nb)Data Augmentation :Data augmentation improves\ndata before retrieval, including techniques such as removing ir-\nrelevant information, eliminating ambiguity, updating outdated\ndocuments, synthesize new data, etc.\nMake-An-Audio [44] uses captioning and audio-text re-\ntrieval to generate captions for language-free audio to mitigate\ndata sparsity, and adds random concept audio to improve the\noriginal audio. LESS [171] strategically selects an optimal\ndataset for downstream tasks by analyzing gradient informa-\ntion. It aims to maximize the dataset\u2019s impact on fine-tuning\nthe model\u2019s performance in response to instructional prompts.\nReACC [132] employs data augmentation (including renaming\nand dead code insertion) to pre-train the code retrieval model.\n2)Retriever Enhancement :The retrieval process is crucial\nin RAG systems. Generally, the better the retrieved content\nquality, the easier it is to stimulate the ability of LLMs in-\ncontext learning as well as other generators and paradigms.9\nFig. 4: Taxonomy of RAG Enhancements.\nThe worse the content quality, the more likely it is to cause\nmodel hallucinations. Therefore, in this section, we will\ndiscuss how to efficiently improve the effectiveness of the\nretrieval process.\na)Recursive Retrieval :Recursive retrieval is to perform\nmultiple searches to retrieve richer and higher-quality contents.\nReACT [172] uses Chain-of-Thought (CoT) [173] to break\nqueries down for recursive retrieval and provide richer in-\nformation.", "start_char_idx": 7101, "end_char_idx": 9530, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "638f46aa-98ea-47c6-9083-3d5c75cc98b4": {"__data__": {"id_": "638f46aa-98ea-47c6-9083-3d5c75cc98b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "node_type": "1", "metadata": {}, "hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "90b32c62-1f04-44ba-b29a-0ca24bb90c12", "node_type": "1", "metadata": {}, "hash": "47aa8341805eb8373995fbe9e89f3cf4f7b160423e5f87be8c6e8eee1b2e8464", "class_name": "RelatedNodeInfo"}]}, "text": "RATP [174] uses the Monte-Carlo Tree Search\n(MCTS) to perform multiple simulations, identifying optimal\nretrieval content. This content is then integrated into a template\nand sent to the generator for final production.", "start_char_idx": 9531, "end_char_idx": 9749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "980c530c-1005-4c72-8d3a-68472f1c9de1": {"__data__": {"id_": "980c530c-1005-4c72-8d3a-68472f1c9de1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "fec8fb2e-3c48-487b-b5b0-a4ab6a3a5aa6", "node_type": "1", "metadata": {}, "hash": "4ada32db1c609fff7297b58456fd595ec117bc332ab3278d018f99014580b905", "class_name": "RelatedNodeInfo"}, {"node_id": "427f2851-d74b-4810-b773-a43b53845b08", "node_type": "1", "metadata": {}, "hash": "e59a88b2df7394ede064070f936b7d0b7a21745d3e6955219538a6f3a2e41770", "class_name": "RelatedNodeInfo"}, {"node_id": "d005b6b4-fbc3-4538-b88d-ebcc6675bb46", "node_type": "1", "metadata": {}, "hash": "85f5b7efc4d50fde33857b29a94dba2885ea3b5e65a020e7b28e761a0c08a969", "class_name": "RelatedNodeInfo"}, {"node_id": "6a816ebb-a509-478b-bd71-41524128c6bd", "node_type": "1", "metadata": {}, "hash": "80a70a850d81d0a81e6f5d559f93ba5e85b769d29b6148cae336b27f471e3bc0", "class_name": "RelatedNodeInfo"}, {"node_id": "57928373-8fa0-4601-abae-1b8a20f7afc4", "node_type": "1", "metadata": {}, "hash": "3270723f80fd1e1d103461e9c648111a8b3d3ac144e889a08c1a7f2613b3fe02", "class_name": "RelatedNodeInfo"}]}, "text": "This content is then integrated into a template\nand sent to the generator for final production.\nb)Chunk Optimization :Chunk optimization refers to\nadjusting chunk size for improved retrieval results. Sentence-\nwindow retrieval [175] is an efficient approach that enhances\nretrieval by fetching small chunks of text and returning\na window of relevant sentences surrounding the retrieved\nsegment. This method ensures that the context before and\nafter the targeted sentence is included, providing a more\ncomprehensive understanding of the retrieved information.\nAuto-merge retrieval is another advanced RAG method of\nLlamaIndex [175] which organizes the document in a tree-\nlike structure, with the parent node containing the content of\nall children nodes. For example, articles and paragraphs, as\nwell as paragraphs and sentences, all follow a parent-child\nrelationship. In the retrieve process, fine-grained search for\nchildren nodes ultimately returns the parent node, effectively\nproviding richer information. To address the lack of contextual\ninformation, RAPTOR [176] employs recursive embedding,\nclustering, and summarization of text chunks until further\nclustering becomes infeasible, thereby constructing a multi-\nlevel tree structure.\nc)Retriever Finetuning :As a core component in the\nRAG system, the retriever plays a crucial role in the entire\nsystem operation process. An effective embedding model can\ncluster semantically similar content in vector space, enhancing\nthe retriever\u2019s capability to provide valuable information for\nthe subsequent generator, thus boosting the RAG system\u2019s effi-\nciency. Hence, the proficiency of the embedding model [177]\u2013\n[180] is vital for the RAG system\u2019s overall effectiveness.\nIn addition, for embedding models that already have good\nexpression power, we can still finetune them using high-quality\ndomain data or task related data to improve their performance\nin specific domains or tasks. REPLUG [127] treats LM as ablack box and update the retriever model based on the final\nresults. APICoder [129] finetunes the retriever with python\nfiles and api names, signature, description. EDITSUM [138]\nfinetunes the retriever to decrease the jaccard distance between\nsummaries after retrieval.", "start_char_idx": 0, "end_char_idx": 2233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89183965-5983-4d40-83bb-bc6c1f2a2d8d": {"__data__": {"id_": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9b6bcd96-26f3-4e02-9ee0-d0b553dfbbbf", "node_type": "1", "metadata": {}, "hash": "2af18ff4c671d075245f18e5c9b1532583c60fcef2c5bc88a264ecec24b9dfe9", "class_name": "RelatedNodeInfo"}, {"node_id": "521b5c8b-39fc-4b8e-bdc5-fc37e17b6a92", "node_type": "1", "metadata": {}, "hash": "c6ca82d92452988f1526b29e489164b5fe0b1ee107937d3690e8e326745b938e", "class_name": "RelatedNodeInfo"}, {"node_id": "f400ddea-f8d1-45c4-b737-76842703e6f5", "node_type": "1", "metadata": {}, "hash": "a8aacd529316f6d96751924db0132e294491418431d06acd5796c913fa2b00ac", "class_name": "RelatedNodeInfo"}, {"node_id": "19e0ec7f-0645-402b-a264-668c85a59adb", "node_type": "1", "metadata": {}, "hash": "ec5e1a59a5c0490d1d6132b3982d37746f29d33ba6199fd8e43079187254a3e8", "class_name": "RelatedNodeInfo"}, {"node_id": "d7938e9c-de67-4c48-8b5b-475d32455dd5", "node_type": "1", "metadata": {}, "hash": "638b42097f590ff117dc31186b6ecd7b95790c779f92fd54ec42e6f887f3259c", "class_name": "RelatedNodeInfo"}]}, "text": "SYNCHROMESH [122] adds tree\ndistance os ASTs in the loss and uses Target Similarity\nTuning to finetune the Retriever. R-ConvED [48] finetunes the\nRetriever with the same data as generator. Kulkarni et al. [181]\napplied infoNCE loss to finetune the Retriever.\nd)Hybrid Retrieval :Hybrid retrieve denotes the concur-\nrent employment of a diverse array of retrieval methodologies\nor the extraction of information from multiple distinct sources.\nRAP-Gen [182] and ReACC [132] use both dense retriever\nand sparse retriever to improve the quality of retrieval. Ren-\ncos [121] uses sparse retriever to retrieve similar code snippets\non syntactic-level and usse dense retriever to retrieve similar\ncode snippets on semantic-level. BASHEXPLAINER [139]\nfirst uses dense retriever to capture semantic information\nand then uses sparse retriever to acquire lexical information.\nRetDream [50] first retrieves with text and then retrieves with\nthe image embedding. CRAG [183] has designed a retrieval\nevaluator to assess the relevance of retrieved documents to the\ninput query, triggering three types of retrieval actions based on\nvarying confidence levels: if deemed correct, it directly uses\nthe retrieval results for Knowledge Refinement; if incorrect,\nit resorts to Web Search; and if ambiguous, it combines both\napproaches. To enhance performance in question-and-answer\ntasks, Huang et al. [184] introduced two metrics, DKS(Dense\nKnowledge Similarity) and RAC(Retriever as Answer Clas-\nsifier), during the retrieval phase. These metrics account for\nboth the pertinence of the answers and the applicability of the\nunderlying knowledge. UniMS-RAG [185] introduces a novel\nkind of token, termed as the \u201cacting token\u201d, which determines\nthe source from which to retrieve information.\ne)Re-ranking :The Rerank technique refers to reorder-\ning the retrieved content in order to achieve greater diversity\nand better results. Re2G [186] applies a re-ranker [187]\nmodel after the traditional retriever.", "start_char_idx": 2234, "end_char_idx": 4216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d7bfce5-d142-44e2-9d76-7001709526dd": {"__data__": {"id_": "6d7bfce5-d142-44e2-9d76-7001709526dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1458b347-0390-4066-8436-60dd46fa3e53", "node_type": "1", "metadata": {}, "hash": "6db050db3e164bae6937244b219837ed0bddb5fffa84978bae5f04ea687868a6", "class_name": "RelatedNodeInfo"}, {"node_id": "ae376509-a845-4656-bc3b-84889c1c4046", "node_type": "1", "metadata": {}, "hash": "85e1ad273eb4ed542b68a830364a3b1906d1ed892b38cf58e1f72fbf4b58f6c6", "class_name": "RelatedNodeInfo"}, {"node_id": "cb450b82-3b7e-48ac-b189-52e905a1d67b", "node_type": "1", "metadata": {}, "hash": "29d80a94794e082ad943c9d3c5bd83d27abae6d86a8c01a0a2c77b981dd7e14e", "class_name": "RelatedNodeInfo"}, {"node_id": "a349c88d-5d16-4818-8c37-f9d948c59dd5", "node_type": "1", "metadata": {}, "hash": "25da6dbf51d8c0ce7397c9711bcb748d4c62ad9529a0e7886fd7f5e8228449f4", "class_name": "RelatedNodeInfo"}, {"node_id": "7bbf6e82-0859-4e81-8ca0-06d8aee22e61", "node_type": "1", "metadata": {}, "hash": "c3d284cc380ace0060370c94024e17d7c7a9078913ecdfc006fcce3b9bdc8fca", "class_name": "RelatedNodeInfo"}]}, "text": "The effect of the re-\nranker model is to re-rank retrieved documents, the purpose\nof which is to reduce the impact of information loss caused\nby compressing text into vectors on the quality of retrieval.\nAceCoder [188] reranks the retrieved programs with a selector\nto reduce redundant programs and obtain diverse retrieved\nprograms. XRICL [189] uses a distillation-based exemplar10\nreranker after retrieval. Rangan [190] employs the Quantized\nInfluence Measure, assessing statistical biases between a query\nand a reference to evaluate the similarity of data subsets\nand rerank retrieval results. UDAPDR [191] uses LLMs to\ncost-effectively generate synthetic queries that train domain-\nspecific rerankers, which then apply multi-teacher knowledge\ndistillation to develop a cohesive retriever. LLM-R [192]\niteratively trains the retriever, using feedback from a frozen\nLLM to rank candidate documents and train the reward model.\nThe retriever is further trained based on knowledge distillation.\nEach iteration of training builds upon the retriever trained\nin the previous cycle, facilitating iterative optimization in\nsubsequent rounds.\nf)Retrieval Transformation :Retrieval Transformation\ninvolves rephrasing retrieved content to better activate the\ngenerator\u2019s potential, resulting in improved output.\nFILCO [193] effectively filters out irrelevant content from\nthe retrieved text chunk, leaving only the precise supporting\ncontent. This process simplifies the task for the generator,\nmaking it easier to predict the correct answer. FiD-Light [194]\ninitially employs an encoder to convert the retrieved content\ninto a vector, which it then compresses, resulting in a sub-\nstantial reduction of latency time. RRR [195] integrates the\ncurrent query with the top-k document in each round through\na template, and subsequently restructures it via a pre-trained\nLLMs(GPT-3.5-Turbo etc.).\ng)Others :In addition to the above optimization meth-\nods, there are also some other optimization methods for the\nretrieve process. For example, Meta-data filtering [196] is\na method to help processing retrieved documents which uses\nmetadata (such as time, purpose, etc.) to filter the retrieved\ndocuments for better results.", "start_char_idx": 4217, "end_char_idx": 6425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e288a398-5ad7-4fe0-9e03-11b716299770": {"__data__": {"id_": "e288a398-5ad7-4fe0-9e03-11b716299770", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b833d9b9-cff4-4e06-9dba-20efb87d02d1", "node_type": "1", "metadata": {}, "hash": "86acf41015531bda8785a43dc2b5ced0fd4a0692d6ac5e7721c155949d42e2d9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1fac2605-63aa-4450-9179-db26c337f758", "node_type": "1", "metadata": {}, "hash": "e84cd8e449a606f23a605b6e2e16f038bde927c870a7a8aa8d100b7796609186", "class_name": "RelatedNodeInfo"}, {"node_id": "ea7e8084-0269-4117-b8a3-23557924e586", "node_type": "1", "metadata": {}, "hash": "7b2173bf2212186a5c0d3803e5fd190e5b454559f43b736791cbb874ef3e0aa5", "class_name": "RelatedNodeInfo"}, {"node_id": "5fcedf03-b441-4c81-a2f1-d9291f04c901", "node_type": "1", "metadata": {}, "hash": "1cdca8bcce56f0f68666044fd67bbb27cc11ba1fb177979eb72b9c113644b107", "class_name": "RelatedNodeInfo"}, {"node_id": "84264c77-f9e8-43b3-ba41-6723f6b6c82f", "node_type": "1", "metadata": {}, "hash": "19d103e403eb6ad162415ea2b0c51181f6f6f938d193e686c8ae61b068e3a2d0", "class_name": "RelatedNodeInfo"}, {"node_id": "3f595dd6-042a-41e4-95d4-e25bb8d30bf6", "node_type": "1", "metadata": {}, "hash": "a455fcd397f5a60d073d0e2e58f8121bf9d3e66dd33e44cadcf5a58a5c46359d", "class_name": "RelatedNodeInfo"}]}, "text": "to filter the retrieved\ndocuments for better results. GENREAD [197] and GRG [198]\nintroduce a novel approach where the retrieval process is\nsupplanted or improved by prompting a LLM to generate\ndocuments in response to a given question.\n3)Generator Enhancement :In RAG systems, the quality\nof the generator often determines the quality of the final output\nresults. Therefore, the ability of the generator determines the\nupper limit of the entire RAG system\u2019s effectiveness.\na)Prompt Engineering :Technologies in prompt engi-\nneering [199] that focus on improving the quality of LLMs\u2019\noutput, such as Prompt compression, Stepback Prompt [200],\nActive Prompt [201], Chain of Thought Prompt [173], etc.,\nare all applicable to LLM generators in RAG systems. LLM-\nLingua [202] applies a small model to compresses the overall\nlength of the query to accelerate model inference, relieving\nthe negative impact of irrelevant information on the model\nand alleviating the phenomenon of \u201cLost in the Middle\u201d\n[155]. ReMoDiffuse [51] decomposes complex descriptions\ninto anatomical text scripts by using ChatGPT. ASAP [203]\nadd exemplar tuples to the prompt for better results. An\nexemplar tuple is composed of the input code, a function\ndefinition, the results of analyzing that definition and its\nassociated comment. CEDAR [130] uses a designed prompt\ntemplate to organize code demonstration, query, and natural\nlanguage instructions into a prompt. XRICL [189] utilizes\nCOT technology to add translation pairs as an intermediate\nstep in cross linguistic semantic parsing and inference. AC-TIVERAG [204] employs the Cognition Nexus mechanism to\ncalibrate the intrinsic cognition of LLMs and applies COT\nprompt in answer generation. Make-An-Audio [44] is able to\nuse other modalities as input which can provide much richer\ninformation for the following process.\nb)Decoding Tuning :Decoding tuning refers to adding\nadditional controls during the generator processing, which can\nbe achieved by adjusting hyperparameters to achieve greater\ndiversity, limiting the output vocabulary in some form, and so\non.\nInferFix [131] balances the diversity and quality of\nresults by adjusting the temperature in decoder.", "start_char_idx": 6372, "end_char_idx": 8562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b833d9b9-cff4-4e06-9dba-20efb87d02d1": {"__data__": {"id_": "b833d9b9-cff4-4e06-9dba-20efb87d02d1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "node_type": "1", "metadata": {}, "hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "58196cf7-31c3-45cb-b305-47780d51c9a2", "node_type": "1", "metadata": {}, "hash": "86acf41015531bda8785a43dc2b5ced0fd4a0692d6ac5e7721c155949d42e2d9", "class_name": "RelatedNodeInfo"}]}, "text": "SYN-\nCHROMESH [122] limits the output vocabulary of the de-\ncoder by implementing a completion engine to eliminate\nimplementation errors.\nc)Generator Finetuning :The finetuning of the gener-\nator can enhance the model\u2019s ability to have more precise\ndomain knowledge or better fit with the retriever.\nRETRO [36] fixes the parameters of the retriever and\nuses the chunked cross attention mechanism in the gen-\nerator to combine the content of the query and retriever.", "start_char_idx": 8563, "end_char_idx": 9028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3307b03b-7942-4e2b-a3ea-a083b2c85f81": {"__data__": {"id_": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "fc1b0850-0803-4987-819c-9783a3f90423", "node_type": "1", "metadata": {}, "hash": "52dcc586282e53f750e2d52165b05aa09ebfdcc29130c111e9d13d6da7efed97", "class_name": "RelatedNodeInfo"}, {"node_id": "dafdee97-421f-4159-ad03-fe6c19162347", "node_type": "1", "metadata": {}, "hash": "467ce2cfea16d32515fbb933ab698db768319784ad6728a65da72c4b035c3614", "class_name": "RelatedNodeInfo"}, {"node_id": "0980800e-b33e-4cc1-8f7c-f0de094be475", "node_type": "1", "metadata": {}, "hash": "a39804cca309068ef58df145563ebb1e520161906152f46fd8c16f25e9b14dbc", "class_name": "RelatedNodeInfo"}, {"node_id": "9732bb2b-0e9f-4259-9387-38c9d98380ce", "node_type": "1", "metadata": {}, "hash": "dd4eaeb8530d58c6052726e28c80215adfbfaf9adb87280195fee778be217929", "class_name": "RelatedNodeInfo"}, {"node_id": "70523c54-2fb7-42c6-ba94-f375b1199c89", "node_type": "1", "metadata": {}, "hash": "6104f65f0b7e32c1e344b3671c76d399384544d7ca92905d6fde656828695bb6", "class_name": "RelatedNodeInfo"}]}, "text": "APICoder [129] finetunes the generator CODEGEN-MONO\n350M [205] with a shuffled new file combined with API\ninformation and code blocks. CARE [160] first uses image\ndata, audio data and vedio-text pairs to train encoders and then\nfinetune the decoder (generator) with the target of decreas-\ning caption loss and concept detection loss together, during\nwhich the encoders and the retriever are frozen. Animate-A-\nStory [206] optimizes the video generator with image data, and\nthen finetunes a LoRA [207] adapter to capture the appearance\ndetails of the given character. RetDream [50] finetunes a LoRA\nadapter [207] with the rendered images.\n4)Result Enhancement :In many scenarios, the result of\nRAG may not achieve the expected effect, and some tech-\nniques of Result Enhancement can help alleviate this problem.\na)Output Rewrite :Output Rewrite refers to rewriting\nthe content generated by the generator in certain scenarios to\nmeet the needs of downstream tasks.\nSARGAM [208] refines outputs in code-related tasks by em-\nploying a special Transformer alongside Deletion, Placeholder,\nand Insertion Classifiers to better align with the real-world\ncode context. Ring [209] obtains diversity results by reranking\ncandidates based on the average of per token log probabilities\nproduced by the generator. CBR-KBQA [54] revises the result\nby aligning generated relations with those presented in the\nlocal neighborhood of the query entity in knowledge graph.\n5)RAG Pipeline Enhancement :RAG Pipeline Enhance-\nment refers to optimizing the processes of RAG at the system\nlevel in order to achieve better performance results.\na)Adaptive Retrieval :Some studies and practices on\nRAG have shown that retrieval is not always beneficial for\nthe final generated results When the parameterized knowledge\nof the model itself is sufficient to answer relevant questions,\nexcessive retrieval will cause resource waste and may increase\nthe model\u2019s confusion. Therefore, in this chapter, we will\ndiscuss two types of methods for determining whether to\nretrieve, named rule-based and model-based methods.11\nRule-based: FLARE [210] actively decides whether and\nwhen to search through the probability in the generation pro-\ncess.", "start_char_idx": 0, "end_char_idx": 2205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5665018f-a756-44be-adc2-ce77d463ad0e": {"__data__": {"id_": "5665018f-a756-44be-adc2-ce77d463ad0e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "925a1d28-1f61-4d13-94eb-e10d7f7f9962", "node_type": "1", "metadata": {}, "hash": "d52b32725149599eed9c76892dd18d75f2eb6828ef8aaf04800546d3aef20934", "class_name": "RelatedNodeInfo"}, {"node_id": "b8e0d74c-77c8-4bd1-b552-ef3c563f3ed8", "node_type": "1", "metadata": {}, "hash": "d331139405b2b3aa94378e91d12e884de0c7bd59668c3b58f4c4037fd6e98ae0", "class_name": "RelatedNodeInfo"}, {"node_id": "ae23095a-71b8-4464-861a-106ac4195c21", "node_type": "1", "metadata": {}, "hash": "4046189438dd0b65201626d4c183cf413ec4c2243c69c03f6ff4db08913e77ae", "class_name": "RelatedNodeInfo"}, {"node_id": "23e2260e-43be-456f-912d-25e99d0379cb", "node_type": "1", "metadata": {}, "hash": "6c8ece06ce7d893fb8d466ca1e06e05e2885d9cce414d8d0a520a418c378c272", "class_name": "RelatedNodeInfo"}, {"node_id": "f14e060e-bf7d-4077-af80-b992fe7858d5", "node_type": "1", "metadata": {}, "hash": "eb8d287dde1768a41557294123f9b85b41ae13b7c91f2a091df7b33e9be9cb7c", "class_name": "RelatedNodeInfo"}]}, "text": "Efficient-KNNLM [38] combines the generation proba-\nbility of KNN-LM [37] and NPM [162] with a hyperparameter\n\u03bbto determine the proportion of generation and retrieval.\nMallen et al. [211] used statistical analysis on questions to\nenable direct answers for high-frequency ones and applied\nRAG for low-frequency ones. Jiang et al. [212] studied Model\nUncertainty, Input Uncertainty, and Input Statistics to compre-\nhensively assess the confidence level of the model. Ultimately,\nbased on the confidence level of the model, a decision is made\nwhether to retrieve. Kandpal et al. [213] studied the correlation\nbetween the number of relevant documents and the model\u2019s\nknowledge mastery to assess the need for retrieval.\nModel-based: Self-RAG [126] uses a trained generator to\ndetermine whether to perform a retrieval based on the retrieve\ntoken under different instructions, and evaluates the relevance\nand level of support of the retrieved text through the Self-\nReflection token. Finally, the quality of the final output result\nis evaluated based on the Critique token. Ren et al. [214]\nused \u201dJudgment Prompting\u201d to determine whether LLMs can\nanswer relevant questions and whether their answers are\ncorrect or not, thereby assisting in determining the necessity of\na retrieval. SKR [215] uses the ability of LLMs themselves to\njudge in advance whether they can answer the question, and\nif they can answer, no retrieval is performed. Rowen [216]\nemploys a model as a sophisticated multilingual detection\nsystem to evaluate the semantic coherence of answers to\nidentical questions posed across various languages. In the\nevent of detected inconsistencies, it decides to retrieve external\ninformation, thereby enhancing the reasoning process and\nrectifying inaccuracies. Conversely, when responses exhibit\nconsistency, the system upholds the initially generated answer,\nwhich is derived from internal reasoning. AdaptiveRAG [217]\ndynamically decides whether to retrieve based on the query\ncomplexity by a classifier, which is a smaller LM.\nb)Iterative RAG :Iterative RAG progressively refines\nresults by repeatedly cycling through retrieval and generation\nphases, rather than a single round.", "start_char_idx": 2206, "end_char_idx": 4390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7471598-449f-4ce6-870e-1a9e120e6ccd": {"__data__": {"id_": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "33406e86-99a8-48de-89db-58f1a36281d2", "node_type": "1", "metadata": {}, "hash": "36d9c719a9e79b3429e76ec2a0d8446b95d959e22e7b613ba4c6f2234fba3f75", "class_name": "RelatedNodeInfo"}, {"node_id": "a4e3f0d5-3d58-4b4a-aefe-64c3b61670ac", "node_type": "1", "metadata": {}, "hash": "20ffce0a9e3e563045a7d3dd94c07e8e0a3941022a3742d6cdaea30cd5709511", "class_name": "RelatedNodeInfo"}, {"node_id": "3c090fe5-243a-41c0-8129-aae9e995378d", "node_type": "1", "metadata": {}, "hash": "95b544fb71173001db2cef997e59cd39d831c936b7e0e5c1bc86f4c93d491941", "class_name": "RelatedNodeInfo"}, {"node_id": "3984af1d-e971-46af-8596-695647deb311", "node_type": "1", "metadata": {}, "hash": "813624b36a4b9f9d5ca140283c54afe415e12d6788b37cfd343417e9702315a5", "class_name": "RelatedNodeInfo"}, {"node_id": "0abdbd95-fcbf-4913-89d8-f527efd2400e", "node_type": "1", "metadata": {}, "hash": "8a4500445f9cd24183393f9bc6371095490678d0e7fff6432a5052e2eb591e8e", "class_name": "RelatedNodeInfo"}]}, "text": "RepoCoder [218] employs an iterative retrieval-generation\npipeline for code completion tasks, enhancing each retrieval\nquery with code generated in prior iterations to more ef-\nfectively leverage information dispersed across various files,\nthereby achieving superior results. ITER-RETGEN [219] syn-\nergizes retrieval and generation in an iterative manner. The\ncurrent output of the generator can to some extent reflect\nthe knowledge it still lacks, and the retrieve can retrieve the\nmissing information as contextual information for the next\nround, which helps to improve the quality of the generated\ncontent in the next round. SelfMemory [220] employs a\nretrieval-augmented generator in an iterative manner to create\nan unlimited memory pool. Following this, a memory selector\nis used to choose one output, which then serves as the\nmemory for the subsequent generation round. RAT [221] initial\ngenerates content by an LLM with a zero-shot CoT prompt,\nthen revises each thought step by retrieving knowledge from\nexternal knowledge base.IV.APPLICATIONS\nIn this section, we focus on RAG applications spanning\nvarious modalities. To echo with the taxonomy of RAG\nfoundations and enhancements, we also demonstrate their\nutilization across different tasks in Table I.\nA.RAG for Text\nTo begin with, text generation is among the most important\nand widely deployed applications for RAG. Here we introduce\npopular works for seven tasks, respectively.\n1)Question Answering :Question Answering involves the\nprocess of providing responses to posed questions by drawing\nfrom a vast and comprehensive collection of textual sources.\nFiD [35] and REALM [33] identify the top-k most pertinent\narticle snippets based on the query and forward each snippet\nalong with the question to LLMs to generate k responses.\nThese responses are then synthesized into a final answer.\nToutanova et al. [222] substituted the text corpus in REALM\nwith subgraphs from a knowledge graph, yielding impressive\nresults. As shown in Fig. 5, RETRO [36] employs attention\nmechanisms to integrate the question with relevant retrieved\ndocuments within the model to produce the final answer.\nSKR [215] observes that using RAG does not invariably\nbenefit Question Answering and thus explored guiding the\nmodel to evaluate its grasp of pertinent knowledge, subse-\nquently adapting its use of external resources for retrieval\nenhancement.", "start_char_idx": 4391, "end_char_idx": 6780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51582689-3bf2-4ba0-b051-10e732354bda": {"__data__": {"id_": "51582689-3bf2-4ba0-b051-10e732354bda", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f", "node_type": "1", "metadata": {}, "hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "6e6fced2-1c4c-44c4-8d7c-2f6fdb25a3a0", "node_type": "1", "metadata": {}, "hash": "1cd39309c519ca0bb144895b03e1a94d3ebd40904ed4fbf72e830e0d774387df", "class_name": "RelatedNodeInfo"}, {"node_id": "f9f5e60a-8be2-4228-8bfa-5d35ca444603", "node_type": "1", "metadata": {}, "hash": "504dcbdb20a5665898226c9c99137af0d19f8890826960d9803bc9ba1363b173", "class_name": "RelatedNodeInfo"}, {"node_id": "9f04f21a-584f-4e6c-9df3-42e9a2f38966", "node_type": "1", "metadata": {}, "hash": "c0793e2ea7ea8fbc6a54769f1fc5a656745c62a2f75d8610db2cf81d8d69d9e8", "class_name": "RelatedNodeInfo"}, {"node_id": "f37a12ea-f7e2-4d3b-8761-660a17587794", "node_type": "1", "metadata": {}, "hash": "043521c99d65eafb211c8d0f1c6a14d092a9fe335c06bd269e35a2e14504f6e4", "class_name": "RelatedNodeInfo"}, {"node_id": "99734d75-ed29-4007-a8a8-dc4f79d9e8b6", "node_type": "1", "metadata": {}, "hash": "ce7095d7b065f243de29d6e4b7a1ab794598f777056463ae1af47a9f761c9e97", "class_name": "RelatedNodeInfo"}]}, "text": "TOG [223] introduces an innovative knowledge\ngraph-augmented LLM framework, which excels by fostering\ninteractions between LLMs and the Knowledge Graph and\nby expanding the inference path space with beam search.\nNPM [162] pioneers the use of nonparametric data distribu-\ntions in lieu of the softmax layer, enabling models with fewer\nparameters to perform effectively. Self-RAG [126] improves\nanswer quality by learning to discern when to retrieve, as-\nsess the retrieved content\u2019s relevance, and evaluate the final\ngenerated results using four types of reflective tokens. CL-\nReLKT [224] employs a language-generalized encoder to\nbridge the gap between question-document pairs across lan-\nguages, thus better leveraging multilingual data. CORE [225]\nmitigates language resource disparities by introducing a novel\ndense passage retrieval algorithm and a multilingual autore-\ngressive generation model. Lastly, EAE [156] enhances answer\nquality by retrieving entity embeddings for query entities and\nintegrating these with hidden states for further processing. UR-\nC C A F FW T r a ns f ormer \nEnc o der \nR etrie v al \nd ataset \nFrozen kNN Retriever \nKV\nRETR O b lo c k ( x L ) N eig hb o ur s \nIn p ut \nt o k ens Ch unk ed cr oss-att en tion ( C C A ) \nB ER T B ER T \nCondition \nA tt ending c h unk s Enc o ded neig hb o ur s \nC A \nC A \nA T T N Q \nEMB REA D A tt end Enc o ded neig hb o ur s \nC1 \nC2 \nC3 H1 \nH2 \nH3 \nHH1+ \nH2+ E1E2E1\nE2\nCA (H1+, E1) \nCA (H2+, E2) \nCCA (H, E) \nX\nFig. 5: Architecture of RETRO [36] model.\nQA [226] found that when encountering unseen problems,\nretrieving QA pairs has a better final effect; When encoun-\ntering problems that have not been seen before, the retrieve12\nTABLE I: Taxonomy of RAG applications across various modalities.", "start_char_idx": 6781, "end_char_idx": 8543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdd04060-a8e4-474c-a338-f65f89616e95": {"__data__": {"id_": "cdd04060-a8e4-474c-a338-f65f89616e95", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ab870226-7a14-42b6-9fee-e893c73c5cd4", "node_type": "1", "metadata": {}, "hash": "78c170cea536e40744c67d3b9109c2452b044dff94ef0b21a9def3ab450354a2", "class_name": "RelatedNodeInfo"}, {"node_id": "90c41dad-e5cd-4732-9250-448c021e4383", "node_type": "1", "metadata": {}, "hash": "b67465ae3f47b8bf0c6d2bca74caac653135d04ec00f72af68ba3ddb7e4c3336", "class_name": "RelatedNodeInfo"}, {"node_id": "15ad2d42-ba2f-4e70-9b47-27a0ddbd9339", "node_type": "1", "metadata": {}, "hash": "b55021b3e46f6715b5b90741fad6a2b7215d07eeb6ad9db40a49c0d2723be1a1", "class_name": "RelatedNodeInfo"}, {"node_id": "47cdba86-01b2-4134-8ce2-89a9a558fc6f", "node_type": "1", "metadata": {}, "hash": "2c759c4e1d4bbf1f31cba44d9dfb074ac0aea35af15e734dd637449c7d92d5aa", "class_name": "RelatedNodeInfo"}, {"node_id": "803a9f00-6ef8-488e-b428-cda0027f4d4f", "node_type": "1", "metadata": {}, "hash": "df59a9100bbecd3559fd871a13502bbc4f7ff2933247da740fa2d2e10942fedf", "class_name": "RelatedNodeInfo"}, {"node_id": "3a7c4090-0eb8-406d-97aa-727111851701", "node_type": "1", "metadata": {}, "hash": "b8282c2ad2503a6110ab799e5ca9f170c077d49ebd84e55d2f696c5eefad8916", "class_name": "RelatedNodeInfo"}, {"node_id": "2ae6e793-db56-4b3f-a68e-609335a5b269", "node_type": "1", "metadata": {}, "hash": "4cbbdb588e4bc63d20bacfa8b9f4b2e14b1c191853f00a6f282c33a9906b8050", "class_name": "RelatedNodeInfo"}]}, "text": "RAG for Text\nQuestion Answering Human-Machine Conversation Neural Machine Translation Summarization Others\nREALM\u2021\u00a7TKEGEN\u00a7TOG\u2021\nSKR\u00a7\u00b6Self-RAG\u00a7\u00b6RIAG\u2021\nFiD\u2021\u00a7RETRO\u00a7NPM\u2021\u00a7CREA-ICL\u2020\u2021BlenderBot3\u2021\u00a7\nCEG\u2021\u2225Internet-Augmented-DG\u2021\u00a7\nConceptFlow\u2021\u00a7Skeleton-to-Response\u2021\u00a7NMT-with-Monolingual-TM\u2020\u2021\u00a7\nTRIME\u2021\u00a7KNN-MT\u2021\u00a7COG\u2021RAMKG\u2021\u00a7RPRR\u2021RIGHT\u2021\u00a7\nUnlimiformer\u00a7CONCRETE\u2021\u00a7Atlas\u2021\u00a7\nKG-BART\u2021\u00a7R-GQA\u2021\u00a7\nRAG for Code\nCode Generation Code Summarization Code CompletionAutomatic\nProgram RepairText-to-SQL and Code\n-based Semantic ParsingOthers\nSKCODER\u00a7RRGCode\u2021\nARKS\u2020\u00b6RECODE\nKNN-TRANX\u2225Toolcoder\u00a7\u2225RACE\u2020BASHEXPLAINER\u2021\nREADSUM\u2225Rencos\u2021\nCoRec\u2021Tram\u00a7\nEDITSUM\u2021ReACC\u2020\u2021RepoCoder\u2020\u00a7\u00b6\nDe-Hallucinator\u00b6REPOFUSE\u00a7\nRepoFusion\u00a7EDITAS\u00a7RING\u2225CEDAR\u00a7\nRAP-Gen\u2021\u00a7InferFix\u00a7\nSARGAM\u00a7RTLFixer\u2021\u00a7XRICL\u2021\u00a7SYNCHROMESH\u2021\u00a7\nRESDSQL\u00a7REFSQL\u2021\u00a7\nCodeICL\u00a7MURRE\u2225\u00b6De-fine\u2021\u2225Code4UIE\u00a7\nE&V StackSpotAI\u2021\u00a7\nImputBlaster\u00b6\nRAG for Knowledge RAG for 3D\nKnowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others", "start_char_idx": 0, "end_char_idx": 927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69604801-7765-4976-81e7-231241d2b2b1": {"__data__": {"id_": "69604801-7765-4976-81e7-231241d2b2b1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "df0e1575-9ac0-4490-b8af-d43a8fa606ef", "node_type": "1", "metadata": {}, "hash": "7716787fa54deb6c06c7d2a449a5b31aa8e5d0fbb5f83f9e61a9d25686773564", "class_name": "RelatedNodeInfo"}, {"node_id": "a1409477-4e3c-4538-96b0-ad47279e810e", "node_type": "1", "metadata": {}, "hash": "43254f2cf24047a828eaa2bb62bc3f66ac4bfe60859c81894dacc2d90e40195d", "class_name": "RelatedNodeInfo"}, {"node_id": "f3d3599d-8950-4295-8694-79a0ddfd1125", "node_type": "1", "metadata": {}, "hash": "163c8055f315b311ed6fc8826b9f1e630fd0e4ec87eef7dcbc2f13e736e84693", "class_name": "RelatedNodeInfo"}, {"node_id": "83ae85ec-a12a-4af1-8b75-888ba8a4b1a0", "node_type": "1", "metadata": {}, "hash": "339b3412669c8da9f7837d1486cb78c4dc2a3fef96f753fa62d8cfc9a67d09bf", "class_name": "RelatedNodeInfo"}, {"node_id": "180aee7f-e742-4903-bab5-cfce033587e9", "node_type": "1", "metadata": {}, "hash": "2a7534500517c93f62de8b2a9f286b9f1d1058af6de3e12d2684a169447aaf39", "class_name": "RelatedNodeInfo"}, {"node_id": "51619bcb-760b-4af8-8b26-ad2d6635661d", "node_type": "1", "metadata": {}, "hash": "15683b26837fdf389ee09e1d95478661de799df79ec01184a2e3e4a963084234", "class_name": "RelatedNodeInfo"}]}, "text": "for 3D\nKnowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others Text-to-3D\nCBR-KBQA\u2021\u00a7\u2225TIARA\u2020\u2021\u00a7Keqing\u2020\u2021\u00a7\nRNG-KBQA\u2021\u2225ReTraCk\u00a7SKP\u2020\u2021\u00a7UniK-QA\u2020\u2021KG-FiD\u2021GRAPE\u2021\nSKURG\u2020\u2021KnowledGPT\u2021EFSUM\u00a7EfficientQA\u2021CORE\u00a7Convinse\u2020\u2021\nRINK\u2021\u00a7T-RAG\u2021\u00a7StructGPT\u2021GRetriever\u00a7SURGE\u00a7\nK-LaMP RHO\u2225ReMoDiffuse\u2020\u2021\nAMD\u2020\nRAG for Image RAG for Video\nImage Generation Image Captioning Others Video Captioning Video QA & Dialogue Others\nRetrieveGAN\u2021IC-GAN\u00a7Re-imagen\u00a7\nRDM Retrieve&Fuse\u00a7KNN-DiffusionMA\u2225REVEAL\u2021SMALLCAP\u2020\nCRSR\u2020RA-TransformerPICa\u2225Maira\u2021\nKIF\u2021RA-VQA\u2021KaVD\u2021\u00a7R-ConvED\u2021\u00a7\nCARE\u00a7\nEgoInstructor\u2020\u2021\u00a7MA-DRNN\u2020\u2021R2A\u2021\nTvqa+\u00a7VGNMN\u2021VidIL\u2020\u2021RAG-Driver\u2021\nAnimate-A-Story\u2020\u00a7\nRAG for Science RAG for Audio\nDrug Discovery Biomedical Informatics Enhancement Math Applications Audio Generation Audio Captioning\nRetMol\u2020\u00a7PromptDiff\u2020\u2021PoET\u2021Chat-Orthopedist\u2020\u00a7BIOREADER\u2020MedWriter\u2021QARAG\u2020\u2021LeanDojo\u2021RAG-for-math-QA\u2020\u2021Re-AudioLDM\u00a7Make-An-Audio\u2020\u00a7RECAP\u2021\u00a7\nQuery-based Latent-based Logit-based Speculative\n Query+Latent\n Latent+Logit \u2020Input \u2021Retriever \u00a7Generator \u2225Output \u00b6Pipeline\ntext chunk performs better. Therefore, it is proposed to simul-\ntaneously retrieve QA pairs and text chunks, and select the\nfinal answer by comparing the calibrated confidences.", "start_char_idx": 848, "end_char_idx": 2040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54f4b631-38f8-41b7-98e8-f94f97d23d56": {"__data__": {"id_": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "31f544b8-fc3e-4aab-aa56-68a1f20f7e14", "node_type": "1", "metadata": {}, "hash": "5ac33e121490e6ec757c4d927dd6662d4f814884a7df62c28d1831e088aae63b", "class_name": "RelatedNodeInfo"}, {"node_id": "1047af7a-6186-49ad-a0f0-9b3504bc95d4", "node_type": "1", "metadata": {}, "hash": "03ac4bdba0fccb257111d37163e87ab7e6738774bd3d9d68ee4000f7b4c9293f", "class_name": "RelatedNodeInfo"}, {"node_id": "fbfb8209-388f-4094-a987-e33a058a0252", "node_type": "1", "metadata": {}, "hash": "7580ff928a48ce4f189bfcede1f0d1ef844838f4366a06d5cc23f477931b7b9c", "class_name": "RelatedNodeInfo"}, {"node_id": "16b516cf-f294-4811-84de-614fd3732235", "node_type": "1", "metadata": {}, "hash": "6e7007578125968a72a37271e7a6ceb5d3ce393a0f4f98dad3fffe1f2ab7e942", "class_name": "RelatedNodeInfo"}, {"node_id": "a664023c-ea54-4d03-805e-474d4063dd8b", "node_type": "1", "metadata": {}, "hash": "4bed89afca62165b8da4bb71e49f0f07a808396c31d8b0276ca3a736e1312d87", "class_name": "RelatedNodeInfo"}]}, "text": "DISC-\nLawLLM [227] constructs a supervised fine-tuning dataset\nthrough a legal syllogism prompting strategy, enabling the\nmodel to receive support from the latest legal information.\nRAG-end2end [228] conducts simultaneous training of the\nretriever (DPR) and the generator (BART) to optimize per-\nformance for the end-to-end question-answering task and to\nfacilitate domain adaptation. MultiHop-RAG [229] is designed\nto extract pertinent information from a variety of distinct\ndocuments, aggregating this knowledge to equip the generator\nwith the necessary context for producing the definitive answer\nto the query.\n2)Fact Verification :Fact Verification involves assessing\nthe veracity of information, a critical function in disciplines\nsuch as Natural Language Processing (NLP), Information Re-\ntrieval, and Data Mining. In today\u2019s digital era, characterized\nby an exponential increase in data, particularly across social\nmedia and online news platforms, there is a rapid proliferation\nof unchecked information. Fact verification plays an essential\nrole in countering the spread of fake news, deceptive content,\nand rumors, thereby preserving the integrity of the information\nlandscape and ensuring the public\u2019s access to accurate knowl-\nedge. Consequently, automated fact verification systems are of\nimmense importance, with broad applications and significant\npractical value. CONCRETE [230] leverages cross-lingual\nretrieval mechanisms to tap into a wealth of multilingual evi-\ndence, effectively bridging the gap in resources for languages\nthat are underrepresented in fact-checking datasets. Hagstr \u00a8om\net al. [231] proved on LLaMA [4] and Atlas [30] that search\naugmentation is more beneficial for solving inconsistencyproblems than increasing model size. Atlas [30] shows that\nusing RAG to support LLMs in knowledge-intensive tasks\nmarkedly improves their few-shot learning performance.\n3)Commonsense Reasoning :Commonsense Reasoning\nentails the capability of machines to infer or make decisions on\nproblems or tasks in a human-like manner, drawing upon their\nacquired external knowledge and its application. However, the\nvast scope of common sense knowledge and the intricacies of\nreasoning processes make Commonsense Reasoning a peren-\nnially challenging and prominent area of research within the\nfield of NLP.", "start_char_idx": 2041, "end_char_idx": 4359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a77345a-c539-4264-a387-adf9022090e0": {"__data__": {"id_": "4a77345a-c539-4264-a387-adf9022090e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4884c63-ab65-499b-a999-c681de0dfcc4", "node_type": "1", "metadata": {}, "hash": "46bd8a4fc3ed4d3290e7003f3407af8e4d2db97749eb64131085bf710c8ab18a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "418fc8e2-aaf2-40b9-9c67-7e349f169503", "node_type": "1", "metadata": {}, "hash": "8bd99ac6d990dab535934da58469b133cb5cba145e84af7c874cf83adcf7d119", "class_name": "RelatedNodeInfo"}, {"node_id": "de618738-3540-432b-9a94-c7d107d99047", "node_type": "1", "metadata": {}, "hash": "346cabd965f44a84879007ecb97d4e84c0ae3c6680bbc62578612e8dc42de8ea", "class_name": "RelatedNodeInfo"}, {"node_id": "9072dec7-70eb-4adf-a06d-8419b381e147", "node_type": "1", "metadata": {}, "hash": "c8e4004b6301fbcc0dde13bd4a98401cbece0a56d8f1f345bd9c28dacb19dd77", "class_name": "RelatedNodeInfo"}, {"node_id": "f4debacb-72ef-420b-80cb-f38869c8f88a", "node_type": "1", "metadata": {}, "hash": "abcdabb421f7aa40586529b8925a82e8e298ae6c056de319b7994e49ae8c6c07", "class_name": "RelatedNodeInfo"}, {"node_id": "8aa981c8-f72d-4180-b9f8-c1019163280e", "node_type": "1", "metadata": {}, "hash": "fc37b98df62da7972342b729efc3faec48860c91aac19ede52ac6c18a908c3d5", "class_name": "RelatedNodeInfo"}]}, "text": "KG-BART [232] expands the conceptual land-\nscape by incorporating intricate interrelations among diverse\nconcepts within a knowledge graph. It employs graph attention\nmechanisms to aid LLMs in crafting more nuanced and\nlogically coherent sentences. This approach not only improves\nthe models\u2019 generalization capabilities but also significantly\nbolsters their Commonsense Reasoning proficiency. Wan et\nal. [233] constructed the CONFLICTINGQA dataset, compris-\ning contentious questions and conflicting answer documents,\nto examine which textual features significantly influence LMs\u2019\nability to independently navigate controversial issues. The\nfindings reveal that LMs often neglect the stylistic aspects of\ntext that are typically valued by people.\n4)Human-Machine Conversation :Human-Machine Con-\nversation encompasses the ability of machines to comprehend\nnatural language and adeptly employ this skill to engage with\nhumans seamlessly. This capability represents a significant\nchallenge within the realms of Artificial Intelligence and Natu-\nral Language Processing and offers a broad spectrum of practi-\ncal applications. As such, Human-Machine Conversation con-\ntinues to be a focal point of research for many scholars. Con-\nceptFlow [234] leverages a commonsense knowledge graph to13\nstructure conversations, directing the flow of dialogue based on\nattention scores, and propelling the conversation forward. This\nmethod achieves commendable results even with a substantial\nreduction in model parameters. Cai et al. [235] reimagined the\ntext generation task as a cloze test by retrieving and distilling\nthe essence of past conversational history, leading to notable\noutcomes. Komeili et al. [236] augmented dialogue generation\nquality by harnessing advanced search engine technologies to\nsource pertinent content from the internet. BlenderBot3 [237]\nbroadens its search horizon, not only mining relevant internet\ncontent but also local dialogue history, and employs entity\nextraction among other techniques to refine the quality of the\nresulting dialogue. Kim et al. [238], PARC [239], and CREA-\nICL [240] improve the caliber of non-English conversations by\nincorporating cross-lingual knowledge, effectively addressing\nthe scarcity of non-English datasets and enhancing the quality\nof the generated dialogue.", "start_char_idx": 4360, "end_char_idx": 6673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4884c63-ab65-499b-a999-c681de0dfcc4": {"__data__": {"id_": "c4884c63-ab65-499b-a999-c681de0dfcc4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d2915bcf-d475-4bab-9409-6cf973c97c17", "node_type": "1", "metadata": {}, "hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "72b96e4e-d8af-49f6-b47b-1a08f90868d5", "node_type": "1", "metadata": {}, "hash": "bc936310148fb4e9f57248298e2722518f33232cd3df7d593ebfa3209a9a9006", "class_name": "RelatedNodeInfo"}, {"node_id": "54b25020-b744-4a8e-b370-c1ebce85705e", "node_type": "1", "metadata": {}, "hash": "956ab1da27c305e031d2bf652135d4845fafe3fcc02ce354f743d93098361d67", "class_name": "RelatedNodeInfo"}]}, "text": "CEG [241] addresses hallucination\nissues through a post-processing mechanism, verifying LLM-\ngenerated answers through retrieval. If the answer is correct,\nthe retrieved document is added to the original answer as a\nreference; if the answer lacks reliable references, it guides the\nLLM to respond to the question anew.\n5)Neural Machine Translation :Neural Machine Transla-\ntion (NMT) is the automated process of translating text from\na source language to a target language [161], [242], [243].\nIt is a pivotal task in the domain of NLP and represents a\nsignificant objective in the pursuit of AI, boasting considerable\nscientific and practical significance. Cai et al.", "start_char_idx": 6674, "end_char_idx": 7342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3acbe4b-37d3-4ad7-8129-81790271bbfc": {"__data__": {"id_": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b9d7ac54-078c-45bd-b1d4-8dfa6dbf170a", "node_type": "1", "metadata": {}, "hash": "d9647231a697ca7bf65e464ed7daff9f0dd2cb2f8b7ba598fbac015a7d253747", "class_name": "RelatedNodeInfo"}, {"node_id": "e1e5279b-e384-4417-b2cb-d9c34616ccca", "node_type": "1", "metadata": {}, "hash": "0aee4a40af52680ca585b1560c0397b012a3d8dbab10a215160f651bfc48b9ab", "class_name": "RelatedNodeInfo"}, {"node_id": "2d1e4e84-60a7-432c-bca6-5189663bcf12", "node_type": "1", "metadata": {}, "hash": "62aa6443caf0f05b663198b701b18cfb9c192b06cf974c870856ec6d7ba4e841", "class_name": "RelatedNodeInfo"}, {"node_id": "bbbda80d-54fa-48fe-a793-c1fb0f178cd6", "node_type": "1", "metadata": {}, "hash": "1312862de7181e050df710916f0cd6a8bb576b5fc5baa7affd058ef99a3bcef9", "class_name": "RelatedNodeInfo"}, {"node_id": "d05fe151-9ada-49db-8ce4-4086284632a5", "node_type": "1", "metadata": {}, "hash": "afde5d25f136883ae4af9cb82655fc3ab85d84e0468f47fed884c00dac997db9", "class_name": "RelatedNodeInfo"}]}, "text": "Cai et al. [242] proposed\nan innovative approach that utilizes monolingual corpora\nalongside multilingual learning techniques, challenging the\ntraditional dependency on bilingual corpora in Neural Machine\nTranslation. This approach ensures that the retrieval system\nprovides ample information while simultaneously optimizing\nboth the retrieval mechanism and the translation model, cul-\nminating in impressive performance. KNN-MT [243] executes\ntranslation tasks at the token level by computing vector space\ndistances. TRIME [161] effectively minimizes the discrepancy\nbetween training and inference phases by jointly training the\nretrieval system and the generation model, thereby enhancing\nthe precision of translations.\n6)Event Extraction :Event Extraction is a specialized\ntask within Natural Language Processing (NLP) that focuses\non pinpointing and extracting instances of particular event\ntypes from unstructured textual data. An event is generally\ncharacterized by a central action or predicate and the related\nentities, which can include participants, temporal indicators,\nlocations, and other relevant attributes. The objective of event\nextraction is to convert the nuanced details embedded within\ntext into a structured format, thereby facilitating advanced\nanalysis, efficient information retrieval, and practical down-\nstream applications. R-GQA [244] employs a retrieval-based\napproach to enhance the context of a given issue by identifying\nand utilizing the most closely aligned Question-Answer pair\nfrom a repository, thereby enriching the information available\nfor processing the current query.\n7)Summarization :In the realm of NLP, Summarization\nis a task aimed at distilling the essential information from\nlengthy texts and producing a concise, coherent summary thatencapsulates the primary themes. Summarization enables users\nto quickly grasp the essence of a text, thereby conserving\ntime that would otherwise be spent on reading extensive\nmaterial. There are two main approaches to Summarization:\nExtractive and Abstractive. Extractive Summarization involves\nthe automatic selection and compilation of key phrases directly\nfrom the source text. A key phrase succinctly captures the\nmain themes, content, or perspectives of the text and is\ntypically composed of one or several words. The generation\nof key phrases is instrumental for understanding, categorizing,\nretrieving, and organizing textual information. It is extensively\napplied in fields such as search engine optimization, aca-\ndemic research, text summarization, and more.", "start_char_idx": 0, "end_char_idx": 2553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2": {"__data__": {"id_": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8d2dd5cc-61b0-40b5-917f-d7371b591334", "node_type": "1", "metadata": {}, "hash": "2ce498d4d0e77d4733c9ea7455880dccf40e9cda6a38dc8b93680100a80a6b81", "class_name": "RelatedNodeInfo"}, {"node_id": "e4876034-8e45-4364-b80d-3e066f5fe57f", "node_type": "1", "metadata": {}, "hash": "4356bb4631422f6bbb91bf37c5e6f44f5cc87bb6a1b05cdce5b76001a1cacb5d", "class_name": "RelatedNodeInfo"}, {"node_id": "a6fba84e-9aff-44df-8957-f463106d4046", "node_type": "1", "metadata": {}, "hash": "0f0d29e362cc918b3699de19f12832cc74568e11534b19f3573a7b67dc56c467", "class_name": "RelatedNodeInfo"}, {"node_id": "1cc459b3-d325-40e8-9cd2-2a80a7a7f523", "node_type": "1", "metadata": {}, "hash": "93c3fc9d2429ef0ab639ab0294b5355fc54830c4a7f05ac63c97601df9c9750c", "class_name": "RelatedNodeInfo"}, {"node_id": "dfabe07a-608f-4828-813d-97c83b376819", "node_type": "1", "metadata": {}, "hash": "a7ef90141bb9fa66607f9921cd3cfc59efd5174dc8ca34beb3eb67045ac9b304", "class_name": "RelatedNodeInfo"}]}, "text": "This technique\nrefrains from creating new sentences, instead repurposing\nsegments from the original text. Abstractive Summarization,\non the other hand, entails comprehending the original text\u2019s\nmeaning and reformulating it into new sentences [153], [245]\u2013\n[247]. This approach can convey the source\u2019s intent more\nfluidly but poses greater challenges in terms of implementation\ndue to its complexity. RAMKG [245] effectively leverages a\ncomprehensive English corpus to bolster the performance of\nKeyphrase Generation in non-English contexts. It does so by\nenhancing the alignment of keywords extracted from texts in\ndifferent languages that share similar subject matter. Unlimi-\nformer [153] addresses the issue of input length constraints in\ntransformer-based models by retrieving and utilizing the top-\nk most relevant hidden states, thereby extending the model\u2019s\ncapacity to handle longer inputs. RPRR [246] employs a\nRetrieve-Plan-Retrieve-Read approach to overcome the limited\ncontext window constraints faced by LLMs, utilizing retrieved\ninformation to generate high-quality Wikipedia documents for\nemerging events. RIGHT [247] chooses to use different types\nof retrievers in different datasets to enhance the generator,\nwhich can effectively improve the quality of automatically\ngenerated labels in simple deployment.\nB.RAG for Code\nSeparate retrieval and generation approaches have histor-\nically been employed for code-related tasks. For retrieval,\nsimilar code snippets can be identified using Abstract Syntax\nTrees (AST) or text edit distance. For generation, sequence-\nto-sequence models are employed to generate code or natural\nlanguage. Recent RAG research combines both retrieval and\ngeneration techniques to enhance the overall performance.\n1)Code Generation :The goal of code generation is to\ntransform natural language (NL) descriptions into code im-\nplementation, which can be seen a process of text-to-code.\nTherefore, LSTM and transformer models are widely used for\ngenerator. Whether to use code-specific retrieval or text-based\nretrieval depends on the contents to be searched.\nRetrieval-based prompt engineering is one of the most\nprevalent scenarios of RAG in code generation. In-context\nlearning includes training samples in prompts as the input for\nsequence-to-sequence generative models. Retrieval techniques\nare adopted to find similar training samples to the test input,\nso that the prompt can be more informative and related.", "start_char_idx": 2554, "end_char_idx": 5009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2": {"__data__": {"id_": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "dbc3a6be-152f-4360-a086-e317b1979737", "node_type": "1", "metadata": {}, "hash": "6f3a016faa77dd4691e65c98393271ca6ba86b60449aa726a2fc9cf818696ef9", "class_name": "RelatedNodeInfo"}, {"node_id": "5206c7a8-aeeb-4e49-b845-ef739b76e3ce", "node_type": "1", "metadata": {}, "hash": "86f0e70f43fcd760c4dea692b88c797580efe42389a0cd517a13b26214324fb5", "class_name": "RelatedNodeInfo"}, {"node_id": "50e13e9b-86a6-4bb9-b538-3c314a67bc03", "node_type": "1", "metadata": {}, "hash": "60e8bc62a163e6eb837552784eafcfc5795233b38dd7113d28ab8c90aca2a87b", "class_name": "RelatedNodeInfo"}, {"node_id": "3a7ebacc-2913-49d8-9dd6-45bcfebe24f8", "node_type": "1", "metadata": {}, "hash": "01dbf15cc102e66fec9160d9dc1168ca57d55e49273ce0fc4bbfa91bf7735186", "class_name": "RelatedNodeInfo"}, {"node_id": "92637d5c-b17b-47a1-9264-bf112dbb6030", "node_type": "1", "metadata": {}, "hash": "1322a0b65c4691879655abbe54e364ee6e6059e1025e15c8a8db3bc14e1477a6", "class_name": "RelatedNodeInfo"}]}, "text": "REDCODER [40] retrieves similar NL descriptions using\ndense retriever CodeBert [25], then concatenates the NL texts,\ntheir paired codes, for downstream generator PLBART [41].14\nCodeT5Mix [248] adopts the paradigm of RECODER, propos-\ning a model that functions dually as both the retriever and\ngenerator. APICoder [129] first train a Bert-based deep re-\ntriever to align the embeddings of NL descriptions and API\ndocumentation; then, it retrieves relevant API information to\nbuild prompt for the generator CODEGEN-MONO [249]. The\nfollowing work [250] uses the same pipline, renaming the\nretriever module as APIFinder. COCOGEN [251] aims at com-\nmonsense reasoning, which generates code-based reasoning-\ngraphs with Codex [2] given NL inputs; it adds an evaluation\nsetting of dynamic prompt selection, which actually retrieves\nrelevant examples for prompts. In DocPrompting [42] given\nan NL intent, the retriver retrieves relevant documentations,\nthen the generator generates codes based on the NL and\nretrieved documents. It evaluates both sparse retrievers and\ndense retrievers, and also tries different generators in ex-\nperiments. CodeT5+ [252] adopts the CodeT5 [106] model\nfor both the retriever and generator, leveraging only the\nencoder part in the retrieval process. AceCoder [188] fixes the\nretriever to BM25 [19], and tests several LLM generators for\ncode generation. A3CodGen [253] extracts local information,\nretrieves relevant global functions using embeddings, and\nincorporates third-party library information to construct the\nprompt for LLM-based code generation. SKCODER [254]\nemploys BM25 to retrieve relevant code snippets, which are\nfurther processed to produce sketch template. The template and\nthe original description are concatenated for final generation.\nCodeGen4Libs [255] uses RAG for both import statements and\ncodes, employing BM25 as retriever and finetuned CodeT5\nas generator. CODEAGENT [256] design agents to search\non web, retrieve relevant documentation, generate programs,\nand test correctness.", "start_char_idx": 5010, "end_char_idx": 7038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a68a403-80ce-43dd-b2dc-b6a49addecdf": {"__data__": {"id_": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad848f9e-6e82-4e71-92f7-1a534a8a184b", "node_type": "1", "metadata": {}, "hash": "58b053e1b7137d45e355adb640ff6ae383be6bfb4865c6fe8d3ada623fe85b69", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ed5469d4-5c93-486d-81d8-c7f56922eb1b", "node_type": "1", "metadata": {}, "hash": "010731eb2a2146cf0b40c785666f86bbc1ff81fb0504fd2b4e274d1ae3b32008", "class_name": "RelatedNodeInfo"}, {"node_id": "206281fa-552c-4bf2-93dd-014d41c7ff14", "node_type": "1", "metadata": {}, "hash": "fc31ca62bd3062edad05cc9cc9db407f9164beea8f54fd3ce681065c4fde5f4a", "class_name": "RelatedNodeInfo"}, {"node_id": "4a7f8864-6fd6-44db-bca1-9b388792455b", "node_type": "1", "metadata": {}, "hash": "85a30c5a8222e21feeaa0fa966a1109023f1f7ba6e04f0b74644aca72475ea68", "class_name": "RelatedNodeInfo"}, {"node_id": "00ca55a6-99e0-4f3d-8c8d-cf2f350b020c", "node_type": "1", "metadata": {}, "hash": "a9b8c1396b5928d35ba73802312dd896c62fd1478d49dbaa2d59a80de63f8f0e", "class_name": "RelatedNodeInfo"}, {"node_id": "69d976e7-b806-444e-ba71-9e7eae0820b8", "node_type": "1", "metadata": {}, "hash": "2e5e42d92dbe93f4e08cfc902be01307824abc995499fa48db3ea761b070ca5d", "class_name": "RelatedNodeInfo"}]}, "text": "RRGCode [257] retrieves relevant code\nsnippets using both sparse and dense retrieval, employs a\ncross-encoder to re-rank the retrieval results, then generates\ncode using the concatenation of the query and the retrieved\ncodes. A recent study [258] shows that retrieval-augmented\nframework for code suggestions, including code generation\nand code completion, can improve the performance by a\nlarge margin. ARKS [259] steps further upon prompt-based\nRAG, incorporating iterative RAG to re-formulate queries and\nupdate knowledge soup (containing documentation, execution\nfeedback, generated code, etc.) for dense retrieval, improving\nfinal generation accuracy.\nRetrieval results can be applied during the generation pro-\ncess as well. RECODE [120] retrieves NL descriptions and\npaired codes using edit distance, then extracts n-gram action\nsubtrees from codes\u2019 ASTs. During LSTM-based generation,\nthe patterns of processed subtrees are leveraged to increase the\ncorresponding word probability at each decoding step. kNN-\nTRANX [163] uses seq2tree model BertranX [260] to convert\nNL to code AST. It constructs a datastore for each code AST\nprefix and NL pair; i.e., for each NL-code pair, the context\nrepresentation of the i-th context is obtained by encoding\nNL and the i-th prefix of code\u2019s AST through the seq2tree\nmodel. At each decoding step of the generation, the hidden\nrepresentations are searched within the datastore to form a\nnew probability, which is later combined with the seq2tree\nmodel\u2019s output through a confidence network.ToolCoder [261] performs normal code generation, and\nconducts online search or offline retrieval when encountering\nspecial < API > token. This paradigm makes the model learn\nto leverage API tools.\n2)Code Summarization :The goal of code summarization\nis to transform code into natural language (NL) descriptions,\nwhich is a process of code-to-text. Same to code generation,\nmany sequence-to-sequence models are applied as generator.\nIn many research works, the retrieval results are processed\nby additional encoders.", "start_char_idx": 7039, "end_char_idx": 9089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad848f9e-6e82-4e71-92f7-1a534a8a184b": {"__data__": {"id_": "ad848f9e-6e82-4e71-92f7-1a534a8a184b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca", "node_type": "1", "metadata": {}, "hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f21507d8-c965-413c-939d-44f3e5f1c095", "node_type": "1", "metadata": {}, "hash": "58b053e1b7137d45e355adb640ff6ae383be6bfb4865c6fe8d3ada623fe85b69", "class_name": "RelatedNodeInfo"}]}, "text": "In many research works, the retrieval results are processed\nby additional encoders. Re2Com [141] and EditSum [138]\nSource Code Repository\nExtract\nJava MethodsComments\nCommentCode\nCommentCode\nCommentCodeTraining Set\nTest SetValidation SetDivided by projectRetrieval Corpus / Training SetRetrieve Module\nRefine ModuleAttention MechanismInput Code RepresentationSimilar Code RepresentationExemplar RepresentationData PreprocessingTraining and Test\nEncodersEncodersEncodersDecoder\nFig.", "start_char_idx": 9006, "end_char_idx": 9487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cb750da-ee3f-4d5f-90eb-46f05cfb3546": {"__data__": {"id_": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "936de599-ea95-4d78-9c3f-1c249d80d80b", "node_type": "1", "metadata": {}, "hash": "f40523a4df87bcb25e7c99f3d66a52d6b6523337c56997506af070d93515e489", "class_name": "RelatedNodeInfo"}, {"node_id": "f30055d2-7f02-460a-8448-94843c6757af", "node_type": "1", "metadata": {}, "hash": "06077b5569c4638c3e429be712f1652ae498b9452d4bc00e72c91449b702f363", "class_name": "RelatedNodeInfo"}, {"node_id": "c99b4aba-fa2d-455d-9d84-77c3d20635f6", "node_type": "1", "metadata": {}, "hash": "b2edc0a803af4671e5d9360245a9965f3afe77298c5f40c04b5f5d7c2148ce57", "class_name": "RelatedNodeInfo"}, {"node_id": "39fab97e-52f2-4e71-88f2-fe89f43d9e96", "node_type": "1", "metadata": {}, "hash": "b67bcf24635157a6caa9356e8f5d1f83a7f9ffdedeb93438a45bc398702179a1", "class_name": "RelatedNodeInfo"}, {"node_id": "27497f07-deb3-4d74-bcb8-7865ae6f8fca", "node_type": "1", "metadata": {}, "hash": "a0bf98d1b8dde780e20ff462ae52525c4448603c69be2dbb296466b947aea8cb", "class_name": "RelatedNodeInfo"}]}, "text": "6: Architecture of Re2Com [141] model.\nboth retrieve similar code using sparse retrieval BM25 and\ngenerate summary using LSTM. As shown in Fig. 6, they\nseparately encode the input, the retrieved code, and the corre-\nsponding summary, then combine the middle representations\nor probabilities in the decoder. HGNN [262] instead uses\ncode edit distance for retrieval, and substitutes the encoders\nfor codes with hybrid GNN on their Code Property Graphs\n(CPG) [263]. RACE [142] aims at generating commit message\nfor code difference. It leverages dense retrieval for similar\nexamples and transformer model for generation. It also uses\nthree encoders for the input, the retrieved code difference,\nand corresponding commit message, then combines the results\nbefore feeding into the decoder. BASHEXPLAINER [139]\nshares the similar idea. Its dense retrieval module is based on\nCodeBert [25], and for generation, the output representations\nof the input and the similar code from CodeBert are directly\nfused for transformer-based decoder. READSUM [264] re-\ntrieves similar code using Levenshtein distance, applies code\nencoder and summary encoder to retrieved pairs, and generates\nsummary using a decoder where a fusion network combines\nthe information.\nRAG for in-context learning, which retrieves similar ex-\namples and build prompt for generation, also works in code\nsummary. REDCODER [40] works for both code generation\nand summary, and it replaces the retriever with GraphCode-\nBert [105] for code summary task. ASAP [203] retrieves\nsimilar code with BM25, and generates summary with GPT\nmodels. Similar to the paradigm described above, research\non pseudocode generation [265] employs the retrieved code\nas input for generation and subsequently replaces the results\nwith the original input. SCCLLM [266] retrieves relevant code\nsnippets by semantic, syntactic, and lexical-based retrieval,\nthen forms in-context prompts for smart contract comment\ngeneration via LLM. UniLog [267] retrieves similar code\nsnippets paired with their log statements to conduct in-context\nlearning for log statement generation.\nLogit-based RAG also prevails in code summarization.", "start_char_idx": 0, "end_char_idx": 2152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb5f8140-17ea-42a8-b530-a5aaee35caa9": {"__data__": {"id_": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "398b9c48-d6a9-41cb-b4c5-d800a081bd55", "node_type": "1", "metadata": {}, "hash": "c731288a38ae45222d36eddc88a86c59e6a38454421934bf6351586eea72e742", "class_name": "RelatedNodeInfo"}, {"node_id": "278e7c07-9191-417a-9fc5-452bfcb6b8ef", "node_type": "1", "metadata": {}, "hash": "f76ce2ced3a02f398c81c9ebc4e1796d9a0f80c824520b0a85560966303bf574", "class_name": "RelatedNodeInfo"}, {"node_id": "692f0364-6df0-4243-bd64-3a5b65ea337c", "node_type": "1", "metadata": {}, "hash": "d213eabab635020e7eadaef204ece34e38863722e9ce62a6f1c154f31c0d7d21", "class_name": "RelatedNodeInfo"}, {"node_id": "acbc0f4c-614d-4494-b95f-1759fde42b04", "node_type": "1", "metadata": {}, "hash": "4ede8bb7e31ea7496e82bf828088b393a3677a8e8cf97be1439a1b5030ab6dd1", "class_name": "RelatedNodeInfo"}, {"node_id": "6847293d-9f0b-427d-b17d-187f2b533b44", "node_type": "1", "metadata": {}, "hash": "0fab70f128917d34f24978c7f836ffd9044fe784f8e6c736e4d706a8e14b57c6", "class_name": "RelatedNodeInfo"}]}, "text": "Logit-based RAG also prevails in code summarization.\nRencos [121] utilizes two different methods to retrieve similar15\ncode snippets, syntactic similarity between abstract syntax\ntrees (AST) and cosine similarity between dense representa-\ntions. For generator, it adopts attention-based LSTM. There are\nthree encoder-decoder models for the code input and two re-\ntrieval results respectively, and the probabilities are combined\nfor final generation. CoRec [268] generates commit message\ngiven code diff and retrieved similar code diff. Multiple LSTM\ngenerators handle the input code diff and the retrieved code\ndiff, then adds the probabilities for final generation. kNN-\nTransformer [269] obtains context vector by feeding code into\nan encoder-decoder generator, then gets three parts of logits:\nthe first is from searching the vector, the second is from normal\nTransformer, and the third is the copy mechanism that reserve\nrare tokens from the input. Tram [270] involves retrieval in\nboth token-level and sentence-level. It encodes source code\nand corresponding AST into hidden states representations; for\ntoken-level, it retrieves similar representations in the database\nto form next-token prediction logits; for sentence-level, it uses\nsimilar code for autoregressive generation logits; it also add the\noriginal autoregressive generation logits. CMR-Sum [271] uses\nencoder-decoder model to generate summaries. It conducts\ncross attention between representations of retrieved summary\nand generated summary, and add the logits to the original\ntransformer logits for final distribution.\n3)Code Completion :Code completion can be thought of\nas the coding equivalent of the \u201cnext sentence prediction\u201d task.\nEarly attempts on function completion [272] employs DPR to\nretrieve relevant template functions using function docstring,\nthen concatenate the information as the input to the code\ngenerator BART. ReACC [132] retrieves similar codes to build\nprompts for generation. For retrieval, it uses hybrid retriever,\nwhich combines sparse and dense retrieval; for generation, it\nuses CodeGPT-adapted [273]. RepoCoder [218] steps further\nto perform iterative retrieval and generation to bridge the gap\nbetween the retrieval context and the intended completion tar-\nget. In each iteration, for retrieval, the code query is augmented\nwith previously generated code; for generation, the prompt\nis formed by combining the newest retrieved codes with the\ncode query.", "start_char_idx": 2100, "end_char_idx": 4555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92c16379-d8bd-463f-9c60-ec9831a037e6": {"__data__": {"id_": "92c16379-d8bd-463f-9c60-ec9831a037e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e5a0007e-aa29-4be4-8b2c-177b5707175b", "node_type": "1", "metadata": {}, "hash": "c8cfef6c2fbad2bddf7bd38fb415981a629d2fefca854437453d8c71ae9ddbda", "class_name": "RelatedNodeInfo"}, {"node_id": "27d32337-66ea-48ac-bc2d-dad3f1ad9f00", "node_type": "1", "metadata": {}, "hash": "3658d0b51ed1b9b7906452aa6727f38756893740f883f3b140a52217bb48ae8f", "class_name": "RelatedNodeInfo"}, {"node_id": "5a65a88d-bb2b-4c2d-9a66-95d2302cc0f2", "node_type": "1", "metadata": {}, "hash": "54fc47517fe02ff2c264738afb2321e73283adeae52bdaa206198ec276fb733d", "class_name": "RelatedNodeInfo"}, {"node_id": "dedc7f2c-f064-44fd-84bd-3f4c428591ad", "node_type": "1", "metadata": {}, "hash": "3b565b9e1b23086a38ba8f4cd58d7a29796157c1d10e6b7d03f56d0e78f2c4f1", "class_name": "RelatedNodeInfo"}, {"node_id": "3759a715-0879-495a-b493-c4b225e963b7", "node_type": "1", "metadata": {}, "hash": "ae651c280891e84d0034439418e38059bab9f5019aaa471f90185f615a0b7d99", "class_name": "RelatedNodeInfo"}]}, "text": "Other than combining retrieval results in prompts,\nthe retrieval-and-edit framework [140] first retrieves similar\ntraining examples using dense retrieval, then encodes the\ninput and the retrieved result separately, finally combine them\nthrough attention for later LSTM decoder. CoCoMic [274]\nbuilds a project context graph for the whole code project, and\nretrieves top-k neighbors of the input source code. It generates\nrepresentations of both source code and retrieved contexts,\nthen jointly processes the embeddings to complete the code.\nRepoFusion [275] follows the idea of Fusion-in-Decoder; it\nemploys multiple encoder to input the concatenation of the\nretrieved repo contexts and the unfinished code, them fuses\nthem and generates the results through a decoder. KNM-\nLM [276] uses the same model for retrieval encoding and\ngeneration, then combines the logits using bayes inference.\nEDITAS [277] aims at assertion generation. It retrieves similar\nqueries and their assertions, encodes the edit sequence (from\nthe retrieved query to the original query) and the retrieved\nassertion, then fuses the information for decoder generation.\nDe-Hallucinator [278] first generates code snippets withoutretrieval, then retrieves relevant API references given gener-\nated contents. In the second pass, retrieved API references are\ncombined in prompt for better generation. REPOFUSE [279]\nuses both rationale context and retrieved similar codes to\nconstruct prompt. To fit in the input length limit, it reserves\nthe contexts that are most relevant to the query.\n4)Automatic Program Repair :Buggy code can take a lot\nof effort to fix. Automatic program repair leverages generative\nmodels to output the correct version. Query-based RAG tech-\ninque is widely used in automatic program repair [130], [131],\n[182], [208], [209]. Among them, RING [209] retrieves similar\nerror messages based on both sparse and dense vectors, then\nbuilds prompt for the generator Codex [2]. CEDAR [130] ap-\nplies for both assertion generation and program repairs tasks; it\nuses sparse and dense retrieval to search for similar codes, then\nforms prompt for Codex to generate results. InferFix [131]\ncrafts a prompt carrying the bug type, location, relevant syntax\nhierarchies, and similar fixes through dense retrieval.", "start_char_idx": 4556, "end_char_idx": 6843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54d96d23-c6e9-45cb-960d-1aedfdd1b580": {"__data__": {"id_": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7b4af96-3f92-422b-a14e-63fbead4b498", "node_type": "1", "metadata": {}, "hash": "c94d5758c800353256e28de210bee124682beb04df926d46fc0e7f952804b1ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "bd1a0ed5-3007-43f1-98ad-4201cb85a042", "node_type": "1", "metadata": {}, "hash": "083dcfee709cf6951da210ad73396a73a02c3db6c94f40be30735ae3b488fd2e", "class_name": "RelatedNodeInfo"}, {"node_id": "133d75a8-9182-42a0-999e-0436128e7872", "node_type": "1", "metadata": {}, "hash": "f5f572420f155b551cd1fcd0753f4be4432c0ceed9ae756f9a35ae8f86c91d6f", "class_name": "RelatedNodeInfo"}, {"node_id": "3bf2fb29-9f66-46f7-859a-6d90dfc99792", "node_type": "1", "metadata": {}, "hash": "0ba5dcc18772d6b6de4d405bd88bf7a16e923a6c80d83565dac6acb2773ebe8d", "class_name": "RelatedNodeInfo"}, {"node_id": "e20b5f0d-7df5-4c8e-932f-7d9935f2f2c2", "node_type": "1", "metadata": {}, "hash": "9a7004724a9337596c37aed41ddada2ee10ce166a5ef528d77f8d9fe3001edcb", "class_name": "RelatedNodeInfo"}, {"node_id": "c2fba76e-41e1-4e4e-ab13-1b470c8216da", "node_type": "1", "metadata": {}, "hash": "90a0ce2703139eda58f06d328f72f331e55a18123f827878b221fb0f76628050", "class_name": "RelatedNodeInfo"}]}, "text": "Then it\nalso uses Codex for generation. RAP-Gen [182] also retrieves\nsimilar buggy codes and corresponding fixes through hybrid\nretriever (including both sparse and dense retriever). It fine-\ntunes CodeT5 [106] with this RAG paradigm. SARGAM [208]\nsearches similar buggy code using dense retrieval, generates\npatches using augmented input, then applies another model to\nmodify the final result. These research works also involve error\nlocalization, which is not our focus. RTLFixer [280] leverages\nReAct and RAG to implement an agent fixing errors in Verilog\ncodes. It iteratively retrieves relevant errors and corresponding\nsolutions, and combines reasoning and action planning to form\nprompts for LLMs.\n5)Text-to-SQL and Code-based Semantic Parsing :Se-\nmantic parsing is the task of translating natural language\nutterances to unambiguous structured meaning representations,\nwhere code language is often leveraged to augment this pro-\ncess. SQL is not only a programming language but can also be\nconsidered as a structured representation, so we place text-to-\nSQL (a special case of code generation) in this subsection. Re-\nlated research works all apply query-based RAG. XRICL [189]\nfocuses on the problem of translating non-English utterances\ninto SQL queries. It searches and reranks English utterance\nusing non-English ones by dense retrieval, then builds prompt\nfor Codex to generate SQL queries. SYNCHROMESH [122]\nproposes constrained semantic decoding to enforce rich syn-\ntactic and semantic constraints during generation of SQL or\nother languages. It uses the similarity between abstract syntax\ntrees (AST) to finetune the dense retriever S-Bert. During\ninference, similar NL and SQL are retrived to form the prompt\nof GPT-3. CodeICL [281] uses Python for semantic parsing,\nand augments prompts with a structured domain description\nfor GPT generation. In few-shot learning setting, it leverages\nBM25 sparse retrieval to find similar training examples. RES-\nDSQL [282] ranks schemas using cross-encoder, then includes\nranked schemas into prompts to generate SQL skeleton and\nSQL query. ReFSQL [283] retrieves relevant questions and\ncorresponding SQL to augment text-to-SQL generation.", "start_char_idx": 6844, "end_char_idx": 9038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7b4af96-3f92-422b-a14e-63fbead4b498": {"__data__": {"id_": "b7b4af96-3f92-422b-a14e-63fbead4b498", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "node_type": "1", "metadata": {}, "hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a33b256d-abe8-4813-9b29-7f34c07e22b2", "node_type": "1", "metadata": {}, "hash": "c94d5758c800353256e28de210bee124682beb04df926d46fc0e7f952804b1ee", "class_name": "RelatedNodeInfo"}]}, "text": "It\ninvolves structure-enhanced retriever with schema linking, and\nMahalanobis contrastive learning to improve the retrieval per-\nformance.", "start_char_idx": 9039, "end_char_idx": 9177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9ef127c-8f59-4316-a43a-a91acee1dc89": {"__data__": {"id_": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a726854f-f8a9-431e-829b-30e4742f7d46", "node_type": "1", "metadata": {}, "hash": "ea52ad687cb13a7ec54c3f74179630253570e15f8d148d494b7942356022b6e8", "class_name": "RelatedNodeInfo"}, {"node_id": "450ef87a-0957-4826-83a2-5d9d8e4773c7", "node_type": "1", "metadata": {}, "hash": "da8cb6a8784d136d0aad2cc17c8d2d18221184da47dc36c66ed3499bdc2b14e4", "class_name": "RelatedNodeInfo"}, {"node_id": "9b24883c-5a4c-4f3b-be38-9c962ce7bc1f", "node_type": "1", "metadata": {}, "hash": "42589916d045fdcf2f3694e659c92851f6f4addd375e78669898df82b6985dc5", "class_name": "RelatedNodeInfo"}, {"node_id": "e862db03-86f6-4080-81bd-d15c046aa835", "node_type": "1", "metadata": {}, "hash": "a9044711eb44f3f5c95ae885b8f5e837f354b88e3c7db9ace3d673be80d928ba", "class_name": "RelatedNodeInfo"}, {"node_id": "843259ee-797d-4210-a4b1-8ec05c4e6bc7", "node_type": "1", "metadata": {}, "hash": "f1909e97b67db94e20e1fb205ed79114bc0462660e7f60a32fbd13a416248f32", "class_name": "RelatedNodeInfo"}]}, "text": "ODIS [284] retrieves in-domain and out-of-domain16\ndemonstrations using BM25, then includes them for in-context\nlearning to generate SQL. Another work [285] retrieves both\nsimilar and diverse demonstrations, and then builds prompt for\nin-context learning to generate SQL. MURRE [286] conducts\nmulti-hop retrieve-rewrite, where relevant tables are retrieved\nthrough dense retrieval and then re-writed to generate new\ntabularized question. A rank module at last integrate the\nretrieval results and select the top tables for the text-to-SQL\nmodule. CodeS [287] retrieves relevant information from table\ndatabases in a coarse-to-fine manner, then includes retrieved\nvalues to build prompts for both finetuning and inference.\n6)Others :There are several other code-related tasks that\nadopt RAG paradigm. In [288] for numerical reasoning task,\nthe Chain-of-Thought is replaced with the programs as the\nintermediate reasoning step, and dense retrieval-based similar\nexamples are augmented in prompt for downstream LLMs.\nDe-fine [289] tries to resolve intricate tasks using programs.\nIt follows the paradigm in SKCODER, retrieves relevant\npairs of query and code, then produces sketch template for\nreal program generation. After generation, it combines the\nfeedback to refine the answer with the same generator. The\nrefined programs, regarded as optimal results, are added back\nto the codebase for subsequent serving. E&V [290] leverages\nan LLM-based agent for program static analysis. The agent\nuses source code retrieval, pseudo-code execution, execution\nspecifications verification, and other tools to form interme-\ndiate results. The retrieval is implemented by AST pattern\nmatching. Code4UIE [291] leverages code representation for\ninformation extraction tasks. It retrieves relevant examples\nthrough dense retrieval, and constructs in-context learning\nprompt using the retrieved queries and their corresponding\ncodes. StackSpotAI [292] builds an AI coding assistant,\nwhich incorporates many code-related tasks. It implements\nan RAG component, identifying the most relevant pieces of\ninformation which serve as the context for GPT generation.\nInputBlaster [293] aims to generate unusual text input that\ncould cause mobile app crash. It combines retrieved relevant\nbuggy text with valid input to form the prompt for generation.", "start_char_idx": 0, "end_char_idx": 2323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3263c986-bb7b-4f7f-8240-c0cd405dec60": {"__data__": {"id_": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8f8e59fd-24fd-49fa-8626-d7723415d3a5", "node_type": "1", "metadata": {}, "hash": "594f9849e7e66a5d46a98c2863f93f4987dd2f10b40fb2c50aebc0ba97812114", "class_name": "RelatedNodeInfo"}, {"node_id": "c144f42f-9652-4ab8-b893-b42cbdcb8a98", "node_type": "1", "metadata": {}, "hash": "76dab4b41a2359b22b37552b5b44de99aac495c844fe1eb99086fe3fe3d92cfe", "class_name": "RelatedNodeInfo"}, {"node_id": "49acf9e1-1fa1-40e3-9801-a7f64e4a132d", "node_type": "1", "metadata": {}, "hash": "66f74b21899b27f7755248d31d26cc574bdc4de4507bb6722e71a35c77f51e25", "class_name": "RelatedNodeInfo"}, {"node_id": "d641491f-61ef-4359-b9fe-9dfa8bd9cb8d", "node_type": "1", "metadata": {}, "hash": "0593dff6bbb31422e64f41c87800c8af17bfe5d1f729322ce6795d4c07a67083", "class_name": "RelatedNodeInfo"}, {"node_id": "aff25138-c9f0-4611-bb45-175028a2ca57", "node_type": "1", "metadata": {}, "hash": "8a6480c1cff870f48f255d1523ae0c99c09e17426f89620ee48e2d9151705ec5", "class_name": "RelatedNodeInfo"}]}, "text": "It combines retrieved relevant\nbuggy text with valid input to form the prompt for generation.\nC.RAG for Knowledge\nStructured knowledge, including KGs (knowledge graph)\nand tables, is widely used in language-related tasks. It usually\nserves as the retrieval source to augment generation. In addi-\ntion to regular sparse and dense retrieval, NER (named-entity\nrecognition) technique and graph-aware neighbor retrieval are\napplied to identify and extract relevant entities and relations.\n1)Knowledge Base Question Answering :KBQA (knowl-\nedge base question answering) typically utilizes a knowledge\nbase to determine the correct answer to a question. Many\nsemantic parsing methods have been proposed, generating\nlogical forms (e.g. SPARQL) based on the question.\nQuery-based RAG is the mainstream approach. For a given\nquery, Unseen Entity Handling [53] retrieves topic entities\nthrough FreeBase [294], and concatenates the query with the\nentity for an encoder-decoder to generate SPARQL output.\nCBR-KBQA [54] retrieves relevant questions and correspond-\ning logical form answers with roberta-based deep retrieval,\nthen concatenates the question and the retrieved pairs for\nencoder-decoder transformer model. It also revises the finalgeneration result to align the generated relations with relations\npresent in the local neighborhood of the query entity in the\nknowledge graph. GMT-KBQA [52] first retrieves relevant\nentities and relations through bert-based deep retrieval, then\nconcatenates the information for T5 generation. To improve\nthe retrieval result, it leverages cross-encoder to rerank the\ncandidates, and uses the same T5 structure to conduct relation\nclassification and entity dismbiguation. RNG-KBQA [123]\nenumerates candidate logical forms in the knowledge graph,\nthen ranks the candidates and concatenates them with query to\nform the prompt input to generate final logical form through\na T5 model. Based on this idea, TIARA [124] also retrieve\nentity and schema besides logical forms, while a follow-\ning work [295] retrieves top-k questions with BM25. Uni-\nParser [133] retrieves relevant entities from knowledge graph\nusing mention detection, cross-encoder ranker, and 2-hop paths\nextraction. It also considers enumerating tables and columns\nfrom databases.", "start_char_idx": 2230, "end_char_idx": 4503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da514732-a3cf-4a31-a994-8bc6710f22ec": {"__data__": {"id_": "da514732-a3cf-4a31-a994-8bc6710f22ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "733f45d2-d38c-4127-a959-e19802493591", "node_type": "1", "metadata": {}, "hash": "93d44df141bc660c8b690aeec3deb44ffab8d3cade1a8ef6a39403ae1c02fffc", "class_name": "RelatedNodeInfo"}, {"node_id": "173fb9b0-9e9f-46a0-9829-c989e9918d6e", "node_type": "1", "metadata": {}, "hash": "f64b62db6925c0ea9576063430929b8dcdafe839a94656d2de46b03f260fc54d", "class_name": "RelatedNodeInfo"}, {"node_id": "914f3071-6fb5-478c-acf8-1ec4f4529e08", "node_type": "1", "metadata": {}, "hash": "e8cf591ff745cb74efc940c41662648433104d08571c7f7ce8ffc561cff7e778", "class_name": "RelatedNodeInfo"}, {"node_id": "14912092-810f-441d-9cae-bb9025dfafb1", "node_type": "1", "metadata": {}, "hash": "d514d5f2a086f1c7d5faadc6d13396b5aa734ac5938f086116e9d04a5658c280", "class_name": "RelatedNodeInfo"}, {"node_id": "3220a14f-bb5c-4a16-9a37-6c03ab5029db", "node_type": "1", "metadata": {}, "hash": "1989e998e491eae739ce7adc26758bcec8eb276fd825941ceda8b6bada38f3e6", "class_name": "RelatedNodeInfo"}]}, "text": "It also considers enumerating tables and columns\nfrom databases. On obtaining the relevant information, it\nconcatenates the top-k primitives with the query and generates\nlogical forms through T5. BLLM augmentation [135] uses\nTIARA as the retrieval for relevant knowledge base elements,\nthen performs in-context learning through black-box LLM\nsuch as GPT-4 for generating logical forms. ECBRF [134]\nfollows the case-based reasoning paradigm [296], retrieving\nsimilar triplet with dense retriever and constructing prompt in-\nput for BART or GPT-2 in-context learning. FC-KBQA [297]\nextracts relevant class, relation, and entity given a question.\nFor class and relation, it uses BM25 as retriever and a\nBert-based cross-encoder as re-ranker. For entity, it follows\nthe mention detection paradigm. To compose all the com-\nponent candidates, it applies T5 model to generate logical\nexpression. StructGPT [298] extracts relevant information,\nincluding triplets and nearest entities, to form prompt for\nsubsequent LLM. KAPING [299] builds prompt including the\nuser query and the retrieved relevant facts (through entity\nmatching), then generates the answer through LLM. Another\nwork [300] also follows the retrieve-then-generate paradigm,\nreplacing the retrieval with a relation distribution generation\nmodel for weighted triplets. Retrieve-Rewrite-Answer [301]\nfirst retrieves subgraph using hop prediction, relation path\nprediction, and triplet sampling. It then performs KG-to-text\nand zero-shot generation with retrieved subgraphs as prompt.\nKeqing [302] first decomposes a complext question into simple\nsub-questions through finetuned LLM, then retrieves similar\nsub-question template by dense retriever RoBERTa to extract\ncandidate entities from knowledge graph, and finally generates\nanswer through ChatGPT given relevant entities as context\ninput. To probe the deep understanding of natural language in\nLLMs with formal languages, a research work [303] explores\nthe capability of formal language understanding and formal\nlanguage generation. It leverages retrieved pairs to perform in-\ncontext learning. For understanding, it uses tree edit distance\nto retrieve similar logical forms, while for generation, it uses\nBM25 to retrieve similar natural language queries. Interactive-\nKBQA [304] treats LLM as agent and KG as environment.", "start_char_idx": 4439, "end_char_idx": 6772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f": {"__data__": {"id_": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "node_type": "1", "metadata": {}, "hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e3480fc6-2d39-40e2-be85-87c2c822cede", "node_type": "1", "metadata": {}, "hash": "0b251a06b01867da1a9cb13084131026715e08ea78acb957a5f1d094b244c6a2", "class_name": "RelatedNodeInfo"}, {"node_id": "8995d339-b032-4fa5-8f3b-43650975ed6e", "node_type": "1", "metadata": {}, "hash": "27a3d5ac32af29ad3d440f795997c9704675c56bdf7d226facd161bb46a46185", "class_name": "RelatedNodeInfo"}, {"node_id": "661a03b4-e664-4d71-88ee-a259b4da4183", "node_type": "1", "metadata": {}, "hash": "fddabeaa9b428e2ebaf58c18a03c64a2f68180cab1c991467f2439f9df3a12c3", "class_name": "RelatedNodeInfo"}, {"node_id": "2988ad8e-3321-476a-a8f9-5a2bbc50646f", "node_type": "1", "metadata": {}, "hash": "ca8349ec39b705fcdc18dcf074d49bcb4e2dc1955b85fa60909ec2c4ef9f949b", "class_name": "RelatedNodeInfo"}]}, "text": "Interactive-\nKBQA [304] treats LLM as agent and KG as environment.\nIn each step, the LLM conducts entity-linking and one-hop\nretrieval on KG, then generates current thought and action,\nuntil obtaining the final answer.17\nLatent representation-based RAG is also employed in\nKBQA. ReTraCk [305] links entities using mention detection,\nand retrieves schema items using dense retriever Bert. It\nthen generates logical forms by LSTM, incorporating re-\ntrieved items through knowledge-specific rules. SKP [145]\nconcatenates triplets with the query and uses fusion-in-decoder\ntechnique in inference. It adds a pretraining stage with a\nknowledge-aware MLM loss on triplets and knowledge con-\nstrastive loss with respect to the retrieved items. DECAF [144]\nforms Resource Description Format knowledge base triplets\ninto sentences, then concatenates sentences with the same\nhead entity into documents. It retrieves relevant documents\nusing BM25 sparse retrieval or Bert dense retrieval, then\nleverages Fusion-in-Decoder technique to generate logical\nform and direct answer given each (query, document) pair as\ninput. It combines the output to obtain the final answer. KD-\nCoT [146] uses the same dense retriever and fusion-in-decoder\ngenerator as DECAF. It follows a Chain-of-Thought paradigm,\niteratively performing retrieval, generation, and verification\nuntil the CoT is finished.\n2)Knowledge-augmented Open-domain Question An-\nswering :Structured knowledge is often leveraged to augment\nODQA (open-domain question answering).\nThe use of latent representation-based RAG, particularly\nthe fusion-in-decoder technique, is prevalent for knowledge-\naugmented open-domain question answering. UniK-QA [143]\nconcatenates the text forms of the components in a triplet\nand build document pool for retrieval. For a given question,\nit leverages dense retriever for relevant documents, then per-\nforms fusion-in-decoder technique to incorporate the infor-\nmation for answer generation.", "start_char_idx": 6706, "end_char_idx": 8672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20062614-22e0-4695-9dae-521fdff26f79": {"__data__": {"id_": "20062614-22e0-4695-9dae-521fdff26f79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3b83c4fe-27e8-4f03-bc18-81e067a18526", "node_type": "1", "metadata": {}, "hash": "b57fbc2ebf590b52b8cb0bbdd48c42d44502010f5dd7ec9f73b1b12e7e91f597", "class_name": "RelatedNodeInfo"}, {"node_id": "93b3e0da-3e52-4b92-8a34-ab241bfe5838", "node_type": "1", "metadata": {}, "hash": "551bee74a9305e1db91f96c09d76ce2a6f35623064d56a1b30e728dcd2e4f134", "class_name": "RelatedNodeInfo"}, {"node_id": "89fb7b72-777a-4cc7-837e-9f5b47dabb85", "node_type": "1", "metadata": {}, "hash": "b82674335f03284752135f4dc8b3acd2ee1cfe7a17372db8afc4a1d5f82cea1b", "class_name": "RelatedNodeInfo"}, {"node_id": "bcc25b7c-9372-44a8-94ca-19e7319a9be9", "node_type": "1", "metadata": {}, "hash": "9865d3e13b72010893102e1184658ada8baeba91690c655a84fea99758db9bc9", "class_name": "RelatedNodeInfo"}, {"node_id": "9ab130ed-3f3d-4917-b41e-406b1094c588", "node_type": "1", "metadata": {}, "hash": "049fe44b6242213c14f99937de30e224b60860bde52cdee87f6e47c834c60590", "class_name": "RelatedNodeInfo"}]}, "text": "KG-FiD [306] conducts the\nEncoder\nL\n1\n \nLayers \nText \nKnowledge \nSource\nDPR \nRetriever\nP1\nP3\nP5\nP6\nP2\nP4\nP7\nP8\nKG\nDecoder\nInput \nQuestion\nEncoder\nL\n1\n \nLayers \nEncoder\nL-L\n1\n \nLayers \nEncoder\nL\n1\n \nLayers \nEncoder\nL\n1\n \nLayers \nConcatenation\nOutput\nAnswer\nP1\nP3\nP5\nP2\nP7\nQuestion \n+ \nP1\nQuestion \n+ \nP2\nQuestion \n+ \nP3\nQuestion \n+ \nP5\nEncoder\nL\n1\n \nLayers \nQuestion \n+ \nP7\nEncoder\nL-L\n1\n \nLayers \nRetrieved\nPassages \n& \nEmbeddings\nStage-1 \nReranking\nReading \nModule\nWhen \ndid \nthe \nYankees \nmove \nto \nNew \nYork?\n1903\nNew \nYork \nYankees\nYankee \nStadium\nStaten \nIsland \nYankees\nNew \nYork \nYankees\nOperator\nYankee \nStadium\nNew \nYork \nYankees\nParent\nClub\nStaten \nIsland \nYankees\n......\nN\n0\n \nPassages\nN\n1\n \nPassages\nN\n2\n \nPassages\nStage-2 \nReranking\nFig. 7: Architecture of KG-FiD [306] model.\nretrieval and generation as normal FiD. It add re-ranking in\ntwo ways: the first is to use a graph attention network on the\ngraph formed by retrieved documents; the second is to use\nthe hidden states in the generator. OREOLM [307] empowers\nLLM with knowledge reasoning paths. Concretely, it uses\nentity linking to determine the initial state, then conducts\ncontextualized random walk on KG to get reasoning paths,\nwhose entity value memory are combined into the hidden\nstates of LLM for better generation. GRAPE [308] constructs\nbipartite graph for each pair of question and retrieved passage,\nthen builds bipartite graph on entity for fusing knowledge.It leverages FiD as backbone model and generate answers.", "start_char_idx": 0, "end_char_idx": 1499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f056fd99-2019-42e5-9b42-718302e68fe2": {"__data__": {"id_": "f056fd99-2019-42e5-9b42-718302e68fe2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "70f5ba7c-7a51-4706-a4cf-fd4b7aadcaa6", "node_type": "1", "metadata": {}, "hash": "015e5673068b5f833b52db799479cab5b46df495ef645d4d06e4de3f63ee94c9", "class_name": "RelatedNodeInfo"}, {"node_id": "47039706-e2d3-4000-ab1b-9d20e70132ee", "node_type": "1", "metadata": {}, "hash": "45bda953d0f366db5c776f2f66a8bc0463baf8344d27f63e79d9ed353c3dc27c", "class_name": "RelatedNodeInfo"}, {"node_id": "976714c0-e4d5-43a0-ba7e-0fab9a51bcff", "node_type": "1", "metadata": {}, "hash": "350ff98415c71c002b25af89b10d0688c29d77d932f3dd9d65ab07666bfb47d5", "class_name": "RelatedNodeInfo"}, {"node_id": "075a15af-6fe7-4aa3-ad2e-93674b13240c", "node_type": "1", "metadata": {}, "hash": "21533d9f05f52b65109bc408bdf8bbaf25b507920bd699dd0c55640e0c18bf4a", "class_name": "RelatedNodeInfo"}, {"node_id": "68a5291a-3462-4b62-a5ec-ebe9bde1e764", "node_type": "1", "metadata": {}, "hash": "06b14469e41df83541f8ec8aebc3be24ae5b01b90b4a6864858eb7358cdc148c", "class_name": "RelatedNodeInfo"}]}, "text": "SKURG [309] forms a knowledge graph using text and image\ndata, then updates each source\u2019s representation with their\nrelevant sources. It then uses a specially designed decoder\nto iteratively retrieve and generate. It conducts cross-attention\nwith all the sources, then retrieves the source with highest\nscore and concatenates to the original input embedding; if a\ngate score does not exceed the threshold, it starts to generate\nthe real answer, otherwise the retrieval stage re-starts.\nWith the rapid development of LLMs, query-based RAG is\nemerging as a new standard. DIVKNOWQA [310] develops\na retrieval-augmentation tool, including sparse retrieval on\nstructured knowledge, dense retrieval on texts, and sparql\ngeneration on KB. Through CoT-based LLM, it retrieves\nand re-ranks in each step, and generates the final answer.\nKnowledGPT [311] uses generated code to retrieve from\nboth public and personal knowledge bases, so as to build\nprompt for LLM question answering. EFSUM [312] generates\nevidence-focused summary with retrieved relevant facts, then\noptimizes the summary to align the QA-specific preference\nwith another generator and the filters for helpfulness and\nfaithfulness. GenTKGQA [313] conducts subgraph retrieval\nthrough relation ranking and time mining, then employs GNN\nto incorporate structural and temporal information into virtual\ntoken representations for subsequent LLM generation. Knowl-\nedgeNavigator [314] analyzes complex questions and performs\nretrieval on knowledge graph through iterative filtering of\nrelations with respect to core entities, so as to obtain relevant\ntriplets. It then includes the triplets into prompt for generation.\n3)Table for Question Answering :Tables, as another form\nof structured knowledge, also facilitates question answering.\nFusion-in-decoder [35] style RAG is often used for table\nQA. EfficientQA [315], a competition held in NeurIPS 2020,\nwitnessed the proposal of numerous retrieval-reader systems\nthat rely on textual and tabular data. Dual Reader-Parser [316]\nre-ranks the retrieved textual and tabular data for generation.\nConvinse [317] retrieves information from heterogeneous re-\nsources (including knowledge bases, tables, and texts) after\nquestion understanding.", "start_char_idx": 1500, "end_char_idx": 3733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32e72675-2960-49e4-afd6-5052ce1b0802": {"__data__": {"id_": "32e72675-2960-49e4-afd6-5052ce1b0802", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "90051f7a-8d7b-40ad-87e1-292a767de344", "node_type": "1", "metadata": {}, "hash": "07aec4a3479cee064cd203ac5f764888cb18ebbc174ab3eddedcc410c956a70d", "class_name": "RelatedNodeInfo"}, {"node_id": "b489fbfc-5190-4afb-a1df-d181d1dfe24a", "node_type": "1", "metadata": {}, "hash": "70d6cbb5f0ddbc2c67b4bed7ab5feae2c924188236f7747ac329686d4f5404a1", "class_name": "RelatedNodeInfo"}, {"node_id": "5813aad4-2cc6-4ea7-8a51-b5b7c512d119", "node_type": "1", "metadata": {}, "hash": "dfe9511d7a7d0489935b1517df9db707251218badbe7a8893f6220eb9df43622", "class_name": "RelatedNodeInfo"}, {"node_id": "da33621c-3be9-444b-8b72-86b42cadccd9", "node_type": "1", "metadata": {}, "hash": "af55e4ebfc1a3ccfe3e179906cb05169564745cbbfd578a4e5bb6f3206fd4d72", "class_name": "RelatedNodeInfo"}, {"node_id": "8d88553b-27a7-4aec-9b20-866a54191319", "node_type": "1", "metadata": {}, "hash": "728721a1408e46e2475b6aade3b1e409f904a943cdae18e75c1b8a532aef53f2", "class_name": "RelatedNodeInfo"}]}, "text": "CORE [318] retrieves relevant tables\nand passages through dense representation, then re-ranks the\nretrieved results using query-generation score from a T0 linker.\nIt uses FiD for final generation. RINK [319] follows the\nretriever-reranker-reader paradigm for table-augmented ques-\ntion answering. It designs a set-level reader-inherited reranker\nto get the relevance score of blocks (table segments). TAG-\nQA [320] retrieves from both tables and texts: for tables, it\nconverts tables to graphs then performs GNN to select relevant\ncontents; for texts, it uses BM25 for retrieval. It then leverages\nFiD for answer generation.\nTables can be incorporated into prompts for query-based\nRAG. T-RAG [321] retrieves relevant tables given a user\nquery, then concatenates the query with the tables as a prompt\nto generate the answer through BART. OmniTab [322] con-\nducts multi-task training to improve the performance of table\nquestion answering model. It retrieves relevant tables given a\nquery, then concatenates them for masked-token pre-training.\nCARP [323] retrieves relevant tables and passages using\nentity linking, then extracts hybrid chain of retrieved knowl-18\nedge, which is later used to construct prompt for generation.\nStructGPT [298] extracts relevant information from knowledge\ngraph, table, or database, to form prompt and generate answers\nthrough LLMs. cTBLS [324] retrieves relevant tables through\ndense retrieval, then for each query, it forms prompt with\nranked tables for answer generation. A recent work [325]\nfirst uses table-to-text techniques to integrate tabular data into\ncorpora, then conducts experiments on both finetuning and\nRAG for question answering.\n4)Others :Prototype-KRG [326] retrieves knowledge facts\nand dialogue prototypes, then integrates them into the\nGRU-based generator by both hidden states and logits.\nSURGE [327] retrieves relevant subgraphs using dense re-\ntrieval, then adds them into the input of the generator for\ndialogue generation. RHO [328] fuses KG embedding of\nrelevant entities and relations into textual embeddings during\nthe generation of open-domain dialogue. K-LaMP [329] re-\ntrieves entity in history queries to construct prompt for query\nsuggestion.", "start_char_idx": 3734, "end_char_idx": 5942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc06d373-fc7c-4d14-b2cb-95d390096739": {"__data__": {"id_": "dc06d373-fc7c-4d14-b2cb-95d390096739", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca", "node_type": "1", "metadata": {}, "hash": "c60ae4ed8fdcfe51dafce5c4af7675cbd46ee8f0bb30e601660346acd0633957", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ddd5a5e0-ea32-47c9-8629-7b251984223f", "node_type": "1", "metadata": {}, "hash": "649f3e3f6eae647dc2c7cccb9a5c26373cc7764f54537472af0fd7698596f9b2", "class_name": "RelatedNodeInfo"}, {"node_id": "49fb7aa6-8e4c-40de-b2f3-cc2a83b89252", "node_type": "1", "metadata": {}, "hash": "2fa5ad0999650b12a5ee597b9f587280bafce0c57a515c53f7cb764b209cd64d", "class_name": "RelatedNodeInfo"}, {"node_id": "44b33897-061e-42bb-aa09-288cb0daa2c4", "node_type": "1", "metadata": {}, "hash": "7aa4aafade0a30f65b28e4167949bd05a7901f2ee7b4ffb647e5204be6a73c39", "class_name": "RelatedNodeInfo"}, {"node_id": "90470b79-c441-4938-a09b-0bed43975300", "node_type": "1", "metadata": {}, "hash": "d97cafae7b1bca2b682ff10bc186a04ba59ffd5d249a66097681d7788f5a55ea", "class_name": "RelatedNodeInfo"}, {"node_id": "a06ca537-930e-469e-8b48-e7a30a705046", "node_type": "1", "metadata": {}, "hash": "92c1d81820d166fdd215c7feb82cb56d5d68ae6ba36a3bf5378615625ca190b9", "class_name": "RelatedNodeInfo"}]}, "text": "ReSKGC [147] linearizes all training triplets into\ntext by concatenation, then retrieves relevant triplets using\nBM25, and generates completed triplet using T5 with fusion-\nin-decoder. G-Retriever [330] retrieves relevant nodes and\nedges from graph-based data, then constructs subgraph and\nperforms graph prompt tuning for question answering based\non textual graphs.\nD.RAG for Image\n1)Image Generation :Image Generation refers to the\nprocess of creating new images, typically using algorithms in\nthe field of artificial intelligence and machine learning.\nThe retrieval process can not only help yield high-quality\nimages even for rare or unseen subjects, but also reduces\nthe parameter count and computational expense [45], [137],\n[148]\u2013[152], [331]. For GAN-based model, RetrieveGAN [45]\nemploys a differentiable retrieval process to select compatible\nimage patches for generation, with Gumbel-softmax trick and\nend-to-end training. IC-GAN [137] models data as a mix\nof conditional distributions around each training instance,\nconditioning both the generator and discriminator on these\ninstances, and can control the semantics and style by swap-\nping class labels or conditional instances. Recently, diffusion\nmodels beat GANs on image generation [332]. KNN-Diffusion\n[149] trains a text-to-image diffusion model without text data,\nby conditions the model on CLIP joint embedding of the\ninstance and k-nearest neighbors from image database. These\nk-NN embeddings bridge the text-image distribution gap and\nallow image generation from different domains by swapping\nthe database. Similarly, RDM [150] conditions diffusion or\nautoregressive models on CLIP embeddings of external image\ndatabases. It enables post-hoc conditioning on class labels,\ntext prompts and zero-shot stylization [151]. Beyond retrieving\nonly images, Re-imagen [148] conditions on both text prompts\nand retrieved image-text pairs for text-to-image generation.\nInterleaved classifier-free guidance is also proposed to balance\nthe alignment between text prompts and retrieval conditions.\nTo avoid information loss of CLIP embeddings and access\nto all visual condition, Retrieve&Fuse [331] concatenates\nthe retrieved conditional image and the noised image before\neach attention block in U-Net, and allowing interaction viaself-attention.", "start_char_idx": 5943, "end_char_idx": 8247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca": {"__data__": {"id_": "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "041dcbc3-b10c-426b-995b-38234f2d3a38", "node_type": "1", "metadata": {}, "hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "397e8007-44af-4d39-827a-d0c027215c52", "node_type": "1", "metadata": {}, "hash": "c60ae4ed8fdcfe51dafce5c4af7675cbd46ee8f0bb30e601660346acd0633957", "class_name": "RelatedNodeInfo"}]}, "text": "RPG [79] retrieves representative images to\nconstruct informative in-context examples (i.e., image-region\npairs), and utilizes multi-modal chain-of-thought reasoning\n[333] to plan out complementary subregions for compositional\ntext-to-image diffusion.", "start_char_idx": 8248, "end_char_idx": 8499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b646a538-109b-47c0-ad7a-ac2283530d47": {"__data__": {"id_": "b646a538-109b-47c0-ad7a-ac2283530d47", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f2b2e28f-36c4-44d8-8e1d-f200916d2b17", "node_type": "1", "metadata": {}, "hash": "2d87addbd47e1638b3ab677261388b22c8684d16547131fa64cdde00c05114f7", "class_name": "RelatedNodeInfo"}, {"node_id": "302545ac-ad76-44da-813d-2c45ea69dcb2", "node_type": "1", "metadata": {}, "hash": "bc383378ddced11c32265c187bc61c78aab6c5e9dd79b0cc874b0b212abcee0d", "class_name": "RelatedNodeInfo"}, {"node_id": "4f3cf145-b04e-4360-857c-a098a8f9d4e9", "node_type": "1", "metadata": {}, "hash": "12bd7949a060e9cfc6589be7b308f0cf831db18ea708fee3dc06c8fb35cba6dd", "class_name": "RelatedNodeInfo"}, {"node_id": "205bf3a5-3e9b-4f95-a13b-506a843bbb2e", "node_type": "1", "metadata": {}, "hash": "2f7bf8a234dca68d77c97b1c2d3bdcf225399f814949b7d74df778417ac25acf", "class_name": "RelatedNodeInfo"}, {"node_id": "fd6e7113-d963-43a0-a7f5-6ad2a59683af", "node_type": "1", "metadata": {}, "hash": "71f625ce05e2377efb7c4d573d190af8262c140bbe59ec678fe52230757616b3", "class_name": "RelatedNodeInfo"}]}, "text": "2)Image Captioning :Image Captioning is the process of\ngenerating a textual description of an image.\nRetrieval-augmented image captioning typically synthesises\ndescription with a collection of retrieved captions, instead\ndepending only on the input image. MA [164] augments via\na memory bank, built with historical context and target word\nof image-text training set, and queried during inference with\nthe current context. V ocabulary distribution is computed based\non retrieved entries, and interpolated with the original predic-\ntion. In adversarial training, RAMP [334] employs retrieved\ncaptions as reference for discriminator training, prompting\nthe generator to make full use of retrieved captions. The\nmemory-augmented attention and copying mechanism are also\nexploited to better use. The RA-Transformer [46] and EXTRA\n[335], both retrieval-augmented transformer-based captioning\nmodels, utilize cross-attention over encoded retrieved captions.\nEXTRA, as depicted in Fig. 8, jointly process the image and\naman slopeSEP...\n... aa\nBOSEOS\nskierskierheads\nmountainsmountains\nthe......\n......\nv1 w1wM v2 vN-1vN CLS...\n\"a man riding skis\ndown a snow covered slope\"Autoregressive\nLanguage DecoderVision-and-Language\nEncoder\nCross-Attention\nCurrent Image Retrieved Caption\n\"a man riding skis\ndown a snow covered slope\"... ......\"a couple of people with\nski 's standing in the snow\"DISTANCESINPUT\nIMAGE CAPTIONDatastore\n215...\nhv1hv2 hvN-1hvNhw1hw2hwM-1hwM...\nFig. 8: Architecture of EXTRA [335] model.\nretrieved captions with V&L encoder, such that the decoder\nattends to both visual and linguistic contexts. Beyond retrieved\ncaptions, REVEAL [336] uniformly encodes and retrieves\nmulti-modal world knowledge, including image-text pairs,\nquestion answering pairs, and knowledge graph triplets, which\nis then integrated with image features by retrieval score-\naware attention module. Straightforwardly, SMALLCAP [47]\nemploys a CLIP vision encoder and a LLM decoder, linked\nby trainable cross-attention layers, where retrieved captions\nserve as input-specific in-context examples for prompt for en-\nhancement.", "start_char_idx": 0, "end_char_idx": 2105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc9ebb92-7688-4767-b8f6-7b76327405bc": {"__data__": {"id_": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7a227faa-e6f0-439d-86e9-94aa2b4ba4c7", "node_type": "1", "metadata": {}, "hash": "ffb23b90835b4e19169323e10e9bafa8967ace6f061bc963630ab0ac6974e036", "class_name": "RelatedNodeInfo"}, {"node_id": "5216234a-e278-4358-b80c-1309c790e54e", "node_type": "1", "metadata": {}, "hash": "72b02ec6981491005b3f5c95435a9f1171570e1eda6e058df6221fe6d1d05170", "class_name": "RelatedNodeInfo"}, {"node_id": "a7cc3307-ed1b-4e3c-93ac-40875c2d3796", "node_type": "1", "metadata": {}, "hash": "c860231e3bea18303a5cd2c93b08d2a6e30193260c92536f247ed7ed2a70a2d4", "class_name": "RelatedNodeInfo"}, {"node_id": "b5e16604-7e38-4ab2-8e19-a62413f43be8", "node_type": "1", "metadata": {}, "hash": "5ee101198383d6ca2370f84ed48e4be0ffe6a2da74daf79a511608317e5893d5", "class_name": "RelatedNodeInfo"}, {"node_id": "659bcfea-85b8-403c-bc6f-60b89fc4ffcd", "node_type": "1", "metadata": {}, "hash": "1c354d09d92bef3ad875a3d1d68361a7d38e4183407b042a1eafe7eb29ad208f", "class_name": "RelatedNodeInfo"}]}, "text": "For remote sensing image, CRSR [337] enhances\nretrieved captions with semantic refinement, i.e. filters out\nmisleading details and emphasizes visually salient content.\nBesides, the visual features is also enriched by transformer\nnetwork with learnable queries, capturing more intricate details\nwithin the images19\n3)Others :There also exist many retrieval augmented\nworks for other image-related tasks. For visual question\nanswering (VQA), PICa [338] leverages GPT-3\u2019s implicit\nknowledge retrieval and reasoning capabilities, which con-\nverts images into textual descriptions, then prompts GPT-3\nto predict answers based on these descriptions and the ques-\ntion, finally ensembles multi-query results. RA-VQA [339]\nidentifies a limitation that the retrieval in previous work is\ntrained separately from answer generation, and propose a joint\ntraining scheme that integrates differentiable retrieval with\nanswer generation, enabling end-to-end training. For visually\ngrounded dialogue, KNN-based Information Fetching (KIF)\n[340] enhances generative Transformer for dialog modeling.\nEach KIF module learns a read operation to access fixed\nexternal knowledge. Maria [341], a neural conversation agent,\nenhances dialog generation by leveraging visual experiences\nretrieved from a large-scale image index as extra context.\nFor multi-modal machine translation, which aims to improve\nNMT with multi-modal information, [342] incorporates visual\ninformation at the phrase level to address the sparsity of paired\nsentence-image, employing a conditional V AE to filters out\nredundant visual information from sentence-image datasets.\nE.RAG for Video\n1)Video Captioning :Video captioning is to describe the\nvisual content with a descriptive utterance. KaVD [343]\ngenerates news video caption with background knowledge\nmined from topically related documents such as named entities\nand events. R-ConvED [48] introduces retrieval-augmented\nmechanism to facilitate the word prediction. It uses Dual\nEncoding [109] for video-text retrieval, and proposes a convo-\nlutional encoder-decoder network for generation. For a given\ninput video, R-ConvED first retrieves top-k relevant sentences\nand their corresponding video from training set, then feeds\nthese pairs and the input video into the generator separately.", "start_char_idx": 2106, "end_char_idx": 4396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc9fe157-ab09-4904-b323-e8019ece8204": {"__data__": {"id_": "fc9fe157-ab09-4904-b323-e8019ece8204", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "777cf26e-c814-4055-ad5e-732d45dba164", "node_type": "1", "metadata": {}, "hash": "acc09292f676e1ffea0794cb34400731fd86968bc22364173d15a53ecdd65676", "class_name": "RelatedNodeInfo"}, {"node_id": "ab6eb898-4f1a-4cb8-8ea8-e3b33547d7e3", "node_type": "1", "metadata": {}, "hash": "ddc6cfcc0005f3fb6ec0474ccb6d775fb1a6e0525e02508ce376f9727deea40c", "class_name": "RelatedNodeInfo"}, {"node_id": "fdcb691e-6311-4440-a5cd-ae83b3f5a07a", "node_type": "1", "metadata": {}, "hash": "a36b46800ce88f00acdc7d1391739a35a19452fd9aca6b16667c158dc2fb244a", "class_name": "RelatedNodeInfo"}, {"node_id": "67eb05b7-2887-4807-a2d6-8703d4e2af90", "node_type": "1", "metadata": {}, "hash": "90db2db81321c29a3625ac42af3f5ab5e5eece66320a591113f02c2ad815e3b2", "class_name": "RelatedNodeInfo"}, {"node_id": "d886886d-ff01-4653-bb1a-adaea32bfb9d", "node_type": "1", "metadata": {}, "hash": "1e6d2cd3af9d14b626de8fcfeae80e420e8d121517c3b3e6303acb294e0c57ac", "class_name": "RelatedNodeInfo"}]}, "text": "The obtained decoder hidden states are combined through\nattention-like read operation, so that the target word can be\npredicted using the final representation. CARE [160] utilizes\nvisual encoder, audio encoder, and text encoder for frame,\naudio, and retrieved texts, respectively. It uses CLIP as re-\ntriever, and transformer decoder as generator. The embeddings\nof the three modalities are combined to augment the decoder,\nproducing global semantic guidance which attends the input\nembedding, and local semantic guidance which attends the\nattention layer. EgoInstructor [49] generates captions for first-\nperson view videos. It retrieves relevant exocentric videos\nand corresponding texts via dense retrieval, then encodes the\ninput egocentric video and the retrieved exocentric videos\nthrough a CLIP-based visual encoder and a transformer-\ndecoder-based bottleneck module. Then it generates captions\nthrough decoder-based LLM which takes the retrieved texts\nas input and interacts with encoded videos in gated cross-\nattention layer.\n2)Video QA&Dialogue :Video QA&Dialogue generates\nsingle or multiple-round responses in alignment with video\ncontent. For video question answering (VideoQA), MA-DRNN\n[344] leverages the differentiable neural computer (DNC) with\nan external memory, for storing and retrieving useful infor-\nmation in questions and videos, and modeling the long-termvisual-textual dependence. Given the video input, R2A [345]\nretrieves semantically similar texts by multi-modal model, e.g.\nCLIP, and query LLM with both the question and the retrieved\ntexts. For video-grounded dialogue, [346] proposes TVQA+\ndataset which enables to retrieve relevant moments and visual\nconcepts to answer questions about videos, and also proposes\nSpatio-Temporal Answerer with Grounded Evidence (STAGE)\nto exploit it. VGNMN [347] also extracts visual cues from\nvideos, while the retrieval is carried out using neural module\nnetworks parameterized by entities and actions in previous\ndialogues.\n3)Others :There also exist many retrieval augmented\nworks for other video-related tasks. VidIL [348] exploits\nimage-language models to translate video content into\ntemporal-aware prompts with few-shot examples, for vari-\nous video-language tasks including video captioning, video\nquestion answering, video caption retrieval, and video fu-\nture event prediction.", "start_char_idx": 4397, "end_char_idx": 6752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4055d84-e057-42ea-901f-7ba1753df337": {"__data__": {"id_": "d4055d84-e057-42ea-901f-7ba1753df337", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c98e98ee-03b5-4bf9-a652-999317a0b671", "node_type": "1", "metadata": {}, "hash": "70828131a4e089a5fd70194bf07328bef70c911b67ea89eae6f21470b4185c0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9a5ec0e0-ea1e-478b-9d66-97f0fb1cfca3", "node_type": "1", "metadata": {}, "hash": "a8dc18a87cea9bdda30c0a9e0da96ab38cc14062c117652010447a571dbdb7dd", "class_name": "RelatedNodeInfo"}, {"node_id": "14288937-76a6-415e-b127-6be38d179ae5", "node_type": "1", "metadata": {}, "hash": "f2e85e1f011008644bd6e77d8b4dbed46579f2a38148f980d78a15a853e342d5", "class_name": "RelatedNodeInfo"}, {"node_id": "ed353fdf-d90b-4d26-9c0f-227c3236f79d", "node_type": "1", "metadata": {}, "hash": "62fdaf3c02618b87c732fe7e851354a9a4e77666b856cdb7f27174241fec6e8b", "class_name": "RelatedNodeInfo"}, {"node_id": "8633af80-72a5-42dd-8ad4-362e972bfc14", "node_type": "1", "metadata": {}, "hash": "5020e1dd3eaf177794475d9b17e8cb34a6ff91027da1d83c4ee2afe0a500a62e", "class_name": "RelatedNodeInfo"}, {"node_id": "c488c930-29d8-4119-800f-e2a195f83bee", "node_type": "1", "metadata": {}, "hash": "ed387b66755b48b0ded2e5c36528bfedebcb85ee626460ebe25911f1aa82c2c1", "class_name": "RelatedNodeInfo"}]}, "text": "Notably, for trustworthy autonomous\ndriving, RAG-Driver [349] grounds the MLLM in relevant\nexpert demonstrations from a memory database, to produce\ndriving action explanations, justifications, and control signal\nprediction. Text-to-video generation is to generate video given\nnatural language descriptions. As shown in Fig. 9, Animate-\nPlot 1Motion structure retrieval\nStoryboard descriptionStructure-guidedtext-to-video synthesisText promptsVideo databaseText queriesPlot iPlot n\u22ef\u22ef\n\u22ef\u22efStory script\nFig. 9: Architecture of Animate-A-Story [206] model.\nA-Story [206] develops a framework which can generate high-\nquality storytelling videos based on texts. It first separates the\ntext into individual plots, and decorates the description using\nLLM. It then retrieves relevant videos for each plot through\na dense retriever [110]. It generates videos through a latent\ndiffusion model, consisting of two branches: a text encoder20\nCLIP, and a structure encoder which takes the estimated depth\nof the retrieved videos as structure control.\nF .RAG for Audio\n1)Audio Generation :The goal of audio generation is to\ngenerate audio with natural language input.\n\u201cA bottle of champagne is popped and then poured into a glass\u201dInputprompt\nOutputWaveform\nCLAPEncoder\nDatabaseAudio FeatureLanguageFeature\u201cSome water pure into the glass\u201d\n\u201cWater pure into the glass\u201d\n\u201cA champagne is popped while a man talks\u201d\nVAEDecoderHiFi-GANRetrievalAudioMAET5Audio & Language FeatureLDMCrossAttention\nFig. 10: Architecture of Re-AudioLDM [159] model.\nRe-AudioLDM [159] adopts dense retriever CLAP [26] to\nretrieve similar caption-audio pairs given input prompt. As\nshown in Fig. 10, the generator, latent diffusion model and\nV AE-based decoder, take the representations of input text and\nretrieved pairs as input and generate output audio. Make-An-\nAudio [44] uses dense retriever CLAP [26] to augment data,\nretrieving related audio given natural language text. It then\nconstructs pseudo prompts for diffusion-based text-to-audio\nmodel training.", "start_char_idx": 6753, "end_char_idx": 8767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c98e98ee-03b5-4bf9-a652-999317a0b671": {"__data__": {"id_": "c98e98ee-03b5-4bf9-a652-999317a0b671", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "node_type": "1", "metadata": {}, "hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b8a28e70-6de3-4ea3-b8bb-68c535a48178", "node_type": "1", "metadata": {}, "hash": "70828131a4e089a5fd70194bf07328bef70c911b67ea89eae6f21470b4185c0c", "class_name": "RelatedNodeInfo"}]}, "text": "It then\nconstructs pseudo prompts for diffusion-based text-to-audio\nmodel training.\n2)Audio Captioning :The goal of audio captioning is\nto generate natural language data with audio data, which is\nbasically a sequence-to-sequence task. RECAP [350] leverages\nCLAP [26] to retrieve related captions given audio data.", "start_char_idx": 8684, "end_char_idx": 8997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25335f95-389e-4dfd-a165-a8c49e4ad787": {"__data__": {"id_": "25335f95-389e-4dfd-a165-a8c49e4ad787", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "92be1258-6cac-48f4-9fc8-c2e56846fb94", "node_type": "1", "metadata": {}, "hash": "8cd0f612e6794f2d275bc30e52ef6cffdd6bb76955978871e9a35d5489de5274", "class_name": "RelatedNodeInfo"}, {"node_id": "bdc7067e-8cb5-4467-8e3e-90737e05f7f2", "node_type": "1", "metadata": {}, "hash": "713d2e9a5a11988662306a934bd30ed4098a19a154b88b70a1a20c14e14b3c1c", "class_name": "RelatedNodeInfo"}, {"node_id": "04338204-b1ff-477a-b016-8e7de89b26d5", "node_type": "1", "metadata": {}, "hash": "74ae8f85b1714060d332c2f5818befb7fc9b486f415fb86caa66924ff5e6cf2d", "class_name": "RelatedNodeInfo"}, {"node_id": "67cd8374-8b19-4e27-b88d-659eaf35c055", "node_type": "1", "metadata": {}, "hash": "f99d525051e1ed451734b0e9fd9683f4232e9362bf8a92182a8c84a4a1e843c9", "class_name": "RelatedNodeInfo"}]}, "text": "The\nretrieved captions are then included into the prompt input for\nGPT-2 model, which interacts with audio embeddings through\ncross attention. In [43], dense retriver VGGish [107] is adopted\nto produce dense embedding of audio data, and GPT-2 is\nadopted to generate representations of the retrieved captions\nwhich are paired with similar audios. After obtaining the\nrepresentations of audios and captions, an extra multi-head\nattention block and a linear layer fuses all the information\nand generates the output. Some research studies transform\naudio modality to text, in order to leverage advancements\nin LLMs [351]\u2013[353]. They take advantage of deep retrieval\nmodels, aligning the modalities into the same latent space for\ndownstream text generation.\nG.RAG for 3D\n1)Text-to-3D :Retrieval can be applied to augment the\ngeneration of 3D contents. ReMoDiffuse [51] aims at gener-\nating motions using diffusion models. It first retrieves relevant\nmotion entites through CLIP given text input, then leverages\nthe information of the text and the retrieved entities through\na semantic-modulated attention layer.\nAMD [158] designs two branches of motion diffusion for\nfidelity and diversity. As shown in Fig. 11, the first branch\ninputs the original prompt text for diffusion; the second branch\ndecomposes the input text into anatomical scripts and retrieve\nsimilar reference motions for diffusion. A transformer-based\nfusion module is further applied to adaptively balance the\nresult from two branches. RetDream [50] targets general 3D\ngeneration, using retrieved 3D assets to augment the process\n...A man is pretending to \nbe a chicken , constantly \npecking at the ground \nand waving his arms like a \nchicken.\n1)Amanlowers his head     \ntowards the ground.\n2) ... opens and closes \nhis mouth rapidly.\n3) ... moves his head up and \ndown, mimicking a \npecking motion .", "start_char_idx": 0, "end_char_idx": 1862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff14e27b-df47-4028-8029-4a6e19eb36aa": {"__data__": {"id_": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7aec402f-4bb8-4eb0-b627-ed7989264302", "node_type": "1", "metadata": {}, "hash": "92c6cc55abd2f8ec86af9cbbe0df8a64e1825894463c00afcc99ef6099a65437", "class_name": "RelatedNodeInfo"}, {"node_id": "b429fff9-0da0-4633-af64-90ad7e215920", "node_type": "1", "metadata": {}, "hash": "6b10c3b746dfdafe48deb59742440fbfac0a9cf84231f58ca8932e71467db6a7", "class_name": "RelatedNodeInfo"}, {"node_id": "13274157-7229-45b0-8997-6fdb2dbae214", "node_type": "1", "metadata": {}, "hash": "ea76a66ba11bdba8a29eb6070d2f0fe53c943a6e0d1574c027b5ec4549f496e9", "class_name": "RelatedNodeInfo"}, {"node_id": "f567d5c9-2dc1-47a3-9193-897d26055ee0", "node_type": "1", "metadata": {}, "hash": "2bf5d5f4d1bbd8192e7da5f07ad81a58d462885c505647410b4bed13178563d1", "class_name": "RelatedNodeInfo"}, {"node_id": "a396e262-669d-4c9d-9a1c-17da560b9118", "node_type": "1", "metadata": {}, "hash": "bbb63ca1a3b8ff529d8b39f3fa9fabcc5e69078985e1e96210a64788ba7c000e", "class_name": "RelatedNodeInfo"}, {"node_id": "a1817d0b-a1d0-4e69-8562-0a506d10f3e7", "node_type": "1", "metadata": {}, "hash": "baf0d0e5f9874e48d3e8a0ae8d89ce07bc6512843bd06bcdbb077fa21e0cb509", "class_name": "RelatedNodeInfo"}]}, "text": "3) ... moves his head up and \ndown, mimicking a \npecking motion .\n4) ... flaps his arms up \nand down ,imitating a \nchicken's wings .\ud835\udc61 MLPCross\nAttentionLinearFusion Block\nTransformer  EncoderCLIP\nText\nCLIP\nText\nLinear\ud835\udc65\ud835\udc611\ud835\udc65\ud835\udc612\ud835\udc65\ud835\udc613\ud835\udc65\ud835\udc61\ud835\udc41\n\ud835\udc5a1\ud835\udc5a2\ud835\udc67\ud835\udc61\ud835\udc58\ud835\udc60\ud835\udc67\ud835\udc61\ud835\udc58\ud835\udc59\n\ud835\udc5a3\ud835\udc5a\ud835\udc45 \u2295\u0ddc\ud835\udc6501\n\u0ddc\ud835\udc6502\n\u0ddc\ud835\udc6503\n\u0ddc\ud835\udc650\ud835\udc41\u22ef\u22ef\n\u22ef\n\u22ef\n\u22ef\u22efLinear\nLinearTransformer  EncoderFusion Block\u2295Origin  Motion Diffusion\nFeature FusionText Decomposition\nfine -tuned\nSelf \nAttention\nDropout\nLayer Norm\nLinear\nGELU\nLayer NormDropout\nLinear\nFusion Block optional\u2131\ud835\udc592\n\u2131\ud835\udc60\u2131\ud835\udc61\ud835\udc9e\ud835\udc59\n\ud835\udc9e\ud835\udc60\nHybrid Retrieval\u2130\ud835\udc60\u2130\ud835\udc59\u2131\ud835\udc591\ud835\udc5d\ud835\udf031\nSearch with Anatomical TextBest Match\nDataset\ud835\udc5a1:\ud835\udc45Reference Action \nTokensRandom Select\nComplex and Decomposed\nText Features\nDiffused Motion and \nReference Action Features \ud835\udc65\ud835\udc611:\ud835\udc41Diffused Motion Data\ud835\udc58,\ud835\udc63\n\ud835\udc5e\nReference Motion Diffusion\ud835\udc5d\ud835\udf032Fig. 11: Architecture of AMD [158] model.\nof variational score distillation from 2D diffusion models.\nGiven an input query, it retrieves relevant 3D assets through\nCLIP, then utilizes the retrieved assets to provide geometric\nprior and adapted 2D prior. Concretely, retrieved assets not\nonly impose an additional velocity on particles for distribution\ninitialization, but also help optimize the 2D diffusion model\nthrough low-rank adaptation.", "start_char_idx": 1797, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46dc9b82-6c13-4332-a2b3-4aa89903801f": {"__data__": {"id_": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "6760dec7-64fb-4236-8b3a-0d596faa42fa", "node_type": "1", "metadata": {}, "hash": "565a1fc0ca9660b9e84cef013466a7b26ee224161c074daadb84e05b26ebaf59", "class_name": "RelatedNodeInfo"}, {"node_id": "4d643c3b-afb5-4c96-be4e-f5d35e44f22b", "node_type": "1", "metadata": {}, "hash": "e259dae028f671e24a833c32bc349fd5bc0d5edb563ae13cb79f4be8228ad506", "class_name": "RelatedNodeInfo"}, {"node_id": "139b6acd-1363-4fe5-9b28-dc397c0a70e1", "node_type": "1", "metadata": {}, "hash": "19b5312e829790b1abc9d8bcdcfde1dcf125d3ef387cc0ac56dd9bce31a90502", "class_name": "RelatedNodeInfo"}, {"node_id": "79244573-93ef-474d-b526-9fad3bb61d58", "node_type": "1", "metadata": {}, "hash": "e8e5317db72c541c28e57aefa4f6752d9b642b49b1edbe4954300336dcc96209", "class_name": "RelatedNodeInfo"}, {"node_id": "50351b61-7460-48e7-ab37-0b865bdee1f3", "node_type": "1", "metadata": {}, "hash": "31ae0c605b4b880a0307a579f48a4c43f2a409d9d941e4d47bb63e6070c2b4ca", "class_name": "RelatedNodeInfo"}, {"node_id": "c2bb9ba0-e566-4971-82e0-7da09e2666fe", "node_type": "1", "metadata": {}, "hash": "1b1c45fae9e01893f0142260fb929b4e7d65dc64e53a288037414d4cd0e9e9f1", "class_name": "RelatedNodeInfo"}]}, "text": "H.RAG for Science\nRAG has also emerged as a promising research direction\nfor many interdisciplinary applications, such as molecular\ngeneration, medical tasks and computational research.\n1)Drug Discovery :The goal of drug discovery is to\ngenerate molecules that concurrently fulfill diverse properties.\nRetMol [55] integrates a lightweight retrieval mechanism and\n-4.9 kcal/molRetrieval databaseRetrieverInformation fusionDecoderEncoder\n-8.4 kcal/mol-10.3 kcal/mol-10.9 kcal/molEncoderShared  weightsInput molecule\nRetrieved exemplar moleculesInput embedding\nRetrieved embeddingsFused embedding-8.4 kcal/mol\nOutput molecule\nRetrieval modulePre-trained module\nFig. 12: Architecture of RetMol [55] model.\nmolecular strings into a pre-trained encoder-decoder gener-\native model to retrieve and fuse exemplar molecules with\nthe input. PromptDiff [354] introduces an interaction-based,\nretrieval-augmented 3D molecular diffusion model that re-\ntrieves a curated set of ligand references to guide the synthesis\nof ligands meeting specific design criteria.\n2)Biomedical Informatics Enhancement :Several re-\ncent studies have improved the expressiveness of LLM\nby retrieving information from biomedical domain-specific\ndatabases, thereby augmenting the model\u2019s capabilities to\nprovide valuable guidance for tasks in the medical field.\nPoET [355] is an autoregressive generative model based on\na variant of Transformer that integrates a retrieval mech-\nanism to enable prompt augmentation, thereby expediting\nthe prediction of fitness properties for protein variants.\nChat-Orthopedist [136] enhances ChatGPT with a retrieval-\naugmented mechanism focused on adolescent idiopathic sco-\nliosis (AIS), utilizing an external knowledge base for precise\nresponses. BIOREADER [356] is the first retrieval-enhanced\ntext-to-text transformer-based model for biomedical natural21\nlanguage processing, incorporating the retrieved literature\nevidence into the model using a chunked-cross attention\nmechanism. MedWriter [357] employs a hierarchical retrieval-\naugmented generation method that combines report-level and\nsentence-level templates to produce coherent and clinically\naccurate medical reports from images. QA-RAG [358] em-\nploys a dual-track RAG strategy to enhance pharmaceutical\ncompliance by effectively retrieving and integrating regula-\ntory guidelines based on language model responses and user\nqueries.", "start_char_idx": 2998, "end_char_idx": 5393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "668623f4-21a5-4b59-b6ac-e78759e6b403": {"__data__": {"id_": "668623f4-21a5-4b59-b6ac-e78759e6b403", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c26f6c18-927a-4e11-b542-6fa85387b872", "node_type": "1", "metadata": {}, "hash": "d4de0904dc60501c95e3fd2e514611431f3b100c9259692bc819ded096879298", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "12baea84-1c2e-406e-9fe6-d6c5ae40afe7", "node_type": "1", "metadata": {}, "hash": "2aeede78d038ba29f40fcaaf9ce6e9204b6a52cd0f6c0d514fa4736fb60543a9", "class_name": "RelatedNodeInfo"}, {"node_id": "a2b02493-0913-4aa1-a942-76a5693942e6", "node_type": "1", "metadata": {}, "hash": "19e1a97de4bd3b6cd38a3d4a6a06edae0fddbbfe16a0808a0c15ec04f3174ed6", "class_name": "RelatedNodeInfo"}, {"node_id": "b2d1d2c9-03fc-4370-8b49-bf0a6482fc52", "node_type": "1", "metadata": {}, "hash": "31968708e17aa7e3e46b6f80cbc31b8100b1dfece13d693db0719b73693f8245", "class_name": "RelatedNodeInfo"}, {"node_id": "20beb164-b808-4fab-bdb1-2cab9205485b", "node_type": "1", "metadata": {}, "hash": "84f799769391b9be09a02d6763ab27bae4d2948e81d675f40d5c82ffc66e2cf2", "class_name": "RelatedNodeInfo"}, {"node_id": "45e7963b-3f3f-4503-beab-9d52b2e33541", "node_type": "1", "metadata": {}, "hash": "e09994106885dd8c04c785edf18c9728e3b00eb7174a10204da1d43b740edbc9", "class_name": "RelatedNodeInfo"}]}, "text": "3)Math Applications :Retrieval-augmented generation\ntechnology in mathematics streamlines problem-solving,\nboosts research innovation, and refines educational strategies.\nLeanDojo [359] boosts theorem proving by using retrieval-\naugmented methods to choose relevant premises from exten-\nsive mathematical libraries, improving automation and theo-\nrem generalization. RAG-for-math-QA [360] improves math\nquestion-answering by integrating a high-quality math text-\nbook with retrieval-augmented generation, enhancing LLM-\ngenerated responses for middle-school algebra and geometry.\nV. B ENCHMARK\nGiven the increasing research interests and applications of\nRAG, there have also been several benchmarks assessing RAG\nfrom certain aspects.\nChen et al. [361] proposed an RAG benchmark, which\nevaluates RAG from four aspects: Noise Robustness, Neg-\native Rejection, Information Integration, and Counterfactual\nRobustness, respectively. Noise Robustness evaluates whether\nLLMs could extract the necessary information from documents\ncontaining noisy information. The noisy information is rele-\nvant to the input query but useless for answering it. Negative\nRejection measures whether LLMs would reject to respond the\nquery when the retrieved content is not enough. Information\nIntegration assesses whether LLMs could acquire knowledge\nand make responses by integrating multiple retrieved contents.\nCounterfactual Robustness refers to the ability of LLMs to\nidentify counterfactual errors in the retrieved content.\nAnother three benchmarks, RAGAS [362], ARES [363] and\nTruLens [364], consider three different aspects: Faithfulness,\nAnswer Relevance, and Context Relevance, respectively. Faith-\nfulness focuses on the factual errors in the results when the\ncorrect answers can be inferred from the retrieved contents.\nAnswer Relevance measures whether the generated results\nactually address the problems (i.e., queries) or not. Context\nRelevance judges whether the retrieved contents contain as\nmuch knowledge as possible to answer the queries, and as\nlittle irrelevant information as possible.\nCRUD-RAG [365] divides all RAG tasks into four cate-\ngories, which are Create, Read, Update, and Delete, respec-\ntively, and also evaluates each category using text continuation,\nquestion answering (with single- and multi-document ques-\ntions), hallucination modification, and open-domain multi-\ndocument summary.", "start_char_idx": 5394, "end_char_idx": 7791, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c26f6c18-927a-4e11-b542-6fa85387b872": {"__data__": {"id_": "c26f6c18-927a-4e11-b542-6fa85387b872", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882", "node_type": "1", "metadata": {}, "hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0d99a087-25e4-4697-aa87-74b61e82a6ad", "node_type": "1", "metadata": {}, "hash": "d4de0904dc60501c95e3fd2e514611431f3b100c9259692bc819ded096879298", "class_name": "RelatedNodeInfo"}]}, "text": "MIRAGE [366] is a benchmark designed\nfor assessing the application of RAG in the medical domain,\nfocusing on the comparison and optimization of medical\nquestion-answering systems\u2019 performance. KILT [367] is an-\nother benchmark focuses on ensuring information accuracyand reliability by aligning Wikipedia pages with specific snap-\nshots and pinpointing the most pertinent text ranges through\nBLEU score evaluations.", "start_char_idx": 7792, "end_char_idx": 8207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad7a59b1-43f1-4324-99d3-262cc9769b05": {"__data__": {"id_": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "bb13df0d-57d8-48be-9f81-b00f3aa17fe0", "node_type": "1", "metadata": {}, "hash": "863a26731ac86bfa63a1b2efcc3bf8dcfd08e3d777e5926cf01add9e54be611f", "class_name": "RelatedNodeInfo"}, {"node_id": "a77cb3bb-d25e-43d3-a9c0-d530dd22739b", "node_type": "1", "metadata": {}, "hash": "9050ce00c526f93dcc82f7a1cce2aab4283c09c0b54e26bb876ee24dbd113811", "class_name": "RelatedNodeInfo"}, {"node_id": "3194219b-ebcd-4142-b47c-5fa24070e90f", "node_type": "1", "metadata": {}, "hash": "72a840308d31a0302150fddd5aaae9f145064ee2ea3630351c964b80e3584a86", "class_name": "RelatedNodeInfo"}, {"node_id": "31fd32b6-a023-49c1-8fa4-a4c977281d19", "node_type": "1", "metadata": {}, "hash": "fb4e74c749db844bc1f37627d035eb64ddaa9c5f30e41156ea390d254a625259", "class_name": "RelatedNodeInfo"}, {"node_id": "f967a8e1-0805-45a8-8ffd-8652afc40ca8", "node_type": "1", "metadata": {}, "hash": "d87ba2f9649be406707cf8943eeac7c5add458702613a4203be3394a750c290f", "class_name": "RelatedNodeInfo"}]}, "text": "It filters out lower-quality data to\nmaintain a high standard of information mapping, offering a\nvariety of retrieval system options like TF-IDF, DPR, RAG,\nand BLINK + flair to support evidence-based predictions or\ncitations according to task requirements.\nVI. D ISCUSSION\nA.Limitations\nDespite the widespread adoption of RAG, it suffers from\nseveral limitations by nature. In this paper, we provide a sum-\nmary of the limitations and engage in an in-depth discussion.\n1)Noises in Retrieval Results :Information retrieval can-\nnot yield perfect results because information loss appears in\nrepresentations generated by encoder models. Additionally,\nANN search can also provide approximate results rather than\nexact ones. Consequently, certain degree of noise is inevitable\nin retrieval results, manifesting as irrelevant objects or mis-\nleading information, which may cause failure points in RAG\nsystems [368]. Though the common sense is that increasing\nthe accuracy of retrieval will contribute to the effectiveness of\nRAG, a recent study surprisingly shows that noisy retrieval\nresults may conversely help improve the generation qual-\nity [369]. A possible explanation is that diversity in retrieval\nresults may also be necessary for prompt construction [370].\nAs a result, the impact of noise in retrieval results remains\nuncertain, leading to confusion in practical uses regarding\nwhich metric to employ for retrieval and how to facilitate the\ninteraction between the retriever and the generator.\n2)Extra Overhead :While retrieval can help mitigate the\ncosts of generation in certain cases [30]\u2013[32], the incorporation\nof retrieval sometimes introduces non-negligible overhead.\nConsidering that RAG is primarily employed to improve the\nperformance of existing generative models, the inclusion of\nadditional retrieval and interaction processes leads to increased\nlatency. Worse still, when combined with complex enhance-\nment methods, such as recursive retrieval [371] and iterative\nRAG [218], the extra overhead will become even more sig-\nnificant. Furthermore, as the scale of retrieval expands, the\nstorage and access complexity associated with data sources\nwill also increase. In presence, RAG systems exhibit a trade-\noff between costs and benefits. Looking ahead, we anticipate\nfurther optimization to alleviate the associated overhead [372].\n3)The Gap between Retrievers and Generators :Seam-\nlessly integrating retrieval and generation components requires\nmeticulous design and optimization.", "start_char_idx": 0, "end_char_idx": 2501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4": {"__data__": {"id_": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e34bb962-915a-4424-974d-2c772aee08b6", "node_type": "1", "metadata": {}, "hash": "53372c64133ba60545e97b15510a4af2a1d8d4a65762fde1e3683b0360d36d95", "class_name": "RelatedNodeInfo"}, {"node_id": "fc7c7c5e-5606-49c5-abc4-f7ae526fab8f", "node_type": "1", "metadata": {}, "hash": "a6ab9ece106b1dc1605f98b3b224a29614a04b2f3bc5dce77d703282ee06d460", "class_name": "RelatedNodeInfo"}, {"node_id": "e1613544-b38c-4ed0-9a78-59133c3529f4", "node_type": "1", "metadata": {}, "hash": "901e70e1c2f5f93fbab7ee3e5ec525e86ff1532cafc6771162a4d1d598a5e261", "class_name": "RelatedNodeInfo"}, {"node_id": "1a866526-497b-49f3-a6da-5c3d580b7c85", "node_type": "1", "metadata": {}, "hash": "5c847c5b287e488f1ad3805947b76e9c6eca2d21dbb0b4c34e3f5fda46a44898", "class_name": "RelatedNodeInfo"}, {"node_id": "217c9a59-07ca-468b-8329-6c3f7031478d", "node_type": "1", "metadata": {}, "hash": "1c7ceaa85c9ae8fbd26360435168711ac8984197cf93af74c0167f8e2e3a65ba", "class_name": "RelatedNodeInfo"}]}, "text": "Since the objectives of\nretrievers and generators may not align, and their latent spaces\nmight differ, designing their interaction poses challenges. As\nintroduced in Section III, numerous approaches have been\nproposed to enable effective RAG, and these approaches\neither disentangle the retrieval and generation processes or\nintegrate them at an intermediate stage. While the former is\nmore modularized, the latter could potentially benefit from\njoint training. Till not, there lacks a sufficient comparison of\ndifferent ways of interaction across various scenarios.22\n4)Increased System Complexity :The introduction of re-\ntrieval unavoidably increases the system complexity and the\nnumber of hyper-parameters to tune. For instance, a recent\nstudy on the trade-off between attribution and fluency in\nprompt-augmentation-style RAG demonstrates that using top-\nk retrieval for generation improves attribution, but hurts flu-\nency in turns [373]. The counter effects of different aspects\nin RAG, such as metric selection, are still under explored.\nTherefore, further refinement of RAG systems, both in terms\nof algorithms, and deployment, is necessary to fully unlock\ntheir potentials.\n5)Lengthy Context :One of the primary shortcomings of\nRAG, in particular the query-based RAG, is that it lengthens\nthe context tremendously, making it infeasible for generators\nwith limited context length. In addition, the lengthened context\nalso slows down the generation process generally. The research\nadvancements in prompt compression [202] and long-context\nsupport [374] have partially mitigated these challenges, albeit\nwith a slight trade-off in accuracy or costs.\nB.Potential Future Directions\nLastly, we wish to outline several potential directions for\nfuture RAG research and applications.\n1)More Advanced Research on RAG Methodologies, En-\nhancements, and Applications :A straight-forward research\ndirection is to develop more advanced methodologies, en-\nhancements, and applications of RAG.\nAs introduced in Section III-A, existing works have ex-\nplored various interaction patterns between retrievers and\ngenerators. However, since the optimization target of these two\ncomponents are distinct, the practical augmentation process\nhas a large impact on the final generation results. Investigation\nof more advanced foundations for augmentation holds promise\nfor fully unleashing the potential of RAG.", "start_char_idx": 2502, "end_char_idx": 4897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce7a1637-31c5-4726-9393-835a332f13f2": {"__data__": {"id_": "ce7a1637-31c5-4726-9393-835a332f13f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a8767cf3-fa04-46e0-85e9-f36df1a1f878", "node_type": "1", "metadata": {}, "hash": "c61aa576e78e57206b42dba50fa8fb45dfd10de9ffa4e9cbe50df835d0cca675", "class_name": "RelatedNodeInfo"}, {"node_id": "4c8c9fd2-3b8c-4a7a-ad1e-a1b74840dc1a", "node_type": "1", "metadata": {}, "hash": "a443d2d5b71c1e86d5d172e72934343995e0e01e41569ec475b45c0ebf15ee1b", "class_name": "RelatedNodeInfo"}, {"node_id": "00eafba3-f472-4835-ab19-ac519d4aa3cb", "node_type": "1", "metadata": {}, "hash": "3430023d3b35f1cc958713d1ccbfe291475be1a641b6e3db62775d6aadc033ea", "class_name": "RelatedNodeInfo"}, {"node_id": "2117da92-d682-475d-aba8-a71d376f683a", "node_type": "1", "metadata": {}, "hash": "064cedaf76dcf26370c23f963fe33543c5c057a71bd040a92e67a796cc05af52", "class_name": "RelatedNodeInfo"}]}, "text": "Based on a constructed RAG system, enhancements are\nhelpful to improve the effectiveness of certain components\nor the entire pipeline. Given the inherent complexity of the\nsystem, there exists significant potential for RAG to improve,\nnecessitating proper tuning and careful engineering. We look\nforward to further experimental analysis and in-depth explo-\nration that will contribute to the development of more effective\nand more robust RAG systems.\nAs introduced in Section IV, RAG is a general technique\nthat has been applied across diverse modalities and tasks. Yet\nmost of existing works straightforwardly integrate external\nknowledge with the specific generation tasks, without thor-\noughly taking into account the key characteristics of the target\ndomains. Therefore, for generation tasks that do not fully\nleverage the power of RAG, we are confident that designing\nproper RAG system will be beneficial.\n2)Efficient Deployment and Processing :Currently, sev-\neral deployment solutions of query-based RAG for LLMs\nhave been proposed, such as LangChain [375], LLAMA-\nIndex [175], and PipeRAG [376]. However, for other founda-\ntions of RAG and/or generation tasks, there lacks a plug-and-\nplay solution. In addition, given the extra overhead introduced\nby retrieval, and considering that the complexities of both\nthe retriever and generator will continue to grow, achievingefficient processing in RAG remains a challenge, necessitating\ntargeted system optimization.\n3)Incorporating Long-tail and Real-time Knowledge :\nWhile a key motivation of RAG is to harness real-time and\nlong-tail knowledge, few studies have explored the pipeline\nfor knowledge updating and expansion. Many existing works\nmake up the retrieval sources with merely the training data\nof generators, thereby neglecting the dynamic and flexible\ninformation advantages that could have been offered by re-\ntrieval. As a consequence, designing a useful RAG system with\ncontinuously updated knowledge and/or flexible knowledge\nsources, along with corresponding system-level optimizations,\nis a growing research direction. With the capability of utilizing\nlong-tail knowledge, we also expect RAG to leverage person-\nalized information and features, so as to adapt to today\u2019s web\nservice.", "start_char_idx": 4898, "end_char_idx": 7152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "001f02ad-43a4-4105-9aab-1eca82b49991": {"__data__": {"id_": "001f02ad-43a4-4105-9aab-1eca82b49991", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45593217-9f9a-4741-a8d5-1545ac7e429e", "node_type": "1", "metadata": {}, "hash": "c97aee1c760250dd596a785c4de320feadd0c4f892ad3cd7d5518a1cd165e1d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7b311141-51f6-48e8-85f4-19dcb6789636", "node_type": "1", "metadata": {}, "hash": "baca1b715cc6c66d857813ffd6ba3f8808089c4ec5417ca5b7d2e9c711a56bfc", "class_name": "RelatedNodeInfo"}, {"node_id": "f2b30c75-d798-43b7-a3dc-3876e3820cef", "node_type": "1", "metadata": {}, "hash": "265a59a04791bb7a2f0fa9ed01900df3453d0573b5a2babd96baeb2cbcb62c4f", "class_name": "RelatedNodeInfo"}, {"node_id": "0906a26a-d932-4a45-b2fd-6b5d80b92412", "node_type": "1", "metadata": {}, "hash": "80bfe8682c1596fcb512c2f3c981f3bede057b1967012b8cfc59bd46143485e8", "class_name": "RelatedNodeInfo"}, {"node_id": "dcd8e83f-99ae-4dab-adea-b18713a1e027", "node_type": "1", "metadata": {}, "hash": "0f9ad2e506a7c97760c83bc3fec86edebf45e91e9c5957b9ec270b479b915bce", "class_name": "RelatedNodeInfo"}, {"node_id": "5093a13c-9d13-46ed-9d22-057236d90bde", "node_type": "1", "metadata": {}, "hash": "7ba01220e68a02efb03eacc795a92bc17df1b0fb16d45fa9d45e2cb3906f3d58", "class_name": "RelatedNodeInfo"}]}, "text": "4)Combined with Other Techniques :In essential, RAG\nis orthogonal to other techniques that share the goal of\nimproving AIGC effectiveness, including fine tuning, rein-\nforcement learning, chain-of-thought, agent-based generation,\nand other potential optimizations. However, the exploration of\nsimultaneously applying these techniques is still in its early\nstages, calling for further research to delve into algorithm\ndesign and fully leverage their potential. It is worthy to note\nthat a recent notion appears \u201clong-context models like Gemini\n1.5 will replace RAG\u201d. Nevertheless, this assertion does not\nhold true \u2014 RAG exhibits greater flexibility in managing\ndynamic information, encompassing both up-to-date and long-\ntail knowledge [377]. We believe that RAG in the future will\ntake advantage of long context generation to achieve even\nbetter performance, rather than simply being weeded out by\nit.\nVII. C ONCLUSION\nIn this paper, we conducted a thorough and comprehensive\nsurvey on RAG within the context of AIGC, with a particular\nfocus on augmentation foundations, enhancements, and ap-\nplications. We first systematically organized and summarize\nthe foundation paradigms in RAG, providing insights into\nthe interaction between retrievers and generators. Then, we\nreviewed the enhancements that further improve the effective-\nness of RAG, including the enhancements on each component\nor the entire pipeline. To facilitate researchers across diverse\ndomains, we showcased practical applications of RAG in a\nrange of modalities and tasks. Finally, we also presented\nexisting benchmarks for RAG, discussed current limitations\nof RAG, and shed light on promising future directions.\nREFERENCES\n[1] T. B. Brown, B. Mann etal., \u201cLanguage models are few-shot learners,\u201d\ninNeurIPS, 2020.\n[2] M. Chen, J. Tworek etal., \u201cEvaluating large language models trained\non code,\u201d arXiv:2107.03374, 2021.\n[3] OpenAI, \u201cGPT-4 technical report,\u201d arXiv:2303.08774, 2023.\n[4] H. Touvron, T. Lavril etal., \u201cLlama: Open and efficient foundation\nlanguage models,\u201d arXiv:2302.13971, 2023.", "start_char_idx": 7153, "end_char_idx": 9219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45593217-9f9a-4741-a8d5-1545ac7e429e": {"__data__": {"id_": "45593217-9f9a-4741-a8d5-1545ac7e429e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "143faccb-70a4-4f32-99ba-66824f74c219", "node_type": "1", "metadata": {}, "hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "479df2ba-adbd-48c0-912c-45351d9583ca", "node_type": "1", "metadata": {}, "hash": "c97aee1c760250dd596a785c4de320feadd0c4f892ad3cd7d5518a1cd165e1d0", "class_name": "RelatedNodeInfo"}]}, "text": "[5] H. Touvron, L. Martin etal., \u201cLlama 2: Open foundation and fine-tuned\nchat models,\u201d arXiv:2307.09288, 2023.", "start_char_idx": 9220, "end_char_idx": 9331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19780472-c500-4db8-9fe0-a3572c15efe9": {"__data__": {"id_": "19780472-c500-4db8-9fe0-a3572c15efe9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "30edca9e-602b-4751-8f1e-8ceea3fa7144", "node_type": "1", "metadata": {}, "hash": "1aa5d5997c67fcf1db567ade1d71c118c3ef1d05ad54d88093e4dae616699bb7", "class_name": "RelatedNodeInfo"}, {"node_id": "e9ac09f7-abbd-4291-8e1b-59826aa8e9fb", "node_type": "1", "metadata": {}, "hash": "ec7bb8e20dd6cf16abe9a86cd42b285ccf6d240cdbcf8c8c396ddb7ffa225bc3", "class_name": "RelatedNodeInfo"}, {"node_id": "9f025190-0e23-42a3-8fa2-bad898a7321e", "node_type": "1", "metadata": {}, "hash": "5d60c2d112691791a8f931895499dfecafe42faabb05fa5597eff5922bb5e13b", "class_name": "RelatedNodeInfo"}, {"node_id": "d6e7f471-d1a0-48aa-93cb-990c54902330", "node_type": "1", "metadata": {}, "hash": "4c006ac775ec69130023d6533d2961725059ebc8f6c4cdee2be43a96acfc8e87", "class_name": "RelatedNodeInfo"}, {"node_id": "a7f52bb1-c615-4d48-a1fa-281c34570180", "node_type": "1", "metadata": {}, "hash": "1f7f77ee4a3d8dd691b67d237409073e092f2232018e33d4ecfc2966fc161df8", "class_name": "RelatedNodeInfo"}]}, "text": "[6] B. Rozi `ere, J. Gehring etal., \u201cCode llama: Open foundation models\nfor code,\u201d arXiv:2308.12950, 2023.\n[7] A. Ramesh, M. Pavlov, G. Goh etal., \u201cZero-shot text-to-image gener-\nation,\u201d in ICML, 2021.23\n[8] A. Ramesh, P. Dhariwal, A. Nichol etal., \u201cHierarchical text-conditional\nimage generation with CLIP latents,\u201d arXiv:2204.06125, 2022.\n[9] J. Betker, G. Goh, L. Jing etal., \u201cImproving image generation with\nbetter captions,\u201d Computer Science, vol. 2, no. 3, p. 8, 2023.\n[10] R. Rombach, A. Blattmann, D. Lorenz etal., \u201cHigh-resolution image\nsynthesis with latent diffusion models,\u201d in IEEE/CVF, 2022.\n[11] OpenAI, \u201cVideo generation models as world simulators,\u201d https://openai.\ncom/research/video-generation-models-as-world-simulators, 2024.\n[12] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\nComput., vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[13] A. Vaswani, N. Shazeer, N. Parmar etal., \u201cAttention is all you need,\u201d\ninNeurIPS, 2017.\n[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza etal., \u201cGenerative adver-\nsarial networks,\u201d CACM, vol. 63, no. 11, pp. 139\u2013144, 2020.\n[15] J. Devlin, M. Chang etal., \u201cBERT: pre-training of deep bidirectional\ntransformers for language understanding,\u201d in NAACL-HLT, 2019.\n[16] C. Raffel, N. Shazeer, A. Roberts etal., \u201cExploring the limits of transfer\nlearning with a unified text-to-text transformer,\u201d JMLR, vol. 21, pp.", "start_char_idx": 0, "end_char_idx": 1373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6190d24-8b42-4a04-b7d0-7020e7d7af23": {"__data__": {"id_": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d643aa0b-a69d-41b5-8731-a0b676b53177", "node_type": "1", "metadata": {}, "hash": "ce2d048fa36bdbb619690152be1d24e33df0e2dfa0b71eb5ab2906c0f4066e95", "class_name": "RelatedNodeInfo"}, {"node_id": "52d9581f-cd10-4219-850b-7cfa60631540", "node_type": "1", "metadata": {}, "hash": "6305023dd1bb84b1de043dc2641644b37cbaf17b5a10b8c2c8593776e5f5f0a2", "class_name": "RelatedNodeInfo"}, {"node_id": "51acae86-bf53-4591-8137-987b5f3a0c6c", "node_type": "1", "metadata": {}, "hash": "504194cd9029ef71a80737bf125a2b63236236f7291b8f4812148675703bbb52", "class_name": "RelatedNodeInfo"}, {"node_id": "f75ff35e-1ac0-488e-829c-f62aa23907e3", "node_type": "1", "metadata": {}, "hash": "83dda942aed08acfee5b36e535c3e1384fc635ad9355b22311195cc2ed168277", "class_name": "RelatedNodeInfo"}, {"node_id": "a91d0b66-8351-440d-b6f8-5bce59073802", "node_type": "1", "metadata": {}, "hash": "aeffa4332e0d98db2bc2c5bcf56cf6c9f9c6a959a6bf4db76a13f9c99d5cddd2", "class_name": "RelatedNodeInfo"}]}, "text": "21, pp.\n140:1\u2013140:67, 2020.\n[17] W. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling to\ntrillion parameter models with simple and efficient sparsity,\u201d JMLR,\nvol. 23, no. 120, pp. 1\u201339, 2022.\n[18] J. Kaplan, S. McCandlish, T. Henighan etal., \u201cScaling laws for neural\nlanguage models,\u201d 2020.\n[19] S. E. Robertson and H. Zaragoza, \u201cThe probabilistic relevance frame-\nwork: BM25 and beyond,\u201d FTIR, vol. 3, no. 4, pp. 333\u2013389, 2009.\n[20] V . Karpukhin, B. Oguz, S. Min etal., \u201cDense passage retrieval for\nopen-domain question answering,\u201d in EMNLP, 2020.\n[21] J. Johnson, M. Douze, and H. J \u00b4egou, \u201cBillion-scale similarity search\nwith gpus,\u201d IEEE Trans. BigData, vol. 7, no. 3, pp. 535\u2013547, 2021.\n[22] Q. Chen, B. Zhao, H. Wang etal., \u201cSPANN: highly-efficient billion-\nscale approximate nearest neighborhood search,\u201d in NeurIPS, 2021.\n[23] R. Datta, D. Joshi, J. Li etal., \u201cImage retrieval: Ideas, influences, and\ntrends of the new age,\u201d CSUR, vol. 40, no. 2, pp. 5:1\u20135:60, 2008.\n[24] A. Radford, J. W. Kim, C. Hallacy etal., \u201cLearning transferable visual\nmodels from natural language supervision,\u201d in ICML, 2021.\n[25] Z. Feng, D. Guo etal., \u201cCodebert: A pre-trained model for program-\nming and natural languages,\u201d in EMNLP Findings, 2020.\n[26] Y . Wu, K. Chen, T. Zhang etal., \u201cLarge-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-caption augmen-\ntation,\u201d in ICASSP, 2023.", "start_char_idx": 1366, "end_char_idx": 2781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17e36111-0937-4893-91bf-173e818746db": {"__data__": {"id_": "17e36111-0937-4893-91bf-173e818746db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b5bc3b1a-4bf1-4261-a969-6e7883ff5c4a", "node_type": "1", "metadata": {}, "hash": "289b69fbd9473307f8b274e9d118151bcc73fe65a2a2ba8ef626f4d27b4ad996", "class_name": "RelatedNodeInfo"}, {"node_id": "a23e8c3e-903b-41c7-a0a0-652cbcebb4a1", "node_type": "1", "metadata": {}, "hash": "fa6387ddcae8c4dd5120c6c3930dacd954fa1745791c842498ef46e449e066b9", "class_name": "RelatedNodeInfo"}, {"node_id": "a4081b4f-d08e-416d-bde6-65a05eccbc1d", "node_type": "1", "metadata": {}, "hash": "75bac725f262aae6b18ffaef66cdee7b0acc1d743322dbb112bd29e25d335f0d", "class_name": "RelatedNodeInfo"}, {"node_id": "451e2b54-d25f-4063-948f-01bd7cebaab7", "node_type": "1", "metadata": {}, "hash": "f7fb578fd52e3cb144aae359abe069f89aa3dbfe4a7fbb3bcf9c013fdd5c7129", "class_name": "RelatedNodeInfo"}, {"node_id": "138049a2-5244-4799-b421-bab9d1feada8", "node_type": "1", "metadata": {}, "hash": "1480e5cf29e0125b1ed50a523a6671aa7cca4f2b09a5cce7e82742e9137b15dc", "class_name": "RelatedNodeInfo"}]}, "text": "[27] A. Mallen, A. Asai, V . Zhong etal., \u201cWhen not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric\nmemories,\u201d in ACL, 2023.\n[28] N. Carlini, F. Tram `eretal., \u201cExtracting training data from large\nlanguage models,\u201d in USENIX, 2021.\n[29] M. Kang, N. M. G \u00a8urel etal., \u201cC-RAG: certified generation risks for\nretrieval-augmented language models,\u201d arXiv:2402.03181, 2024.\n[30] G. Izacard, P. Lewis, M. Lomeli etal., \u201cAtlas: Few-shot learning with\nretrieval augmented language models,\u201d arXiv:2208.03299, 2022.\n[31] Y . Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, \u201cMemorizing\ntransformers,\u201d in ICLR, 2022.\n[32] Z. He, Z. Zhong, T. Cai etal., \u201cREST: retrieval-based speculative\ndecoding,\u201d arxiv:2311.08252, 2023.\n[33] K. Guu, K. Lee, Z. Tung etal., \u201cREALM: retrieval-augmented language\nmodel pre-training,\u201d ICML, 2020.\n[34] P. S. H. Lewis, E. Perez, A. Piktus etal., \u201cRetrieval-augmented\ngeneration for knowledge-intensive NLP tasks,\u201d in NeurIPS, 2020.\n[35] G. Izacard and E. Grave, \u201cLeveraging passage retrieval with generative\nmodels for open domain question answering,\u201d in EACL, 2021.\n[36] S. Borgeaud, A. Mensch etal., \u201cImproving language models by\nretrieving from trillions of tokens,\u201d in ICML, 2022.\n[37] U. Khandelwal, O. Levy, D. Jurafsky etal., \u201cGeneralization through\nmemorization: Nearest neighbor language models,\u201d in ICLR, 2020.", "start_char_idx": 2782, "end_char_idx": 4156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b9d1983-3c1b-423b-b132-1100f72674af": {"__data__": {"id_": "3b9d1983-3c1b-423b-b132-1100f72674af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e209caf1-ead6-467f-9555-1a2e27ae1c9f", "node_type": "1", "metadata": {}, "hash": "e5f3c064f008e09ee6687542919075f8b54044aab99ed4a359a26a7af4ea6f39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1686d5fe-b11e-433d-b2ae-ae0d0ac166d6", "node_type": "1", "metadata": {}, "hash": "49ed764b19c7b75918ac9854e42c9af38bf8f5a91de6c84936998c0c8edd6510", "class_name": "RelatedNodeInfo"}, {"node_id": "ed2d2138-7296-4372-9bbf-171bee1866c2", "node_type": "1", "metadata": {}, "hash": "5a7f0b35a6e18fdd8d16de1e6ae8ab7dc202871d3bf5e21bf87d34520c59d4ac", "class_name": "RelatedNodeInfo"}, {"node_id": "a2b13420-35ca-40d2-9df7-87435b80f27f", "node_type": "1", "metadata": {}, "hash": "74ef3c302d5b60735e4860d4a66b8605a663f1e378f1ff254f411e9ae9076276", "class_name": "RelatedNodeInfo"}, {"node_id": "d05a8f34-087a-44e4-ad06-8a128b226335", "node_type": "1", "metadata": {}, "hash": "c898d8cbf75ec59da38a8d33d028a1cdfc531750ed9e011deee04be3aa8c0f3b", "class_name": "RelatedNodeInfo"}, {"node_id": "4452e0c4-d1eb-470e-8439-f7cdfd816f13", "node_type": "1", "metadata": {}, "hash": "0f7be290d0887dc9fb9a71e161ab04749b91195a6a6db670d2a470a7dd087fa3", "class_name": "RelatedNodeInfo"}]}, "text": "[38] J. He, G. Neubig, and T. Berg-Kirkpatrick, \u201cEfficient nearest neighbor\nlanguage models,\u201d in EMNLP, 2021.\n[39] zilliztech. (2023) Gptcache. [Online]. Available: https://github.com/\nzilliztech/GPTCache\n[40] M. R. Parvez, W. U. Ahmad etal., \u201cRetrieval augmented code gener-\nation and summarization,\u201d in EMNLP Findings, 2021.\n[41] W. U. Ahmad, S. Chakraborty, B. Ray etal., \u201cUnified pre-training for\nprogram understanding and generation,\u201d in NAACL-HLT, 2021.\n[42] S. Zhou, U. Alon, F. F. Xu etal., \u201cDocprompting: Generating code by\nretrieving the docs,\u201d in ICLR, 2023.\n[43] Y . Koizumi, Y . Ohishi etal., \u201cAudio captioning using pre-trained large-\nscale language model guided by audio-based similar caption retrieval,\u201d\narXiv:2012.07331, 2020.[44] R. Huang, J. Huang, D. Yang etal., \u201cMake-an-audio: Text-to-audio\ngeneration with prompt-enhanced diffusion models,\u201d in ICML, 2023.\n[45] H.-Y . Tseng, H.-Y . Lee etal., \u201cRetrievegan: Image synthesis via\ndifferentiable patch retrieval,\u201d in ECCV, 2020.\n[46] S. Sarto, M. Cornia, L. Baraldi, and R. Cucchiara, \u201cRetrieval-\naugmented transformer for image captioning,\u201d in CBMI, 2022.\n[47] R. Ramos, B. Martins etal., \u201cSmallcap: lightweight image captioning\nprompted with retrieval augmentation,\u201d in CVPR, 2023.\n[48] J. Chen, Y . Pan, Y . Li etal., \u201cRetrieval augmented convolutional\nencoder-decoder networks for video captioning,\u201d TOMCCAP, vol. 19,\nno. 1s, pp. 48:1\u201348:24, 2023.\n[49] J. Xu, Y .", "start_char_idx": 4157, "end_char_idx": 5593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e209caf1-ead6-467f-9555-1a2e27ae1c9f": {"__data__": {"id_": "e209caf1-ead6-467f-9555-1a2e27ae1c9f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "node_type": "1", "metadata": {}, "hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e02bfa91-da6a-4d9d-9f4e-2347f1e94a0f", "node_type": "1", "metadata": {}, "hash": "e5f3c064f008e09ee6687542919075f8b54044aab99ed4a359a26a7af4ea6f39", "class_name": "RelatedNodeInfo"}]}, "text": "[49] J. Xu, Y . Huang, J. Hou etal., \u201cRetrieval-augmented egocentric video\ncaptioning,\u201d arXiv:2401.00789, 2024.", "start_char_idx": 5578, "end_char_idx": 5689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9a22696-df65-46d8-9ae7-bc9360e912fb": {"__data__": {"id_": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e6a2f908-211f-415d-81d3-09169bfb6813", "node_type": "1", "metadata": {}, "hash": "eb97d5aaae1c9e28e4d03c9bb640716c99b6bf98136f2e020dfc0c8a8023ab8f", "class_name": "RelatedNodeInfo"}, {"node_id": "4e243190-6938-47b3-8006-d15ada0261cc", "node_type": "1", "metadata": {}, "hash": "c30a7ec0dd443d67dc1637e359654d89346b4ef7f16e37b8beb995cbb5a53abe", "class_name": "RelatedNodeInfo"}, {"node_id": "c514d792-de01-46d1-87ca-abf33eda5d0e", "node_type": "1", "metadata": {}, "hash": "d90631eb36dc84c8523b1e5398103b24dde746c1a8f2755ec083c5b6f35afbe8", "class_name": "RelatedNodeInfo"}, {"node_id": "e0992dd4-0c0d-498e-a846-4f7f7b68f59f", "node_type": "1", "metadata": {}, "hash": "753cd5a5e55d964c4bcef72ce1ae5071592ae9a5dae07cef279d37843b465b8f", "class_name": "RelatedNodeInfo"}, {"node_id": "e5c3898d-8380-4d44-bfa2-269be17f7145", "node_type": "1", "metadata": {}, "hash": "dc1ccf44a6c6fe428aa18abb96ebc067dabf366a6b268661ffe61a5da184f600", "class_name": "RelatedNodeInfo"}]}, "text": "[50] J. Seo, S. Hong etal., \u201cRetrieval-augmented score distillation for text-\nto-3d generation,\u201d arXiv:2402.02972, 2024.\n[51] M. Zhang, X. Guo, L. Pan etal., \u201cRemodiffuse: Retrieval-augmented\nmotion diffusion model,\u201d in ICCV, 2023.\n[52] X. Hu, X. Wu, Y . Shu, and Y . Qu, \u201cLogical form generation via multi-\ntask learning for complex question answering over knowledge bases,\u201d\ninCOLING, 2022.\n[53] X. Huang, J. Kim, and B. Zou, \u201cUnseen entity handling in complex\nquestion answering over knowledge base via language generation,\u201d in\nEMNLP Findings, 2021.\n[54] R. Das, M. Zaheer, D. Thai etal., \u201cCase-based reasoning for natural\nlanguage queries over knowledge bases,\u201d in EMNLP, 2021.\n[55] Z. Wang, W. Nie, Z. Qiao etal., \u201cRetrieval-based controllable molecule\ngeneration,\u201d in ICLR, 2022.\n[56] Q. Jin, Y . Yang, Q. Chen, and Z. Lu, \u201cGenegpt: Augmenting large\nlanguage models with domain tools for improved access to biomedical\ninformation,\u201d Bioinformatics, vol. 40, no. 2, p. btae075, 2024.\n[57] H. Li, Y . Su, D. Cai etal., \u201cA survey on retrieval-augmented text\ngeneration,\u201d arxiv:2202.01110, 2022.\n[58] A. Asai, S. Min, Z. Zhong, and D. Chen, \u201cAcl 2023 tutorial: Retrieval-\nbased language models and applications,\u201d ACL 2023, 2023.\n[59] Y . Gao, Y . Xiong etal., \u201cRetrieval-augmented generation for large\nlanguage models: A survey,\u201d arxiv:2312.10997, 2023.\n[60] R. Zhao, H. Chen etal., \u201cRetrieving multimodal information for\naugmented generation: A survey,\u201d in EMNLP, 2023.", "start_char_idx": 0, "end_char_idx": 1470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fa08b5f-e917-4b42-9ebe-3e710e3c170e": {"__data__": {"id_": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "bff354a3-9750-4889-a0dd-11699b472edd", "node_type": "1", "metadata": {}, "hash": "a090b4b62ad158c3d1eff259bd59dc8e13e83c4660da5ec4ceb8349a9d702ac4", "class_name": "RelatedNodeInfo"}, {"node_id": "e895353f-be88-42b4-934b-393a825705db", "node_type": "1", "metadata": {}, "hash": "99a6434b29246bcfbef8dd3840bd8bbaf891b077d8466cb6e17e1bee332b9f6d", "class_name": "RelatedNodeInfo"}, {"node_id": "3dba9662-1f9c-4b25-8f7a-cd783c2ae84c", "node_type": "1", "metadata": {}, "hash": "33efce2539999b18ca35a5ab149bb5b9a5874cad5595a96a87c250c2c760379e", "class_name": "RelatedNodeInfo"}, {"node_id": "9c2e66f1-7761-4419-a5df-cd1f3ea06672", "node_type": "1", "metadata": {}, "hash": "2826c44898df37a4742b621df8f67423537a661ad09aaa96ccdc91d0f5a0f669", "class_name": "RelatedNodeInfo"}, {"node_id": "7089157f-0053-48fd-a59b-f1648da859b3", "node_type": "1", "metadata": {}, "hash": "867d8cd5a95f027f7c856bf49abab2ac13e43610e40f1bb579ad7a67a1f5372a", "class_name": "RelatedNodeInfo"}]}, "text": "[61] J. Chen, H. Guo, K. Yi etal., \u201cVisualgpt: Data-efficient adaptation of\npretrained language models for image captioning,\u201d in CVPR, 2022.\n[62] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, \u201cEfficient transformers:\nA survey,\u201d CSUR, vol. 55, no. 6, pp. 109:1\u2013109:28, 2023.\n[63] G. V . Houdt etal., \u201cA review on the long short-term memory model,\u201d\nArtif. Intell. Rev., vol. 53, no. 8, pp. 5929\u20135955, 2020.\n[64] L. Yang, Z. Zhang etal., \u201cDiffusion models: A comprehensive survey\nof methods and applications,\u201d CSUR, vol. 56, no. 4, pp. 1\u201339, 2023.\n[65] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d\ninICML, 2015.\n[66] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d in NeurIPS, 2020.\n[67] A. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffusion proba-\nbilistic models,\u201d in ICML, 2021.\n[68] Y . Song and S. Ermon, \u201cGenerative modeling by estimating gradients\nof the data distribution,\u201d in NeurIPS, 2019.\n[69] \u2014\u2014, \u201cImproved techniques for training score-based generative mod-\nels,\u201d in NeurIPS, 2020.\n[70] Y . Song, J. Sohl-Dickstein, D. P. Kingma etal., \u201cScore-based generative\nmodeling through stochastic differential equations,\u201d in ICLR, 2021.\n[71] Y . Song, C. Durkan, I. Murray, and S. Ermon, \u201cMaximum likelihood\ntraining of score-based diffusion models,\u201d in NeurIPS, 2021.", "start_char_idx": 1471, "end_char_idx": 2861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "381ee8e8-4d6a-4558-b143-621cfd55728c": {"__data__": {"id_": "381ee8e8-4d6a-4558-b143-621cfd55728c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "745f7e7c-d8e7-4889-9eb5-618fcad78057", "node_type": "1", "metadata": {}, "hash": "ecea6b6fcd3f012d1896fabc7c08adc74e40a9fcf5ad790037736e49a09d34ad", "class_name": "RelatedNodeInfo"}, {"node_id": "a1e8f74d-fa3f-413c-8b62-dd170422eaff", "node_type": "1", "metadata": {}, "hash": "e956286d16e8242e1a80157a7936c287286453d76c2cd5f424873c3060d9189f", "class_name": "RelatedNodeInfo"}, {"node_id": "1800ede4-c7b2-4de9-b55d-2b34066e14ae", "node_type": "1", "metadata": {}, "hash": "625e626d11766e2f788af3cc6c6fc2cbff06939a111f51df981f8e0afae50807", "class_name": "RelatedNodeInfo"}, {"node_id": "abde9574-d6b3-4840-bcc0-e33de5bd3070", "node_type": "1", "metadata": {}, "hash": "8e0eaaa16361aebfbf01015d519265747ac47f753454a3641f90fe706c402a06", "class_name": "RelatedNodeInfo"}, {"node_id": "c9c2b5a6-8b09-40ba-ac0f-b2a6873f5f23", "node_type": "1", "metadata": {}, "hash": "3be0b42916ed68b31f592385e2c5b9aaf74b99ab37a70e501f53c53b16664b1f", "class_name": "RelatedNodeInfo"}]}, "text": "[72] L. Yang, H. Qian, Z. Zhang etal., \u201cStructure-guided adversarial training\nof diffusion models,\u201d in CVPR, 2024.\n[73] X. Zhang, L. Yang, Y . Cai etal., \u201cRealcompo: Dynamic equilibrium\nbetween realism and compositionality improves text-to-image diffusion\nmodels,\u201d arXiv:2402.12908, 2024.\n[74] R. Rombach, A. Blattmann, D. Lorenz etal., \u201cHigh-resolution image\nsynthesis with latent diffusion models,\u201d in CVPR, 2022.\n[75] A. Ramesh, P. Dhariwal, A. Nichol etal., \u201cHierarchical text-conditional\nimage generation with clip latents,\u201d arXiv:2204.06125, 2022.\n[76] H. Li, Y . Yang, M. Chang etal., \u201cSrdiff: Single image super-resolution\nwith diffusion probabilistic models,\u201d Neurocomputing, vol. 479, pp.\n47\u201359, 2022.\n[77] J. Ho, C. Saharia, W. Chan etal., \u201cCascaded diffusion models for high\nfidelity image generation,\u201d JMLR, vol. 23, no. 1, pp. 2249\u20132281, 2022.\n[78] L. Yang, J. Liu, S. Hong etal., \u201cImproving diffusion-based image\nsynthesis with context prediction,\u201d in NeurIPS, 2024.24\n[79] L. Yang, Z. Yu, C. Meng etal., \u201cMastering text-to-image diffu-\nsion: Recaptioning, planning, and generating with multimodal llms,\u201d\narXiv:2401.11708, 2024.\n[80] S. Gong, M. Li, J. Feng etal., \u201cDiffuseq: Sequence to sequence text\ngeneration with diffusion models,\u201d in ICLR, 2023.\n[81] X. Li, J. Thickstun, I. Gulrajani etal., \u201cDiffusion-lm improves control-\nlable text generation,\u201d in NeurIPS, 2022.", "start_char_idx": 2862, "end_char_idx": 4248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56d2b4a8-ff0a-4335-939d-9aeb172ee460": {"__data__": {"id_": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d4afe67-524b-4449-a53a-5b1d6ae6c70f", "node_type": "1", "metadata": {}, "hash": "8edc8e65283541f8c15f5ecce593f0b9e2b2228dce4d420f53679e94054d3b16", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e58987c4-bd55-4bf0-bb09-1698c797a463", "node_type": "1", "metadata": {}, "hash": "d3b2f988a6ede2eb558eab53553d2703ee3e43096808515c01d041e841a36687", "class_name": "RelatedNodeInfo"}, {"node_id": "0e90a33d-f58e-4365-a8a9-4ffa41e09c95", "node_type": "1", "metadata": {}, "hash": "c284589570f68a43f745abde60a0ebdac3cfbb5bf6b619af0d3f4f8b64928d6a", "class_name": "RelatedNodeInfo"}, {"node_id": "e5bbbc6e-ba7f-4ffd-a27f-c7606dca596e", "node_type": "1", "metadata": {}, "hash": "b40023c0fe1a3fd176e7b8872af8036701a46b6d7294d7d2ee00c9cc1be41bbb", "class_name": "RelatedNodeInfo"}, {"node_id": "77e00ea8-8fb5-4044-91fd-7b108537e696", "node_type": "1", "metadata": {}, "hash": "bf3777df857681a33c3915572d2ba0608a557201169a931fa5351735775bc0b2", "class_name": "RelatedNodeInfo"}, {"node_id": "afeebcea-17f8-4375-bf26-5a290876fe4b", "node_type": "1", "metadata": {}, "hash": "e48d2e31b94aaf985dde26b9365c57ea82b6f086ca79bf9968b94305633f7803", "class_name": "RelatedNodeInfo"}]}, "text": "[82] J. Austin, D. D. Johnson, J. Ho etal., \u201cStructured denoising diffusion\nmodels in discrete state-spaces,\u201d in NeurIPS, 2021.\n[83] T. Chen, R. Zhang, and G. Hinton, \u201cAnalog bits: Generating discrete\ndata using diffusion models with self-conditioning,\u201d in ICLR, 2023.\n[84] J. Ho, W. Chan, C. Saharia etal., \u201cImagen video: High definition video\ngeneration with diffusion models,\u201d arXiv:2210.02303, 2022.\n[85] W. Harvey, S. Naderiparizi, V . Masrani etal., \u201cFlexible diffusion\nmodeling of long videos,\u201d in NeurIPS, 2022.\n[86] R. Yang, P. Srivastava, and S. Mandt, \u201cDiffusion probabilistic modeling\nfor video generation,\u201d Entropy, vol. 25, no. 10, p. 1469, 2023.\n[87] M. Zhang, Z. Cai, L. Pan etal., \u201cMotiondiffuse: Text-driven human\nmotion generation with diffusion model,\u201d TPAMI, 2024.\n[88] L. Yang, Z. Zhang, Z. Yu etal., \u201cCross-modal contextualized diffusion\nmodels for text-guided visual generation and editing,\u201d in ICLR, 2024.\n[89] N. Anand and T. Achim, \u201cProtein structure and sequence gen-\neration with equivariant denoising diffusion probabilistic models,\u201d\narXiv:2205.15019, 2022.\n[90] M. Xu, L. Yu, Y . Song etal., \u201cGeodiff: A geometric diffusion model\nfor molecular conformation generation,\u201d in ICLR, 2022.\n[91] E. Hoogeboom, V . G. Satorras, C. Vignac, and M. Welling, \u201cEquivari-\nant diffusion for molecule generation in 3d,\u201d in ICML, 2022.\n[92] B. Jing, G. Corso, J. Chang etal., \u201cTorsional diffusion for molecular\nconformer generation,\u201d in NeurIPS, 2022.", "start_char_idx": 4249, "end_char_idx": 5715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d4afe67-524b-4449-a53a-5b1d6ae6c70f": {"__data__": {"id_": "4d4afe67-524b-4449-a53a-5b1d6ae6c70f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "node_type": "1", "metadata": {}, "hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "cbc3690b-efdf-4a99-943f-660533e7b5f9", "node_type": "1", "metadata": {}, "hash": "8edc8e65283541f8c15f5ecce593f0b9e2b2228dce4d420f53679e94054d3b16", "class_name": "RelatedNodeInfo"}]}, "text": "[93] Z. Huang, L. Yang, X. Zhou etal., \u201cProtein-ligand interaction prior for\nbinding-aware 3d molecule diffusion models,\u201d in ICLR, 2024.", "start_char_idx": 5716, "end_char_idx": 5852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8034349-37aa-434a-baba-a1780c471578": {"__data__": {"id_": "c8034349-37aa-434a-baba-a1780c471578", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "32985451-0f54-4e07-80c7-552d7245cb21", "node_type": "1", "metadata": {}, "hash": "6e1a5a210dc7d842d97c68742a1cdfb9ee154be44ce2d8b8d5d9f15e7e2f84b0", "class_name": "RelatedNodeInfo"}, {"node_id": "82a40930-4bf1-484d-8e80-27b16b533ce9", "node_type": "1", "metadata": {}, "hash": "c3fab7c99fcaa1e6924d82710f438cb1a5b46b08f11ddd7c8353261eafaa9e7c", "class_name": "RelatedNodeInfo"}, {"node_id": "cbb1b4f2-8ae6-42b3-b5a5-32c0fd1a45bc", "node_type": "1", "metadata": {}, "hash": "4c977108c3535d2913c7e9f1c7b448f2d8043b89fc8b530eac08f82f82ba2c71", "class_name": "RelatedNodeInfo"}, {"node_id": "401146da-c25d-4ba5-ad60-1e09269f229f", "node_type": "1", "metadata": {}, "hash": "ad0a048b31dfa982b05f70d4998785d5e24708a8bedb3a284200f1244c6b2c1b", "class_name": "RelatedNodeInfo"}, {"node_id": "cf048c23-77df-48f8-b313-a9669d8d1d1c", "node_type": "1", "metadata": {}, "hash": "a8320f9c6c05fb94f30817a491c529d5fc38ddcea3f49ffca8a3bd6fc1e5c2d2", "class_name": "RelatedNodeInfo"}, {"node_id": "1691a577-07ca-4fb1-ae78-7496bf7c34f2", "node_type": "1", "metadata": {}, "hash": "5152f74bb3173eb01ee428cb6017b0d6fd1cdc9e2995df6a3d7df4421d5a47a5", "class_name": "RelatedNodeInfo"}]}, "text": "[94] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit mod-\nels,\u201d in ICLR, 2021.\n[95] X. Liu, C. Gong, and Q. Liu, \u201cFlow straight and fast: Learning to\ngenerate and transfer data with rectified flow,\u201d 2023.\n[96] Y . Song, P. Dhariwal, M. Chen, and I. Sutskever, \u201cConsistency models,\u201d\ninICML, 2023.\n[97] J. Gui, Z. Sun, Y . Wen etal., \u201cA review on generative adversarial\nnetworks: Algorithms, theory, and applications,\u201d TKDE, vol. 35, no. 4,\npp. 3313\u20133332, 2023.\n[98] S. E. Robertson and S. Walker, \u201cOn relevance weights with little\nrelevance information,\u201d in SIGIR, 1997.\n[99] J. D. Lafferty and C. Zhai, \u201cDocument language models, query models,\nand risk minimization for information retrieval,\u201d in SIGIR, 2001.\n[100] Y . Liu, M. Ott etal., \u201cRoberta: A robustly optimized BERT pretraining\napproach,\u201d arxiv:1907.11692, 2019.\n[101] L. Xiong, C. Xiong, Y . Li etal., \u201cApproximate nearest neighbor\nnegative contrastive learning for dense text retrieval,\u201d in ICLR, 2021.\n[102] H. Zhang, Y . Gong, Y . Shen etal., \u201cAdversarial retriever-ranker for\ndense text retrieval,\u201d in ICLR, 2022.\n[103] Y . Qu, Y . Ding, J. Liu etal., \u201cRocketqa: An optimized training approach\nto dense passage retrieval for open-domain question answering,\u201d in\nNAACL-HLT, 2021.\n[104] L. Gao and J. Callan, \u201cCondenser: a pre-training architecture for dense\nretrieval,\u201d in EMNLP, 2021.\n[105] D. Guo, S. Ren etal., \u201cGraphcodebert: Pre-training code representa-\ntions with data flow,\u201d in ICLR, 2021.\n[106] Y .", "start_char_idx": 0, "end_char_idx": 1483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2effa91-65ac-4805-976d-d7b90f4eb09e": {"__data__": {"id_": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0ec3eea0-5965-4197-86c4-c6de0f49eb59", "node_type": "1", "metadata": {}, "hash": "016fa8a704ebeaa45b595a36bbe1b9d94b0ffe29f004eafa485e71a61313cd8c", "class_name": "RelatedNodeInfo"}, {"node_id": "b4d029c3-4be1-41fb-b582-9b251b13ce44", "node_type": "1", "metadata": {}, "hash": "bb607afb8ab9e4c79a6f9644d555f42617a7b6173500270a2801506da8dc44be", "class_name": "RelatedNodeInfo"}, {"node_id": "3ed89d36-44f5-49b6-b641-ec69f55c4c45", "node_type": "1", "metadata": {}, "hash": "a2912157e1bc157e274d9de5f00b6f9f1bb535d826099043061d1adce90e4b0a", "class_name": "RelatedNodeInfo"}, {"node_id": "d3afa8dc-ece3-4538-859d-2395de671669", "node_type": "1", "metadata": {}, "hash": "fb7bb6e405e2dac3f190c757accbfe4e8de75e9d4fc5b225c5e1fe7d625e20ab", "class_name": "RelatedNodeInfo"}, {"node_id": "284e6c20-913d-45df-943d-2415e097f426", "node_type": "1", "metadata": {}, "hash": "61c247edd1304705944f140830d9a1d9e9bea189fff7831e14ce337d565ef1fa", "class_name": "RelatedNodeInfo"}]}, "text": "[106] Y . Wang, W. Wang, S. R. Joty, and S. C. H. Hoi, \u201cCodet5: Identifier-\naware unified pre-trained encoder-decoder models for code understand-\ning and generation,\u201d in EMNLP, 2021.\n[107] S. Hershey, S. Chaudhuri etal., \u201cCNN architectures for large-scale\naudio classification,\u201d in ICASSP, 2017.\n[108] X. Yuan, Z. Lin, J. Kuen etal., \u201cMultimodal contrastive training for\nvisual representation learning,\u201d in CVPR, 2021.\n[109] J. Dong, X. Li, C. Xu etal., \u201cDual encoding for zero-example video\nretrieval,\u201d in CVPR, 2019.\n[110] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, \u201cFrozen in time:\nA joint video and image encoder for end-to-end retrieval,\u201d in ICCV,\n2021.\n[111] J. Zhan, J. Mao, Y . Liu etal., \u201cOptimizing dense retrieval model\ntraining with hard negatives,\u201d in SIGIR, 2021.\n[112] J. L. Bentley, \u201cMultidimensional binary search trees used for associa-\ntive searching,\u201d CACM, vol. 18, no. 9, pp. 509\u2013517, 1975.\n[113] W. Li, C. Feng, D. Lian etal., \u201cLearning balanced tree indexes for\nlarge-scale vector retrieval,\u201d in SIGKDDg, 2023.[114] M. Datar, N. Immorlica, P. Indyk etal., \u201cLocality-sensitive hashing\nscheme based on p-stable distributions,\u201d in SCG, 2004.\n[115] Y . A. Malkov and D. A. Yashunin, \u201cEfficient and robust approxi-\nmate nearest neighbor search using hierarchical navigable small world\ngraphs,\u201d TPAMI, vol. 42, no. 4, pp. 824\u2013836, 2018.\n[116] S. Jayaram Subramanya, F. Devvrit etal., \u201cDiskann: Fast accurate\nbillion-point nearest neighbor search on a single node,\u201d NeurIPS, 2019.", "start_char_idx": 1474, "end_char_idx": 2977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64e5a052-01aa-40bb-aa0a-d181d256bf60": {"__data__": {"id_": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "fb895d59-7721-489d-8f8f-a2433ddff395", "node_type": "1", "metadata": {}, "hash": "3a87a652a6f86075d377ce9afc2ccce3b28bdfcaffbe872dce1c347d018b12f6", "class_name": "RelatedNodeInfo"}, {"node_id": "e905510b-f520-4148-ac32-cfde6e07b550", "node_type": "1", "metadata": {}, "hash": "6671f6f4a810ac84da61d0269bb1e30d5229215a50a58043608b430e970fc719", "class_name": "RelatedNodeInfo"}, {"node_id": "e7fdee8d-fb8b-4b70-88d7-28387deff320", "node_type": "1", "metadata": {}, "hash": "4d7e1a39e009e85483a05336a17d42c29f88e56145e0203d53721033c59a77b3", "class_name": "RelatedNodeInfo"}, {"node_id": "d97b4cae-b4b0-4f06-a7be-1069f141ac10", "node_type": "1", "metadata": {}, "hash": "a1b1aaf1104de8f0bfa389a154d097aec46b0cfa48570c87403bc96b16c6acf2", "class_name": "RelatedNodeInfo"}, {"node_id": "dec47915-f50b-44a2-be1d-c858cfbae020", "node_type": "1", "metadata": {}, "hash": "c3171852d81b620af7e61e3eeb8c6fa8ddafc17b0f9dcfbee952350af953a8f5", "class_name": "RelatedNodeInfo"}]}, "text": "[117] J. Ren, M. Zhang, and D. Li, \u201cHm-ann: Efficient billion-point nearest\nneighbor search on heterogeneous memory,\u201d NeurIPS, 2020.\n[118] Y . Wang, Y . Hou, H. Wang etal., \u201cA neural corpus indexer for\ndocument retrieval,\u201d in NeurIPS, 2022.\n[119] H. Zhang, Y . Wang, Q. Chen etal., \u201cModel-enhanced vector index,\u201d\ninNeurIPS, 2023.\n[120] S. A. Hayati, R. Olivier, P. Avvaru etal., \u201cRetrieval-based neural code\ngeneration,\u201d in EMNLP, 2018.\n[121] J. Zhang, X. Wang, H. Zhang etal., \u201cRetrieval-based neural source\ncode summarization,\u201d in ICSE, 2020.\n[122] G. Poesia, A. Polozov, V . Le etal., \u201cSynchromesh: Reliable code\ngeneration from pre-trained language models,\u201d in ICLR, 2022.\n[123] X. Ye, S. Yavuz etal., \u201cRNG-KBQA: generation augmented iterative\nranking for knowledge base question answering,\u201d in ACL, 2022.\n[124] Y . Shu etal., \u201cTIARA: multi-grained retrieval for robust question\nanswering over large knowledge bases,\u201d arXiv:2210.12925, 2022.\n[125] X. V . Lin, R. Socher etal., \u201cBridging textual and tabular data for\ncross-domain text-to-sql semantic parsing,\u201d arXiv:2012.12627, 2020.\n[126] A. Asai, Z. Wu, Y . Wang etal., \u201cSelf-rag: Learning to retrieve, generate,\nand critique through self-reflection,\u201d arxiv:2310.11511, 2023.\n[127] W. Shi, S. Min, M. Yasunaga etal., \u201cReplug: Retrieval-augmented\nblack-box language models,\u201d arXiv:2301.12652, 2023.\n[128] O. Ram, Y .", "start_char_idx": 2978, "end_char_idx": 4349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0": {"__data__": {"id_": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be6792a4-da55-47a2-b289-7b1b5869811d", "node_type": "1", "metadata": {}, "hash": "f96435e0fc6aced74c8216eab27599c4dced159add03bf04dae32d2430393afc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "476c0ba3-36ff-4975-a779-221211e47d3b", "node_type": "1", "metadata": {}, "hash": "7fc2dfa101bea461f9d42ba6b5081e56374181aa78e977c8627aaf7190dab339", "class_name": "RelatedNodeInfo"}, {"node_id": "6448425b-2559-4546-8595-c3b65dd0e3e4", "node_type": "1", "metadata": {}, "hash": "0286e9e3d50cd3d87b69996feff4938ccd8fdfcdff7d4511d1eb04f082addb5c", "class_name": "RelatedNodeInfo"}, {"node_id": "7a35ffa7-9401-4a09-98af-479d127abace", "node_type": "1", "metadata": {}, "hash": "3ddd875a82ae87b6dacee134dfac0046dc0fc56803b10cda69076fb4a54f5115", "class_name": "RelatedNodeInfo"}, {"node_id": "c1814bc3-fec5-4b96-82b3-676e2b6b0347", "node_type": "1", "metadata": {}, "hash": "af2299a56891f2f862fccb94236f7866a0f5c59ef49ba60e0f5f0b1b84490e4b", "class_name": "RelatedNodeInfo"}, {"node_id": "c7191792-a0cf-4120-a368-bbd2841d09a2", "node_type": "1", "metadata": {}, "hash": "83ca15596edab9d4a4e4fa43e7bcc948adb4d49d2e4f199086788cd0f6913299", "class_name": "RelatedNodeInfo"}]}, "text": "[128] O. Ram, Y . Levine, I. Dalmedigos etal., \u201cIn-context retrieval-\naugmented language models,\u201d arXiv:2302.00083, 2023.\n[129] D. Zan, B. Chen, Z. Lin etal., \u201cWhen language model meets private\nlibrary,\u201d in EMNLP Findings, 2022.\n[130] N. Nashid, M. Sintaha, and A. Mesbah, \u201cRetrieval-based prompt\nselection for code-related few-shot learning,\u201d in ICSE, 2023.\n[131] M. Jin, S. Shahriar, M. Tufano etal., \u201cInferfix: End-to-end program\nrepair with llms,\u201d in ESEC/FSE, 2023.\n[132] S. Lu, N. Duan, H. Han etal., \u201cReacc: A retrieval-augmented code\ncompletion framework,\u201d in ACL, 2022.\n[133] Y . Liu etal., \u201cUni-parser: Unified semantic parser for question answer-\ning on knowledge base and database,\u201d in EMNLP, 2022.\n[134] Z. Yang, X. Du, E. Cambria etal., \u201cEnd-to-end case-based reasoning\nfor commonsense knowledge base completion,\u201d in EACL, 2023.\n[135] M. Patidar, A. K. Singh, R. Sawhney etal., \u201cCombining transfer\nlearning with in-context learning using blackbox llms for zero-shot\nknowledge base question answering,\u201d arXiv:2311.08894, 2023.\n[136] W. Shi, Y . Zhuang, Y . Zhu etal., \u201cRetrieval-augmented large language\nmodels for adolescent idiopathic scoliosis patients in shared decision-\nmaking,\u201d in ACM-BCB, 2023.\n[137] A. Casanova, M. Careil, J. Verbeek etal., \u201cInstance-conditioned gan,\u201d\ninNeurIPS, 2021.\n[138] J. Li, Y . Li, G. Li etal., \u201cEditsum: A retrieve-and-edit framework for\nsource code summarization,\u201d in ASE, 2021.", "start_char_idx": 4332, "end_char_idx": 5760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be6792a4-da55-47a2-b289-7b1b5869811d": {"__data__": {"id_": "be6792a4-da55-47a2-b289-7b1b5869811d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "26864623-e72d-4b91-b583-8d12ace4d8e1", "node_type": "1", "metadata": {}, "hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0f91f59a-b476-412b-9245-f889cbc210fd", "node_type": "1", "metadata": {}, "hash": "f96435e0fc6aced74c8216eab27599c4dced159add03bf04dae32d2430393afc", "class_name": "RelatedNodeInfo"}]}, "text": "[139] C. Yu, G. Yang, X. Chen etal., \u201cBashexplainer: Retrieval-augmented\nbash code comment generation based on fine-tuned codebert,\u201d in\nICSME, 2022.\n[140] T. B. Hashimoto, K. Guu, Y .", "start_char_idx": 5761, "end_char_idx": 5944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99895f25-3030-456c-a22c-ea588744446b": {"__data__": {"id_": "99895f25-3030-456c-a22c-ea588744446b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "60d49ff3-1b7b-4a96-8e1f-afabc2cee86c", "node_type": "1", "metadata": {}, "hash": "242fa1918ffb0da90b5b82992a5e4878ed0839be50c891420bee4863cbe09834", "class_name": "RelatedNodeInfo"}, {"node_id": "02c2a2a6-761e-4211-bfd5-86c926d63d22", "node_type": "1", "metadata": {}, "hash": "dbdd689ba75bfdd2280cc0d59256b3918b8eb1e8240b75a88b9ea05f218205b7", "class_name": "RelatedNodeInfo"}, {"node_id": "749657c9-194f-478d-b477-5fa7311abb91", "node_type": "1", "metadata": {}, "hash": "8edefe3efa2b413f716f0df12a5a3daa56a1c34812b2309e6bf3e96a2178e828", "class_name": "RelatedNodeInfo"}, {"node_id": "02ca04f3-521e-43e5-8af5-2acd94eed1c6", "node_type": "1", "metadata": {}, "hash": "09815c57fa0b93b03b3d2afa08a681a76e740d5cbbd73ac4a269f79f02a8fea3", "class_name": "RelatedNodeInfo"}, {"node_id": "0c4d1346-cb20-48bd-8b99-723773cb2c04", "node_type": "1", "metadata": {}, "hash": "464640c70193231b16ee89b72e5fbd7f8e746a1f99bc02e0b9ec8c04a71dde62", "class_name": "RelatedNodeInfo"}, {"node_id": "280e9ac2-9291-48a8-9bf0-2cbb0074a5b2", "node_type": "1", "metadata": {}, "hash": "3b39c5e6ce412a9abca8fc16f6cb3b40899ca3563cd916833c688215bc969554", "class_name": "RelatedNodeInfo"}]}, "text": "[140] T. B. Hashimoto, K. Guu, Y . Oren, and P. Liang, \u201cA retrieve-and-edit\nframework for predicting structured outputs,\u201d in NeurIPS, 2018.\n[141] B. Wei, Y . Li, G. Li etal., \u201cRetrieve and refine: Exemplar-based neural\ncomment generation,\u201d in ASE, 2020.\n[142] E. Shi, Y . Wang, W. Tao etal., \u201cRACE: retrieval-augmented commit\nmessage generation,\u201d in EMNLP, 2022.\n[143] B. Oguz, X. Chen, V . Karpukhin etal., \u201cUnik-qa: Unified repre-\nsentations of structured and unstructured knowledge for open-domain\nquestion answering,\u201d in NAACL Findings, 2022.\n[144] D. Yu, S. Zhang etal., \u201cDecaf: Joint decoding of answers and logical\nforms for question answering over knowledge bases,\u201d in ICLR, 2023.\n[145] G. Dong, R. Li, S. Wang etal., \u201cBridging the kb-text gap: Leveraging\nstructured knowledge-aware pre-training for KBQA,\u201d in CIKM, 2023.\n[146] K. Wang, F. Duan, S. Wang etal., \u201cKnowledge-driven cot: Exploring\nfaithful reasoning in llms for knowledge-intensive question answering,\u201d\narXiv:2308.13259, 2023.\n[147] D. Yu and Y . Yang, \u201cRetrieval-enhanced generative model for large-\nscale knowledge graph completion,\u201d in SIGIR, 2023.\n[148] W. Chen, H. Hu, C. Saharia, and W. W. Cohen, \u201cRe-imagen: Retrieval-\naugmented text-to-image generator,\u201d in ICLR, 2023.25\n[149] S. Sheynin, O. Ashual, A. Polyak etal., \u201cKnn-diffusion: Image gener-\nation via large-scale retrieval,\u201d in ICLR, 2023.\n[150] A. Blattmann, R. Rombach, K. Oktay etal., \u201cRetrieval-augmented\ndiffusion models,\u201d in NeurIPS, 2022.", "start_char_idx": 0, "end_char_idx": 1479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f1a8ce1-3de5-4339-af32-d75e11da34f5": {"__data__": {"id_": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9f60617e-1a2a-40e2-a93c-1dc9b6ab3bb4", "node_type": "1", "metadata": {}, "hash": "e4be48d349161dfcfb7e9ddb57c73f23fbe67cdf8afc366a4df412f0d7e81b65", "class_name": "RelatedNodeInfo"}, {"node_id": "3dc9760a-cda7-4939-8472-c521a7f46e66", "node_type": "1", "metadata": {}, "hash": "fcd46a50fcef1ddbfdd3e2b562f84c6a8692fbfea8bac39ea1ebfdf67970e8db", "class_name": "RelatedNodeInfo"}, {"node_id": "894a1219-36f1-4b31-986c-03bc089e1262", "node_type": "1", "metadata": {}, "hash": "de1d4b99c10db9aa551edd6112c6ea4ef5cd7a3bd8089bf226f53f9f7bdfdeb8", "class_name": "RelatedNodeInfo"}, {"node_id": "33afe81c-7321-4870-b761-ee193aa81ad2", "node_type": "1", "metadata": {}, "hash": "2b43f7fd4bbc49c5c11bce548c27d32b4e777aac016348e4a783bccf50e0bc91", "class_name": "RelatedNodeInfo"}, {"node_id": "6d1760c6-4e1b-40df-9be3-f2f77ecd10fd", "node_type": "1", "metadata": {}, "hash": "50e827384787f4f73008f65caef654179164d6ecb748091579c0581712b234af", "class_name": "RelatedNodeInfo"}]}, "text": "[151] R. Rombach, A. Blattmann, and B. Ommer, \u201cText-guided synthe-\nsis of artistic images with retrieval-augmented diffusion models,\u201d\narXiv:2207.13038, 2022.\n[152] B. Li, P. H. Torr, and T. Lukasiewicz, \u201cMemory-driven text-to-image\ngeneration,\u201d arXiv:2208.07022, 2022.\n[153] A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley, \u201cUnlimiformer:\nLong-range transformers with unlimited length input,\u201d 2023.\n[154] Y . Kuratov, A. Bulatov etal., \u201cIn search of needles in a 10m haystack:\nRecurrent memory finds what llms miss,\u201d arXiv:2402.10790, 2024.\n[155] N. F. Liu, K. Lin, J. Hewitt etal., \u201cLost in the middle: How language\nmodels use long contexts,\u201d arxiv:2307.03172, 2023.\n[156] T. F \u00b4evry, L. B. Soares etal., \u201cEntities as experts: Sparse memory access\nwith entity supervision,\u201d in EMNLP, 2020.\n[157] M. de Jong, Y . Zemlyanskiy, N. FitzGerald etal., \u201cMention memory:\nincorporating textual knowledge into transformers through entity men-\ntion attention,\u201d in ICLR, 2021.\n[158] B. Jing, Y . Zhang, Z. Song etal., \u201cAmd: Anatomical motion diffusion\nwith interpretable motion decomposition and fusion,\u201d in AAAI, 2024.\n[159] Y . Yuan, H. Liu, X. Liu etal., \u201cRetrieval-augmented text-to-audio\ngeneration,\u201d in ICASSP, 2024.\n[160] B. Yang, M. Cao, and Y . Zou, \u201cConcept-aware video captioning:\nDescribing videos with effective prior information,\u201d TIP, vol. 32, pp.\n5366\u20135378, 2023.", "start_char_idx": 1480, "end_char_idx": 2850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2690e2d-178c-4921-a734-f9334530f2e6": {"__data__": {"id_": "e2690e2d-178c-4921-a734-f9334530f2e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d9b944ae-7d4c-4001-9ede-131a7bae71ed", "node_type": "1", "metadata": {}, "hash": "e14a8fc720d36aaf7ebaf31e04a2330924feb61e5ec30599b093090550d214a1", "class_name": "RelatedNodeInfo"}, {"node_id": "e6ec21f9-4ef7-4c2f-9981-ebb4ea136d25", "node_type": "1", "metadata": {}, "hash": "c87f7538e91a66d29f4e65c5c400e389214e34bcc169fb018b5051b089013b82", "class_name": "RelatedNodeInfo"}, {"node_id": "e300f2d2-8346-4926-9e45-958579e50006", "node_type": "1", "metadata": {}, "hash": "6a7c02ced3fd4ac1205faed566ef63c127c203b47ba1cec9c0dc3a954355a87c", "class_name": "RelatedNodeInfo"}, {"node_id": "61eeeea0-f16d-4f99-861b-ab4e26d8f559", "node_type": "1", "metadata": {}, "hash": "e2e7fcd8947c9df3c2ea51aab17d9f5d0bd7ce6b71d0d262a20b1ea65e57c59a", "class_name": "RelatedNodeInfo"}, {"node_id": "cf9e4349-a22c-4b71-b1f4-43964496d78d", "node_type": "1", "metadata": {}, "hash": "ea193ec17f467678656fd450d8273954166a37c7dff016e229440b011ec23f24", "class_name": "RelatedNodeInfo"}]}, "text": "32, pp.\n5366\u20135378, 2023.\n[161] Z. Zhong, T. Lei, and D. Chen, \u201cTraining language models with\nmemory augmentation,\u201d in EMNLP, 2022.\n[162] S. Min, W. Shi, M. Lewis etal., \u201cNonparametric masked language\nmodeling,\u201d in ACL Findings, 2023.\n[163] X. Zhang, Y . Zhou, G. Yang, and T. Chen, \u201cSyntax-aware retrieval\naugmented code generation,\u201d in EMNLP Findings, 2023.\n[164] Z. Fei, \u201cMemory-augmented image captioning,\u201d in AAAI, 2021.\n[165] Y . Leviathan, M. Kalman, and Y . Matias, \u201cFast inference from trans-\nformers via speculative decoding,\u201d in ICML, 2023.\n[166] T. Lan, D. Cai, Y . Wang etal., \u201cCopy is all you need,\u201d in ICLR, 2023.\n[167] B. Cao, D. Cai, L. Cui etal., \u201cRetrieval is accurate generation,\u201d\narXiv:2402.17532, 2024.\n[168] L. Wang, N. Yang, and F. Wei, \u201cQuery2doc: Query expansion with\nlarge language models,\u201d in EMNLP, 2023.\n[169] L. Gao, X. Ma, J. Lin, and J. Callan, \u201cPrecise zero-shot dense retrieval\nwithout relevance labels,\u201d in ACL, 2023.\n[170] G. Kim, S. Kim, B. Jeon etal., \u201cTree of clarifications: Answering\nambiguous questions with retrieval-augmented large language models,\u201d\ninEMNLP, 2023.\n[171] M. Xia, S. Malladi, S. Gururangan etal., \u201cLESS: selecting influential\ndata for targeted instruction tuning,\u201d arXiv:2402.04333, 2024.\n[172] S. Yao, J. Zhao, D. Yu etal., \u201cReact: Synergizing reasoning and acting\nin language models,\u201d in ICLR, 2023.", "start_char_idx": 2826, "end_char_idx": 4186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d88f6d7f-5944-408b-bd56-1536e37a36df": {"__data__": {"id_": "d88f6d7f-5944-408b-bd56-1536e37a36df", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3592c0a4-8fb2-4091-8a28-67a42ea67b94", "node_type": "1", "metadata": {}, "hash": "d363c37c0208a96b75cdf2c8c5083629ba0e987c7fb6517541f69fbf03d4867c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "92eb3717-2b38-491d-baf3-6c0ff9b342e7", "node_type": "1", "metadata": {}, "hash": "b8e0c472b81098d79b399af1a212347f5ae96ad64c2652b5dee776368aeec2c2", "class_name": "RelatedNodeInfo"}, {"node_id": "3d25af56-930f-403f-8f20-f02c3a197bc7", "node_type": "1", "metadata": {}, "hash": "eef2f65a892389cd1e9b956738993356488fc8ad644dd1fc6ea75dce8ef8ff24", "class_name": "RelatedNodeInfo"}, {"node_id": "5414bd03-37a1-473a-858d-a56a90cd023b", "node_type": "1", "metadata": {}, "hash": "379e713ebe54fdbd426cd7c7ee6304a56209e04313ba11b9c925945aa7c8ca07", "class_name": "RelatedNodeInfo"}, {"node_id": "8e7379bb-e063-4df2-8867-a18129fcda65", "node_type": "1", "metadata": {}, "hash": "be9119a02e9d2969b0135681db778caa41f5b3a7f4deb0e4e4e11118aaee5598", "class_name": "RelatedNodeInfo"}, {"node_id": "abf97515-a241-4826-aae1-39007c84f619", "node_type": "1", "metadata": {}, "hash": "756e1017767c48066deec1d3e8e016d8f66895d088a09215e924c76dba96e7f6", "class_name": "RelatedNodeInfo"}]}, "text": "[173] J. Wei, X. Wang, D. Schuurmans etal., \u201cChain-of-thought prompting\nelicits reasoning in large language models,\u201d in NeurIPS, 2022.\n[174] T. Pouplin, H. Sun, S. Holt, and M. Van der Schaar, \u201cRetrieval-\naugmented thought process as sequential decision making,\u201d\narXiv:2402.07812, 2024.\n[175] J. Liu, \u201cLlamaIndex,\u201d 11 2022. [Online]. Available: https://github.\ncom/jerryjliu/llama index\n[176] P. Sarthi, S. Abdullah, A. Tuli etal., \u201cRaptor: Recursive abstractive\nprocessing for tree-organized retrieval,\u201d in ICLR, 2023.\n[177] S. Xiao, Z. Liu, P. Zhang etal., \u201cC-pack: Packaged resources to advance\ngeneral chinese embedding,\u201d arxiv:2309.07597, 2023.\n[178] J. Chen, S. Xiao, P. Zhang etal., \u201cBge m3-embedding: Multi-lingual,\nmulti-functionality, multi-granularity text embeddings through self-\nknowledge distillation,\u201d arxiv:2309.07597, 2023.\n[179] S. Xiao, Z. Liu, P. Zhang, and X. Xing, \u201cLm-cocktail: Resilient tuning\nof language models via model merging,\u201d arxiv:2311.13534, 2023.\n[180] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, \u201cRetrieve anything\nto augment large language models,\u201d arxiv:2310.07554, 2023.\n[181] M. Kulkarni, P. Tangarajan, K. Kim etal., \u201cReinforcement learning for\noptimizing RAG for domain chatbots,\u201d arXiv:2401.06800, 2024.\n[182] W. Wang, Y . Wang etal., \u201cRap-gen: Retrieval-augmented patch gener-\nation with codet5 for automatic program repair,\u201d in ESEC/FSE, 2023.\n[183] S.-Q. Yan, J.-C. Gu, Y .", "start_char_idx": 4187, "end_char_idx": 5613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3592c0a4-8fb2-4091-8a28-67a42ea67b94": {"__data__": {"id_": "3592c0a4-8fb2-4091-8a28-67a42ea67b94", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd", "node_type": "1", "metadata": {}, "hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4acb61f4-1b83-4081-9d70-8affb4aa5e0c", "node_type": "1", "metadata": {}, "hash": "d363c37c0208a96b75cdf2c8c5083629ba0e987c7fb6517541f69fbf03d4867c", "class_name": "RelatedNodeInfo"}]}, "text": "Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, \u201cCorrective retrieval\naugmented generation,\u201d arXiv:2401.15884, 2024.\n[184] W. Huang, M. Lapata, P. V ougiouklis etal., \u201cRetrieval augmented\ngeneration with rich answer encoding,\u201d in IJCNLP-AACL, 2023.", "start_char_idx": 5595, "end_char_idx": 5835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d": {"__data__": {"id_": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4530412d-0461-4aec-8c2e-94451c75012d", "node_type": "1", "metadata": {}, "hash": "ae2cb352b8a9a5b6a37bb9a45fdbd374d533da31d0634b37b84c984be158a440", "class_name": "RelatedNodeInfo"}, {"node_id": "0e1b6ac2-7a00-4b97-981b-5fc244c31c5e", "node_type": "1", "metadata": {}, "hash": "796cc5569f7b79f17e863cb81c44dd080744263907b9b900251d4c9fff9c50c2", "class_name": "RelatedNodeInfo"}, {"node_id": "045dbdf7-80fc-4477-9cdd-fe8ba3dffe30", "node_type": "1", "metadata": {}, "hash": "db6d4dbab74a08a5e05a2caf0c64a41251f0051f7446415202216221934bce42", "class_name": "RelatedNodeInfo"}, {"node_id": "f9735bdd-5304-4a97-9c17-ebaf054f6327", "node_type": "1", "metadata": {}, "hash": "4a6ccc1997816d156334f654f25e0bd57f793ede736e3f22d03a2d5844eb3b0e", "class_name": "RelatedNodeInfo"}, {"node_id": "19526a39-41ed-4d66-bb7a-142aa63cc92d", "node_type": "1", "metadata": {}, "hash": "040830d10fababd13698456acb44eb487bca10f3b354605bd93448d378206a39", "class_name": "RelatedNodeInfo"}]}, "text": "[185] H. Wang, W. Huang, Y . Deng etal., \u201cUnims-rag: A unified multi-source\nretrieval-augmented generation for personalized dialogue systems,\u201d\narXiv:2401.13256, 2024.\n[186] M. R. Glass, G. Rossiello, M. F. M. Chowdhury etal., \u201cRe2g: Retrieve,\nrerank, generate,\u201d in NAACL, 2022.\n[187] R. F. Nogueira and K. Cho, \u201cPassage re-ranking with BERT,\u201d\narxiv:1901.04085, 2019.\n[188] J. Li, Y . Zhao, Y . Li etal., \u201cAcecoder: Utilizing existing code to\nenhance code generation,\u201d arXiv:2303.17780, 2023.\n[189] P. Shi, R. Zhang, H. Bai, and J. Lin, \u201cXRICL: cross-lingual retrieval-\naugmented in-context learning for cross-lingual text-to-sql semantic\nparsing,\u201d in EMNLP Findings, 2022.\n[190] K. Rangan and Y . Yin, \u201cA fine-tuning enhanced rag system with\nquantized influence measure as ai judge,\u201d arXiv:2402.17081, 2024.\n[191] J. Saad-Falcon, O. Khattab, K. Santhanam etal., \u201cUdapdr: Unsu-\npervised domain adaptation via llm prompting and distillation of\nrerankers,\u201d in EMNLP, 2023.\n[192] L. Wang, N. Yang, and F. Wei, \u201cLearning to retrieve in-context\nexamples for large language models,\u201d arXiv:2307.07164, 2023.\n[193] Z. Wang, J. Araki, Z. Jiang etal., \u201cLearning to filter context for\nretrieval-augmented generation,\u201d arxiv:2311.08377, 2023.\n[194] S. Hofst \u00a8atter, J. Chen, K. Raman, and H. Zamani, \u201cFid-light: Efficient\nand effective retrieval-augmented text generation,\u201d in SIGIR, 2023.", "start_char_idx": 0, "end_char_idx": 1376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbdf320d-0210-4751-bc8e-420cdcb3c46a": {"__data__": {"id_": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7b9fbb45-d937-4b73-a820-43f0c42e1d9d", "node_type": "1", "metadata": {}, "hash": "abe78ff677d5d2c4973ee48b761aa71102ea35ec9a4edb501adcd551f115bc81", "class_name": "RelatedNodeInfo"}, {"node_id": "e5ab0743-bfe3-4e21-9e1b-3113313cb509", "node_type": "1", "metadata": {}, "hash": "7ddafac6cdd173978a7f489d4d9bd4dc8ca2c4955cc3746a23219de709851f9e", "class_name": "RelatedNodeInfo"}, {"node_id": "4041d27a-1bdd-45ff-a06e-af9f1e2f8b98", "node_type": "1", "metadata": {}, "hash": "d8cbe2d91a7c82a65608b245827df267ceb113705fb9d98880f941550e9a03c8", "class_name": "RelatedNodeInfo"}, {"node_id": "cefee77b-cebd-48e7-a40a-9c1f7c770a2b", "node_type": "1", "metadata": {}, "hash": "995f0a9f1316a9dc1420e1b4e77e03e8c616a3ad556862778514c819d2edadc0", "class_name": "RelatedNodeInfo"}, {"node_id": "50ce162f-34f2-4eda-811c-936d27b9fb31", "node_type": "1", "metadata": {}, "hash": "b3168d27d763c71b013c14ba72d78326d639ad460b2f2afc750b798b102db7bc", "class_name": "RelatedNodeInfo"}]}, "text": "[195] D. Arora, A. Kini, S. R. Chowdhury etal., \u201cGar-meets-rag paradigm\nfor zero-shot information retrieval,\u201d arXiv:2310.20158, 2023.\n[196] https://www.pinecone.io.\n[197] W. Yu, D. Iter etal., \u201cGenerate rather than retrieve: Large language\nmodels are strong context generators,\u201d arXiv:2209.10063, 2022.\n[198] A. Abdallah and A. Jatowt, \u201cGenerator-retriever-generator: A novel ap-\nproach to open-domain question answering,\u201d arXiv:2307.11278, 2023.\n[199] E. Saravia, \u201cPrompt Engineering Guide,\u201d\nhttps://github.com/dair-ai/Prompt-Engineering-Guide, 12 2022.\n[200] H. S. Zheng, S. Mishra etal., \u201cTake a step back: Evoking reasoning\nvia abstraction in large language models,\u201d arxiv:2310.06117, 2023.\n[201] S. Diao, P. Wang, Y . Lin, and T. Zhang, \u201cActive prompting with chain-\nof-thought for large language models,\u201d arxiv:2302.12246, 2023.\n[202] H. Jiang, Q. Wu, C. Lin etal., \u201cLlmlingua: Compressing prompts for\naccelerated inference of large language models,\u201d in EMNLP, 2023.\n[203] T. Ahmed, K. S. Pai, P. Devanbu, and E. T. Barr, \u201cAutomatic semantic\naugmentation of language model prompts (for code summarization),\u201d\narXiv:2304.06815, 2024.\n[204] Z. Xu, Z. Liu, Y . Liu etal., \u201cActiverag: Revealing the treasures of\nknowledge via active learning,\u201d arXiv:2402.13547, 2024.\n[205] E. Nijkamp, B. Pang, H. Hayashi etal., \u201cA conversational paradigm\nfor program synthesis,\u201d arxiv:2203.13474, 2022.\n[206] Y .", "start_char_idx": 1377, "end_char_idx": 2775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb544f90-4fa9-4e51-9b79-d85553445142": {"__data__": {"id_": "eb544f90-4fa9-4e51-9b79-d85553445142", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1a369f9b-0d6f-4bbe-9c0b-fc461b2629a8", "node_type": "1", "metadata": {}, "hash": "2241eb8ba9fbe7c1fc140f0461669da8bd0095626d26d246ccb691a6df0e5244", "class_name": "RelatedNodeInfo"}, {"node_id": "22e7c4c1-23b8-4bb4-ae60-589ba5c3d7a3", "node_type": "1", "metadata": {}, "hash": "7db6672cf96271b26bad2242b171ec987ed22abb86de121d54cac6d6b82f4e23", "class_name": "RelatedNodeInfo"}, {"node_id": "691ba182-53b9-4313-9afc-5c68f3bfb72a", "node_type": "1", "metadata": {}, "hash": "b81ddc7e1d9920321aaf31e9b029618e53314c01d589f49270cf601f0cb98cd3", "class_name": "RelatedNodeInfo"}, {"node_id": "0567f877-e780-4e95-a4a9-238de74ab8fa", "node_type": "1", "metadata": {}, "hash": "2ab0ba8a9e45669097f5dbaec7306fcbdf937cfca7513d37cdddbff8d8b45e89", "class_name": "RelatedNodeInfo"}, {"node_id": "fe3b96dc-a5d8-4db6-9f81-0c2bab67df04", "node_type": "1", "metadata": {}, "hash": "e4fc90625dc3578202c399fca6ea56894a2d8d85bf0fa1c8ab71f41e30d5250a", "class_name": "RelatedNodeInfo"}]}, "text": "[206] Y . He, M. Xia, H. Chen etal., \u201cAnimate-a-story: Storytelling with\nretrieval-augmented video generation,\u201d arXiv:2307.06940, 2023.\n[207] E. J. Hu, Y . Shen, P. Wallis etal., \u201cLora: Low-rank adaptation of large\nlanguage models,\u201d in ICLR, 2022.\n[208] C. Liu, P. C \u00b8 etin, Y . Patodia etal., \u201cAutomated code editing with search-\ngenerate-modify,\u201d arXiv:2306.06490, 2023.\n[209] H. Joshi, J. P. C. S \u00b4anchez, S. Gulwani etal., \u201cRepair is nearly\ngeneration: Multilingual program repair with llms,\u201d in AAAI, 2023.\n[210] Z. Jiang, F. F. Xu, L. Gao etal., \u201cActive retrieval augmented genera-\ntion,\u201d arXiv:2305.06983, 2023.\n[211] A. Mallen, A. Asai, V . Zhong etal., \u201cWhen not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric\nmemories,\u201d in ACL, 2023.\n[212] Z. Jiang, J. Araki, H. Ding, and G. Neubig, \u201cHow can we know When\nlanguage models know? on the calibration of language models for\nquestion answering,\u201d TACL, 2021.\n[213] N. Kandpal, H. Deng, A. Roberts etal., \u201cLarge language models\nstruggle to learn long-tail knowledge,\u201d in ICML, 2023.\n[214] R. Ren, Y . Wang, Y . Qu etal., \u201cInvestigating the factual knowledge\nboundary of large language models with retrieval augmentation,\u201d\narxiv:2307.11019, 2023.\n[215] Y . Wang, P. Li, M. Sun, and Y . Liu, \u201cSelf-knowledge guided retrieval\naugmentation for large language models,\u201d in EMNLP Findings, 2023.", "start_char_idx": 2766, "end_char_idx": 4147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed003dcb-d68a-41b8-88ef-93e05e14cee3": {"__data__": {"id_": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87de77de-8c9a-4dc5-987b-0dc007f852a7", "node_type": "1", "metadata": {}, "hash": "1cea245af87ea4c2c0f46e0709808b42ed0bfb92ef38349f3939fecce04411e0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8dbfd7da-3238-4854-8260-ae26e9f8190d", "node_type": "1", "metadata": {}, "hash": "6cdf8def1341f4a14a5bfef8a35884e5690195447b399c386f528728919c0864", "class_name": "RelatedNodeInfo"}, {"node_id": "83b74235-3739-4666-9e38-f63ed2c59679", "node_type": "1", "metadata": {}, "hash": "fc75bc8f6cbd4323e33271d315807750b35481ba08a9c0eb577694f585121312", "class_name": "RelatedNodeInfo"}, {"node_id": "e688435b-aaaf-4141-87e8-f1e22fecb5a6", "node_type": "1", "metadata": {}, "hash": "d20b7023b89841ef24f0bec946084949fa971a3bd849d37250dbceb014a45811", "class_name": "RelatedNodeInfo"}, {"node_id": "ac07f681-2700-4b32-af6a-a4597a472e52", "node_type": "1", "metadata": {}, "hash": "5e839c61ab73b9c5265881ee99630c477ca003ccbf54646d98f82db2fa0dd7ce", "class_name": "RelatedNodeInfo"}, {"node_id": "9e12552e-e877-4bb0-84ae-6c51230f6ad8", "node_type": "1", "metadata": {}, "hash": "921fd551544d041ad3c293aea7109b6ae8f1c7ef39bc70ab8e194cbe80fe0271", "class_name": "RelatedNodeInfo"}]}, "text": "[216] H. Ding, L. Pang, Z. Wei etal., \u201cRetrieve only when it needs: Adaptive\nretrieval augmentation for hallucination mitigation in large language\nmodels,\u201d arXiv:2402.10612, 2024.\n[217] S. Jeong, J. Baek, S. Cho etal., \u201cAdaptive-rag: Learning to adapt\nretrieval-augmented large language models through question complex-\nity,\u201d arXiv:2403.14403, 2024.\n[218] F. Zhang, B. Chen etal., \u201cRepocoder: Repository-level code completion\nthrough iterative retrieval and generation,\u201d in EMNLP, 2023.26\n[219] Z. Shao, Y . Gong, Y . Shen etal., \u201cEnhancing retrieval-augmented\nlarge language models with iterative retrieval-generation synergy,\u201d in\nEMNLP Findings, 2023.\n[220] X. Cheng, D. Luo, X. Chen etal., \u201cLift yourself up: Retrieval-\naugmented text generation with self-memory,\u201d in NeurIPS, 2023.\n[221] Z. Wang, A. Liu, H. Lin etal., \u201cRat: Retrieval augmented\nthoughts elicit context-aware reasoning in long-horizon generation,\u201d\narXiv:2403.05313, 2024.\n[222] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, \u201cKnowledge graph\nbased synthetic corpus generation for knowledge-enhanced language\nmodel pre-training,\u201d in NAACL-HLT, 2021.\n[223] J. Sun, C. Xu, L. Tang etal., \u201cThink-on-graph: Deep and respon-\nsible reasoning of large language model with knowledge graph,\u201d\narXiv:2307.07697, 2023.\n[224] P. Limkonchotiwat, W. Ponwitayarat, C. Udomcharoenchaikit etal.,\n\u201cCl-relkt: Cross-lingual language knowledge transfer for multilingual\nretrieval question answering,\u201d in NAACL Findings, 2022.", "start_char_idx": 4148, "end_char_idx": 5620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87de77de-8c9a-4dc5-987b-0dc007f852a7": {"__data__": {"id_": "87de77de-8c9a-4dc5-987b-0dc007f852a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "node_type": "1", "metadata": {}, "hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0de97f25-2e9e-4277-9c7a-25200581dd02", "node_type": "1", "metadata": {}, "hash": "1cea245af87ea4c2c0f46e0709808b42ed0bfb92ef38349f3939fecce04411e0", "class_name": "RelatedNodeInfo"}]}, "text": "[225] A. Asai, X. Yu, J. Kasai, and H. Hajishirzi, \u201cOne question answering\nmodel for many languages with cross-lingual dense passage retrieval,\u201d\ninNeurIPS, 2021.\n[226] K. Lee, S. Han etal., \u201cWhen to read documents or QA history: On\nunified and selective open-domain QA,\u201d in ACL Findings, 2023.", "start_char_idx": 5621, "end_char_idx": 5914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab": {"__data__": {"id_": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "710a03ff-d5ec-48fc-bb24-240df35a4ac8", "node_type": "1", "metadata": {}, "hash": "8682a7d99b5b1ed95823e92a26c87cdb76b67db2ede71a6adb89122de519e5b1", "class_name": "RelatedNodeInfo"}, {"node_id": "25839326-3a8d-4ccc-bfd8-f565f47279db", "node_type": "1", "metadata": {}, "hash": "a36bbdda813e718fbde1f001e6b5bfd3f5ab2e5d1ccc70c92aa998800e5c543e", "class_name": "RelatedNodeInfo"}, {"node_id": "3551135e-b305-4d1f-ad3f-740b6e77e337", "node_type": "1", "metadata": {}, "hash": "7bf987c4b8fb0a7d480d24cf0c53f4fc31d506fe7713a833ca31712cd59d305c", "class_name": "RelatedNodeInfo"}, {"node_id": "ca24b290-c3e9-4f9d-912a-914981fc368f", "node_type": "1", "metadata": {}, "hash": "c128cbfca05c7ca28b713879b0a9b437579fd2b75867d0ffabd4973579c544e2", "class_name": "RelatedNodeInfo"}, {"node_id": "c091822f-cf84-43c4-912a-f283e9175500", "node_type": "1", "metadata": {}, "hash": "b6c6136f6c1d3b5b265e2c1e0ea846eb144283e68f089251b111209ff0ec99f9", "class_name": "RelatedNodeInfo"}]}, "text": "[227] S. Yue, W. Chen etal., \u201cDisc-lawllm: Fine-tuning large language\nmodels for intelligent legal services,\u201d arXiv:2309.11325, 2023.\n[228] S. Siriwardhana, R. Weerasekera, T. Kaluarachchi etal., \u201cImproving\nthe domain adaptation of retrieval augmented generation (RAG) models\nfor open domain question answering,\u201d TACL, vol. 11, pp. 1\u201317, 2023.\n[229] Y . Tang and Y . Yang, \u201cMultihop-rag: Benchmarking retrieval-\naugmented generation for multi-hop queries,\u201d arXiv:2401.15391, 2024.\n[230] K. Huang, C. Zhai, and H. Ji, \u201cCONCRETE: improving cross-lingual\nfact-checking with cross-lingual retrieval,\u201d in COLING, 2022.\n[231] L. Hagstr \u00a8om, D. Saynova, T. Norlund etal., \u201cThe effect of scaling,\nretrieval augmentation and form on the factual consistency of language\nmodels,\u201d arXiv:2311.01307, 2023.\n[232] Y . Liu, Y . Wan etal., \u201cKG-BART: knowledge graph-augmented BART\nfor generative commonsense reasoning,\u201d in AAAI, 2021.\n[233] A. Wan, E. Wallace, and D. Klein, \u201cWhat evidence do language models\nfind convincing?\u201d arXiv:2402.11782, 2024.\n[234] H. Zhang, Z. Liu etal., \u201cGrounded conversation generation as guided\ntraverses in commonsense knowledge graphs,\u201d in ACL, 2020.\n[235] D. Cai, Y . Wang, W. Bi etal., \u201cSkeleton-to-response: Dialogue gener-\nation guided by retrieval memory,\u201d in NAACL-HLT, 2019.\n[236] M. Komeili, K. Shuster, and J. Weston, \u201cInternet-augmented dialogue\ngeneration,\u201d in ACL, 2022.\n[237] K. Shuster, J. Xu etal., \u201cBlenderbot 3: a deployed conversational agent\nthat continually learns to responsibly engage,\u201d arXiv:2208.03188, 2022.", "start_char_idx": 0, "end_char_idx": 1547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086": {"__data__": {"id_": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "978a5215-1623-4a61-8feb-9255f630d968", "node_type": "1", "metadata": {}, "hash": "128ace82b6ae2563c7a381efeb4fbb6624ccacb97311a365bce02e073c1f601f", "class_name": "RelatedNodeInfo"}, {"node_id": "f126c652-bc73-4eae-90a8-9e0fdf8e0aa7", "node_type": "1", "metadata": {}, "hash": "c3839d871f54de06c2a76bc8a3eb9cc4526d9b321cce19a8654890a5f4a60ced", "class_name": "RelatedNodeInfo"}, {"node_id": "7d4e9a1a-35c4-4795-96f8-97258f6ae872", "node_type": "1", "metadata": {}, "hash": "c383037de9ecff9b72bb6cddc18dd274e506c81fcd081b5919146741709e6520", "class_name": "RelatedNodeInfo"}, {"node_id": "0932b200-84c6-4488-bfc5-2396cd5c9b95", "node_type": "1", "metadata": {}, "hash": "cf9c85b69747bc4b53da10ddab70e413839c435a1ab4dd5df6f68768c47472b3", "class_name": "RelatedNodeInfo"}, {"node_id": "759f6c8c-8c8c-4cad-a6b3-a5984bcc99d5", "node_type": "1", "metadata": {}, "hash": "2136eb08e48e73e21fe2062daf9e91b8273cc994dbc7f57b7759fada6378125e", "class_name": "RelatedNodeInfo"}]}, "text": "[238] S. Kim, J. Y . Jang, M. Jung, and S. Shin, \u201cA model of cross-lingual\nknowledge-grounded response generation for open-domain dialogue\nsystems,\u201d in EMNLP Findings, 2021.\n[239] E. Nie, S. Liang, H. Schmid, and H. Sch \u00a8utze, \u201cCross-lingual retrieval\naugmented prompt for low-resource languages,\u201d in ACL, 2023.\n[240] X. Li, E. Nie, and S. Liang, \u201cFrom classification to generation: Insights\ninto crosslingual retrieval augmented icl,\u201d in NeurIPS, 2023.\n[241] W. Li, J. Li, W. Ma, and Y . Liu, \u201cCitation-enhanced generation for\nllm-based chatbot,\u201d arXiv:2402.16063, 2024.\n[242] D. Cai, Y . Wang, H. Li etal., \u201cNeural machine translation with\nmonolingual translation memory,\u201d in ACL/IJCNLP, 2021.\n[243] U. Khandelwal, A. Fan, D. Jurafsky etal., \u201cNearest neighbor machine\ntranslation,\u201d in ICLR, 2021.\n[244] X. Du and H. Ji, \u201cRetrieval-augmented generative question answering\nfor event argument extraction,\u201d in EMNLP, 2022.\n[245] Y . Gao, Q. Yin, Z. Li etal., \u201cRetrieval-augmented multilingual\nkeyphrase generation with retriever-generator iterative training,\u201d in\nNAACL Findings, 2022.\n[246] J. Zhang, E. J. Yu, Q. Chen etal., \u201cRetrieval-based full-length wikipedia\ngeneration for emergent events,\u201d arXiv:2402.18264, 2024.\n[247] R. Fan, Y . Fan, J. Chen etal., \u201cRIGHT: retrieval-augmented generation\nfor mainstream hashtag recommendation,\u201d arxiv:2312.10466, 2023.\n[248] Y . Wang, H. Le, A. D. Gotmare etal., \u201cCodet5mix: A pretrained\nmixture of encoder-decoder transformers for code understanding and\ngeneration,\u201d 2022.", "start_char_idx": 1548, "end_char_idx": 3063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ceb9831-1954-4b8d-a153-1a613db8408a": {"__data__": {"id_": "1ceb9831-1954-4b8d-a153-1a613db8408a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a043749e-7ef5-4b9e-aeb1-6890d7db8c8d", "node_type": "1", "metadata": {}, "hash": "fbbeb9a212c3f4f49364acb3a9eba0c5bbd72e63a8fb61d3324a6916da91bb0a", "class_name": "RelatedNodeInfo"}, {"node_id": "fb6f45f0-2d5b-41ac-9bfd-8950cde6e539", "node_type": "1", "metadata": {}, "hash": "86e4f89610c7e3535b341ebae1ad370be8bf7ce6a888f135c323c73ea208fd29", "class_name": "RelatedNodeInfo"}, {"node_id": "3077a316-a55b-413f-9eb6-138dbe78451a", "node_type": "1", "metadata": {}, "hash": "2c71fd75f5b6d894bbe32eab16472a1741facda3bc0bc5e60a16c91c6d83c10d", "class_name": "RelatedNodeInfo"}, {"node_id": "03f038cc-c3c7-4d93-91da-42d3daa5a276", "node_type": "1", "metadata": {}, "hash": "06dd0c1be81a8bfbab95f733151d92c790302e982e3bab37cb514ae9bea4098e", "class_name": "RelatedNodeInfo"}, {"node_id": "6575adcf-4350-4942-834d-e5fa50c56dd1", "node_type": "1", "metadata": {}, "hash": "5a6039f86ea79bced90d965f8685dc525aad4dd7a9c2eedeaafd5f97495220f8", "class_name": "RelatedNodeInfo"}]}, "text": "[249] E. Nijkamp, B. Pang, H. Hayashi etal., \u201cA conversational paradigm\nfor program synthesis,\u201d arXiv:2203.13474, 2022.\n[250] D. Zan, B. Chen, Y . Gong etal., \u201cPrivate-library-oriented code gener-\nation with large language models,\u201d arXiv:2307.15370, 2023.\n[251] A. Madaan, S. Zhou, U. Alon etal., \u201cLanguage models of code are\nfew-shot commonsense learners,\u201d in EMNLP, 2022.[252] Y . Wang, H. Le, A. Gotmare etal., \u201cCodet5+: Open code large language\nmodels for code understanding and generation,\u201d in EMNLP, 2023.\n[253] D. Liao, S. Pan, Q. Huang etal., \u201cContext-aware code generation\nframework for code repositories: Local, global, and third-party library\nawareness,\u201d arXiv:2312.05772, 2023.\n[254] J. Li, Y . Li, G. Li etal., \u201cSkcoder: A sketch-based approach for\nautomatic code generation,\u201d in ICSE, 2023.\n[255] M. Liu, T. Yang, Y . Lou etal., \u201cCodegen4libs: A two-stage approach\nfor library-oriented code generation,\u201d in ASE, 2023.\n[256] K. Zhang, J. Li, G. Li etal., \u201cCodeagent: Enhancing code generation\nwith tool-integrated agent systems for real-world repo-level coding\nchallenges,\u201d arXiv:2401.07339, 2024.\n[257] Q. Gou, Y . Dong, Y . Wu, and Q. Ke, \u201cRrgcode: Deep hierarchical\nsearch-based code generation,\u201d Journal ofSystems andSoftware, vol.\n211, p. 111982, 2024.\n[258] J. Chen, X. Hu, Z. Li etal., \u201cCode search is all you need? improving\ncode suggestions with code search,\u201d in ICSE, 2024.\n[259] H. Su, S. Jiang, Y .", "start_char_idx": 3064, "end_char_idx": 4487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c62f556-ced5-4c6a-8278-780ade4f4643": {"__data__": {"id_": "9c62f556-ced5-4c6a-8278-780ade4f4643", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "22825eff-8ad1-4d03-823a-8e232176aa19", "node_type": "1", "metadata": {}, "hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0fbf824b-c772-4db5-b5fa-ef25a6ab47fa", "node_type": "1", "metadata": {}, "hash": "af3a2449cf55f4b962a86b0e5675b18594cef35a79afe06b94acc77f291806bd", "class_name": "RelatedNodeInfo"}, {"node_id": "cec0d187-467b-4b39-9224-7ca575532cbb", "node_type": "1", "metadata": {}, "hash": "9a9344b675485242253c35109b798d6986e8a79712271ea270e2495f86921f06", "class_name": "RelatedNodeInfo"}, {"node_id": "b2f6ade5-eaca-44b1-af47-fb893e56fa1d", "node_type": "1", "metadata": {}, "hash": "225257a78780b4879ed5be2d5ddec36375a986deff3558cd3c9e5afe06806331", "class_name": "RelatedNodeInfo"}, {"node_id": "6ea88205-e0b2-44cf-9548-75623465ac25", "node_type": "1", "metadata": {}, "hash": "888afc592a16b6180226fa6e42d716f2662fe7d039489c45f3becc4e656666e9", "class_name": "RelatedNodeInfo"}, {"node_id": "113fc76d-16b2-4b81-ba17-6abb5a94f511", "node_type": "1", "metadata": {}, "hash": "257e8d2008a1e29564a58175d585c69ab6279e4f4cfce81ba8f838a13e99344f", "class_name": "RelatedNodeInfo"}]}, "text": "[259] H. Su, S. Jiang, Y . Lai etal., \u201cArks: Active retrieval in knowledge soup\nfor code generation,\u201d arXiv:2402.12317, 2024.\n[260] N. Beau and B. Crabb \u00b4e, \u201cThe impact of lexical and grammatical pro-\ncessing on generating code from natural language,\u201d in ACL Findings,\n2022.\n[261] K. Zhang, G. Li, J. Li etal., \u201cToolcoder: Teach code generation models\nto use API search tools,\u201d arXiv:2305.04032, 2023.\n[262] S. Liu, Y . Chen, X. Xie etal., \u201cRetrieval-augmented generation for\ncode summarization via hybrid GNN,\u201d in ICLR, 2021.\n[263] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck, \u201cModeling and\ndiscovering vulnerabilities with code property graphs,\u201d in S&P, 2014.\n[264] Y . Choi, C. Na etal., \u201cReadsum: Retrieval-augmented adaptive trans-\nformer for source code summarization,\u201d IEEE Access, 2023.\n[265] A. Alokla, W. Gad, W. Nazih etal., \u201cRetrieval-based transformer\npseudocode generation,\u201d Mathematics, vol. 10, no. 4, p. 604, 2022.\n[266] J. Zhao, X. Chen, G. Yang, and Y . Shen, \u201cAutomatic smart contract\ncomment generation via large language models and in-context learn-\ning,\u201d IST, vol. 168, p. 107405, 2024.\n[267] J. Xu, Z. Cui etal., \u201cUnilog: Automatic logging via LLM and in-\ncontext learning,\u201d in ICSE, 2024.\n[268] H. Wang, X. Xia etal., \u201cContext-aware retrieval-based deep commit\nmessage generation,\u201d TOSEM, vol. 30, no. 4, pp. 56:1\u201356:30, 2021.\n[269] X. Zhu, C. Sha, and J. Niu, \u201cA simple retrieval-based method for code\ncomment generation,\u201d in SANER, 2022.", "start_char_idx": 4461, "end_char_idx": 5926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40459286-87b0-4707-b53e-47333e6c46d3": {"__data__": {"id_": "40459286-87b0-4707-b53e-47333e6c46d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e687371a-384d-4866-bbed-8676f1b0a469", "node_type": "1", "metadata": {}, "hash": "03c166355cfcabed5a64a146b3e7ef2187f51a7bb239fe34b196ef1cdfcf55e0", "class_name": "RelatedNodeInfo"}, {"node_id": "dd6520ae-784e-4cca-9dbf-a7a4daa3831b", "node_type": "1", "metadata": {}, "hash": "87af8aef80571944f8d4745d45cbb599ca7c0d86c1a2433dbaa098d509a8d94a", "class_name": "RelatedNodeInfo"}, {"node_id": "7ced47e8-c50b-41d1-9e66-05758ec081c7", "node_type": "1", "metadata": {}, "hash": "82d340f379882ea34926b5b83ac223777c323ab548df5bb41f3d93e9dae525d7", "class_name": "RelatedNodeInfo"}, {"node_id": "5478d272-b834-4aea-b564-51d14663a9e3", "node_type": "1", "metadata": {}, "hash": "91899be3da3e19910bbc91cadb966b3d5486e4f05c54dc2f536e2efcfd2994be", "class_name": "RelatedNodeInfo"}, {"node_id": "2e51a6da-ad2d-453a-bfe9-5c2c4fc099da", "node_type": "1", "metadata": {}, "hash": "1593eff9db65962ae3c58ff20c4d01235f1b4d597bb3fea27990036c3ccfceda", "class_name": "RelatedNodeInfo"}]}, "text": "[270] T. Ye, L. Wu, T. Ma etal., \u201cTram: A token-level retrieval-augmented\nmechanism for source code summarization,\u201d arXiv:2305.11074, 2023.\n[271] L. Li, B. Liang, L. Chen, and X. Zhang, \u201cCross-modal retrieval-\nenhanced code summarization based on joint learning for retrieval and\ngeneration,\u201d Available atSSRN 4724884.\n[272] D. Drain, C. Hu, C. Wu etal., \u201cGenerating code with the\nhelp of retrieved template functions and stack overflow answers,\u201d\narXiv:2104.05310, 2021.\n[273] S. Lu, D. Guo etal., \u201cCodexglue: A machine learning benchmark\ndataset for code understanding and generation,\u201d in NeurIPS Datasets\nandBenchmarks, 2021.\n[274] Y . Ding, Z. Wang etal., \u201cCocomic: Code completion by jointly\nmodeling in-file and cross-file context,\u201d arXiv:2212.10007, 2022.\n[275] D. Shrivastava, D. Kocetkov etal., \u201cRepofusion: Training code models\nto understand your repository,\u201d arXiv:2306.10998, 2023.\n[276] Z. Tang, J. Ge, S. Liu etal., \u201cDomain adaptive code completion via\nlanguage models and decoupled domain databases,\u201d in ASE, 2023.\n[277] W. Sun, H. Li, M. Yan etal., \u201cRevisiting and improving retrieval-\naugmented deep assertion generation,\u201d in ASE, 2023.\n[278] A. Eghbali and M. Pradel, \u201cDe-hallucinator: Iterative grounding for\nllm-based code completion,\u201d arXiv:2401.01701, 2024.\n[279] M. Liang, X. Xie, G. Zhang etal., \u201cRepofuse: Repository-level code\ncompletion with fused dual context,\u201d arXiv:2402.14323, 2024.\n[280] Y .", "start_char_idx": 0, "end_char_idx": 1422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2c1168f-777a-4fc0-965c-9ce04fe596de": {"__data__": {"id_": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "68e6cf96-535c-4e35-ac82-1e87ef0e7c77", "node_type": "1", "metadata": {}, "hash": "895087b36c2e9880905458ebe4e6847e198c7f48851cbcdec837dd8cb700cec9", "class_name": "RelatedNodeInfo"}, {"node_id": "04b6beaf-8804-4a39-9022-0b734e37e3a8", "node_type": "1", "metadata": {}, "hash": "5cefee2f7197dc42816f47d743c8cebb3b40797cf4ca13fe97833f58feb6cd8b", "class_name": "RelatedNodeInfo"}, {"node_id": "fedcb3ca-1430-4371-ad76-fc95ec58436f", "node_type": "1", "metadata": {}, "hash": "10d2e443e04e8b7f2b7380c20f3121ebb8cedd466df183dd8693f52b6a8db9e8", "class_name": "RelatedNodeInfo"}, {"node_id": "6f374abd-ef51-4f8f-a1da-f098442c5477", "node_type": "1", "metadata": {}, "hash": "9e92bd3b9337b987ffa729df8aef5a0b7d38d316e2bf222405e8f24e8ed41b11", "class_name": "RelatedNodeInfo"}, {"node_id": "981cb88b-4244-44d6-a938-3aa5646afe1d", "node_type": "1", "metadata": {}, "hash": "b7098b5232963024be15a4d4b1a4e59e97678966cad9ef2227bca4a44454177e", "class_name": "RelatedNodeInfo"}, {"node_id": "dfd31fdd-e9a9-4dab-a171-abb51b1b01e3", "node_type": "1", "metadata": {}, "hash": "dc8aaa2901a56043184176caa0df80ec6aa48c90174ce09e8c13da928d1b34c0", "class_name": "RelatedNodeInfo"}]}, "text": "[280] Y . Tsai, M. Liu, and H. Ren, \u201cRtlfixer: Automatically fixing RTL syntax\nerrors with large language models,\u201d arXiv:2311.16543, 2023.\n[281] B. Bogin, S. Gupta, P. Clark etal., \u201cLeveraging code to improve in-\ncontext learning for semantic parsing,\u201d arXiv:2311.09519, 2023.\n[282] H. Li, J. Zhang, C. Li, and H. Chen, \u201cResdsql: Decoupling schema\nlinking and skeleton parsing for text-to-sql,\u201d in AAAI, 2023.\n[283] K. Zhang, X. Lin, Y . Wang etal., \u201cRefsql: A retrieval-augmentation\nframework for text-to-sql generation,\u201d in EMNLP Findings, 2023.\n[284] S. Chang and E. Fosler-Lussier, \u201cSelective demonstrations for cross-\ndomain text-to-sql,\u201d arXiv:2310.06302, 2023.\n[285] L. Nan, Y . Zhao, W. Zou etal., \u201cEnhancing text-to-sql capabilities\nof large language models: A study on prompt design strategies,\u201d in\nEMNLP Findings, 2023.27\n[286] X. Zhang, D. Wang, L. Dou etal., \u201cMulti-hop table retrieval for open-\ndomain text-to-sql,\u201d arXiv:2402.10666, 2024.\n[287] H. Li, J. Zhang, H. Liu etal., \u201cCodes: Towards building open-source\nlanguage models for text-to-sql,\u201d arXiv:2402.16347, 2024.\n[288] Z. Jie and W. Lu, \u201cLeveraging training data in few-shot prompting for\nnumerical reasoning,\u201d arXiv:2305.18170, 2023.\n[289] M. Gao, J. Li, H. Fei etal., \u201cDe-fine: Decomposing and refining visual\nprograms with auto-feedback,\u201d arXiv:2311.12890, 2023.\n[290] Y .", "start_char_idx": 1413, "end_char_idx": 2761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2600ede9-5e4f-4920-99c6-a34d23a10d23": {"__data__": {"id_": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f0b5557e-d64e-44ba-a140-95adb1f5dfbd", "node_type": "1", "metadata": {}, "hash": "2f0d5bb64538c3dc0d9d82db5eb56041b9af6b28ff96ea525a9193f102e0ec8f", "class_name": "RelatedNodeInfo"}, {"node_id": "528e2c6f-bd10-4799-8f30-9da620bfa474", "node_type": "1", "metadata": {}, "hash": "a39560e3eb4e7d1e0c3e78ce1542cb9290915fef8e27df389806183b9ff531ca", "class_name": "RelatedNodeInfo"}, {"node_id": "8901e7d9-2c64-4f66-b5e7-84bee0d8cbab", "node_type": "1", "metadata": {}, "hash": "72b8ca19f09ee16ba22b64ec55eae35eec56c9d2cef23c5318923e6b6b83b06e", "class_name": "RelatedNodeInfo"}, {"node_id": "8c245c70-d293-4329-bf96-74de97202104", "node_type": "1", "metadata": {}, "hash": "fffb6554c2c675722446ebc493afebfde76e754f47831d2786ca3d97665692e2", "class_name": "RelatedNodeInfo"}, {"node_id": "488f77e7-8395-4fc0-b72f-91c002ec5a4f", "node_type": "1", "metadata": {}, "hash": "cc7d985edcd477bcdbb0ebd21f1d2800106095479c22a6c6833961e52b24f030", "class_name": "RelatedNodeInfo"}]}, "text": "[290] Y . Hao, W. Chen, Z. Zhou, and W. Cui, \u201cE&v: Prompting large\nlanguage models to perform static analysis by pseudo-code execution\nand verification,\u201d arXiv:2312.08477, 2023.\n[291] Y . Guo, Z. Li etal., \u201cRetrieval-augmented code generation for universal\ninformation extraction,\u201d arXiv:2311.02962, 2023.\n[292] G. Pinto, C. de Souza etal., \u201cLessons from building stackspot ai: A\ncontextualized ai coding assistant,\u201d arXiv:2311.18450, 2024.\n[293] Z. Liu, C. Chen, J. Wang etal., \u201cTesting the limits: Unusual text inputs\ngeneration for mobile app crash detection with large language model,\u201d\narXiv:2310.15657, 2023.\n[294] K. D. Bollacker, C. Evans etal., \u201cFreebase: a collaboratively created\ngraph database for structuring human knowledge,\u201d in SIGMOD, 2008.\n[295] Y . Shu and Z. Yu, \u201cData distribution bottlenecks in grounding language\nmodels to knowledge bases,\u201d arXiv:2309.08345, 2023.\n[296] D. Leake and D. J. Crandall, \u201cOn bringing case-based reasoning\nmethodology to deep learning,\u201d in ICCBR, 2020.\n[297] L. Zhang, J. Zhang etal., \u201cFC-KBQA: A fine-to-coarse composition\nframework for knowledge base question answering,\u201d in ACL, 2023.\n[298] J. Jiang, K. Zhou etal., \u201cStructgpt: A general framework for large\nlanguage model to reason over structured data,\u201d in EMNLP, 2023.\n[299] J. Baek, A. F. Aji, and A. Saffari, \u201cKnowledge-augmented language\nmodel prompting for zero-shot knowledge graph question answering,\u201d\narXiv:2306.04136, 2023.\n[300] P. Sen, S. Mavadia, and A. Saffari, \u201cKnowledge graph-augmented\nlanguage models for complex question answering,\u201d in NLRSE, 2023.\n[301] Y .", "start_char_idx": 2752, "end_char_idx": 4332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af068cb7-b5e2-45f7-819e-91abfb0c0dd8": {"__data__": {"id_": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2b79e9f-0df5-46f7-b817-a49c288a6411", "node_type": "1", "metadata": {}, "hash": "1c7c217317296557c05da625f77cc0c9529a58c4230c59a18a99f88ff08fe221", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1e703c2b-dba4-46cf-8ded-6734778d1d5a", "node_type": "1", "metadata": {}, "hash": "d844c544b26f303eed76930fbe0f0d359d0469eede8e1f3fb98e92e4109569f9", "class_name": "RelatedNodeInfo"}, {"node_id": "3ac57fb1-b698-4d77-b606-aa7ec2dde7bc", "node_type": "1", "metadata": {}, "hash": "55cb36b23880a53e722c7e524a501507fab326f9e42e4fa40dde0b2c89ed33da", "class_name": "RelatedNodeInfo"}, {"node_id": "5221a53c-2c0c-44db-8829-db95e656070e", "node_type": "1", "metadata": {}, "hash": "b080948b3ede0ac5e0e92dabee796077f7c390758f9948c28fd0a3dce7dbf31c", "class_name": "RelatedNodeInfo"}, {"node_id": "cc43bea9-8e24-4e6d-9241-e795f49eb960", "node_type": "1", "metadata": {}, "hash": "f3832e65463c8ed6b6d493856bf09835dd6fa7e6f864ece74cba14172708f6d1", "class_name": "RelatedNodeInfo"}, {"node_id": "e6947143-c9bc-4abe-9c9d-79a5fe1e7f8b", "node_type": "1", "metadata": {}, "hash": "f03ead6bf70d24c0db8c9a736e0b53c2b0276168cb84342e26ab62593d4db33d", "class_name": "RelatedNodeInfo"}]}, "text": "[301] Y . Wu, N. Hu, S. Bi etal., \u201cRetrieve-rewrite-answer: A kg-to-text\nenhanced llms framework for knowledge graph question answering,\u201d\narXiv:2309.11206, 2023.\n[302] C. Wang, Y . Xu, Z. Peng etal., \u201ckeqing: knowledge-based ques-\ntion answering is a nature chain-of-thought mentor of LLM,\u201d\narXiv:2401.00426, 2024.\n[303] J. Liu, S. Cao, J. Shi etal., \u201cProbing structured semantics under-\nstanding and generation of language models via question answering,\u201d\narXiv:2401.05777, 2024.\n[304] G. Xiong, J. Bao, and W. Zhao, \u201cInteractive-kbqa: Multi-turn inter-\nactions for knowledge base question answering with large language\nmodels,\u201d arXiv:2402.15131, 2024.\n[305] S. Chen, Q. Liu, Z. Yu etal., \u201cRetrack: A flexible and efficient\nframework for knowledge base question answering,\u201d in ACL, 2021.\n[306] D. Yu, C. Zhu, Y . Fang etal., \u201cKg-fid: Infusing knowledge graph in\nfusion-in-decoder for open-domain question answering,\u201d in ACL, 2022.\n[307] Z. Hu, Y . Xu, W. Yu etal., \u201cEmpowering language models with\nknowledge graph reasoning for open-domain question answering,\u201d in\nEMNLP, 2022.\n[308] M. Ju, W. Yu, T. Zhao etal., \u201cGrape: Knowledge graph enhanced\npassage reader for open-domain question answering,\u201d in EMNLP\nFindings, 2022.\n[309] Q. Yang, Q. Chen, W. Wang etal., \u201cEnhancing multi-modal multi-\nhop question answering via structured knowledge and unified retrieval-\ngeneration,\u201d in MM, 2023.\n[310] W. Zhao, Y . Liu, T. Niu etal., \u201cDIVKNOWQA: assessing the reasoning\nability of llms via open-domain question answering over knowledge\nbase and text,\u201d arXiv:2310.20170, 2023.", "start_char_idx": 4323, "end_char_idx": 5890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2b79e9f-0df5-46f7-b817-a49c288a6411": {"__data__": {"id_": "b2b79e9f-0df5-46f7-b817-a49c288a6411", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "node_type": "1", "metadata": {}, "hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a7dec7ad-180d-4584-aba7-1440a17e4926", "node_type": "1", "metadata": {}, "hash": "1c7c217317296557c05da625f77cc0c9529a58c4230c59a18a99f88ff08fe221", "class_name": "RelatedNodeInfo"}]}, "text": "[311] X. Wang, Q. Yang, Y . Qiu etal., \u201cKnowledgpt: Enhancing large\nlanguage models with retrieval and storage access on knowledge bases,\u201d\narXiv:2308.11761, 2023.", "start_char_idx": 5891, "end_char_idx": 6053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b862e92-4358-47af-950d-ebbb46d47f6d": {"__data__": {"id_": "3b862e92-4358-47af-950d-ebbb46d47f6d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1f3a2651-8e67-4f13-b519-4fb05133d539", "node_type": "1", "metadata": {}, "hash": "df56259fc0d40d1b77dc1eac26c524023327b96d67363bf07004bb33e60d14a2", "class_name": "RelatedNodeInfo"}, {"node_id": "7ab974a1-c2d5-4f5b-ba7f-7cb8895494fb", "node_type": "1", "metadata": {}, "hash": "2521929a6f40b4bdb1fe61250b278fac3f35e880df83ab5e0f092eeb0c504945", "class_name": "RelatedNodeInfo"}, {"node_id": "3daf5567-08fc-4895-9fc4-a5ddd2814134", "node_type": "1", "metadata": {}, "hash": "da05fb640e2428ef73a5f2dcf8e208b19e24f5f5f39778e6010d087e1cd8fd91", "class_name": "RelatedNodeInfo"}, {"node_id": "515b74fb-2324-44c3-bd65-4fb244f6e74d", "node_type": "1", "metadata": {}, "hash": "75f2d308eac87f9e29d3d3a24a6aa8be868a72e30cf3b1a30cadf415e7f25997", "class_name": "RelatedNodeInfo"}, {"node_id": "a4af6706-eb29-45ef-b74d-d7de98e6c324", "node_type": "1", "metadata": {}, "hash": "f5356c305bd9ede44480cbf292b58f3594d28989d3823c65fe9b8a26a1c9f0a1", "class_name": "RelatedNodeInfo"}]}, "text": "[312] S. Ko, H. Cho, H. Chae etal., \u201cEvidence-focused fact summa-\nrization for knowledge-augmented zero-shot question answering,\u201d\narXiv:2403.02966, 2024.\n[313] Y . Gao, L. Qiao, Z. Kan etal., \u201cTwo-stage generative question\nanswering on temporal knowledge graph using large language models,\u201d\narXiv:2402.16568, 2024.\n[314] T. Guo, Q. Yang, C. Wang etal., \u201cKnowledgenavigator: Leveraging\nlarge language models for enhanced reasoning over knowledge graph,\u201d\narXiv:2312.15880, 2023.\n[315] S. Min, J. Boyd-Graber, C. Alberti etal., \u201cNeurips 2020 efficientqa\ncompetition: Systems, analyses and lessons learned,\u201d in NeurIPS 2020\nCompetition andDemonstration Track, 2021.[316] A. H. Li, P. Ng, P. Xu etal., \u201cDual reader-parser on hybrid tex-\ntual and tabular evidence for open domain question answering,\u201d in\nACL/IJCNLP, 2021.\n[317] P. Christmann, R. S. Roy, and G. Weikum, \u201cConversational question\nanswering on heterogeneous sources,\u201d in SIGIR, 2022.\n[318] K. Ma, H. Cheng, X. Liu etal., \u201cOpen-domain question answering\nvia chain of reasoning over heterogeneous knowledge,\u201d in EMNLP\nFindings, 2022.\n[319] E. Park, S.-M. Lee etal., \u201cRink: reader-inherited evidence reranker for\ntable-and-text open domain question answering,\u201d in AAAI, 2023.\n[320] W. Zhao, Y . Liu, Y . Wan etal., \u201cLocalize, retrieve and fuse: A\ngeneralized framework for free-form question answering over tables,\u201d\narXiv:2309.11049, 2023.\n[321] F. Pan, M. Canim etal., \u201cEnd-to-end table question answering via\nretrieval-augmented generation,\u201d arXiv:2203.16714, 2022.\n[322] Z. Jiang, Y .", "start_char_idx": 0, "end_char_idx": 1541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "453b70bb-0eba-49cb-b453-f5e535d0ef69": {"__data__": {"id_": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2f4221be-6049-4901-8efb-e694d6bf02d3", "node_type": "1", "metadata": {}, "hash": "5ec09b093f3b6b6ab1d21c44c2cc76e7e59ee729cdeb4d1e8e52c5eb351196a9", "class_name": "RelatedNodeInfo"}, {"node_id": "ce5eeb5c-6acf-4e7b-930a-a01bbad620b5", "node_type": "1", "metadata": {}, "hash": "3648978ac2cd3e3d1f759bcfa6ced7d418797432a9bbdc3b867201c3a78e4da4", "class_name": "RelatedNodeInfo"}, {"node_id": "cb6869a9-dce1-4e01-b5f5-ab824ac0c734", "node_type": "1", "metadata": {}, "hash": "0c3eedac0486cfabe18c592fef3f7b353a44feec5286614ab7a0b58824261e93", "class_name": "RelatedNodeInfo"}, {"node_id": "d4eed92e-1087-49a4-a506-5e1c481ca07c", "node_type": "1", "metadata": {}, "hash": "938e50e1d287bff1ee90f2f42c7099ba96aecbf80240efaaff4af1d05db3ba20", "class_name": "RelatedNodeInfo"}, {"node_id": "d6078340-927d-4d92-b7c9-fd480fb21ce4", "node_type": "1", "metadata": {}, "hash": "4b195bc9bf2fd8e04dfb0e3b5f7ceddf9fbb40deffcbf83af2b677f720e01433", "class_name": "RelatedNodeInfo"}]}, "text": "[322] Z. Jiang, Y . Mao, P. He etal., \u201cOmnitab: Pretraining with natural\nand synthetic data for few-shot table-based question answering,\u201d in\nNAACL, 2022.\n[323] W. Zhong, J. Huang, Q. Liu etal., \u201cReasoning over hybrid chain for\ntable-and-text open domain question answering,\u201d in IJCAI, 2022.\n[324] A. S. Sundar and L. Heck, \u201cctbl: Augmenting large language models\nfor conversational tables,\u201d arXiv:2303.12024, 2023.\n[325] D. Min, N. Hu, R. Jin etal., \u201cExploring the impact of table-to-text\nmethods on augmenting llm-based question answering with domain\nhybrid data,\u201d arXiv:2402.12869, 2024.\n[326] S. Wu, Y . Li, D. Zhang, and Z. Wu, \u201cImproving knowledge-aware\ndialogue response generation by using human-written prototype dia-\nlogues,\u201d in EMNLP Findings, 2020.\n[327] M. Kang, J. M. Kwak etal., \u201cKnowledge-consistent dialogue genera-\ntion with knowledge graphs,\u201d in ICML Workshop, 2022.\n[328] Z. Ji, Z. Liu, N. Lee etal., \u201cRHO: reducing hallucination in open-\ndomain dialogues with knowledge grounding,\u201d in ACL Findings, 2023.\n[329] J. Baek, N. Chandrasekaran, S. Cucerzan etal., \u201cKnowledge-\naugmented large language models for personalized contextual query\nsuggestion,\u201d arXiv:2311.06318, 2023.\n[330] X. He, Y . Tian, Y . Sun etal., \u201cG-retriever: Retrieval-augmented\ngeneration for textual graph understanding and question answering,\u201d\narXiv:2402.07630, 2024.\n[331] Y . Kirstain, O. Levy, and A. Polyak, \u201cX&fuse: Fusing visual informa-\ntion in text-to-image generation,\u201d arXiv:2303.01000, 2023.", "start_char_idx": 1522, "end_char_idx": 3013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93e520eb-8d02-43c4-b427-0a8ba52aa2e8": {"__data__": {"id_": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d5d3ba2f-8b66-4e08-8aaf-5ece6ba7cf19", "node_type": "1", "metadata": {}, "hash": "8b714012ffb895a7b06bb6e0345c6c61df4fe702a3c7a24b89209d1b957abf7b", "class_name": "RelatedNodeInfo"}, {"node_id": "f4317992-1e40-424e-8538-d9ad7e83c557", "node_type": "1", "metadata": {}, "hash": "f5b2d4b4be2c9562849c991a94f84bcb1c3fca254e320b22e3526cb8a2ad023a", "class_name": "RelatedNodeInfo"}, {"node_id": "e7ea3364-fb19-4d7e-baa9-ea9b311062e0", "node_type": "1", "metadata": {}, "hash": "b97b335fef6dd815fbff2f8cbd6bf8b4d73a2fb1c65116096af0d3012d06e40e", "class_name": "RelatedNodeInfo"}, {"node_id": "0a07698f-dd77-4a84-a079-4546508d342a", "node_type": "1", "metadata": {}, "hash": "2dda67a7b890fa906707af3141ecb84314bb10db8a59b929b909f5a7f0be1851", "class_name": "RelatedNodeInfo"}, {"node_id": "848f1cde-102a-4379-ada5-a556f70b4b2b", "node_type": "1", "metadata": {}, "hash": "6d1114011a1106eceded9c900877c93a6a1d3cc3a1797fa194a358940c91b055", "class_name": "RelatedNodeInfo"}, {"node_id": "a1157e1f-ad11-4ca4-9184-987e74252bae", "node_type": "1", "metadata": {}, "hash": "ee4e1b792d4171e086d0235b7b2ffc7ea240936beb813429c737c7f2ffcc6cf6", "class_name": "RelatedNodeInfo"}]}, "text": "[332] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d NeurIPS, 2021.\n[333] Z. Zhang, A. Zhang, M. Li etal., \u201cMultimodal chain-of-thought\nreasoning in language models,\u201d arXiv:2302.00923, 2023.\n[334] C. Xu, M. Yang, X. Ao etal., \u201cRetrieval-enhanced adversarial train-\ning with dynamic memory-augmented attention for image paragraph\ncaptioning,\u201d Knowledge-Based Systems, vol. 214, p. 106730, 2021.\n[335] R. Ramos, D. Elliott, and B. Martins, \u201cRetrieval-augmented image\ncaptioning,\u201d in EACL, 2023.\n[336] Z. Hu, A. Iscen, C. Sun etal., \u201cReveal: Retrieval-augmented visual-\nlanguage pre-training with multi-source multimodal knowledge mem-\nory,\u201d in CVPR, 2023.\n[337] Z. Li, W. Zhao, X. Du etal., \u201cCross-modal retrieval and semantic\nrefinement for remote sensing image captioning,\u201d Remote Sensing,\nvol. 16, no. 1, p. 196, 2024.\n[338] Z. Yang, Z. Gan, J. Wang etal., \u201cAn empirical study of gpt-3 for\nfew-shot knowledge-based vqa,\u201d in AAAI, 2022.\n[339] W. Lin and B. Byrne, \u201cRetrieval augmented visual question answering\nwith outside knowledge,\u201d in EMNLP, 2022.\n[340] A. Fan, C. Gardent, C. Braud, and A. Bordes, \u201cAugmenting transform-\ners with knn-based composite memory for dialog,\u201d TACL, vol. 9, pp.\n82\u201399, 2021.\n[341] Z. Liang, H. Hu, C. Xu etal., \u201cMaria: A visual experience powered\nconversational agent,\u201d in ACL-IJCNLP, 2021.\n[342] Q. Fang and Y . Feng, \u201cNeural machine translation with phrase-level\nuniversal visual representations,\u201d in ACL, 2022.", "start_char_idx": 3014, "end_char_idx": 4487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db514a78-d261-471f-8efb-8514117e6f4e": {"__data__": {"id_": "db514a78-d261-471f-8efb-8514117e6f4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9278baa9-0cba-447f-b50c-77148e46b013", "node_type": "1", "metadata": {}, "hash": "29be3e881568d726fbf2459b3f5c8063aaf55426eea9a659531cae58f1f57c27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "317486b6-b619-44da-b962-cbee9a56f424", "node_type": "1", "metadata": {}, "hash": "76590c98284586dee98cf2ad0f5a22ade960808cf462ab763c3de0211a6b5a7d", "class_name": "RelatedNodeInfo"}, {"node_id": "b2b58adf-a680-4275-ae91-7be5c85e20a7", "node_type": "1", "metadata": {}, "hash": "b25ea2969b66a8fd75bb1ceed26c4f1cce59f24f62a1e89e7d15ffe28ac7a2f8", "class_name": "RelatedNodeInfo"}, {"node_id": "236a59e2-54bc-4212-a555-5ed4d2bce5c0", "node_type": "1", "metadata": {}, "hash": "1183292c66a8eb663f72dddc27f2a0b9f766082cfcd86b3ccb422b6ee8daf1d5", "class_name": "RelatedNodeInfo"}, {"node_id": "787786c1-3692-41ff-aaba-cf5b6b1c3e57", "node_type": "1", "metadata": {}, "hash": "327c353a260d80583864df84f0830ed5bfa4b5d7a76e8e616b49b5f657ba6535", "class_name": "RelatedNodeInfo"}, {"node_id": "089dcf6d-5b10-43cf-9ea5-1dea702c7438", "node_type": "1", "metadata": {}, "hash": "f2a6a931615fc6906432bbe4c85bbdee305a3ea8aef2b7471bce942f95f4518a", "class_name": "RelatedNodeInfo"}]}, "text": "[343] S. Whitehead, H. Ji, M. Bansal etal., \u201cIncorporating background\nknowledge into video description generation,\u201d in EMNLP, 2018.\n[344] C. Yin, J. Tang, Z. Xu, and Y . Wang, \u201cMemory augmented deep\nrecurrent neural network for video question answering,\u201d TNNLS,\nvol. 31, no. 9, pp. 3159\u20133167, 2019.\n[345] J. Pan, Z. Lin, Y . Ge etal., \u201cRetrieving-to-answer: Zero-shot video\nquestion answering with frozen large language models,\u201d in ICCV, 2023.\n[346] J. Lei, L. Yu, T. L. Berg, and M. Bansal, \u201cTvqa+: Spatio-temporal\ngrounding for video question answering,\u201d in ACL, 2020.\n[347] H. Le, N. Chen, and S. Hoi, \u201cVgnmn: Video-grounded neural module\nnetworks for video-grounded dialogue systems,\u201d in NAACL, 2022.28\n[348] Z. Wang, M. Li, R. Xu etal., \u201cLanguage models with image descriptors\nare strong few-shot video-language learners,\u201d in NeurIPS, 2022.\n[349] J. Yuan, S. Sun, D. Omeiza etal., \u201cRag-driver: Generalisable driving\nexplanations with retrieval-augmented in-context learning in multi-\nmodal large language model,\u201d arXiv:2402.10828, 2024.\n[350] S. Ghosh, S. Kumar, C. K. R. Evuru etal., \u201cRecap: retrieval-augmented\naudio captioning,\u201d in ICASSP, 2024.\n[351] B. Elizalde, S. Deshmukh, and H. Wang, \u201cNatural language supervision\nfor general-purpose audio representations,\u201d in ICASSP, 2024.\n[352] T. Kouzelis and V . Katsouros, \u201cWeakly-supervised automated audio\ncaptioning via text only training,\u201d in DCASE Workshop, 2023.", "start_char_idx": 4488, "end_char_idx": 5910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9278baa9-0cba-447f-b50c-77148e46b013": {"__data__": {"id_": "9278baa9-0cba-447f-b50c-77148e46b013", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "node_type": "1", "metadata": {}, "hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "94d116e3-498d-4027-89f8-a5254567289c", "node_type": "1", "metadata": {}, "hash": "29be3e881568d726fbf2459b3f5c8063aaf55426eea9a659531cae58f1f57c27", "class_name": "RelatedNodeInfo"}]}, "text": "[353] S. Deshmukh, B. Elizalde, D. Emmanouilidou etal., \u201cTraining audio\ncaptioning models without audio,\u201d in ICASSP, 2024.\n[354] L. Yang, Z. Huang, X. Zhou etal., \u201cPrompt-based 3d molecular\ndiffusion models for structure-based drug design,\u201d 2023.", "start_char_idx": 5911, "end_char_idx": 6157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3caf8b77-3b63-4094-aee3-964c13c6c0f8": {"__data__": {"id_": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "node_type": "1", "metadata": {}, "hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "node_type": "1", "metadata": {}, "hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "78085d7d-4ea8-45e2-9aee-16f04562ec6e", "node_type": "1", "metadata": {}, "hash": "f765b393ec1faf2e6cf4a0fa1053401bde53029b79c42ea1efff2b6c3bbaee73", "class_name": "RelatedNodeInfo"}, {"node_id": "4b46f14d-21db-4664-98ba-8af0ed85d3ee", "node_type": "1", "metadata": {}, "hash": "0e3b0c278f9b142b890269cb5fd2d3e6d2d522d1387c3b904022f52fb0e2e3db", "class_name": "RelatedNodeInfo"}, {"node_id": "4930c5a3-29a2-47bb-bd40-8bae48133f38", "node_type": "1", "metadata": {}, "hash": "87e54c77c26f8671e26ce5feab1e894a573ee11e436db9488a51e71f8d5f7700", "class_name": "RelatedNodeInfo"}, {"node_id": "1034634c-de88-4479-b5e0-5222c05e3d8b", "node_type": "1", "metadata": {}, "hash": "ac0162af9ac43490214d70974394e0fd821b517f64ed5a95ec81fd40c47cba1f", "class_name": "RelatedNodeInfo"}, {"node_id": "8a76ebce-b70f-40d6-ab38-599e67907623", "node_type": "1", "metadata": {}, "hash": "5ef82369c213bb59ca1ce528702c523e591cdf7564fe7dc68076942b7010ef1f", "class_name": "RelatedNodeInfo"}]}, "text": "[355] T. Truong Jr and T. Bepler, \u201cPoet: A generative model of protein\nfamilies as sequences-of-sequences,\u201d NeurIPS, 2024.\n[356] G. Frisoni, M. Mizutani, G. Moro, and L. Valgimigli, \u201cBioreader: a\nretrieval-enhanced text-to-text transformer for biomedical literature,\u201d\ninEMNLP, 2022.\n[357] X. Yang, M. Ye, Q. You etal., \u201cWriting by memorizing: Hierarchical\nretrieval-based medical report generation,\u201d arXiv:2106.06471, 2021.\n[358] J. Kim and M. Min, \u201cFrom rag to qa-rag: Integrating generative ai\nfor pharmaceutical regulatory compliance process,\u201d arXiv:2402.01717,\n2024.\n[359] K. Yang, A. Swope etal., \u201cLeandojo: Theorem proving with retrieval-\naugmented language models,\u201d in NeurIPS, 2024.\n[360] Z. Levonian, C. Li, W. Zhu etal., \u201cRetrieval-augmented generation to\nimprove math question-answering: Trade-offs between groundedness\nand human preference,\u201d arXiv:2310.03184, 2023.\n[361] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large language\nmodels in retrieval-augmented generation,\u201d arxiv:2309.01431, 2023.\n[362] S. ES, J. James, L. E. Anke, and S. Schockaert, \u201cRAGAS: automated\nevaluation of retrieval augmented generation,\u201d arxiv:2309.15217, 2023.\n[363] J. Saad-Falcon, O. Khattab, C. Potts etal., \u201cARES: an automated\nevaluation framework for retrieval-augmented generation systems,\u201d\narxiv:2311.09476, 2023.\n[364] https://github.com/truera/trulens.\n[365] Y .", "start_char_idx": 0, "end_char_idx": 1370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03": {"__data__": {"id_": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "node_type": "1", "metadata": {}, "hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea3280e2-9061-4d36-86ab-2bc4d70fce08", "node_type": "1", "metadata": {}, "hash": "d84a4fa0aa5d5dfaf3467309165bb22c19101c2ed6dec85cb948cdc96c82bf34", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "node_type": "1", "metadata": {}, "hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "be0e2e87-03db-4fa1-8995-760715058d4c", "node_type": "1", "metadata": {}, "hash": "068dc30bc98aa568bf15ee519f219abb956711d691e7c3f8ddb632a39405530e", "class_name": "RelatedNodeInfo"}, {"node_id": "83de8f90-ecb7-4af1-889c-436add074d8c", "node_type": "1", "metadata": {}, "hash": "2d62cf3862b30951c57ac1d83c0e2b8371a5de387c8f8fcd17baa5b6f27c9e99", "class_name": "RelatedNodeInfo"}, {"node_id": "2b8bb825-14cb-4e09-83a1-e561d198f0db", "node_type": "1", "metadata": {}, "hash": "b0964e6394cad3acb35ab2e1c357ddaefeafb92228ea5e0f9c110039ee57b831", "class_name": "RelatedNodeInfo"}, {"node_id": "eea9192d-fb03-4210-8a8b-4f4422ced3b3", "node_type": "1", "metadata": {}, "hash": "ef934910724b5ea4ca55e31e62ff250182d6b12bc5eb6e635a319767c6f46b2d", "class_name": "RelatedNodeInfo"}, {"node_id": "d9c00693-e096-43b0-9234-78e0bb1e380e", "node_type": "1", "metadata": {}, "hash": "001ae1d1b6916cf1d6b95233f50099b663812c53fac4d7b7a594c640f7083fbb", "class_name": "RelatedNodeInfo"}]}, "text": "[364] https://github.com/truera/trulens.\n[365] Y . Lyu, Z. Li, S. Niu etal., \u201cCRUD-RAG: A comprehensive chinese\nbenchmark for retrieval-augmented generation of large language mod-\nels,\u201d arxiv:2401.17043, 2024.\n[366] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, \u201cBenchmarking retrieval-\naugmented generation for medicine,\u201d arXiv:2402.13178, 2024.\n[367] F. Petroni, A. Piktus etal., \u201cKilt: a benchmark for knowledge intensive\nlanguage tasks,\u201d in NAACL-HLT, 2021.\n[368] S. Barnett, S. Kurniawan, S. Thudumu etal., \u201cSeven failure\npoints when engineering a retrieval augmented generation system,\u201d\narXiv:2401.05856, 2024.\n[369] F. Cuconasu, G. Trappolini, F. Siciliano etal., \u201cThe power of noise:\nRedefining retrieval for RAG systems,\u201d arXiv:2401.14887, 2024.\n[370] L. Qiu, P. Shaw, P. Pasupat etal., \u201cEvaluating the impact of\nmodel scale for compositional generalization in semantic parsing,\u201d\narXiv:2205.12253, 2022.\n[371] R. Jagerman, H. Zhuang, Z. Qin etal., \u201cQuery expansion by prompting\nlarge language models,\u201d arxiv:2305.03653, 2023.\n[372] H. Zhang, P. Zhao, X. Miao etal., \u201cExperimental analysis of large-scale\nlearnable vector storage compression,\u201d VLDB, 2023.\n[373] R. Aksitov, C. Chang, D. Reitter etal., \u201cCharacterizing attribution\nand fluency tradeoffs for retrieval-augmented large language models,\u201d\narXiv:2302.05578, 2023.\n[374] C. Han, Q. Wang, W. Xiong etal., \u201cLm-infinite: Simple on-the-fly\nlength generalization for large language models,\u201d arXiv:2308.16137,\n2023.", "start_char_idx": 1320, "end_char_idx": 2789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea3280e2-9061-4d36-86ab-2bc4d70fce08": {"__data__": {"id_": "ea3280e2-9061-4d36-86ab-2bc4d70fce08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "node_type": "1", "metadata": {}, "hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5", "node_type": "1", "metadata": {}, "hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "6e31af5c-b9e7-4a58-a655-31047102064b", "node_type": "1", "metadata": {}, "hash": "d84a4fa0aa5d5dfaf3467309165bb22c19101c2ed6dec85cb948cdc96c82bf34", "class_name": "RelatedNodeInfo"}]}, "text": "[375] H. Chase, \u201cLangchain,\u201d https://github.com/langchain-ai/langchain,\n2022.\n[376] W. Jiang, S. Zhang, B. Han etal., \u201cPiperag: Fast retrieval-augmented\ngeneration via algorithm-system co-design,\u201d arXiv:2403.05676, 2024.\n[377] S. Jindal, \u201cDid google gemini 1.5 really kill rag?\u201d https://\nanalyticsindiamag.com/did-google-gemini-1-5-really-kill-rag/, 2024.", "start_char_idx": 2790, "end_char_idx": 3145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af53148f-7cee-4843-ad16-bf3bb0321242": {"__data__": {"id_": "af53148f-7cee-4843-ad16-bf3bb0321242", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d19f54d-598e-4495-8f5b-08c18d8e9da1", "node_type": "1", "metadata": {}, "hash": "347331191612ed06eab5a9c310a913709ba4d9575417ddcf11d6b55441e9ac26", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}}, "text": "1\nRetrieval-Augmented Generation for\nAI-Generated Content: A Survey\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\nFangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui\nAbstract \u2014Advancements in model algorithms, the growth of\nfoundational models, and access to high-quality datasets have\npropelled the evolution of Artificial Intelligence Generated Con-\ntent (AIGC).", "start_char_idx": 0, "end_char_idx": 392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d19f54d-598e-4495-8f5b-08c18d8e9da1": {"__data__": {"id_": "8d19f54d-598e-4495-8f5b-08c18d8e9da1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af53148f-7cee-4843-ad16-bf3bb0321242", "node_type": "1", "metadata": {}, "hash": "866f7c66be8396524be9584d90ae3a7613dce39989d2655a236c128d947807bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e33ebd69-c0b6-48d0-a472-cc8f6eeba54b", "node_type": "1", "metadata": {}, "hash": "90a8a0c5263affd2d6674b130a9634e7f2b7716d1e334f4f70270efee4b263f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}}, "text": "Despite its notable successes, AIGC still faces\nhurdles such as updating knowledge, handling long-tail data,\nmitigating data leakage, and managing high training and infer-\nence costs. Retrieval-Augmented Generation (RAG) has recently\nemerged as a paradigm to address such challenges. In partic-\nular, RAG introduces the information retrieval process, which\nenhances the generation process by retrieving relevant objects\nfrom available data stores, leading to higher accuracy and better\nrobustness.", "start_char_idx": 393, "end_char_idx": 890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e33ebd69-c0b6-48d0-a472-cc8f6eeba54b": {"__data__": {"id_": "e33ebd69-c0b6-48d0-a472-cc8f6eeba54b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d19f54d-598e-4495-8f5b-08c18d8e9da1", "node_type": "1", "metadata": {}, "hash": "347331191612ed06eab5a9c310a913709ba4d9575417ddcf11d6b55441e9ac26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea666fa4-20be-459b-881c-fb0a89ce9d54", "node_type": "1", "metadata": {}, "hash": "0f634c2ab09acf1f889da7c595adf8f12a4f7a1b247c26988828da1984de4d11", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}}, "text": "In this paper, we comprehensively review existing\nefforts that integrate RAG technique into AIGC scenarios. We\nfirst classify RAG foundations according to how the retriever\naugments the generator, distilling the fundamental abstrac-\ntions of the augmentation methodologies for various retrievers\nand generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies\nthat help with potential future progress. We also summarize\nadditional enhancements methods for RAG, facilitating effective\nengineering and implementation of RAG systems.", "start_char_idx": 891, "end_char_idx": 1480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea666fa4-20be-459b-881c-fb0a89ce9d54": {"__data__": {"id_": "ea666fa4-20be-459b-881c-fb0a89ce9d54", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e33ebd69-c0b6-48d0-a472-cc8f6eeba54b", "node_type": "1", "metadata": {}, "hash": "90a8a0c5263affd2d6674b130a9634e7f2b7716d1e334f4f70270efee4b263f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e04f5524-c742-40cc-a345-c5fec7d32d50", "node_type": "1", "metadata": {}, "hash": "379cd72cb5a5165ce5f6728716016e1b26e865949f5fe62c125577aee89ab89a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}}, "text": "Then from\nanother view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for\nresearchers and practitioners. Furthermore, we introduce the\nbenchmarks for RAG, discuss the limitations of current RAG\nsystems, and suggest potential directions for future research.\nGithub: https://github.com/PKU-DAIR/RAG-Survey.\nIndex Terms \u2014Retrieval-augmented generation, AI-generated\ncontent, generative models, information retrieval.", "start_char_idx": 1481, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e04f5524-c742-40cc-a345-c5fec7d32d50": {"__data__": {"id_": "e04f5524-c742-40cc-a345-c5fec7d32d50", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea666fa4-20be-459b-881c-fb0a89ce9d54", "node_type": "1", "metadata": {}, "hash": "0f634c2ab09acf1f889da7c595adf8f12a4f7a1b247c26988828da1984de4d11", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2587bb96-4640-4265-9076-9e1d531a2381", "node_type": "1", "metadata": {}, "hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "class_name": "RelatedNodeInfo"}}, "text": "I.INTRODUCTION\nA.Background\nRECENT years have witnessed the surge in interests\nsurrounding Artificial Intelligence Generated Content\n(AIGC).", "start_char_idx": 1960, "end_char_idx": 2100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af6b7a8d-77d9-4a37-8955-6eb0c86e7669": {"__data__": {"id_": "af6b7a8d-77d9-4a37-8955-6eb0c86e7669", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd22f499-9d78-4f2d-ad79-a53ad1f46361", "node_type": "1", "metadata": {}, "hash": "da3d7e3d3284975ddf529101a852570f368f18f6e85446545f1e3e7188dd6f02", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}}, "text": "Various content generation tools have been metic-\nulously crafted to produce diverse outputs across various\nmodalities, such as Large Language Models (LLMs) including\nthe GPT series [1]\u2013[3] and the LLAMA series [4]\u2013[6] for\ntexts and codes, DALL-E [7]\u2013[9] and Stable Diffusion [10]\nfor images, and Sora [11] for videos. The word \u201cAIGC\u201d\nemphasizes that the contents are produced by advanced gen-\nerative models other than human beings or rule-based ap-\nproaches.", "start_char_idx": 0, "end_char_idx": 460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd22f499-9d78-4f2d-ad79-a53ad1f46361": {"__data__": {"id_": "dd22f499-9d78-4f2d-ad79-a53ad1f46361", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af6b7a8d-77d9-4a37-8955-6eb0c86e7669", "node_type": "1", "metadata": {}, "hash": "bf0ae4fab0c3be5444915814833542f3ed98f2b32f86b28f76ebf583d1e26c65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fbcab7a-aeb1-4153-ab79-2b54badcec49", "node_type": "1", "metadata": {}, "hash": "cfc8a6aa8e58551d569f4405bd319a5080a341da2c636413b7818cc8228c0428", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}}, "text": "These generative models have achieved remarkable\nperformance due to the utilization of novel model algorithms,\n\u2022Penghao Zhao and Hailin Zhang contributed equally to this paper.\n\u2022Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng\nGeng, Fangcheng Fu, Ling Yang, Wentao Zhang and Bin Cui are with\nPeking University (e-mail: penghao.zhao@stu.pku.edu.cn, z.hl@pku.edu.cn,\nyuqinhan@stu.pku.edu.cn,", "start_char_idx": 461, "end_char_idx": 863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fbcab7a-aeb1-4153-ab79-2b54badcec49": {"__data__": {"id_": "2fbcab7a-aeb1-4153-ab79-2b54badcec49", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd22f499-9d78-4f2d-ad79-a53ad1f46361", "node_type": "1", "metadata": {}, "hash": "da3d7e3d3284975ddf529101a852570f368f18f6e85446545f1e3e7188dd6f02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33c2d5cf-ea46-493f-a3e7-c702f5cc33af", "node_type": "1", "metadata": {}, "hash": "08a1bde488e1f091cdc30800f1ed032404445623c5e324821944130ca8ebe2dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}}, "text": "edu.cn,\nyuqinhan@stu.pku.edu.cn, wzr@stu.pku.edu.cn, 1800012997@pku.edu.cn,\nccchengff@pku.edu.cn, yangling0818@163.com, wentao.zhang@pku.edu.cn,\nbin.cui@pku.edu.cn).\n\u2022Jie Jiang is with Tencent Inc.", "start_char_idx": 831, "end_char_idx": 1028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33c2d5cf-ea46-493f-a3e7-c702f5cc33af": {"__data__": {"id_": "33c2d5cf-ea46-493f-a3e7-c702f5cc33af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fbcab7a-aeb1-4153-ab79-2b54badcec49", "node_type": "1", "metadata": {}, "hash": "cfc8a6aa8e58551d569f4405bd319a5080a341da2c636413b7818cc8228c0428", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e89ba5c4-3f8d-45e0-b6cc-e385486d7eff", "node_type": "1", "metadata": {}, "hash": "e16c5fa766882a4cebdb6bd5466a6834e7a19828da5649095b05c17b1ebca04e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}}, "text": "edu.cn).\n\u2022Jie Jiang is with Tencent Inc. (email: zeus@tencent.com)\n\u2022Bin Cui is Corresponding Author.explosive scale of foundation models, and massive high-\nquality datasets. Specifically, sequence-to-sequence tasks have\ntransitioned from utilizing Long Short-Term Memory (LSTM)\nnetworks [12] to Transformer-based models [13], and image-\ngeneration tasks have shifted from Generative Adversarial Net-\nworks (GANs) [14] to Latent Diffusion Models (LDMs) [10]\nas well.", "start_char_idx": 988, "end_char_idx": 1453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e89ba5c4-3f8d-45e0-b6cc-e385486d7eff": {"__data__": {"id_": "e89ba5c4-3f8d-45e0-b6cc-e385486d7eff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33c2d5cf-ea46-493f-a3e7-c702f5cc33af", "node_type": "1", "metadata": {}, "hash": "08a1bde488e1f091cdc30800f1ed032404445623c5e324821944130ca8ebe2dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45966987-f98a-421f-9271-5c503d67344d", "node_type": "1", "metadata": {}, "hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "class_name": "RelatedNodeInfo"}}, "text": "Notably, the architecture of foundation models, ini-\ntially constituted by millions of parameters [15], [16], has now\ngrown to billions or even trillions of parameters [1], [4], [17].\nThese advancements are further bolstered by the availability\nof rich, high-quality datasets [1], [18], which provide ample\ntraining samples to fully optimize model parameters.\nInformation retrieval is another pivotal application within\nthe field of computer science.", "start_char_idx": 1454, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ac6f711-0a4c-4e61-907b-5adb6944be21": {"__data__": {"id_": "2ac6f711-0a4c-4e61-907b-5adb6944be21", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5639ad72-ec31-424b-982e-a75e3975ca22", "node_type": "1", "metadata": {}, "hash": "b353c1d2f59611a400cebe1c774a7e9d5cb5339cae16b0b8a24d953a59d50008", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}}, "text": "Information retrieval is another pivotal application within\nthe field of computer science. Different from generation,\nretrieval aims to locate relevant existing objects from a vast\npool of resources. The most prevalent application of retrieval\nlies in web search engines, which primarily focus on the task\nof document retrieval [19], [20]. In the present era, efficient\ninformation retrieval systems can handle document collections\non the order of billions [21], [22]. Besides documents, retrieval\nhas also been applied for many other modalities [23]\u2013[26].", "start_char_idx": 0, "end_char_idx": 556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5639ad72-ec31-424b-982e-a75e3975ca22": {"__data__": {"id_": "5639ad72-ec31-424b-982e-a75e3975ca22", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ac6f711-0a4c-4e61-907b-5adb6944be21", "node_type": "1", "metadata": {}, "hash": "3464011a2cb4059692c14d8527315f1dba17774d87c447fc33b773f28cd3c392", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "988d88de-c8fb-44a0-bfcc-d9d9ebb6f2ea", "node_type": "1", "metadata": {}, "hash": "a328839604cca5ee1243766b1505cfdc95b7fa143f41a7f80698ba25c8198164", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}}, "text": "Despite the remarkable progress made by advanced gen-\nerative models, AIGC continues to face a number of well-\nknown challenges, including the struggle to maintain up-to-\ndate knowledge, the inability to incorporate long-tail knowl-\nedge [27], and the risk of leaking private training data [28].\nRetrieval-Augmented Generation (RAG) is proposed to allevi-\nate, if not completely address, the aforementioned challenges\nthrough its adaptable data repository [29].", "start_char_idx": 557, "end_char_idx": 1018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "988d88de-c8fb-44a0-bfcc-d9d9ebb6f2ea": {"__data__": {"id_": "988d88de-c8fb-44a0-bfcc-d9d9ebb6f2ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5639ad72-ec31-424b-982e-a75e3975ca22", "node_type": "1", "metadata": {}, "hash": "b353c1d2f59611a400cebe1c774a7e9d5cb5339cae16b0b8a24d953a59d50008", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3db498c9-6714-4117-8544-f0cbc7c7ba24", "node_type": "1", "metadata": {}, "hash": "59d8d14f952e64cd64764abf93c5e33633a4ffb20c4b4dec0bdb45a6fb0d6491", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}}, "text": "The knowledge\nstored for retrieval can be conceptualized as non-parametric\nmemory, which is easily modifiable, capable of accommo-\ndating broad long-tail knowledge, and also able to encode\nconfidential data. In addition, retrieval can also be employed\nto reduce the generation costs. For example, RAG can reduce\nthe size of large generative models [30], provide support for\nlong contexts [31], and eliminate certain generation steps [32].\nA typical RAG process is depicted in Fig. 1.", "start_char_idx": 1019, "end_char_idx": 1502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3db498c9-6714-4117-8544-f0cbc7c7ba24": {"__data__": {"id_": "3db498c9-6714-4117-8544-f0cbc7c7ba24", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "988d88de-c8fb-44a0-bfcc-d9d9ebb6f2ea", "node_type": "1", "metadata": {}, "hash": "a328839604cca5ee1243766b1505cfdc95b7fa143f41a7f80698ba25c8198164", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9c019c1-3346-44e1-89aa-8b9ce920f21c", "node_type": "1", "metadata": {}, "hash": "be7e5b2089d0ff6c9291ceab1e3fd6412df95bea66674a6d6630e8d40af875ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}}, "text": "A typical RAG process is depicted in Fig. 1. Given an\ninput query, the retriever locates and looks up relevant data\nsources, then the retrieved results interact with the generator\nto enhance the overall generation process.", "start_char_idx": 1458, "end_char_idx": 1680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9c019c1-3346-44e1-89aa-8b9ce920f21c": {"__data__": {"id_": "a9c019c1-3346-44e1-89aa-8b9ce920f21c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3db498c9-6714-4117-8544-f0cbc7c7ba24", "node_type": "1", "metadata": {}, "hash": "59d8d14f952e64cd64764abf93c5e33633a4ffb20c4b4dec0bdb45a6fb0d6491", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b2e91e1-6e16-4f33-9934-a2862d8270d6", "node_type": "1", "metadata": {}, "hash": "ddca79e02d5b90a77e6f8aa43a7fe26217b9450e1678ea7ec37299f667dc2219", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}}, "text": "There are sev-\neralfoundational paradigms (foundations in short) according\nto how the retrieved results augment the generation: they\ncan serve as augmented input to the generator [33], [34];\nthey can join at the middle stage of generation as latent\nrepresentations [35], [36]; they can contribute to the final\ngeneration results in the form of logits [37], [38]; they can\neven influence or omit certain generation steps [32], [39].arXiv:2402.19473v3  [cs.CV]  14 Apr 20242\nFig.", "start_char_idx": 1681, "end_char_idx": 2158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b2e91e1-6e16-4f33-9934-a2862d8270d6": {"__data__": {"id_": "2b2e91e1-6e16-4f33-9934-a2862d8270d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9c019c1-3346-44e1-89aa-8b9ce920f21c", "node_type": "1", "metadata": {}, "hash": "be7e5b2089d0ff6c9291ceab1e3fd6412df95bea66674a6d6630e8d40af875ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "node_type": "1", "metadata": {}, "hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "class_name": "RelatedNodeInfo"}}, "text": "1: A generic RAG architecture.", "start_char_idx": 2159, "end_char_idx": 2189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "461b359e-1bb5-4912-bc7d-3a73a3c00b25": {"__data__": {"id_": "461b359e-1bb5-4912-bc7d-3a73a3c00b25", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fde5e6b-04c9-4ac1-bf71-09a1c76b804b", "node_type": "1", "metadata": {}, "hash": "4603ff28f1c0e33013749ce1b15466d652003b6b49ecf2cc15839cf8bd130de7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}}, "text": "1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and\nthe generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results\nand ultimately produces outcomes of various modalities.\nMoreover, beyond the foundational RAG process, researchers\nhave proposed numerous enhancements to elevate the overall\nquality. These methods encompass specific optimizations for\nindividual components as well as holistic enhancements aimed\nat the entire pipeline.", "start_char_idx": 0, "end_char_idx": 568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fde5e6b-04c9-4ac1-bf71-09a1c76b804b": {"__data__": {"id_": "2fde5e6b-04c9-4ac1-bf71-09a1c76b804b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "461b359e-1bb5-4912-bc7d-3a73a3c00b25", "node_type": "1", "metadata": {}, "hash": "8945a6cd35fd64a02d7491742402b0fd31272bc02f9c239e754cb33f9ad4f567", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32a36560-4b02-4f0a-a52e-a1ee6003e14b", "node_type": "1", "metadata": {}, "hash": "a148dc182352bbfd29ea06de2d176c5da69665699191df60b55dddc94330b211", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}}, "text": "In addition, while the concept of RAG initially emerged\nin text-to-text generation [34], this technique has also found\napplications across various domains, including codes [40]\u2013\n[42], audios [43], [44], images [45]\u2013[47], videos [48], [49],\n3D [50], [51], knowledge [52]\u2013[54], and AI for science [55],\n[56]. In particular, the essential idea and process of RAG are\nlargely consistent across modalities.", "start_char_idx": 569, "end_char_idx": 970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32a36560-4b02-4f0a-a52e-a1ee6003e14b": {"__data__": {"id_": "32a36560-4b02-4f0a-a52e-a1ee6003e14b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fde5e6b-04c9-4ac1-bf71-09a1c76b804b", "node_type": "1", "metadata": {}, "hash": "4603ff28f1c0e33013749ce1b15466d652003b6b49ecf2cc15839cf8bd130de7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3be1b9c-cdfc-424a-b473-b1897295dd57", "node_type": "1", "metadata": {}, "hash": "176a10f956a1fb498b8fc006f5f597d844c43bc9e2c56c9736c6b7352de7e847", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}}, "text": "However, it necessitates\nminor adjustments in augmentation techniques, and the se-\nlection of retrievers and generators varies depending on the\nspecific modalities and applications.\nDespite the rapid growth in recent research on RAG and\nthe booming applications, a systematic review encompassing\nall foundations, enhancements, and applications is notably\nabsent, hindering the development of this field. For one thing,\nthe absence of discussion on RAG foundations significantly\nundermines the practical value of the research in this do-\nmain, leaving the potential of RAG not fully explored.", "start_char_idx": 971, "end_char_idx": 1562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3be1b9c-cdfc-424a-b473-b1897295dd57": {"__data__": {"id_": "f3be1b9c-cdfc-424a-b473-b1897295dd57", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32a36560-4b02-4f0a-a52e-a1ee6003e14b", "node_type": "1", "metadata": {}, "hash": "a148dc182352bbfd29ea06de2d176c5da69665699191df60b55dddc94330b211", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30557630-9b86-470e-a715-5603ee609c82", "node_type": "1", "metadata": {}, "hash": "73f3d26a92a23a10a934a1ae6f3813f014cafffe9d0e40b91745e5fab450a52c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}}, "text": "While\nthe majority of research interest, particularly among LLM\nresearchers, centers on query-based RAG in text-generation\ntasks, it is essential to acknowledge that other RAG foun-\ndations are also effective and with significant potential for\nusage and further development. For another, the lack of an\noverview on RAG applications causes researchers and practi-\ntioners to overlook RAG\u2019s progress across multiple modalities\nand remain unaware of how RAG can be effectively applied.", "start_char_idx": 1563, "end_char_idx": 2045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30557630-9b86-470e-a715-5603ee609c82": {"__data__": {"id_": "30557630-9b86-470e-a715-5603ee609c82", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3be1b9c-cdfc-424a-b473-b1897295dd57", "node_type": "1", "metadata": {}, "hash": "176a10f956a1fb498b8fc006f5f597d844c43bc9e2c56c9736c6b7352de7e847", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6e438b00-2436-4718-b18e-6835511b6962", "node_type": "1", "metadata": {}, "hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "class_name": "RelatedNodeInfo"}}, "text": "Although text generation is typically considered as the main\napplication of RAG, we emphasize that the development ofRAG in other modalities has also begun to catch on and\nhas yielded promising advancements. Certain modalities have\na rich historical connection to retrieval techniques, infusing\nRAG with distinctive characteristics.", "start_char_idx": 2046, "end_char_idx": 2378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09fba7a0-ea1b-4ddf-991c-663c646563c6": {"__data__": {"id_": "09fba7a0-ea1b-4ddf-991c-663c646563c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f7399abd-cb1b-48f9-886a-8746f5f1e173", "node_type": "1", "metadata": {}, "hash": "0f63f2d3865eb73bcbe7d9640eb81ce9aaf264d8fc768e2598f9366304350af2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f7399abd-cb1b-48f9-886a-8746f5f1e173", "node_type": "1", "metadata": {}, "hash": "0f63f2d3865eb73bcbe7d9640eb81ce9aaf264d8fc768e2598f9366304350af2", "class_name": "RelatedNodeInfo"}}, "text": "Inspired by this, in this\npaper, our objective is to present a comprehensive survey to\nprovide a systematic overview of RAG.\nB.Contribution\nThis survey offers a comprehensive overview of RAG, cov-\nering foundations, enhancements, applications, benchmarks,\nlimitations, and potential future directions. While retrievers\nand generators exhibit variations across modalities and tasks,\nwe distill the fundamental abstractions of RAG foundations,\nconsidering applications as adaptations stemming from these\nabstractions.", "start_char_idx": 0, "end_char_idx": 515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b298a12-d1e7-402e-ac06-abf397573cc9": {"__data__": {"id_": "9b298a12-d1e7-402e-ac06-abf397573cc9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b602e68-392e-4e7a-9923-1045a4070a95", "node_type": "1", "metadata": {}, "hash": "97823f7b067ccf61c9f5cdf75c41ff33bf257b5f41220aa95cbe18e4c2449ae5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}}, "text": "We aim to offer references and guidelines to\nresearchers and practitioners, providing valuable insights for\nadvancing RAG methodologies and related applications. In\nsummary, we list our contributions as follows:\n\u2022We conduct a comprehensive review of RAG, and distill\nthe abstractions of RAG foundations for various retrievers\nand generators.\n\u2022We investigate the enhancements in the literature of\nRAG, elaborating the techniques leveraged to enable\nmore effective RAG systems.", "start_char_idx": 0, "end_char_idx": 475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b602e68-392e-4e7a-9923-1045a4070a95": {"__data__": {"id_": "4b602e68-392e-4e7a-9923-1045a4070a95", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b298a12-d1e7-402e-ac06-abf397573cc9", "node_type": "1", "metadata": {}, "hash": "28500d7bb796994b26ac0e5962b36a6456ee7a2cfc38b53e38f9b53634161352", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11ccb175-7cb7-493c-9654-5ff59910f24d", "node_type": "1", "metadata": {}, "hash": "2fda599b9d1537e2d126793d666912a1dd6324a10d5df6c6c9ca1d2536546990", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}}, "text": "\u2022For various modalities and tasks, we survey existing\nAIGC methods that incorporate RAG techniques, exhibit-\ning how RAG contributes to current generative models.\n\u2022We discuss the limitations and promising research di-\nrections of RAG, shedding light on its potential future\ndevelopment.\nC.Related Work\nAs the field of RAG advances, several surveys have\nemerged; yet they address only specific facets of the area. In3\nparticular, they either exclusively focus on a single RAG foun-\ndation or provide only a brief overview of RAG augmentation\nmethodologies for limited scenarios.", "start_char_idx": 476, "end_char_idx": 1053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11ccb175-7cb7-493c-9654-5ff59910f24d": {"__data__": {"id_": "11ccb175-7cb7-493c-9654-5ff59910f24d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b602e68-392e-4e7a-9923-1045a4070a95", "node_type": "1", "metadata": {}, "hash": "97823f7b067ccf61c9f5cdf75c41ff33bf257b5f41220aa95cbe18e4c2449ae5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f750e47e-2259-45fc-a520-500bf6a0d0dc", "node_type": "1", "metadata": {}, "hash": "5e1886df3a599483dc5a2cce1e016a486444b6cae97ce671fe6d01f1fca65c92", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}}, "text": "Most of the existing works focus on text-related RAG tasks\nthat are facilitated by LLMs, without in-depth investigation\nin other modalities. The survey by Li et al. [57] offers a\nbasic overview of RAG and discusses specific applications\nwithin the scope of text generation tasks. In a similar vein,\nthe tutorial crafted by Asai et al. [58] centers on retrieval-\nbased language models, detailing their structures and training\nstrategies. Meanwhile, a recent survey by Gao et al.", "start_char_idx": 1054, "end_char_idx": 1531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f750e47e-2259-45fc-a520-500bf6a0d0dc": {"__data__": {"id_": "f750e47e-2259-45fc-a520-500bf6a0d0dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11ccb175-7cb7-493c-9654-5ff59910f24d", "node_type": "1", "metadata": {}, "hash": "2fda599b9d1537e2d126793d666912a1dd6324a10d5df6c6c9ca1d2536546990", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a702c1b-9836-4918-beb5-5562d850aa84", "node_type": "1", "metadata": {}, "hash": "748db795c9935cd715683a62c64256119c87f6332e6c9f65237935c1c9fd8f63", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}}, "text": "Meanwhile, a recent survey by Gao et al. [59]\nexplores RAG in the context of LLMs, with a particular\nemphasis on enhancement approaches for query-based RAG.\nRecognizing that RAG has extended beyond the text domain,\nour work broadens its reach to the entire AIGC landscape,\nfacilitating a more comprehensive coverage of RAG research.\nIn addition, another survey proposed by Zhao et al. [60]\nintroduces RAG applications across multiple modalities, but\nignoring the discussion on RAG foundations.", "start_char_idx": 1491, "end_char_idx": 1984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a702c1b-9836-4918-beb5-5562d850aa84": {"__data__": {"id_": "8a702c1b-9836-4918-beb5-5562d850aa84", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f750e47e-2259-45fc-a520-500bf6a0d0dc", "node_type": "1", "metadata": {}, "hash": "5e1886df3a599483dc5a2cce1e016a486444b6cae97ce671fe6d01f1fca65c92", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cc987eab-006b-480a-8593-97ad22ce1f35", "node_type": "1", "metadata": {}, "hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "class_name": "RelatedNodeInfo"}}, "text": "While existing\nresearch has explored various aspects of RAG, there remains\na need for a comprehensive overview that covers RAG foun-\ndations, enhancements, and its applicability across different\ndomains. In this paper, we aim to address the gap by presenting\na systematic survey of RAG.", "start_char_idx": 1985, "end_char_idx": 2271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "873991ab-7ccf-4af5-bb28-c4b73edd709a": {"__data__": {"id_": "873991ab-7ccf-4af5-bb28-c4b73edd709a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65a5d11a-33fc-43e0-9aa2-49305de58cfd", "node_type": "1", "metadata": {}, "hash": "d7c01fceb9f999eb6749330aace0566051bb4dea21bfeb60262e964fb0671e7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}}, "text": "In this paper, we aim to address the gap by presenting\na systematic survey of RAG.\nD.Roadmap\nThe rest of the paper is organized as follows. Section II elab-\norates on the preliminary of RAG, introducing retrievers and\ngenerators. Section III presents RAG foundations and further\nenhancements on RAG. Section IV reviews existing research\non RAG across various applications. Section V investigates the\nbenchmark frameworks for RAG. Section VI discusses current\nlimitations of RAG and potential future directions. Finally,\nSection VII concludes this paper.\nII.", "start_char_idx": 0, "end_char_idx": 557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65a5d11a-33fc-43e0-9aa2-49305de58cfd": {"__data__": {"id_": "65a5d11a-33fc-43e0-9aa2-49305de58cfd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "873991ab-7ccf-4af5-bb28-c4b73edd709a", "node_type": "1", "metadata": {}, "hash": "79deaa4bb2eb8ae20ce2a74ac4175e0643b01c33b404c18c04bf704ec62372fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "566ee55e-c714-4b11-a525-4fca02f448fd", "node_type": "1", "metadata": {}, "hash": "cc6746ec2074e0f9c722d9fef2fa36b7a4e52d08bf176da8573157e39a5abdc6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}}, "text": "Finally,\nSection VII concludes this paper.\nII. PRELIMINARY\nIn this section, we provide an overview of the general RAG\narchitecture and explore the generators and the retrievers in\ntoday\u2019s RAG-based AIGC.\nA.Overview\nAs shown in Fig. 1, the entire RAG system consists of\ntwo core modules: the retriever and the generator, where the\nretriever searches for relevant information from the data store\nand the generator produces the required contents.", "start_char_idx": 511, "end_char_idx": 954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "566ee55e-c714-4b11-a525-4fca02f448fd": {"__data__": {"id_": "566ee55e-c714-4b11-a525-4fca02f448fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65a5d11a-33fc-43e0-9aa2-49305de58cfd", "node_type": "1", "metadata": {}, "hash": "d7c01fceb9f999eb6749330aace0566051bb4dea21bfeb60262e964fb0671e7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ff83fd5-cee9-4712-8271-478f9ddb56cc", "node_type": "1", "metadata": {}, "hash": "ea67e182e6425e4c8675a79ab0a108a3d47f0c3f5a7c90b389a77768fa147b0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}}, "text": "The RAG\nprocess unfolds as follows: (i) the retriever initially receives\nthe input query and searches for relevant information; (ii) then,\nthe original query and the retrieval results are fed into the\ngenerator through a specific augmentation methodology; (iii)\nfinally, the generator produces the desired outcomes.\nB.Generator\nThe remarkable performance of generative AI across di-\nverse tasks has ushered in the era of AIGC. The generation\nmodule plays a crucial role within the RAG system.", "start_char_idx": 955, "end_char_idx": 1447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ff83fd5-cee9-4712-8271-478f9ddb56cc": {"__data__": {"id_": "7ff83fd5-cee9-4712-8271-478f9ddb56cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "566ee55e-c714-4b11-a525-4fca02f448fd", "node_type": "1", "metadata": {}, "hash": "cc6746ec2074e0f9c722d9fef2fa36b7a4e52d08bf176da8573157e39a5abdc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2ad5fef-179a-446f-a02a-c82560506f03", "node_type": "1", "metadata": {}, "hash": "b215b851556d71c698c41fae930e384481cd27d2cdff11de5c1ed17f62fcc398", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}}, "text": "The generation\nmodule plays a crucial role within the RAG system. Different\ngenerative models are applied for different scenarios, such\nas transformer models for text-to-text tasks, VisualGPT [61]\nfor image-to-text tasks, Stable Diffusion [10] for text-to-\nimage tasks, Codex [2] for text-to-code tasks, etc.", "start_char_idx": 1382, "end_char_idx": 1690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2ad5fef-179a-446f-a02a-c82560506f03": {"__data__": {"id_": "e2ad5fef-179a-446f-a02a-c82560506f03", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ff83fd5-cee9-4712-8271-478f9ddb56cc", "node_type": "1", "metadata": {}, "hash": "ea67e182e6425e4c8675a79ab0a108a3d47f0c3f5a7c90b389a77768fa147b0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23", "node_type": "1", "metadata": {}, "hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "class_name": "RelatedNodeInfo"}}, "text": "Here we\nintroduce 4 typical generators that are frequently used in RAG:\ntransformer model, LSTM, diffusion model, and GAN.1)Transformer Model :Transformer models are one of the\nbest performing models in the field of Natural Language Pro-\ncessing (NLP), consisting of self-attention mechanisms, feed-\nforward networks, layer normalization modules, and residual\nnetworks [62]. As shown in Fig.", "start_char_idx": 1691, "end_char_idx": 2082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f7fd9ca-0cf7-4fb7-9873-c9b592738bdc": {"__data__": {"id_": "2f7fd9ca-0cf7-4fb7-9873-c9b592738bdc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "843191dc-4cee-4491-b706-4214fcde7797", "node_type": "1", "metadata": {}, "hash": "15c631742fc154993f28ddd810e0d9c996fe80de582c88fa6d3bb3356d7ab941", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}}, "text": "As shown in Fig. 2, the input of the transformer\nis mapped to a tensor xinwith a shape of ( b,s,h) after the\ntokenization process and embedding model, where brepresents\nbatch size, srepresents sequence length and hrepresents\nhidden dimension. Next, the position encoding will be sent\nto the self attention layer along with this tensor. The input\nxinand the output xoutof the self-attention module will be\nconnected by the residual network and the layer normalization\nmodule.", "start_char_idx": 0, "end_char_idx": 474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "843191dc-4cee-4491-b706-4214fcde7797": {"__data__": {"id_": "843191dc-4cee-4491-b706-4214fcde7797", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f7fd9ca-0cf7-4fb7-9873-c9b592738bdc", "node_type": "1", "metadata": {}, "hash": "d09340f2882e335df5e06604099dbd05404f2ee214e76d4e22a4d1eda04b58d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "195f3a73-cdcf-4c70-a5b2-1c2cf4b85211", "node_type": "1", "metadata": {}, "hash": "ede1e45f85f765d5f9be10c8cabb9afc2a6cd703028f6fe71201a313e51a5516", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}}, "text": "Finally, the output of the \u201dAdd & Norm\u201d module xout\nwill be sent to the feed forward network.", "start_char_idx": 475, "end_char_idx": 568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "195f3a73-cdcf-4c70-a5b2-1c2cf4b85211": {"__data__": {"id_": "195f3a73-cdcf-4c70-a5b2-1c2cf4b85211", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "843191dc-4cee-4491-b706-4214fcde7797", "node_type": "1", "metadata": {}, "hash": "15c631742fc154993f28ddd810e0d9c996fe80de582c88fa6d3bb3356d7ab941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5596ba26-93d2-4d1f-8f45-2a37cacbbd60", "node_type": "1", "metadata": {}, "hash": "4a9c2e8b557cdb642dd6c1f722d256d6b893715893101fc743604efe255c3498", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}}, "text": "The entire process can be defined as follows:\nQ=xin\u2217Wq+bq\nK=xin\u2217Wk+bk\nV=xin\u2217Wv+bv\nxout=LayerNorm 1(Softmax (Q\u2217KT\n\u221a\nh)\u2217V\u2217Wo+bo)+xin\ny=LayerNorm 2((xout\u2217W1+b1)\u2217W2+b2) +xout\nIt should be noted that wq, wk, wv, woare learnable tensors\nwith shape ( h,h);bq,", "start_char_idx": 569, "end_char_idx": 821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5596ba26-93d2-4d1f-8f45-2a37cacbbd60": {"__data__": {"id_": "5596ba26-93d2-4d1f-8f45-2a37cacbbd60", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "195f3a73-cdcf-4c70-a5b2-1c2cf4b85211", "node_type": "1", "metadata": {}, "hash": "ede1e45f85f765d5f9be10c8cabb9afc2a6cd703028f6fe71201a313e51a5516", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4d4f399-d234-4f87-a944-243f12a6b60b", "node_type": "1", "metadata": {}, "hash": "772cf8d71c580420436a1510accd393345c93d7c0ff6357695d358e5f2ebd70b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}}, "text": "wv, woare learnable tensors\nwith shape ( h,h);bq, bk, bv, boare a learnable tensors with\nshape ( h,).\n2)LSTM :Long Short-Term Memory (LSTM) [63] is a\nspecial Recurrent Neural Network (RNN) model that over-\ncomes the exploding/vanishing gradient problems of RNN in\nprocessing long-term dependency information by introducing\ncell state and gate mechanisms. The LSTM model consists\nof three gates: Input Gate, Forget Gate, and Output Gate.", "start_char_idx": 772, "end_char_idx": 1208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4d4f399-d234-4f87-a944-243f12a6b60b": {"__data__": {"id_": "e4d4f399-d234-4f87-a944-243f12a6b60b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5596ba26-93d2-4d1f-8f45-2a37cacbbd60", "node_type": "1", "metadata": {}, "hash": "4a9c2e8b557cdb642dd6c1f722d256d6b893715893101fc743604efe255c3498", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3b0c525-5be2-4ddd-a29f-c9a87792531b", "node_type": "1", "metadata": {}, "hash": "df8dbbb4c5dae6a952eaef0e3264696c3ecd2174061bd7bd1a48941b57f620bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}}, "text": "The LSTM model consists\nof three gates: Input Gate, Forget Gate, and Output Gate.\nThese gates update the cell state by controlling the information\nflow, enabling the model to remember long-term dependent\ninformation. Cell State is the core module of the LSTM model\nwhich can memorize and maintain information. The Input Gate\ndecides which input data should be retained in the cell state.\nForget Gate determines which cell state information should be\ndiscarded to avoid excessive memory. Output Gate determines\nhow the information in the cell state affects the current output.", "start_char_idx": 1127, "end_char_idx": 1702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3b0c525-5be2-4ddd-a29f-c9a87792531b": {"__data__": {"id_": "b3b0c525-5be2-4ddd-a29f-c9a87792531b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4d4f399-d234-4f87-a944-243f12a6b60b", "node_type": "1", "metadata": {}, "hash": "772cf8d71c580420436a1510accd393345c93d7c0ff6357695d358e5f2ebd70b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "node_type": "1", "metadata": {}, "hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "class_name": "RelatedNodeInfo"}}, "text": "Output Gate determines\nhow the information in the cell state affects the current output.\nThe flow of data and the collaborative work process between\ncomponents are shown in the Fig. 2.", "start_char_idx": 1614, "end_char_idx": 1798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52c28400-6e09-4062-b489-6291b73575ec": {"__data__": {"id_": "52c28400-6e09-4062-b489-6291b73575ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9b29251-dbf2-403d-9baf-1f9dbcca521e", "node_type": "1", "metadata": {}, "hash": "27e00ad0fb86e64f8660b771e78be1a457d3d99e23257d8799ff20a08739a7f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}}, "text": "2.\nThe entire process can be defined as follows:\nf=sigmoid (Wf\u2217xt+Uf\u2217yt\u22121+bf)\nz=tanh(Wz\u2217xt+Uz\u2217yt\u22121+bz)\ni=sigmoid (Wi\u2217xt+Ui\u2217yt\u22121+bi)\no=sigmoid (Wo\u2217xt+Uo\u2217yt\u22121+bo)\nct=z\u2299i+f\u2299ct\u22121\nyt=o\u2299tanh(ct)4\n(a) General transformer model architecture.\n (b) General LSTM block architecture.", "start_char_idx": 0, "end_char_idx": 271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9b29251-dbf2-403d-9baf-1f9dbcca521e": {"__data__": {"id_": "e9b29251-dbf2-403d-9baf-1f9dbcca521e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52c28400-6e09-4062-b489-6291b73575ec", "node_type": "1", "metadata": {}, "hash": "31463060176385aa624f33fc503b9991212c4cdc6288d626b9d44a6fc4861c11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56eb2784-6b7f-4a78-8646-50d89033c3fa", "node_type": "1", "metadata": {}, "hash": "6fda48f7eb64f054bc5f51e56354a7dc48d0caca7dbdb79b66f1634bb01ab283", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}}, "text": "(b) General LSTM block architecture.\n(c) General latent diffusion model architecture.\n (d) General GAN architecture.\nFig. 2: General architectures of several generators.\n3)Diffusion Model :Diffusion models [64] are a family\nof deep generative models that can create realistic and di-\nverse samples of data [65]\u2013[72], such as images [73]\u2013[79],\ntexts [80]\u2013[83], videos [84]\u2013[88], and molecules [89]\u2013[93].\nAs shown in Fig.", "start_char_idx": 235, "end_char_idx": 654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56eb2784-6b7f-4a78-8646-50d89033c3fa": {"__data__": {"id_": "56eb2784-6b7f-4a78-8646-50d89033c3fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9b29251-dbf2-403d-9baf-1f9dbcca521e", "node_type": "1", "metadata": {}, "hash": "27e00ad0fb86e64f8660b771e78be1a457d3d99e23257d8799ff20a08739a7f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9395cc04-3804-4a99-b757-9923150dc466", "node_type": "1", "metadata": {}, "hash": "03d485375ae0b899ea6f66194472dfc66985b777ce69e233e348e5f88d9bba5e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}}, "text": "As shown in Fig. 2, diffusion models work by gradually\nadding noise to data until it becomes random, then reversing\nthe process to generate new data from noise. This process\nis based on probabilistic modeling and neural networks.", "start_char_idx": 638, "end_char_idx": 867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9395cc04-3804-4a99-b757-9923150dc466": {"__data__": {"id_": "9395cc04-3804-4a99-b757-9923150dc466", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56eb2784-6b7f-4a78-8646-50d89033c3fa", "node_type": "1", "metadata": {}, "hash": "6fda48f7eb64f054bc5f51e56354a7dc48d0caca7dbdb79b66f1634bb01ab283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86fc9790-90b1-4c30-a828-0b1d798b9b7c", "node_type": "1", "metadata": {}, "hash": "59f1fa94b6a045018deef756912ca8cf75564b2112e2f85c8531952cfd102210", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}}, "text": "This process\nis based on probabilistic modeling and neural networks.\nDiffusion models mainly have three equivalent formulations:\ndenoising diffusion probabilistic models [65]\u2013[67], score-\nbased generative models [68], [69], and stochastic differen-\ntial equations [70], [71], with following improvements like\nDDIM [94], Rectified Flow [95], Consistency Model [96] and\nRPG-DiffusionMaster [79].", "start_char_idx": 799, "end_char_idx": 1192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86fc9790-90b1-4c30-a828-0b1d798b9b7c": {"__data__": {"id_": "86fc9790-90b1-4c30-a828-0b1d798b9b7c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9395cc04-3804-4a99-b757-9923150dc466", "node_type": "1", "metadata": {}, "hash": "03d485375ae0b899ea6f66194472dfc66985b777ce69e233e348e5f88d9bba5e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "node_type": "1", "metadata": {}, "hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "class_name": "RelatedNodeInfo"}}, "text": "Especially, let x0be a random variable that follows the\ndata distribution q(x0), and let xtbe a random variable that\nfollows the distribution q(xt|x0)after adding noise at time\nstept. Then, DDPM can be formulated as follows:\n\u2022Forward Process The forward process perturbs data with\na sequence of Gaussian noise injections, transforming the\ndata distribution q(x0)into a simple prior distribution\nq(xT)\u2248N(0, I).", "start_char_idx": 1193, "end_char_idx": 1602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c73d00c-a783-4a14-ae37-a8248bcc62cd": {"__data__": {"id_": "1c73d00c-a783-4a14-ae37-a8248bcc62cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71", "node_type": "1", "metadata": {}, "hash": "0cb925af162397fad1504f1a6f778ebaff0c22db95a3980d89546b4d103ab791", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9147c0b4-957d-4bab-b394-3aec4a36c6f2", "node_type": "1", "metadata": {}, "hash": "7627320f42e1ff59feeb8974a33ce815410340af46f69417dff0f36a9c018276", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71", "node_type": "1", "metadata": {}, "hash": "0cb925af162397fad1504f1a6f778ebaff0c22db95a3980d89546b4d103ab791", "class_name": "RelatedNodeInfo"}}, "text": "The transition kernel at each time step\nis given by\nq(xt|xt\u22121) =N(xt;p\n1\u2212\u03b2txt\u22121, \u03b2tI),\nwhere \u03b2t\u2208(0,1)is a hyperparameter. The marginal\ndistribution of xtconditioned on x0is\nq(xt|x0) =N(xt;\u221a\u00af\u03b1tx0,(1\u2212\u00af\u03b1t)I),\nwhere \u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=0\u03b1s.", "start_char_idx": 0, "end_char_idx": 236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9147c0b4-957d-4bab-b394-3aec4a36c6f2": {"__data__": {"id_": "9147c0b4-957d-4bab-b394-3aec4a36c6f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71", "node_type": "1", "metadata": {}, "hash": "0cb925af162397fad1504f1a6f778ebaff0c22db95a3980d89546b4d103ab791", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c73d00c-a783-4a14-ae37-a8248bcc62cd", "node_type": "1", "metadata": {}, "hash": "f35556f35ab7cd771b0f8860bcb865c52d138dab0eb7290d1207f7ffcbb24221", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71", "node_type": "1", "metadata": {}, "hash": "0cb925af162397fad1504f1a6f778ebaff0c22db95a3980d89546b4d103ab791", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Reverse Process The reverse process generates new\ndata samples by reversing the forward process with alearnable Markov chain.", "start_char_idx": 237, "end_char_idx": 363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab9b36f9-5822-4d55-8dc2-a0c2334f7256": {"__data__": {"id_": "ab9b36f9-5822-4d55-8dc2-a0c2334f7256", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a547e43-731c-454a-9b43-aa6128e5d71c", "node_type": "1", "metadata": {}, "hash": "524dd1964c06f3d7675bb36972b3b339af91fcbb234d8c81408462b17be67142", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}}, "text": "The prior distribution is p(xT) =\nN(xT; 0, I)and the transition kernel is\np\u03b8(xt\u22121|xt) =N(xt\u22121;\u00b5\u03b8(xt, t),\u03a3\u03b8(xt, t)),\nwhere \u03b8denotes model parameters, and \u00b5\u03b8(xt, t)and\n\u03a3\u03b8(xt, t)are parameterized by deep neural networks.", "start_char_idx": 0, "end_char_idx": 217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a547e43-731c-454a-9b43-aa6128e5d71c": {"__data__": {"id_": "4a547e43-731c-454a-9b43-aa6128e5d71c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab9b36f9-5822-4d55-8dc2-a0c2334f7256", "node_type": "1", "metadata": {}, "hash": "ef4fa561d6aed9c408bdc9c4bf8c8ceba6886bfb415a196a2df45845dca3dab3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02f8004b-01f4-49f6-a3a6-adc81e2de14b", "node_type": "1", "metadata": {}, "hash": "6f377e222014d759c6dfaea880d0140c54a26d7c0b72f4c74d8d732f221b70dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}}, "text": "The\nreverse process starts from sampling xT\u223cp(xT)and\niteratively samples xt\u22121\u223cp\u03b8(xt\u22121|xt)until t= 0.\n\u2022Model Training For each sample x0\u223cq(x0), the model\ntraining objective is to maximizing the variational lower\nbound (VLB) of the log-likelihood of the data x0.", "start_char_idx": 218, "end_char_idx": 478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02f8004b-01f4-49f6-a3a6-adc81e2de14b": {"__data__": {"id_": "02f8004b-01f4-49f6-a3a6-adc81e2de14b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a547e43-731c-454a-9b43-aa6128e5d71c", "node_type": "1", "metadata": {}, "hash": "524dd1964c06f3d7675bb36972b3b339af91fcbb234d8c81408462b17be67142", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0a2ad27-0fd6-4daa-b109-1ccbbad1a3a4", "node_type": "1", "metadata": {}, "hash": "c2351f2fec996d1e208ac30d9a16876479790b03e069383a295f5ee867a243d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}}, "text": "The\nsimplified form LVLB(x0)is given by\nEq(x1:T|x0)\"\n\u2212logp(xT)\u2212TX\nt=1logp\u03b8(xt\u22121|xt)\nq(xt|xt\u22121)#\nWith simplication and reparameterization trick, the over-\nall objective Eq(x0)\u0002\nLVLB(x0)\u0003\ncan be simplified into the\nfinal form\nEt\u223cU[1,T],x0\u223cq(x0),\u03f5\u223cN(0,", "start_char_idx": 479, "end_char_idx": 728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0a2ad27-0fd6-4daa-b109-1ccbbad1a3a4": {"__data__": {"id_": "d0a2ad27-0fd6-4daa-b109-1ccbbad1a3a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02f8004b-01f4-49f6-a3a6-adc81e2de14b", "node_type": "1", "metadata": {}, "hash": "6f377e222014d759c6dfaea880d0140c54a26d7c0b72f4c74d8d732f221b70dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b600cf7c-debb-4303-a317-d75ff0849894", "node_type": "1", "metadata": {}, "hash": "f2b60bcf8228604996d8bf4bfe72713723d26f70e7b57031a9577fd7042e851e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}}, "text": "T],x0\u223cq(x0),\u03f5\u223cN(0,I)[\u03bb(t)\u2225\u03f5\u2212\u03f5\u03b8(xt, t)\u2225]\nwhere \u03bb(t)is a positive weighting function, U[1, T]is\na uniform distribution over the set {1,2, . . . , T }, and \u03f5\u03b8\nis a deep neural network with parameter \u03b8that predicts\nthe noise vector \u03f5given xtandt.", "start_char_idx": 710, "end_char_idx": 952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b600cf7c-debb-4303-a317-d75ff0849894": {"__data__": {"id_": "b600cf7c-debb-4303-a317-d75ff0849894", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0a2ad27-0fd6-4daa-b109-1ccbbad1a3a4", "node_type": "1", "metadata": {}, "hash": "c2351f2fec996d1e208ac30d9a16876479790b03e069383a295f5ee867a243d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee606a3f-6129-4795-9c3f-d7c743aaa714", "node_type": "1", "metadata": {}, "hash": "4ce7d5b1795c44c295eec329b71d2b2afc46556d4d31eea300ba77a808410574", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}}, "text": "Note that, the overall\nobjective is also equivalent to matching the joint distri-\nbution of the reverse process p\u03b8(x0, x1, . . . , x T)to that\nof the forward process q(x0, x1, . . . , x T)by minimizing\nthe KL divergence between them.\n4)GAN :Generative Adversarial Networks (GANs) [14]\nare highly anticipated deep learning models with amazing\ncapabilities which can simulate and generate realistic images,\naudio, and other data.", "start_char_idx": 953, "end_char_idx": 1380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee606a3f-6129-4795-9c3f-d7c743aaa714": {"__data__": {"id_": "ee606a3f-6129-4795-9c3f-d7c743aaa714", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b600cf7c-debb-4303-a317-d75ff0849894", "node_type": "1", "metadata": {}, "hash": "f2b60bcf8228604996d8bf4bfe72713723d26f70e7b57031a9577fd7042e851e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "37fb7373-7684-4059-9b8a-f119e12a9483", "node_type": "1", "metadata": {}, "hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "class_name": "RelatedNodeInfo"}}, "text": "Due to its outstanding performance,\nGANs have achieved significant achievements in various5\nfields [97].", "start_char_idx": 1381, "end_char_idx": 1485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edf2a2b8-ee8c-4db5-95c2-41153e5155e3": {"__data__": {"id_": "edf2a2b8-ee8c-4db5-95c2-41153e5155e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad84ebeb-7552-4a56-a95c-9bd66c84eb7b", "node_type": "1", "metadata": {}, "hash": "93cf02f3b171215472aa74e19b6dc943807c49aadfae97b599ccd02db18afe50", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}}, "text": "The design inspiration of GANs comes from the\nzero-sum game in game theory.\nAs shown in Fig. 2, a typical GAN consists of two main\ncomponents: a generator and a discriminator. These two parts\ncompete with each other through adversarial learning, allowing\nthe generator to continuously improve its ability to generate re-\nalistic samples, while the discriminator continuously improves\nits ability to distinguish between true and false samples.\nC.Retriever\nRetrieval is to identify and obtain relevant information given\nan information need.", "start_char_idx": 0, "end_char_idx": 538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad84ebeb-7552-4a56-a95c-9bd66c84eb7b": {"__data__": {"id_": "ad84ebeb-7552-4a56-a95c-9bd66c84eb7b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edf2a2b8-ee8c-4db5-95c2-41153e5155e3", "node_type": "1", "metadata": {}, "hash": "0cb7d262d972da8edd2f256f796aff89e8e4002ab357ab90135fe7557797d35a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7adf1354-3259-464f-bed2-aeeb85ded3a7", "node_type": "1", "metadata": {}, "hash": "d3bb99f609dfd449331b064089a29027712f890adf3fa4c6a87a49dfe2883a71", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}}, "text": "Specifically, let\u2019s consider information\nresources that can be conceptualized as a key-value store\n{(ki, vi)}N\ni=1, where each key kicorresponds to a value vi\n(kiandvican be identical). Given a query q, the objective\nis to search for the top- kmost similar keys using a similarity\nfunction s, and obtain the paired values. Based on different\nsimilarity functions, existing retrieval methods can be cate-\ngorized into sparse retrieval, dense retrieval, and others.", "start_char_idx": 539, "end_char_idx": 1002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7adf1354-3259-464f-bed2-aeeb85ded3a7": {"__data__": {"id_": "7adf1354-3259-464f-bed2-aeeb85ded3a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad84ebeb-7552-4a56-a95c-9bd66c84eb7b", "node_type": "1", "metadata": {}, "hash": "93cf02f3b171215472aa74e19b6dc943807c49aadfae97b599ccd02db18afe50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2228bbc-835c-4746-9b2a-fa52a330de44", "node_type": "1", "metadata": {}, "hash": "2f366efbd57daa81af8bf8b34a96ddba48f07feeb03c6bfe59214b00f36cd44b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}}, "text": "For\nwidely used sparse and dense retrieval, the whole process can\nbe divided into two distinct phases: in the first phase, each\nobject is encoded into a specific representation; and in the\nsecond phase, an index is constructed to organize the data\nsource for efficient search.\n1)Sparse Retriever :Sparse retrieval methods are widely\nused in document retrieval, where the keys are actually doc-\numents to be searched (values are the same documents in\nthis scenario).", "start_char_idx": 1003, "end_char_idx": 1468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2228bbc-835c-4746-9b2a-fa52a330de44": {"__data__": {"id_": "d2228bbc-835c-4746-9b2a-fa52a330de44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7adf1354-3259-464f-bed2-aeeb85ded3a7", "node_type": "1", "metadata": {}, "hash": "d3bb99f609dfd449331b064089a29027712f890adf3fa4c6a87a49dfe2883a71", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "node_type": "1", "metadata": {}, "hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "class_name": "RelatedNodeInfo"}}, "text": "These methods leverage term matching metrics\nsuch as TF-IDF [98], query likelihood [99], and BM25 [19],\nwhich analyze word statistics from texts and construct inverted\nindices for efficient searching. Among them, BM25 is a hard-\nto-beat baseline in industrial-scale web search.", "start_char_idx": 1469, "end_char_idx": 1746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5823942-5724-49f0-bdcc-35b8eea56ee0": {"__data__": {"id_": "c5823942-5724-49f0-bdcc-35b8eea56ee0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b66452e2-8ce6-4fa9-9050-d4e0f3728888", "node_type": "1", "metadata": {}, "hash": "b3f08b94deeb1151858ca99daf69914479f6932e2fdae49240db89a369ea3276", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}}, "text": "Among them, BM25 is a hard-\nto-beat baseline in industrial-scale web search. For a query q\ncontaining keywords {qi}n\ni=1, the BM25 score of a document\nDis:\ns(q, D) =nX\ni=1IDF(qi)\u00b7f(qi, D)\u00b7(a+ 1)\nf(qi, D) +a\u00b7(1\u2212b+b\u00b7|D|\navgdl)\nwhere IDF is the inverse document frequency weight, f(qi, D)\nis the number of times that qioccurs in the document D,", "start_char_idx": 0, "end_char_idx": 341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b66452e2-8ce6-4fa9-9050-d4e0f3728888": {"__data__": {"id_": "b66452e2-8ce6-4fa9-9050-d4e0f3728888", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5823942-5724-49f0-bdcc-35b8eea56ee0", "node_type": "1", "metadata": {}, "hash": "5750a8ed7dc03f2dbad8f4a9134ee3069242bbd4cf24f7d26c9daab8814f5cc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d95c5819-7658-4df8-8296-49d7e5cf845c", "node_type": "1", "metadata": {}, "hash": "4be66e81cfcdb363195b3275e58c2cadb5e648ce4c0c640771753ff2d90a6f21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}}, "text": "f(qi, D)\nis the number of times that qioccurs in the document D,|D|\nis the length of D,avgdl is the average document length in\nthe corpus collection, aandbare tunable parameters.\nIDF is computed as:\nIDF(qi) = ln(N\u2212n(qi) + 0.5\nn(qi) + 0.5+ 1)\nwhere Nis the number of documents, and n(qi)is the number\nof documents containing qi.IDF score is also used in TF-IDF.", "start_char_idx": 277, "end_char_idx": 637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d95c5819-7658-4df8-8296-49d7e5cf845c": {"__data__": {"id_": "d95c5819-7658-4df8-8296-49d7e5cf845c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b66452e2-8ce6-4fa9-9050-d4e0f3728888", "node_type": "1", "metadata": {}, "hash": "b3f08b94deeb1151858ca99daf69914479f6932e2fdae49240db89a369ea3276", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0200ef30-c427-45b4-a9d9-64b6cbc20ca6", "node_type": "1", "metadata": {}, "hash": "867fd0d62c4e70740f0a74c5fccb102ffec38f067da618a9940a1d4cc4863a39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}}, "text": "To enable efficient search, sparse retrieval typically lever-\nages an inverted index to organize documents. Concretely, each\nterm from the query performs a lookup to obtain a list of\ncandidate documents, which are subsequently ranked based\non their statistical scores.\n2)Dense Retriever :Unlike sparse retrieval, dense retrieval\nmethods represent queries and keys using dense embedding\nvectors, and build approximate nearest neighbor (ANN) index\nto speed up the search. This can be applied to all modalities.", "start_char_idx": 638, "end_char_idx": 1146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0200ef30-c427-45b4-a9d9-64b6cbc20ca6": {"__data__": {"id_": "0200ef30-c427-45b4-a9d9-64b6cbc20ca6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d95c5819-7658-4df8-8296-49d7e5cf845c", "node_type": "1", "metadata": {}, "hash": "4be66e81cfcdb363195b3275e58c2cadb5e648ce4c0c640771753ff2d90a6f21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98cbfd2c-a158-4fba-9f6c-1a9279934bae", "node_type": "1", "metadata": {}, "hash": "3eeb808bd67c8491ab0ed7e46319290a43e2dc8b98e9b68eea007eca637d02e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}}, "text": "This can be applied to all modalities.\nFor text data, recent advancements in pre-trained models,including BERT [15] and RoBERTa [100], have been em-\nployed to encode queries and keys individually [20], [101]\u2013\n[104]. Similar to text, models have been proposed to encode\ncode data [25], [105], [106], audio data [26], [107], image\ndata [24], [108], video data [109], [110]. etc.", "start_char_idx": 1108, "end_char_idx": 1484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98cbfd2c-a158-4fba-9f6c-1a9279934bae": {"__data__": {"id_": "98cbfd2c-a158-4fba-9f6c-1a9279934bae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0200ef30-c427-45b4-a9d9-64b6cbc20ca6", "node_type": "1", "metadata": {}, "hash": "867fd0d62c4e70740f0a74c5fccb102ffec38f067da618a9940a1d4cc4863a39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2415d406-648d-488e-adee-8bd0fb74a5f5", "node_type": "1", "metadata": {}, "hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "class_name": "RelatedNodeInfo"}}, "text": "etc. The similarity\nscore between dense representations are usually computed\nwith metrics such as cosine, inner product, L2-distance.\nDuring training, dense retrieval usually follows a contrastive\nlearning paradigm, making positive samples more similar and\nnegative samples less similar. Several hard negative tech-\nniques [101], [111] have been proposed to further improve the\nmodel quality.", "start_char_idx": 1480, "end_char_idx": 1872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d96d7a26-cf40-465c-a85b-e3e272e7b8b2": {"__data__": {"id_": "d96d7a26-cf40-465c-a85b-e3e272e7b8b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "725f8aab-d383-442b-a039-3f59785fa12d", "node_type": "1", "metadata": {}, "hash": "c5f5eb951d0813ca20637583c09c4e3723f826e8e42418599ac200f5b64cea0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}}, "text": "During inference, approximate nearest neighbor\n(ANN) methods are applied for efficient searching. Various\nindices are developed to serve ANN search, such as tree [112],\n[113], locality sensitive hashing [114], neighbor graph index\n(e.g., HNSW [115], DiskANN [116], HMANN [117]), and\nthe combination of graph index and inverted index (e.g.,\nSPANN [22]).\n3)Others :In addition to sparse retrieval and dense re-\ntrieval, there are alternative methods for retrieving relevant\nobjects [118], [119].", "start_char_idx": 0, "end_char_idx": 493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "725f8aab-d383-442b-a039-3f59785fa12d": {"__data__": {"id_": "725f8aab-d383-442b-a039-3f59785fa12d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d96d7a26-cf40-465c-a85b-e3e272e7b8b2", "node_type": "1", "metadata": {}, "hash": "af39deba15a30236adc1be3bc1713da6e884d3929e95c0dfab3857df1c947d8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b270e70d-0842-46d4-aec3-b2325f853782", "node_type": "1", "metadata": {}, "hash": "9f82351a0c19c16dd19ba00171a7fae601df89158aa923f206da092993c290a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}}, "text": "Instead of calculating representations,\nsome research works directly use the edit distance between\nnatural language texts [120] or abstract syntax trees (AST)\nof code snippets [121], [122]. For knowledge graph, entities\nare linked with relations, which can be regarded as a pre-\nbuilt index for retrieval searching. Therefore, RAG methods\nwhich involve knowledge graph can use k-hop neighbor search\nas retrieval process [123], [124]. Named entity recognition\n(NER) [125] is another way of retrieval, where the input is\nthe query and the entites are the keys.\nIII.", "start_char_idx": 494, "end_char_idx": 1057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b270e70d-0842-46d4-aec3-b2325f853782": {"__data__": {"id_": "b270e70d-0842-46d4-aec3-b2325f853782", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "725f8aab-d383-442b-a039-3f59785fa12d", "node_type": "1", "metadata": {}, "hash": "c5f5eb951d0813ca20637583c09c4e3723f826e8e42418599ac200f5b64cea0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fdd4dd2-7a43-4965-ae4c-ef91448aece7", "node_type": "1", "metadata": {}, "hash": "4c80eb3165b3c06c748289ad7aa84c5eb49fb4fa284c1cbb799b13f2a2938867", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}}, "text": "III. METHODS\nIn this section, we introduce RAG foundations and outline\nenhancement methods that further improve the effectiveness.\nA.RAG Foundations\nBased on how the retriever augments the generator, we\ncategorize RAG foundations into 4 classes, as shown in Fig. 3.\n1)Query-based RAG :Query-based RAG, originated from\nthe idea of prompt augmentation, integrates the user\u2019s query\nwith insights from information fetched during the retrieval\nprocess, directly into the initial stage of the language model\u2019s\ninput.", "start_char_idx": 1053, "end_char_idx": 1563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fdd4dd2-7a43-4965-ae4c-ef91448aece7": {"__data__": {"id_": "1fdd4dd2-7a43-4965-ae4c-ef91448aece7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b270e70d-0842-46d4-aec3-b2325f853782", "node_type": "1", "metadata": {}, "hash": "9f82351a0c19c16dd19ba00171a7fae601df89158aa923f206da092993c290a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a35fa84-92c0-4504-bc57-83425e0b900b", "node_type": "1", "metadata": {}, "hash": "b34420caa937cadacf76dd62cad12261d129a4457c4940bf6df16e2ab3a62c6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}}, "text": "This paradigm stands as a widely adopted approach\nwithin the applications of RAG. After retrieval, the retrieved\ncontent will be merged with the original user query to create a\ncomposite input sequence which is subsequently fed into the\ngenerator for response generation. Query-based RAG has been\nwidely applied across various modalities.\nFor Text Generation, REALM [33] employs a dual-BERT\nframework to streamline knowledge retrieval and integration,\nmarrying pre-trained models with knowledge extractors.", "start_char_idx": 1564, "end_char_idx": 2070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a35fa84-92c0-4504-bc57-83425e0b900b": {"__data__": {"id_": "0a35fa84-92c0-4504-bc57-83425e0b900b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fdd4dd2-7a43-4965-ae4c-ef91448aece7", "node_type": "1", "metadata": {}, "hash": "4c80eb3165b3c06c748289ad7aa84c5eb49fb4fa284c1cbb799b13f2a2938867", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "node_type": "1", "metadata": {}, "hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "class_name": "RelatedNodeInfo"}}, "text": "The\ninitial BERT module processes the input question alongside\ndocuments to facilitate retrieval, utilizing MIPS for selecting\nthe top-k documents with the highest probability and periodi-\ncally updating the index.", "start_char_idx": 2071, "end_char_idx": 2285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75700ca3-a4eb-4c65-a733-d621db10c1a3": {"__data__": {"id_": "75700ca3-a4eb-4c65-a733-d621db10c1a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d81e32e-a897-46af-85a9-02e278286cd0", "node_type": "1", "metadata": {}, "hash": "1c581ea8f27640b23541bcda4f611dd2bb777d2ed23f9a757478ea4264ea11a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9d81e32e-a897-46af-85a9-02e278286cd0", "node_type": "1", "metadata": {}, "hash": "1c581ea8f27640b23541bcda4f611dd2bb777d2ed23f9a757478ea4264ea11a1", "class_name": "RelatedNodeInfo"}}, "text": "The document snippets obtained are\nthen integrated with the query, feeding into the second BERT\nmodule to produce multiple outputs that are aggregated into\na singular, comprehensive response. Lewis et al. [34] syner-\ngized pre-trained language models with knowledge retrieval6\nFig. 3: Taxonomy of RAG foundations.\nmechanisms, leveraging DPR and BART structures to ac-\ncomplish retrieval-augmented generation tasks. DPR serves as\nthe retrieval component, sourcing pertinent information from\nvast document databases, while BART uses this information\nfor text generation.", "start_char_idx": 0, "end_char_idx": 568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c450e73d-15e3-4bdb-a091-1ec7e6dca295": {"__data__": {"id_": "c450e73d-15e3-4bdb-a091-1ec7e6dca295", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07038048-4e61-4bd9-8db8-e5de3caa80bb", "node_type": "1", "metadata": {}, "hash": "eedaed2218a3cd897167bc1608e4df7dd1e3b2f72b1a07d7563cd1fa21a941e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}}, "text": "RAG-Token and RAG-Sequence differ in\ntheir retrieval timings, with the former retrieving information\nat each token generation and the latter conducting a single\nretrieval for the entire sequence. SELF-RAG [126] enhances\nthe accuracy and relevance of responses by integrating a\nretrieval and critique strategy. Initially, the model employs a\nretriever to search for information paragraphs closely related to\nthe input question. Subsequently, the critique model evaluates\nthese paragraphs to determine their relevance and level of\nsupport of the retrieved text, assessing their impact on the\ngeneration of responses.", "start_char_idx": 0, "end_char_idx": 614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07038048-4e61-4bd9-8db8-e5de3caa80bb": {"__data__": {"id_": "07038048-4e61-4bd9-8db8-e5de3caa80bb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c450e73d-15e3-4bdb-a091-1ec7e6dca295", "node_type": "1", "metadata": {}, "hash": "e3acc618300ff01dbd232052cfe306eccb2854ebc332a79567274eef6e95a625", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70e6d4c2-9123-48b5-9496-517b95606d04", "node_type": "1", "metadata": {}, "hash": "38b4d78d9bc27300083d3bde2e5c1eb8b334564fc52d284947505999b74310f4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}}, "text": "Finally, the generator model con-\nstructs responses based on this information and evaluates the\nquality of these responses through critique marks. In addition\nto being compatible with local generators, Query-based RAG\nis also applicable to scenarios that use LLM through API\ncalls. REPLUG [127] illustrates this methodology by treating\nthe language model as a \u201cblack box\u201d, utilizing Contriever to\nseamlessly incorporate relevant external documents into the\nquery.", "start_char_idx": 615, "end_char_idx": 1078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70e6d4c2-9123-48b5-9496-517b95606d04": {"__data__": {"id_": "70e6d4c2-9123-48b5-9496-517b95606d04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07038048-4e61-4bd9-8db8-e5de3caa80bb", "node_type": "1", "metadata": {}, "hash": "eedaed2218a3cd897167bc1608e4df7dd1e3b2f72b1a07d7563cd1fa21a941e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fab5a902-7a88-4285-acd8-0558cd7a3974", "node_type": "1", "metadata": {}, "hash": "ba4ca6bbf960bacc69853c2d66ebc687e3f3421111d91b7e160683b0d651aeef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}}, "text": "REPLUG LSR, a variant with LM-Supervised Retrieval,\nfurther refines this process by optimizing retrieval through\nlanguage model-supervised insights, aiming to reduce per-\nplexity scores and improve model performance by enriching\nits contextual understanding. In-Context RALM [128] uses\nBM25 for document retrieval and trains a predictive reranker\nto reorder and integrate the top-ranked documents.\nIn contemporary research on other modalities, augmenting\ninputs with retrieved contents (which are not limited to texts)\nhas proven highly effective in enhancing the performance of\nvarious tasks.", "start_char_idx": 1079, "end_char_idx": 1672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fab5a902-7a88-4285-acd8-0558cd7a3974": {"__data__": {"id_": "fab5a902-7a88-4285-acd8-0558cd7a3974", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70e6d4c2-9123-48b5-9496-517b95606d04", "node_type": "1", "metadata": {}, "hash": "38b4d78d9bc27300083d3bde2e5c1eb8b334564fc52d284947505999b74310f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "647c2fe0-ab0c-45db-82b0-9af9eaf4feac", "node_type": "1", "metadata": {}, "hash": "4919cb79ee556e0ae4d86eefb39a90956910dbf1366a7b3d81de2c4e66d3cf27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}}, "text": "This strategy is applicable across several critical\ndomains, including code generation, audio generation, and\nKnowledge Base Question Answering (KBQA).\nFor Text-to-Code task, APICoder [129] and DocPrompt-\ning [42] demonstrate how effectively integrating retrieved\ninformation into language models can improve the accuracy\nand relevance of generated code. In Automatic Program Repair\ntask, CEDAR [130] and InferFix [131] utilize retrieved code\nsnippets to aid the repair process, enhancing the model\u2019s\nunderstanding and application of repair strategies by combin-\ning them with the original input.", "start_char_idx": 1673, "end_char_idx": 2269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "647c2fe0-ab0c-45db-82b0-9af9eaf4feac": {"__data__": {"id_": "647c2fe0-ab0c-45db-82b0-9af9eaf4feac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fab5a902-7a88-4285-acd8-0558cd7a3974", "node_type": "1", "metadata": {}, "hash": "ba4ca6bbf960bacc69853c2d66ebc687e3f3421111d91b7e160683b0d651aeef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "59f340a8-342e-491c-94b2-43642ff312f4", "node_type": "1", "metadata": {}, "hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "class_name": "RelatedNodeInfo"}}, "text": "For Code Completion task,\nReACC [132] employs a prompting mechanism, leveraging\nretrieved code snippets as part of the new input to increase\nthe accuracy and efficiency of code completion.", "start_char_idx": 2270, "end_char_idx": 2458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfeff0d1-aadf-4d52-8c3b-0e986b4cd6ce": {"__data__": {"id_": "cfeff0d1-aadf-4d52-8c3b-0e986b4cd6ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20581bbc-4949-4403-b086-da6b3edb5099", "node_type": "1", "metadata": {}, "hash": "c49587207e9182413b4e17af9ab7d8e81b11c4569c388e439b0f0b728cd667b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}}, "text": "Recent researches in Knowledge Base Question Answering(KBQA) has also shown significant effects of combining\nretrieval and language models. For instance, Uni-Parser [133],\nRNG-KBQA [123], and ECBRF [134] effectively improve\nthe performance and accuracy of QA systems by merging\nqueries and retrieved information into prompts. BLLM aug-\nmentation [135] represents an innovative attempt at zero-\nshot KBQA using black-box large language models.", "start_char_idx": 0, "end_char_idx": 442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20581bbc-4949-4403-b086-da6b3edb5099": {"__data__": {"id_": "20581bbc-4949-4403-b086-da6b3edb5099", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfeff0d1-aadf-4d52-8c3b-0e986b4cd6ce", "node_type": "1", "metadata": {}, "hash": "bb303f85a73ee0e2a6e9b5a3f85514eed6fc574efb2e5fa7040e62656d90b57e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d6adc2c-4482-41e2-8db4-1d7c490f0a90", "node_type": "1", "metadata": {}, "hash": "258c9adf36230883a49a612423d7da47361fc5d8d8da64202310bb0d37713a5f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}}, "text": "This\nmethod, by directly integrating retrieved information into the\nmodel input without the need for additional sample training,\ndemonstrates the great potential of combining retrieval and\nlanguage models to enhance the model\u2019s generalization ability\nin understanding and answering unseen questions.\nIn the AI-for-Science domain, Chat-Orthopedist [136] pro-\nvides support for shared decision-making among adolescents\nwith idiopathic scoliosis. It enhances the application effec-\ntiveness and information accuracy of LLMs by integrating\nretrieved information into the prompts of the model.", "start_char_idx": 443, "end_char_idx": 1031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d6adc2c-4482-41e2-8db4-1d7c490f0a90": {"__data__": {"id_": "9d6adc2c-4482-41e2-8db4-1d7c490f0a90", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20581bbc-4949-4403-b086-da6b3edb5099", "node_type": "1", "metadata": {}, "hash": "c49587207e9182413b4e17af9ab7d8e81b11c4569c388e439b0f0b728cd667b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b70cee44-ce5a-4788-8ff4-c52b0d7696f2", "node_type": "1", "metadata": {}, "hash": "f1548a6b1ad6a89c9ee9bab6feadf0fe3f2cec50d8a73e615c203f7111fd8ac3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}}, "text": "In the task of Image Generation, RetrieveGAN [45] en-\nhances the relevance and accuracy of generated images by\nintegrating retrieved information, including selected image\npatches and their corresponding bounding boxes, into the\ninput stage of the generator. IC-GAN [137] modulates the\nspecific conditions and details of the generated images by\nconcatenating noise vectors with instance features.\nFor 3D Generation, RetDream [50] initially utilizes\nCLIP [24] to retrieve relevant 3D assets, then merges the\nretrieved contents with the user input during the input phase.", "start_char_idx": 1032, "end_char_idx": 1600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b70cee44-ce5a-4788-8ff4-c52b0d7696f2": {"__data__": {"id_": "b70cee44-ce5a-4788-8ff4-c52b0d7696f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d6adc2c-4482-41e2-8db4-1d7c490f0a90", "node_type": "1", "metadata": {}, "hash": "258c9adf36230883a49a612423d7da47361fc5d8d8da64202310bb0d37713a5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f447e72d-4b10-4ac6-92c8-2e00aed7fe48", "node_type": "1", "metadata": {}, "hash": "8ec63b70c55853ccdad77674647c54e4852946db45bfda073a76591a38bf13a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}}, "text": "Query-based RAG, often paired with LLM generators,\noffers modular flexibility, allowing swift integration of pre-\ntrained components for quick deployment. Prompt design is\ncrucial for utilizing retrieved data within this setup.\n2)Latent Representation-based RAG :In Latent\nRepresentation-based RAG framework, retrieved objects are\nincorporated into generative models as latent representations.\nThis enhances the model\u2019s comprehension abilities and\nimproves the quality of the generated content.\nThe Fusion-in-Decoder (FiD) [35] technique leverages both\nBM25 and DPR for sourcing supportive paragraphs.", "start_char_idx": 1601, "end_char_idx": 2202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f447e72d-4b10-4ac6-92c8-2e00aed7fe48": {"__data__": {"id_": "f447e72d-4b10-4ac6-92c8-2e00aed7fe48", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b70cee44-ce5a-4788-8ff4-c52b0d7696f2", "node_type": "1", "metadata": {}, "hash": "f1548a6b1ad6a89c9ee9bab6feadf0fe3f2cec50d8a73e615c203f7111fd8ac3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "70f446ab-c0c8-4500-a291-c9562ad75f60", "node_type": "1", "metadata": {}, "hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "class_name": "RelatedNodeInfo"}}, "text": "It con-\ncatenates each retrieved paragraph and its title with the query,\nprocessing them individually through the encoder.", "start_char_idx": 2203, "end_char_idx": 2325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "446508f1-2908-4bc4-a220-f7638b3b0714": {"__data__": {"id_": "446508f1-2908-4bc4-a220-f7638b3b0714", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7bcbfc6-a287-44da-824a-07ea4bcb9eba", "node_type": "1", "metadata": {}, "hash": "d80e6780de80cc16cf05ef1dca2f8639b01b6bba7b5891ecab3041e1458e91a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}}, "text": "FiD re-\nduces computational complexity and efficiently utilizes rel-\nevant information to generate answers by fusing information\nfrom multiple retrieved paragraphs in the decoder, rather than\nprocessing each paragraph in the encoder. The application\nof Fusion-in-Decoder methodologies transcends the realm of\ntextual content processing, demonstrating substantial potential\nand adaptability in processing code, structured knowledge,7\nand diverse multimodal datasets.", "start_char_idx": 0, "end_char_idx": 465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7bcbfc6-a287-44da-824a-07ea4bcb9eba": {"__data__": {"id_": "c7bcbfc6-a287-44da-824a-07ea4bcb9eba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "446508f1-2908-4bc4-a220-f7638b3b0714", "node_type": "1", "metadata": {}, "hash": "898c8a93a0eb4e100148740afe8e4f05259180e4e65e5e5066b93294abb89988", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf28dde5-d99f-4207-a394-11b22ff17b04", "node_type": "1", "metadata": {}, "hash": "844ad0c835ce614b7d7395bc902ccb91c97ed1430419c0b6001a67aee95de240", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}}, "text": "Specifically within the code-\nrelated domain, technologies such as EDITSUM [138], BASH-\nEXPLAINER [139], and RetrieveNEdit [140] adopt the FiD\napproach, facilitating integration through encoder-processed\nfusion. Re2Com [141], and RACE [142] , among other meth-\nods, also feature the design of multiple encoders for different\ntypes of inputs.", "start_char_idx": 466, "end_char_idx": 807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf28dde5-d99f-4207-a394-11b22ff17b04": {"__data__": {"id_": "bf28dde5-d99f-4207-a394-11b22ff17b04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7bcbfc6-a287-44da-824a-07ea4bcb9eba", "node_type": "1", "metadata": {}, "hash": "d80e6780de80cc16cf05ef1dca2f8639b01b6bba7b5891ecab3041e1458e91a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2fae188-76b6-4255-862b-8c0d22114254", "node_type": "1", "metadata": {}, "hash": "fb60aed20fb772e19ba17d8488314d0091d5167c8a07165a5194ff1c51421afb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}}, "text": "In the field of Science, RetMol [55] also employs the\nFusion-in-Decoder strategy, integrating information at the\ndecoder stage to enhance the relevance and quality of the\ngenerated molecular structures.\nIn the field of Knowledge Base Question Answer-\ning (KBQA), the FiD method has been widely adopted,\ndemonstrating significant effectiveness. UniK-QA [143], DE-\nCAF [144], SKP [145], KD-CoT [146], and ReSKGC [147]\nhave effectively enhanced the performance of QA systems\nthrough the application of Fusion-in-Decoder technology.", "start_char_idx": 808, "end_char_idx": 1336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2fae188-76b6-4255-862b-8c0d22114254": {"__data__": {"id_": "d2fae188-76b6-4255-862b-8c0d22114254", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf28dde5-d99f-4207-a394-11b22ff17b04", "node_type": "1", "metadata": {}, "hash": "844ad0c835ce614b7d7395bc902ccb91c97ed1430419c0b6001a67aee95de240", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5157b639-edab-4c47-a9c6-771b6464b8a3", "node_type": "1", "metadata": {}, "hash": "9ab7544abb879d39377f58d84bd644ea3306f7e8b15210b7c8c3fe89ef36d91f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}}, "text": "This\nillustrates that by integrating RAG for KBQA, the\nRetro [36] pioneers the integration of retrieved text via\n\u201cChunked Cross-Attention\u201d a novel mechanism that segments\nthe input sequence into discrete chunks. Each chunk indepen-\ndently executes cross-attention operations, thereby mitigating\ncomputational burdens. This technique enables the model to\nselectively retrieve and assimilate distinct documents for var-\nied sequence segments, fostering dynamic retrieval throughout\nthe generation process. This enhances the model\u2019s adaptability\nand enriches the contextual backdrop of generated content.", "start_char_idx": 1337, "end_char_idx": 1938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5157b639-edab-4c47-a9c6-771b6464b8a3": {"__data__": {"id_": "5157b639-edab-4c47-a9c6-771b6464b8a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2fae188-76b6-4255-862b-8c0d22114254", "node_type": "1", "metadata": {}, "hash": "fb60aed20fb772e19ba17d8488314d0091d5167c8a07165a5194ff1c51421afb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3", "node_type": "1", "metadata": {}, "hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "class_name": "RelatedNodeInfo"}}, "text": "This enhances the model\u2019s adaptability\nand enriches the contextual backdrop of generated content. In\nthe domain of image generation, cross-attention mechanisms\nhave been widely adopted within RAG frameworks. Methods\nsuch as Re-imagen [148], KNN-Diffusion [149], RDM [150]\nand LAION-RDM & ImageNet-RDM [151] utilize cross-\nattention to integrate multiple retrieval results, effectively\nenhancing the overall performance of the models.", "start_char_idx": 1841, "end_char_idx": 2274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e910ae6a-e737-4702-b02a-6eb60c5e82fa": {"__data__": {"id_": "e910ae6a-e737-4702-b02a-6eb60c5e82fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4617e78-1587-42e9-baff-320cf53f0c30", "node_type": "1", "metadata": {}, "hash": "e77c39de403e8a735b83488b744ffa6721f4e9096ebf3a923eb6c6f4977b2b0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}}, "text": "In addition, there are also some other novel structures worth\nour attention, Li [152] introduced the ACM, a text-image affine\ncombination module, which notably does not employ any form\nof attention mechanism. Memorizing Transformers [31] revo-\nlutionize long document processing through the integration of\na kNN-augmented attention mechanism within a Transformer\nlayer. This innovation triggers a kNN search amidst input se-\nquence processing, fetching data based on similarities between\nthe sequence and stored key-value pairs, thereby elevating\nperformance without necessitating complete retraining.", "start_char_idx": 0, "end_char_idx": 601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4617e78-1587-42e9-baff-320cf53f0c30": {"__data__": {"id_": "f4617e78-1587-42e9-baff-320cf53f0c30", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e910ae6a-e737-4702-b02a-6eb60c5e82fa", "node_type": "1", "metadata": {}, "hash": "b248567956e50d66624a62467f52786021bb02342a7b0bb21da3b937bf6b11d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7182afb7-8ad9-4544-96da-88dd70391ad6", "node_type": "1", "metadata": {}, "hash": "443b17deeaab2e6bb7b31d5f23b69420e9579246d367a7d5c59baf4bb717b4ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}}, "text": "This\napproach not only bolsters processing efficiency but also\nbroadens the model\u2019s memory span, enabling self-retrieval\nfrom its generated outputs and fine-tuning for extensive\nknowledge bases or code repositories. Unlimiformer [153],\nby embedding a k-nearest neighbors (kNN) index within a\npre-trained encoder-decoder transformer framework, pioneers\nhandling inputs of indefinite length. Storing hidden states\nof input tokens in the kNN index allows for the efficient\nretrieval of highly relevant tokens during decoding.", "start_char_idx": 602, "end_char_idx": 1124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7182afb7-8ad9-4544-96da-88dd70391ad6": {"__data__": {"id_": "7182afb7-8ad9-4544-96da-88dd70391ad6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4617e78-1587-42e9-baff-320cf53f0c30", "node_type": "1", "metadata": {}, "hash": "e77c39de403e8a735b83488b744ffa6721f4e9096ebf3a923eb6c6f4977b2b0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d11a8593-e158-4491-a23f-387fa20a2949", "node_type": "1", "metadata": {}, "hash": "9ad9fe17ae4654d2a4442d753dde3e5e34e59a46712df8c5dbe9e4d1eb663936", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}}, "text": "This\ninnovation extends the model\u2019s capacity to manage prolonged\nsequences. Kuratov et al. [154] integrated Transformer with\nRNN, utilizing the model\u2019s intermediate output as the content\nfor retrieval. This process was executed at each layer of the\nTransformer, thereby significantly extending the text window\u2019s\nlength and effectively mitigating the issue of \u201cLost in theMiddle\u201d [155]. Diverging from prior methods for knowledge,\nEaE [156] empowers language models to internalize explicit\nentity knowledge.", "start_char_idx": 1125, "end_char_idx": 1631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d11a8593-e158-4491-a23f-387fa20a2949": {"__data__": {"id_": "d11a8593-e158-4491-a23f-387fa20a2949", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7182afb7-8ad9-4544-96da-88dd70391ad6", "node_type": "1", "metadata": {}, "hash": "443b17deeaab2e6bb7b31d5f23b69420e9579246d367a7d5c59baf4bb717b4ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67354aca-f02d-489b-8076-768319861122", "node_type": "1", "metadata": {}, "hash": "944c0a0b19482c05747ab66de688411e786894d90558f161ef81ee4457836e2e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}}, "text": "EaE introduces an entity-specific param-\neterization, optimizing inference efficacy through an entity\nmemory layer embedded within the transformer architecture.\nThis layer directly acquires entity representations from textual\ndata, utilizing a sparse retrieval strategy to fetch the nearest\nentities based on their embeddings, thus refining the model\u2019s\ncomprehension through a calculated aggregation of entity-\nspecific information. on this basis , TOME [157] shifts the\nfocus towards comprehensive mention encodings, prioritizing\nthe granularity of mention over mere entity representations.", "start_char_idx": 1632, "end_char_idx": 2223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67354aca-f02d-489b-8076-768319861122": {"__data__": {"id_": "67354aca-f02d-489b-8076-768319861122", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d11a8593-e158-4491-a23f-387fa20a2949", "node_type": "1", "metadata": {}, "hash": "9ad9fe17ae4654d2a4442d753dde3e5e34e59a46712df8c5dbe9e4d1eb663936", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "node_type": "1", "metadata": {}, "hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "class_name": "RelatedNodeInfo"}}, "text": "It meticulously creates a database that stores key and value\nencodings along with entity IDs, enabling the retrieval of much\nmore fine-grained information.", "start_char_idx": 2224, "end_char_idx": 2379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "521bc758-b6b8-4767-ba22-0b5c7695f7e6": {"__data__": {"id_": "521bc758-b6b8-4767-ba22-0b5c7695f7e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d78ca6ea-0bd4-4a06-a4de-9956656051dd", "node_type": "1", "metadata": {}, "hash": "5e1f7e76883b83dfadeac9a72b929a9ede1ad8ef5be9ea2275c5e90a49093aa6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d78ca6ea-0bd4-4a06-a4de-9956656051dd", "node_type": "1", "metadata": {}, "hash": "5e1f7e76883b83dfadeac9a72b929a9ede1ad8ef5be9ea2275c5e90a49093aa6", "class_name": "RelatedNodeInfo"}}, "text": "TOME integrates an initial\ntransformer block to process input texts, followed by TOME\nblocks with memory attention layers, facilitating the synthesis\nof multifaceted information sources and enhancing inferential\nreasoning capabilities, even for unencountered entities.", "start_char_idx": 0, "end_char_idx": 268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fce0132-01b6-44ae-a579-03dddea37351": {"__data__": {"id_": "0fce0132-01b6-44ae-a579-03dddea37351", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff9fbb87-c761-46f6-a6ab-81f552c06371", "node_type": "1", "metadata": {}, "hash": "9e297e1e6d0ad11041908374be148a28c4936d88e235a043f9a94e16b3095e60", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}}, "text": "In the field of 3D generation, ReMoDiffuse [51] introduces\na semantics-modulated attention mechanism which enhances\nthe accuracy of generating corresponding 3D motions based on\ntextual descriptions. AMD [158] achieves efficient conversion\nfrom text to 3D motion by fusing the original diffusion process\nwith the reference diffusion process.\nIn the Audio domain, Koizumi et al. [43] utilized an\nLLM, incorporating encoded dense features in the atten-\ntion module to guide the generation of audio captions.", "start_char_idx": 0, "end_char_idx": 504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff9fbb87-c761-46f6-a6ab-81f552c06371": {"__data__": {"id_": "ff9fbb87-c761-46f6-a6ab-81f552c06371", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fce0132-01b6-44ae-a579-03dddea37351", "node_type": "1", "metadata": {}, "hash": "df5df7a3d7632afe85b819f047ab0cd93d28f96e1ffb607b3508128f16e7a23f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "833e21b8-65c9-44c2-980d-9da78f2499ec", "node_type": "1", "metadata": {}, "hash": "c6a57722d21e1eb0004da56067ae071eebd079f844d4f65db34fed921c337800", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}}, "text": "Re-\nAudioLDM [159] utilizes distinct encoders to extract deep\nfeatures from text and audio, which are then integrated into the\nattention mechanism of its Latent Diffusion Model (LDM).\nFor video captioning, R-ConvED [48] uses a convolutional\nencoder-decoder network to process retrieved video-sentence\npairs with an attention mechanism, generating hidden states to\nproduce captions. CARE [160] introduces a concept detector\nto produce concept probabilities, and incorporates concept\nrepresentations into a hybrid attention mechanism.", "start_char_idx": 505, "end_char_idx": 1037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "833e21b8-65c9-44c2-980d-9da78f2499ec": {"__data__": {"id_": "833e21b8-65c9-44c2-980d-9da78f2499ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff9fbb87-c761-46f6-a6ab-81f552c06371", "node_type": "1", "metadata": {}, "hash": "9e297e1e6d0ad11041908374be148a28c4936d88e235a043f9a94e16b3095e60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06fba012-1729-4a78-b242-4b7f23225683", "node_type": "1", "metadata": {}, "hash": "443c5d8a8fba2f3bb196aa65a1f98dcd476d2414ec911f1113b77f8cbf481034", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}}, "text": "EgoIn-\nstructor [49] employs gated-cross attention to integrate textual\ninputs with encoded video features, enhancing the relevance\nand coherence of the generated captions for egocentric video\ncontent.\nFinally, Latent Representation-based RAG is adaptable to\nvarious modalities and tasks. It obtains the hidden states of\nretrieved data, enabling seamless integration between retrievers\nand generators. However, additional training is required to\nalign the latent space. Within this paradigm, we have the flex-\nibility to design intricate and novel algorithms that effectively\ncombine the information from retrieved contents.", "start_char_idx": 1038, "end_char_idx": 1662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06fba012-1729-4a78-b242-4b7f23225683": {"__data__": {"id_": "06fba012-1729-4a78-b242-4b7f23225683", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "833e21b8-65c9-44c2-980d-9da78f2499ec", "node_type": "1", "metadata": {}, "hash": "c6a57722d21e1eb0004da56067ae071eebd079f844d4f65db34fed921c337800", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4947b6b4-0407-4482-a4c2-6ede5a612a61", "node_type": "1", "metadata": {}, "hash": "a83c5c00f76247cc6deb0a886bb04e96740a53be43fc33841cd3c613d01ce650", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}}, "text": "3)Logit-based RAG :In logit-based RAG, generative mod-\nels integrate retrieval information through logits during the\ndecoding process. Typically, the logits are combined through\nsimple summation or models to compute the probabilities for\nstep-wise generation.\nThe kNN-LM [37] model integrates a pre-trained neural\nlanguage model with the k-nearest neighbor search.", "start_char_idx": 1663, "end_char_idx": 2027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4947b6b4-0407-4482-a4c2-6ede5a612a61": {"__data__": {"id_": "4947b6b4-0407-4482-a4c2-6ede5a612a61", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06fba012-1729-4a78-b242-4b7f23225683", "node_type": "1", "metadata": {}, "hash": "443c5d8a8fba2f3bb196aa65a1f98dcd476d2414ec911f1113b77f8cbf481034", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9dc57153-6fd5-499b-a634-fc574002380e", "node_type": "1", "metadata": {}, "hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "class_name": "RelatedNodeInfo"}}, "text": "It employs\nthe pre-trained model to generate a list of candidate words and\ntheir probability distribution, while simultaneously performing\nretrieval from a data repository to find the k most relevant8\nneighbors based on the current context, thus enhancing the\noutput of the original language model.", "start_char_idx": 2028, "end_char_idx": 2326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d495a1-fa6d-4e6b-b58e-d71d21fc6dcc": {"__data__": {"id_": "93d495a1-fa6d-4e6b-b58e-d71d21fc6dcc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf11fe80-d25f-479b-ad30-4a45fd1b4ee4", "node_type": "1", "metadata": {}, "hash": "7650665160ddcadabdaf6ed5f29669f2ad40fafd4b0aa01982e41d56c408ece2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}}, "text": "The innovation at the\ncore of this model lies in its ability for dynamic retrieval of\ninformation from a broad text corpus, significantly improving\nthe accuracy and relevance of its predictions, particularly in\naddressing rare patterns and adapting to various fields. He et\nal. [38] introduced a new framework that is predicated on\nperforming retrieval operations only when necessary, aimed\nat enhancing the inference efficiency of the kNN-LM model\nthrough an adaptive retrieval.", "start_char_idx": 0, "end_char_idx": 479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf11fe80-d25f-479b-ad30-4a45fd1b4ee4": {"__data__": {"id_": "cf11fe80-d25f-479b-ad30-4a45fd1b4ee4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93d495a1-fa6d-4e6b-b58e-d71d21fc6dcc", "node_type": "1", "metadata": {}, "hash": "ef24e4c88670f09703fc785f860654a130018a2cf3ff4ffb7db3596545470e27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "916c7dd8-b06b-4893-8ee9-4802ad8ba187", "node_type": "1", "metadata": {}, "hash": "bb48b4a67eb4de213d26ba81df501946bb33c5681a9a0994f4e038d7eba69bd2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}}, "text": "This framework accelerates the\nmodel\u2019s inference speed by training a retrieval adapter, which\nautomatically identifies and eliminates unnecessary retrieval\nactions in certain scenarios. This method allows the model to\ndynamically decide on the necessity of retrieval based on the\ncurrent context, thereby balancing the trade-off between per-\nformance and efficiency, and substantially increasing inference\nspeed while maintaining model performance.\nUnlike previous methods that only merge memories during\nthe testing time, TRIME [161] achieves memory merging\nduring both training and testing phases, treating in-batch\nexamples as accessible memory.", "start_char_idx": 480, "end_char_idx": 1128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "916c7dd8-b06b-4893-8ee9-4802ad8ba187": {"__data__": {"id_": "916c7dd8-b06b-4893-8ee9-4802ad8ba187", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf11fe80-d25f-479b-ad30-4a45fd1b4ee4", "node_type": "1", "metadata": {}, "hash": "7650665160ddcadabdaf6ed5f29669f2ad40fafd4b0aa01982e41d56c408ece2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3c637f9-57fc-4689-b0ab-401bf0f103cd", "node_type": "1", "metadata": {}, "hash": "fdcbf9f039f932d129a769237f162be470154d20738827ae577b560820dd7396", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}}, "text": "It leverages new data batching\nand memory construction techniques to effectively utilize\nexternal memory. It employs BM25 scores to pack paragraphs\nwith high lexical overlap into the same batch, constructing\nthe training memory to further optimize model performance.\nNPM [162] is a non-parametric masked language model com-\nprised of an encoder and a reference corpus. Unlike traditional\nmodels that apply a softmax over a finite vocabulary, NPM\nmodels a non-parametric distribution over the corpus.", "start_char_idx": 1129, "end_char_idx": 1628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3c637f9-57fc-4689-b0ab-401bf0f103cd": {"__data__": {"id_": "c3c637f9-57fc-4689-b0ab-401bf0f103cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "916c7dd8-b06b-4893-8ee9-4802ad8ba187", "node_type": "1", "metadata": {}, "hash": "bb48b4a67eb4de213d26ba81df501946bb33c5681a9a0994f4e038d7eba69bd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5c1bab0-af3f-4195-9a99-494366bdfaeb", "node_type": "1", "metadata": {}, "hash": "70d586b0b3b6757f09540c5ff6b3369e4cf4f456f9bc1d14e9bbdd4710dc19fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}}, "text": "The\nencoder\u2019s role is to map phrases from the corpus into fixed-\nsize vectors, filling in [MASK] by retrieving the phrase most\nsimilar to the masked position.\nBeyond Text, other modalities, such as code and Image,\nalso leverage logit-based RAG. For code-to-text conversion\ntask, Rencos [121] generates multiple summary candidates in\nparallel from the retrieved code. It then normalizes these can-\ndidates using edit distance and calculates the final probability\nto select the summary output that best matches the original\ncode.", "start_char_idx": 1629, "end_char_idx": 2156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5c1bab0-af3f-4195-9a99-494366bdfaeb": {"__data__": {"id_": "b5c1bab0-af3f-4195-9a99-494366bdfaeb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3c637f9-57fc-4689-b0ab-401bf0f103cd", "node_type": "1", "metadata": {}, "hash": "fdcbf9f039f932d129a769237f162be470154d20738827ae577b560820dd7396", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "node_type": "1", "metadata": {}, "hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "class_name": "RelatedNodeInfo"}}, "text": "In code summarization task, EDITSUM [138] enhances\nthe quality of summary generation by integrating prototype\nsummaries at the probability level. For text-to-code tasks,\nthe kNN-TRANX [163] model employs a combination of a\nconfidence network and meta-knowledge to merge retrieved\ncode fragments.", "start_char_idx": 2157, "end_char_idx": 2452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5291ac61-aa0c-4268-aa6c-4431e2115530": {"__data__": {"id_": "5291ac61-aa0c-4268-aa6c-4431e2115530", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db893a9b-7adf-4d69-a247-e0187299a6a5", "node_type": "1", "metadata": {}, "hash": "09159869813772405435517b8005b4db6f8d3d3285d16ca1afa985eec6600bc5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}}, "text": "It utilizes a seq2tree structure to generate\ntarget code that closely matches the input query, thereby\nincreasing the accuracy and relevance of code generation. In\nimage captioning tasks, MA [164] combines an attention-based\nencoder-decoder, using the image encoder to extract visual\nfeatures to construct the semantic part, and decodes it word by\nword with the information retrieved. MA interpolates between\ntwo distributions generated by the caption decoder and the\nmemory-augmented module to determine the distribution of\nthe next word.", "start_char_idx": 0, "end_char_idx": 539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db893a9b-7adf-4d69-a247-e0187299a6a5": {"__data__": {"id_": "db893a9b-7adf-4d69-a247-e0187299a6a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5291ac61-aa0c-4268-aa6c-4431e2115530", "node_type": "1", "metadata": {}, "hash": "00c625443826b23f67f5255c081d9ed10a712cc6dc61dc66af70dbdf3a9b45f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6cb972f-9382-4be4-8251-374170f5deaf", "node_type": "1", "metadata": {}, "hash": "eac9829ece5ce364de5719e2f67a1c72c733e9ce951d5d3e858aabe6c1433caa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}}, "text": "In conclusion, Logit-based RAG effectively leverages histor-\nical states (or other data sources) to infer the current state and\ncombines information at the logit level. This approach is well-\nsuited for sequence generation tasks. It couples retrieval and\ngeneration, primarily requiring training of the generator. Novelapproaches can be designed to effectively utilize the obtained\nprobability distributions and adapt to subsequent tasks.\n4)Speculative RAG :Speculative RAG seeks opportunities\nto use retrieval instead of pure generation, aiming to save\nresources and accelerate response speed.", "start_char_idx": 540, "end_char_idx": 1134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6cb972f-9382-4be4-8251-374170f5deaf": {"__data__": {"id_": "b6cb972f-9382-4be4-8251-374170f5deaf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db893a9b-7adf-4d69-a247-e0187299a6a5", "node_type": "1", "metadata": {}, "hash": "09159869813772405435517b8005b4db6f8d3d3285d16ca1afa985eec6600bc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8bbd7aa-500f-4460-b15c-c8959ea97ed7", "node_type": "1", "metadata": {}, "hash": "db71fc1a77cb8df69ca9fd2ad78c71ecf08144091bcb67a8a1acd3e455bd856d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}}, "text": "REST [32] replaces\nthe small models in speculative decoding [165] with retrieval,\nenabling the generation of drafts. GPTCache [39] addresses the\nissue of high latency when using the LLM APIs by building\na semantic cache for storing LLM responses. COG [166]\ndecomposes the text generation process into a series of copy-\nand-paste operations, retrieving words or phrases from the\ndocuments instead of generation. Cao et al.", "start_char_idx": 1135, "end_char_idx": 1556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8bbd7aa-500f-4460-b15c-c8959ea97ed7": {"__data__": {"id_": "e8bbd7aa-500f-4460-b15c-c8959ea97ed7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6cb972f-9382-4be4-8251-374170f5deaf", "node_type": "1", "metadata": {}, "hash": "eac9829ece5ce364de5719e2f67a1c72c733e9ce951d5d3e858aabe6c1433caa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc6d7f07-411a-40b5-b47f-5332e2253a43", "node_type": "1", "metadata": {}, "hash": "db6346e86c2a29df736cb6f745d44a7b111b1a149dfe97522bcf7cab30133aab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}}, "text": "Cao et al. [167] proposed a\nnew paradigm to eliminate the dependence of the final result\non the quality of the first-stage retrieved content, replacing\ngeneration with directly retrieved phrase level content.\nIn conclusion, Speculative RAG is currently primarily ap-\nplicable to sequential data. It decouples the generator and\nthe retriever, enabling the direct use of pre-trained models as\ncomponents. Within this paradigm, we can explore a wider\nrange of strategies to effectively utilize the retrieved content.", "start_char_idx": 1546, "end_char_idx": 2059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc6d7f07-411a-40b5-b47f-5332e2253a43": {"__data__": {"id_": "bc6d7f07-411a-40b5-b47f-5332e2253a43", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8bbd7aa-500f-4460-b15c-c8959ea97ed7", "node_type": "1", "metadata": {}, "hash": "db71fc1a77cb8df69ca9fd2ad78c71ecf08144091bcb67a8a1acd3e455bd856d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "node_type": "1", "metadata": {}, "hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "class_name": "RelatedNodeInfo"}}, "text": "Within this paradigm, we can explore a wider\nrange of strategies to effectively utilize the retrieved content.\nB.RAG Enhancements\nIn this section, we introduce methods which enhance the\nperformance of a constructed RAG system. We categorize\nexisting methods into 5 groups based on their enhancement tar-\ngets: input, retriever, generator, result, and the entire pipeline.", "start_char_idx": 1949, "end_char_idx": 2320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "236dd4eb-04a5-45f2-84ca-1d849255c593": {"__data__": {"id_": "236dd4eb-04a5-45f2-84ca-1d849255c593", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1887afa-cd93-4a27-b62f-d7f1d01e26cb", "node_type": "1", "metadata": {}, "hash": "c69585fb915b622cc5e4cb4273bc2131e5e30cc2a978be334bff95c847346264", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}}, "text": "1)Input Enhancement :The input, initially fed into the re-\ntriever, significantly impacts the final outcome of the retrieval\nstage. In this section, we introduce two methods for input\nenhancement: query transformation and data augmentation.\na)Query Transformation :Query transformation can\nenhance the result of retrieval by modifying the input query.\nQuery2doc [168] and HyDE [169] use the original query to\ngenerate a pseudo document, which is later used as the query\nfor retrieval. The pseudo document contains richer relevant\ninformation, which helps to retrieve more accurate results.", "start_char_idx": 0, "end_char_idx": 589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1887afa-cd93-4a27-b62f-d7f1d01e26cb": {"__data__": {"id_": "c1887afa-cd93-4a27-b62f-d7f1d01e26cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "236dd4eb-04a5-45f2-84ca-1d849255c593", "node_type": "1", "metadata": {}, "hash": "88b8e3bdcfc87b9878aa403bc13c7999170b44f9a32c7180d2a27852c8f06d99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "037972d6-86da-41d6-85fd-6f03cf5d8c9a", "node_type": "1", "metadata": {}, "hash": "d830658810f3425ba1435f16d727e21d94d20d4957acd423a0b36ebc0b685672", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}}, "text": "The pseudo document contains richer relevant\ninformation, which helps to retrieve more accurate results.\nTOC [170] employs recursive retrieval augmented clarification\non ambiguous questions to construct a tree of disambiguated\nquestions, thereby generating comprehensive answers to these\nambiguous questions. During the construction process of this\ntree structure, TOC utilizes a self-verification pruning method\nto ensure the factual relevance of each node.\nb)Data Augmentation :Data augmentation improves\ndata before retrieval, including techniques such as removing ir-\nrelevant information, eliminating ambiguity, updating outdated\ndocuments, synthesize new data, etc.", "start_char_idx": 485, "end_char_idx": 1156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "037972d6-86da-41d6-85fd-6f03cf5d8c9a": {"__data__": {"id_": "037972d6-86da-41d6-85fd-6f03cf5d8c9a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1887afa-cd93-4a27-b62f-d7f1d01e26cb", "node_type": "1", "metadata": {}, "hash": "c69585fb915b622cc5e4cb4273bc2131e5e30cc2a978be334bff95c847346264", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "071b41de-61a2-4b47-8be3-d14fd05f5538", "node_type": "1", "metadata": {}, "hash": "46ffbf322df316c22fec8e4897f85b727381ef659acb96dbb7dd4ab3824be140", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}}, "text": "Make-An-Audio [44] uses captioning and audio-text re-\ntrieval to generate captions for language-free audio to mitigate\ndata sparsity, and adds random concept audio to improve the\noriginal audio. LESS [171] strategically selects an optimal\ndataset for downstream tasks by analyzing gradient informa-\ntion. It aims to maximize the dataset\u2019s impact on fine-tuning\nthe model\u2019s performance in response to instructional prompts.\nReACC [132] employs data augmentation (including renaming\nand dead code insertion) to pre-train the code retrieval model.", "start_char_idx": 1157, "end_char_idx": 1701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "071b41de-61a2-4b47-8be3-d14fd05f5538": {"__data__": {"id_": "071b41de-61a2-4b47-8be3-d14fd05f5538", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "037972d6-86da-41d6-85fd-6f03cf5d8c9a", "node_type": "1", "metadata": {}, "hash": "d830658810f3425ba1435f16d727e21d94d20d4957acd423a0b36ebc0b685672", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e656c762-fee1-489b-8bbf-5f3172ddcef9", "node_type": "1", "metadata": {}, "hash": "701b642bf94593756a8b62edfddb7cbad92bd19637fc9973dbc83231747cc9b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}}, "text": "2)Retriever Enhancement :The retrieval process is crucial\nin RAG systems. Generally, the better the retrieved content\nquality, the easier it is to stimulate the ability of LLMs in-\ncontext learning as well as other generators and paradigms.9\nFig. 4: Taxonomy of RAG Enhancements.\nThe worse the content quality, the more likely it is to cause\nmodel hallucinations. Therefore, in this section, we will\ndiscuss how to efficiently improve the effectiveness of the\nretrieval process.", "start_char_idx": 1702, "end_char_idx": 2180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e656c762-fee1-489b-8bbf-5f3172ddcef9": {"__data__": {"id_": "e656c762-fee1-489b-8bbf-5f3172ddcef9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "071b41de-61a2-4b47-8be3-d14fd05f5538", "node_type": "1", "metadata": {}, "hash": "46ffbf322df316c22fec8e4897f85b727381ef659acb96dbb7dd4ab3824be140", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fac41315-4d32-4359-b216-55c151e8b5e1", "node_type": "1", "metadata": {}, "hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "class_name": "RelatedNodeInfo"}}, "text": "a)Recursive Retrieval :Recursive retrieval is to perform\nmultiple searches to retrieve richer and higher-quality contents.\nReACT [172] uses Chain-of-Thought (CoT) [173] to break\nqueries down for recursive retrieval and provide richer in-\nformation.", "start_char_idx": 2181, "end_char_idx": 2429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90b32c62-1f04-44ba-b29a-0ca24bb90c12": {"__data__": {"id_": "90b32c62-1f04-44ba-b29a-0ca24bb90c12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "638f46aa-98ea-47c6-9083-3d5c75cc98b4", "node_type": "1", "metadata": {}, "hash": "47aa8341805eb8373995fbe9e89f3cf4f7b160423e5f87be8c6e8eee1b2e8464", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "638f46aa-98ea-47c6-9083-3d5c75cc98b4", "node_type": "1", "metadata": {}, "hash": "47aa8341805eb8373995fbe9e89f3cf4f7b160423e5f87be8c6e8eee1b2e8464", "class_name": "RelatedNodeInfo"}}, "text": "RATP [174] uses the Monte-Carlo Tree Search\n(MCTS) to perform multiple simulations, identifying optimal\nretrieval content. This content is then integrated into a template\nand sent to the generator for final production.", "start_char_idx": 0, "end_char_idx": 218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fec8fb2e-3c48-487b-b5b0-a4ab6a3a5aa6": {"__data__": {"id_": "fec8fb2e-3c48-487b-b5b0-a4ab6a3a5aa6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "427f2851-d74b-4810-b773-a43b53845b08", "node_type": "1", "metadata": {}, "hash": "e59a88b2df7394ede064070f936b7d0b7a21745d3e6955219538a6f3a2e41770", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}}, "text": "This content is then integrated into a template\nand sent to the generator for final production.\nb)Chunk Optimization :Chunk optimization refers to\nadjusting chunk size for improved retrieval results. Sentence-\nwindow retrieval [175] is an efficient approach that enhances\nretrieval by fetching small chunks of text and returning\na window of relevant sentences surrounding the retrieved\nsegment. This method ensures that the context before and\nafter the targeted sentence is included, providing a more\ncomprehensive understanding of the retrieved information.", "start_char_idx": 0, "end_char_idx": 558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "427f2851-d74b-4810-b773-a43b53845b08": {"__data__": {"id_": "427f2851-d74b-4810-b773-a43b53845b08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fec8fb2e-3c48-487b-b5b0-a4ab6a3a5aa6", "node_type": "1", "metadata": {}, "hash": "4ada32db1c609fff7297b58456fd595ec117bc332ab3278d018f99014580b905", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d005b6b4-fbc3-4538-b88d-ebcc6675bb46", "node_type": "1", "metadata": {}, "hash": "85f5b7efc4d50fde33857b29a94dba2885ea3b5e65a020e7b28e761a0c08a969", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}}, "text": "Auto-merge retrieval is another advanced RAG method of\nLlamaIndex [175] which organizes the document in a tree-\nlike structure, with the parent node containing the content of\nall children nodes. For example, articles and paragraphs, as\nwell as paragraphs and sentences, all follow a parent-child\nrelationship. In the retrieve process, fine-grained search for\nchildren nodes ultimately returns the parent node, effectively\nproviding richer information.", "start_char_idx": 559, "end_char_idx": 1010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d005b6b4-fbc3-4538-b88d-ebcc6675bb46": {"__data__": {"id_": "d005b6b4-fbc3-4538-b88d-ebcc6675bb46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "427f2851-d74b-4810-b773-a43b53845b08", "node_type": "1", "metadata": {}, "hash": "e59a88b2df7394ede064070f936b7d0b7a21745d3e6955219538a6f3a2e41770", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a816ebb-a509-478b-bd71-41524128c6bd", "node_type": "1", "metadata": {}, "hash": "80a70a850d81d0a81e6f5d559f93ba5e85b769d29b6148cae336b27f471e3bc0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}}, "text": "To address the lack of contextual\ninformation, RAPTOR [176] employs recursive embedding,\nclustering, and summarization of text chunks until further\nclustering becomes infeasible, thereby constructing a multi-\nlevel tree structure.\nc)Retriever Finetuning :As a core component in the\nRAG system, the retriever plays a crucial role in the entire\nsystem operation process.", "start_char_idx": 1011, "end_char_idx": 1379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a816ebb-a509-478b-bd71-41524128c6bd": {"__data__": {"id_": "6a816ebb-a509-478b-bd71-41524128c6bd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d005b6b4-fbc3-4538-b88d-ebcc6675bb46", "node_type": "1", "metadata": {}, "hash": "85f5b7efc4d50fde33857b29a94dba2885ea3b5e65a020e7b28e761a0c08a969", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57928373-8fa0-4601-abae-1b8a20f7afc4", "node_type": "1", "metadata": {}, "hash": "3270723f80fd1e1d103461e9c648111a8b3d3ac144e889a08c1a7f2613b3fe02", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}}, "text": "An effective embedding model can\ncluster semantically similar content in vector space, enhancing\nthe retriever\u2019s capability to provide valuable information for\nthe subsequent generator, thus boosting the RAG system\u2019s effi-\nciency. Hence, the proficiency of the embedding model [177]\u2013\n[180] is vital for the RAG system\u2019s overall effectiveness.\nIn addition, for embedding models that already have good\nexpression power, we can still finetune them using high-quality\ndomain data or task related data to improve their performance\nin specific domains or tasks.", "start_char_idx": 1380, "end_char_idx": 1935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57928373-8fa0-4601-abae-1b8a20f7afc4": {"__data__": {"id_": "57928373-8fa0-4601-abae-1b8a20f7afc4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a816ebb-a509-478b-bd71-41524128c6bd", "node_type": "1", "metadata": {}, "hash": "80a70a850d81d0a81e6f5d559f93ba5e85b769d29b6148cae336b27f471e3bc0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "980c530c-1005-4c72-8d3a-68472f1c9de1", "node_type": "1", "metadata": {}, "hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "class_name": "RelatedNodeInfo"}}, "text": "REPLUG [127] treats LM as ablack box and update the retriever model based on the final\nresults. APICoder [129] finetunes the retriever with python\nfiles and api names, signature, description. EDITSUM [138]\nfinetunes the retriever to decrease the jaccard distance between\nsummaries after retrieval.", "start_char_idx": 1936, "end_char_idx": 2233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b6bcd96-26f3-4e02-9ee0-d0b553dfbbbf": {"__data__": {"id_": "9b6bcd96-26f3-4e02-9ee0-d0b553dfbbbf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "521b5c8b-39fc-4b8e-bdc5-fc37e17b6a92", "node_type": "1", "metadata": {}, "hash": "c6ca82d92452988f1526b29e489164b5fe0b1ee107937d3690e8e326745b938e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}}, "text": "SYNCHROMESH [122] adds tree\ndistance os ASTs in the loss and uses Target Similarity\nTuning to finetune the Retriever. R-ConvED [48] finetunes the\nRetriever with the same data as generator. Kulkarni et al. [181]\napplied infoNCE loss to finetune the Retriever.\nd)Hybrid Retrieval :Hybrid retrieve denotes the concur-\nrent employment of a diverse array of retrieval methodologies\nor the extraction of information from multiple distinct sources.", "start_char_idx": 0, "end_char_idx": 441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "521b5c8b-39fc-4b8e-bdc5-fc37e17b6a92": {"__data__": {"id_": "521b5c8b-39fc-4b8e-bdc5-fc37e17b6a92", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b6bcd96-26f3-4e02-9ee0-d0b553dfbbbf", "node_type": "1", "metadata": {}, "hash": "2af18ff4c671d075245f18e5c9b1532583c60fcef2c5bc88a264ecec24b9dfe9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f400ddea-f8d1-45c4-b737-76842703e6f5", "node_type": "1", "metadata": {}, "hash": "a8aacd529316f6d96751924db0132e294491418431d06acd5796c913fa2b00ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}}, "text": "RAP-Gen [182] and ReACC [132] use both dense retriever\nand sparse retriever to improve the quality of retrieval. Ren-\ncos [121] uses sparse retriever to retrieve similar code snippets\non syntactic-level and usse dense retriever to retrieve similar\ncode snippets on semantic-level. BASHEXPLAINER [139]\nfirst uses dense retriever to capture semantic information\nand then uses sparse retriever to acquire lexical information.\nRetDream [50] first retrieves with text and then retrieves with\nthe image embedding.", "start_char_idx": 442, "end_char_idx": 949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f400ddea-f8d1-45c4-b737-76842703e6f5": {"__data__": {"id_": "f400ddea-f8d1-45c4-b737-76842703e6f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "521b5c8b-39fc-4b8e-bdc5-fc37e17b6a92", "node_type": "1", "metadata": {}, "hash": "c6ca82d92452988f1526b29e489164b5fe0b1ee107937d3690e8e326745b938e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19e0ec7f-0645-402b-a264-668c85a59adb", "node_type": "1", "metadata": {}, "hash": "ec5e1a59a5c0490d1d6132b3982d37746f29d33ba6199fd8e43079187254a3e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}}, "text": "RetDream [50] first retrieves with text and then retrieves with\nthe image embedding. CRAG [183] has designed a retrieval\nevaluator to assess the relevance of retrieved documents to the\ninput query, triggering three types of retrieval actions based on\nvarying confidence levels: if deemed correct, it directly uses\nthe retrieval results for Knowledge Refinement; if incorrect,\nit resorts to Web Search; and if ambiguous, it combines both\napproaches. To enhance performance in question-and-answer\ntasks, Huang et al.", "start_char_idx": 865, "end_char_idx": 1379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19e0ec7f-0645-402b-a264-668c85a59adb": {"__data__": {"id_": "19e0ec7f-0645-402b-a264-668c85a59adb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f400ddea-f8d1-45c4-b737-76842703e6f5", "node_type": "1", "metadata": {}, "hash": "a8aacd529316f6d96751924db0132e294491418431d06acd5796c913fa2b00ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7938e9c-de67-4c48-8b5b-475d32455dd5", "node_type": "1", "metadata": {}, "hash": "638b42097f590ff117dc31186b6ecd7b95790c779f92fd54ec42e6f887f3259c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}}, "text": "To enhance performance in question-and-answer\ntasks, Huang et al. [184] introduced two metrics, DKS(Dense\nKnowledge Similarity) and RAC(Retriever as Answer Clas-\nsifier), during the retrieval phase. These metrics account for\nboth the pertinence of the answers and the applicability of the\nunderlying knowledge. UniMS-RAG [185] introduces a novel\nkind of token, termed as the \u201cacting token\u201d, which determines\nthe source from which to retrieve information.", "start_char_idx": 1314, "end_char_idx": 1768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7938e9c-de67-4c48-8b5b-475d32455dd5": {"__data__": {"id_": "d7938e9c-de67-4c48-8b5b-475d32455dd5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19e0ec7f-0645-402b-a264-668c85a59adb", "node_type": "1", "metadata": {}, "hash": "ec5e1a59a5c0490d1d6132b3982d37746f29d33ba6199fd8e43079187254a3e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "node_type": "1", "metadata": {}, "hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "class_name": "RelatedNodeInfo"}}, "text": "e)Re-ranking :The Rerank technique refers to reorder-\ning the retrieved content in order to achieve greater diversity\nand better results. Re2G [186] applies a re-ranker [187]\nmodel after the traditional retriever.", "start_char_idx": 1769, "end_char_idx": 1982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1458b347-0390-4066-8436-60dd46fa3e53": {"__data__": {"id_": "1458b347-0390-4066-8436-60dd46fa3e53", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae376509-a845-4656-bc3b-84889c1c4046", "node_type": "1", "metadata": {}, "hash": "85e1ad273eb4ed542b68a830364a3b1906d1ed892b38cf58e1f72fbf4b58f6c6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}}, "text": "The effect of the re-\nranker model is to re-rank retrieved documents, the purpose\nof which is to reduce the impact of information loss caused\nby compressing text into vectors on the quality of retrieval.\nAceCoder [188] reranks the retrieved programs with a selector\nto reduce redundant programs and obtain diverse retrieved\nprograms. XRICL [189] uses a distillation-based exemplar10\nreranker after retrieval.", "start_char_idx": 0, "end_char_idx": 408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae376509-a845-4656-bc3b-84889c1c4046": {"__data__": {"id_": "ae376509-a845-4656-bc3b-84889c1c4046", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1458b347-0390-4066-8436-60dd46fa3e53", "node_type": "1", "metadata": {}, "hash": "6db050db3e164bae6937244b219837ed0bddb5fffa84978bae5f04ea687868a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb450b82-3b7e-48ac-b189-52e905a1d67b", "node_type": "1", "metadata": {}, "hash": "29d80a94794e082ad943c9d3c5bd83d27abae6d86a8c01a0a2c77b981dd7e14e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}}, "text": "Rangan [190] employs the Quantized\nInfluence Measure, assessing statistical biases between a query\nand a reference to evaluate the similarity of data subsets\nand rerank retrieval results. UDAPDR [191] uses LLMs to\ncost-effectively generate synthetic queries that train domain-\nspecific rerankers, which then apply multi-teacher knowledge\ndistillation to develop a cohesive retriever. LLM-R [192]\niteratively trains the retriever, using feedback from a frozen\nLLM to rank candidate documents and train the reward model.\nThe retriever is further trained based on knowledge distillation.", "start_char_idx": 409, "end_char_idx": 993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb450b82-3b7e-48ac-b189-52e905a1d67b": {"__data__": {"id_": "cb450b82-3b7e-48ac-b189-52e905a1d67b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae376509-a845-4656-bc3b-84889c1c4046", "node_type": "1", "metadata": {}, "hash": "85e1ad273eb4ed542b68a830364a3b1906d1ed892b38cf58e1f72fbf4b58f6c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a349c88d-5d16-4818-8c37-f9d948c59dd5", "node_type": "1", "metadata": {}, "hash": "25da6dbf51d8c0ce7397c9711bcb748d4c62ad9529a0e7886fd7f5e8228449f4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}}, "text": "The retriever is further trained based on knowledge distillation.\nEach iteration of training builds upon the retriever trained\nin the previous cycle, facilitating iterative optimization in\nsubsequent rounds.\nf)Retrieval Transformation :Retrieval Transformation\ninvolves rephrasing retrieved content to better activate the\ngenerator\u2019s potential, resulting in improved output.\nFILCO [193] effectively filters out irrelevant content from\nthe retrieved text chunk, leaving only the precise supporting\ncontent. This process simplifies the task for the generator,\nmaking it easier to predict the correct answer.", "start_char_idx": 928, "end_char_idx": 1533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a349c88d-5d16-4818-8c37-f9d948c59dd5": {"__data__": {"id_": "a349c88d-5d16-4818-8c37-f9d948c59dd5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb450b82-3b7e-48ac-b189-52e905a1d67b", "node_type": "1", "metadata": {}, "hash": "29d80a94794e082ad943c9d3c5bd83d27abae6d86a8c01a0a2c77b981dd7e14e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bbf6e82-0859-4e81-8ca0-06d8aee22e61", "node_type": "1", "metadata": {}, "hash": "c3d284cc380ace0060370c94024e17d7c7a9078913ecdfc006fcce3b9bdc8fca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}}, "text": "This process simplifies the task for the generator,\nmaking it easier to predict the correct answer. FiD-Light [194]\ninitially employs an encoder to convert the retrieved content\ninto a vector, which it then compresses, resulting in a sub-\nstantial reduction of latency time. RRR [195] integrates the\ncurrent query with the top-k document in each round through\na template, and subsequently restructures it via a pre-trained\nLLMs(GPT-3.5-Turbo etc.).", "start_char_idx": 1434, "end_char_idx": 1882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bbf6e82-0859-4e81-8ca0-06d8aee22e61": {"__data__": {"id_": "7bbf6e82-0859-4e81-8ca0-06d8aee22e61", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a349c88d-5d16-4818-8c37-f9d948c59dd5", "node_type": "1", "metadata": {}, "hash": "25da6dbf51d8c0ce7397c9711bcb748d4c62ad9529a0e7886fd7f5e8228449f4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6d7bfce5-d142-44e2-9d76-7001709526dd", "node_type": "1", "metadata": {}, "hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "class_name": "RelatedNodeInfo"}}, "text": "g)Others :In addition to the above optimization meth-\nods, there are also some other optimization methods for the\nretrieve process. For example, Meta-data filtering [196] is\na method to help processing retrieved documents which uses\nmetadata (such as time, purpose, etc.) to filter the retrieved\ndocuments for better results.", "start_char_idx": 1883, "end_char_idx": 2208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fac2605-63aa-4450-9179-db26c337f758": {"__data__": {"id_": "1fac2605-63aa-4450-9179-db26c337f758", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea7e8084-0269-4117-b8a3-23557924e586", "node_type": "1", "metadata": {}, "hash": "7b2173bf2212186a5c0d3803e5fd190e5b454559f43b736791cbb874ef3e0aa5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}}, "text": "to filter the retrieved\ndocuments for better results. GENREAD [197] and GRG [198]\nintroduce a novel approach where the retrieval process is\nsupplanted or improved by prompting a LLM to generate\ndocuments in response to a given question.\n3)Generator Enhancement :In RAG systems, the quality\nof the generator often determines the quality of the final output\nresults. Therefore, the ability of the generator determines the\nupper limit of the entire RAG system\u2019s effectiveness.", "start_char_idx": 0, "end_char_idx": 473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea7e8084-0269-4117-b8a3-23557924e586": {"__data__": {"id_": "ea7e8084-0269-4117-b8a3-23557924e586", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fac2605-63aa-4450-9179-db26c337f758", "node_type": "1", "metadata": {}, "hash": "e84cd8e449a606f23a605b6e2e16f038bde927c870a7a8aa8d100b7796609186", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fcedf03-b441-4c81-a2f1-d9291f04c901", "node_type": "1", "metadata": {}, "hash": "1cdca8bcce56f0f68666044fd67bbb27cc11ba1fb177979eb72b9c113644b107", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}}, "text": "a)Prompt Engineering :Technologies in prompt engi-\nneering [199] that focus on improving the quality of LLMs\u2019\noutput, such as Prompt compression, Stepback Prompt [200],\nActive Prompt [201], Chain of Thought Prompt [173], etc.,\nare all applicable to LLM generators in RAG systems. LLM-\nLingua [202] applies a small model to compresses the overall\nlength of the query to accelerate model inference, relieving\nthe negative impact of irrelevant information on the model\nand alleviating the phenomenon of \u201cLost in the Middle\u201d\n[155].", "start_char_idx": 474, "end_char_idx": 1001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fcedf03-b441-4c81-a2f1-d9291f04c901": {"__data__": {"id_": "5fcedf03-b441-4c81-a2f1-d9291f04c901", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea7e8084-0269-4117-b8a3-23557924e586", "node_type": "1", "metadata": {}, "hash": "7b2173bf2212186a5c0d3803e5fd190e5b454559f43b736791cbb874ef3e0aa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84264c77-f9e8-43b3-ba41-6723f6b6c82f", "node_type": "1", "metadata": {}, "hash": "19d103e403eb6ad162415ea2b0c51181f6f6f938d193e686c8ae61b068e3a2d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}}, "text": "ReMoDiffuse [51] decomposes complex descriptions\ninto anatomical text scripts by using ChatGPT. ASAP [203]\nadd exemplar tuples to the prompt for better results. An\nexemplar tuple is composed of the input code, a function\ndefinition, the results of analyzing that definition and its\nassociated comment. CEDAR [130] uses a designed prompt\ntemplate to organize code demonstration, query, and natural\nlanguage instructions into a prompt.", "start_char_idx": 1002, "end_char_idx": 1435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84264c77-f9e8-43b3-ba41-6723f6b6c82f": {"__data__": {"id_": "84264c77-f9e8-43b3-ba41-6723f6b6c82f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fcedf03-b441-4c81-a2f1-d9291f04c901", "node_type": "1", "metadata": {}, "hash": "1cdca8bcce56f0f68666044fd67bbb27cc11ba1fb177979eb72b9c113644b107", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f595dd6-042a-41e4-95d4-e25bb8d30bf6", "node_type": "1", "metadata": {}, "hash": "a455fcd397f5a60d073d0e2e58f8121bf9d3e66dd33e44cadcf5a58a5c46359d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}}, "text": "XRICL [189] utilizes\nCOT technology to add translation pairs as an intermediate\nstep in cross linguistic semantic parsing and inference. AC-TIVERAG [204] employs the Cognition Nexus mechanism to\ncalibrate the intrinsic cognition of LLMs and applies COT\nprompt in answer generation. Make-An-Audio [44] is able to\nuse other modalities as input which can provide much richer\ninformation for the following process.", "start_char_idx": 1436, "end_char_idx": 1846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f595dd6-042a-41e4-95d4-e25bb8d30bf6": {"__data__": {"id_": "3f595dd6-042a-41e4-95d4-e25bb8d30bf6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84264c77-f9e8-43b3-ba41-6723f6b6c82f", "node_type": "1", "metadata": {}, "hash": "19d103e403eb6ad162415ea2b0c51181f6f6f938d193e686c8ae61b068e3a2d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e288a398-5ad7-4fe0-9e03-11b716299770", "node_type": "1", "metadata": {}, "hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "class_name": "RelatedNodeInfo"}}, "text": "b)Decoding Tuning :Decoding tuning refers to adding\nadditional controls during the generator processing, which can\nbe achieved by adjusting hyperparameters to achieve greater\ndiversity, limiting the output vocabulary in some form, and so\non.\nInferFix [131] balances the diversity and quality of\nresults by adjusting the temperature in decoder.", "start_char_idx": 1847, "end_char_idx": 2190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58196cf7-31c3-45cb-b305-47780d51c9a2": {"__data__": {"id_": "58196cf7-31c3-45cb-b305-47780d51c9a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b833d9b9-cff4-4e06-9dba-20efb87d02d1", "node_type": "1", "metadata": {}, "hash": "86acf41015531bda8785a43dc2b5ced0fd4a0692d6ac5e7721c155949d42e2d9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b833d9b9-cff4-4e06-9dba-20efb87d02d1", "node_type": "1", "metadata": {}, "hash": "86acf41015531bda8785a43dc2b5ced0fd4a0692d6ac5e7721c155949d42e2d9", "class_name": "RelatedNodeInfo"}}, "text": "SYN-\nCHROMESH [122] limits the output vocabulary of the de-\ncoder by implementing a completion engine to eliminate\nimplementation errors.\nc)Generator Finetuning :The finetuning of the gener-\nator can enhance the model\u2019s ability to have more precise\ndomain knowledge or better fit with the retriever.\nRETRO [36] fixes the parameters of the retriever and\nuses the chunked cross attention mechanism in the gen-\nerator to combine the content of the query and retriever.", "start_char_idx": 0, "end_char_idx": 465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc1b0850-0803-4987-819c-9783a3f90423": {"__data__": {"id_": "fc1b0850-0803-4987-819c-9783a3f90423", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dafdee97-421f-4159-ad03-fe6c19162347", "node_type": "1", "metadata": {}, "hash": "467ce2cfea16d32515fbb933ab698db768319784ad6728a65da72c4b035c3614", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}}, "text": "APICoder [129] finetunes the generator CODEGEN-MONO\n350M [205] with a shuffled new file combined with API\ninformation and code blocks. CARE [160] first uses image\ndata, audio data and vedio-text pairs to train encoders and then\nfinetune the decoder (generator) with the target of decreas-\ning caption loss and concept detection loss together, during\nwhich the encoders and the retriever are frozen.", "start_char_idx": 0, "end_char_idx": 398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dafdee97-421f-4159-ad03-fe6c19162347": {"__data__": {"id_": "dafdee97-421f-4159-ad03-fe6c19162347", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc1b0850-0803-4987-819c-9783a3f90423", "node_type": "1", "metadata": {}, "hash": "52dcc586282e53f750e2d52165b05aa09ebfdcc29130c111e9d13d6da7efed97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0980800e-b33e-4cc1-8f7c-f0de094be475", "node_type": "1", "metadata": {}, "hash": "a39804cca309068ef58df145563ebb1e520161906152f46fd8c16f25e9b14dbc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}}, "text": "Animate-A-\nStory [206] optimizes the video generator with image data, and\nthen finetunes a LoRA [207] adapter to capture the appearance\ndetails of the given character. RetDream [50] finetunes a LoRA\nadapter [207] with the rendered images.\n4)Result Enhancement :In many scenarios, the result of\nRAG may not achieve the expected effect, and some tech-\nniques of Result Enhancement can help alleviate this problem.\na)Output Rewrite :Output Rewrite refers to rewriting\nthe content generated by the generator in certain scenarios to\nmeet the needs of downstream tasks.", "start_char_idx": 399, "end_char_idx": 962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0980800e-b33e-4cc1-8f7c-f0de094be475": {"__data__": {"id_": "0980800e-b33e-4cc1-8f7c-f0de094be475", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dafdee97-421f-4159-ad03-fe6c19162347", "node_type": "1", "metadata": {}, "hash": "467ce2cfea16d32515fbb933ab698db768319784ad6728a65da72c4b035c3614", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9732bb2b-0e9f-4259-9387-38c9d98380ce", "node_type": "1", "metadata": {}, "hash": "dd4eaeb8530d58c6052726e28c80215adfbfaf9adb87280195fee778be217929", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}}, "text": "SARGAM [208] refines outputs in code-related tasks by em-\nploying a special Transformer alongside Deletion, Placeholder,\nand Insertion Classifiers to better align with the real-world\ncode context. Ring [209] obtains diversity results by reranking\ncandidates based on the average of per token log probabilities\nproduced by the generator. CBR-KBQA [54] revises the result\nby aligning generated relations with those presented in the\nlocal neighborhood of the query entity in knowledge graph.", "start_char_idx": 963, "end_char_idx": 1451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9732bb2b-0e9f-4259-9387-38c9d98380ce": {"__data__": {"id_": "9732bb2b-0e9f-4259-9387-38c9d98380ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0980800e-b33e-4cc1-8f7c-f0de094be475", "node_type": "1", "metadata": {}, "hash": "a39804cca309068ef58df145563ebb1e520161906152f46fd8c16f25e9b14dbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70523c54-2fb7-42c6-ba94-f375b1199c89", "node_type": "1", "metadata": {}, "hash": "6104f65f0b7e32c1e344b3671c76d399384544d7ca92905d6fde656828695bb6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}}, "text": "5)RAG Pipeline Enhancement :RAG Pipeline Enhance-\nment refers to optimizing the processes of RAG at the system\nlevel in order to achieve better performance results.\na)Adaptive Retrieval :Some studies and practices on\nRAG have shown that retrieval is not always beneficial for\nthe final generated results When the parameterized knowledge\nof the model itself is sufficient to answer relevant questions,\nexcessive retrieval will cause resource waste and may increase\nthe model\u2019s confusion.", "start_char_idx": 1452, "end_char_idx": 1938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70523c54-2fb7-42c6-ba94-f375b1199c89": {"__data__": {"id_": "70523c54-2fb7-42c6-ba94-f375b1199c89", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9732bb2b-0e9f-4259-9387-38c9d98380ce", "node_type": "1", "metadata": {}, "hash": "dd4eaeb8530d58c6052726e28c80215adfbfaf9adb87280195fee778be217929", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81", "node_type": "1", "metadata": {}, "hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, in this chapter, we will\ndiscuss two types of methods for determining whether to\nretrieve, named rule-based and model-based methods.11\nRule-based: FLARE [210] actively decides whether and\nwhen to search through the probability in the generation pro-\ncess.", "start_char_idx": 1939, "end_char_idx": 2205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "925a1d28-1f61-4d13-94eb-e10d7f7f9962": {"__data__": {"id_": "925a1d28-1f61-4d13-94eb-e10d7f7f9962", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8e0d74c-77c8-4bd1-b552-ef3c563f3ed8", "node_type": "1", "metadata": {}, "hash": "d331139405b2b3aa94378e91d12e884de0c7bd59668c3b58f4c4037fd6e98ae0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}}, "text": "Efficient-KNNLM [38] combines the generation proba-\nbility of KNN-LM [37] and NPM [162] with a hyperparameter\n\u03bbto determine the proportion of generation and retrieval.\nMallen et al. [211] used statistical analysis on questions to\nenable direct answers for high-frequency ones and applied\nRAG for low-frequency ones. Jiang et al. [212] studied Model\nUncertainty, Input Uncertainty, and Input Statistics to compre-\nhensively assess the confidence level of the model.", "start_char_idx": 0, "end_char_idx": 464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8e0d74c-77c8-4bd1-b552-ef3c563f3ed8": {"__data__": {"id_": "b8e0d74c-77c8-4bd1-b552-ef3c563f3ed8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "925a1d28-1f61-4d13-94eb-e10d7f7f9962", "node_type": "1", "metadata": {}, "hash": "d52b32725149599eed9c76892dd18d75f2eb6828ef8aaf04800546d3aef20934", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae23095a-71b8-4464-861a-106ac4195c21", "node_type": "1", "metadata": {}, "hash": "4046189438dd0b65201626d4c183cf413ec4c2243c69c03f6ff4db08913e77ae", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}}, "text": "Ultimately,\nbased on the confidence level of the model, a decision is made\nwhether to retrieve. Kandpal et al. [213] studied the correlation\nbetween the number of relevant documents and the model\u2019s\nknowledge mastery to assess the need for retrieval.\nModel-based: Self-RAG [126] uses a trained generator to\ndetermine whether to perform a retrieval based on the retrieve\ntoken under different instructions, and evaluates the relevance\nand level of support of the retrieved text through the Self-\nReflection token. Finally, the quality of the final output result\nis evaluated based on the Critique token.", "start_char_idx": 465, "end_char_idx": 1066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae23095a-71b8-4464-861a-106ac4195c21": {"__data__": {"id_": "ae23095a-71b8-4464-861a-106ac4195c21", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8e0d74c-77c8-4bd1-b552-ef3c563f3ed8", "node_type": "1", "metadata": {}, "hash": "d331139405b2b3aa94378e91d12e884de0c7bd59668c3b58f4c4037fd6e98ae0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23e2260e-43be-456f-912d-25e99d0379cb", "node_type": "1", "metadata": {}, "hash": "6c8ece06ce7d893fb8d466ca1e06e05e2885d9cce414d8d0a520a418c378c272", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}}, "text": "Finally, the quality of the final output result\nis evaluated based on the Critique token. Ren et al. [214]\nused \u201dJudgment Prompting\u201d to determine whether LLMs can\nanswer relevant questions and whether their answers are\ncorrect or not, thereby assisting in determining the necessity of\na retrieval. SKR [215] uses the ability of LLMs themselves to\njudge in advance whether they can answer the question, and\nif they can answer, no retrieval is performed.", "start_char_idx": 977, "end_char_idx": 1429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23e2260e-43be-456f-912d-25e99d0379cb": {"__data__": {"id_": "23e2260e-43be-456f-912d-25e99d0379cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae23095a-71b8-4464-861a-106ac4195c21", "node_type": "1", "metadata": {}, "hash": "4046189438dd0b65201626d4c183cf413ec4c2243c69c03f6ff4db08913e77ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f14e060e-bf7d-4077-af80-b992fe7858d5", "node_type": "1", "metadata": {}, "hash": "eb8d287dde1768a41557294123f9b85b41ae13b7c91f2a091df7b33e9be9cb7c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}}, "text": "Rowen [216]\nemploys a model as a sophisticated multilingual detection\nsystem to evaluate the semantic coherence of answers to\nidentical questions posed across various languages. In the\nevent of detected inconsistencies, it decides to retrieve external\ninformation, thereby enhancing the reasoning process and\nrectifying inaccuracies. Conversely, when responses exhibit\nconsistency, the system upholds the initially generated answer,\nwhich is derived from internal reasoning. AdaptiveRAG [217]\ndynamically decides whether to retrieve based on the query\ncomplexity by a classifier, which is a smaller LM.", "start_char_idx": 1430, "end_char_idx": 2032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f14e060e-bf7d-4077-af80-b992fe7858d5": {"__data__": {"id_": "f14e060e-bf7d-4077-af80-b992fe7858d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23e2260e-43be-456f-912d-25e99d0379cb", "node_type": "1", "metadata": {}, "hash": "6c8ece06ce7d893fb8d466ca1e06e05e2885d9cce414d8d0a520a418c378c272", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5665018f-a756-44be-adc2-ce77d463ad0e", "node_type": "1", "metadata": {}, "hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "class_name": "RelatedNodeInfo"}}, "text": "b)Iterative RAG :Iterative RAG progressively refines\nresults by repeatedly cycling through retrieval and generation\nphases, rather than a single round.", "start_char_idx": 2033, "end_char_idx": 2184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33406e86-99a8-48de-89db-58f1a36281d2": {"__data__": {"id_": "33406e86-99a8-48de-89db-58f1a36281d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4e3f0d5-3d58-4b4a-aefe-64c3b61670ac", "node_type": "1", "metadata": {}, "hash": "20ffce0a9e3e563045a7d3dd94c07e8e0a3941022a3742d6cdaea30cd5709511", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}}, "text": "RepoCoder [218] employs an iterative retrieval-generation\npipeline for code completion tasks, enhancing each retrieval\nquery with code generated in prior iterations to more ef-\nfectively leverage information dispersed across various files,\nthereby achieving superior results. ITER-RETGEN [219] syn-\nergizes retrieval and generation in an iterative manner. The\ncurrent output of the generator can to some extent reflect\nthe knowledge it still lacks, and the retrieve can retrieve the\nmissing information as contextual information for the next\nround, which helps to improve the quality of the generated\ncontent in the next round.", "start_char_idx": 0, "end_char_idx": 627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4e3f0d5-3d58-4b4a-aefe-64c3b61670ac": {"__data__": {"id_": "a4e3f0d5-3d58-4b4a-aefe-64c3b61670ac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33406e86-99a8-48de-89db-58f1a36281d2", "node_type": "1", "metadata": {}, "hash": "36d9c719a9e79b3429e76ec2a0d8446b95d959e22e7b613ba4c6f2234fba3f75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c090fe5-243a-41c0-8129-aae9e995378d", "node_type": "1", "metadata": {}, "hash": "95b544fb71173001db2cef997e59cd39d831c936b7e0e5c1bc86f4c93d491941", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}}, "text": "SelfMemory [220] employs a\nretrieval-augmented generator in an iterative manner to create\nan unlimited memory pool. Following this, a memory selector\nis used to choose one output, which then serves as the\nmemory for the subsequent generation round. RAT [221] initial\ngenerates content by an LLM with a zero-shot CoT prompt,\nthen revises each thought step by retrieving knowledge from\nexternal knowledge base.IV.APPLICATIONS\nIn this section, we focus on RAG applications spanning\nvarious modalities.", "start_char_idx": 628, "end_char_idx": 1126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c090fe5-243a-41c0-8129-aae9e995378d": {"__data__": {"id_": "3c090fe5-243a-41c0-8129-aae9e995378d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4e3f0d5-3d58-4b4a-aefe-64c3b61670ac", "node_type": "1", "metadata": {}, "hash": "20ffce0a9e3e563045a7d3dd94c07e8e0a3941022a3742d6cdaea30cd5709511", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3984af1d-e971-46af-8596-695647deb311", "node_type": "1", "metadata": {}, "hash": "813624b36a4b9f9d5ca140283c54afe415e12d6788b37cfd343417e9702315a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}}, "text": "To echo with the taxonomy of RAG\nfoundations and enhancements, we also demonstrate their\nutilization across different tasks in Table I.\nA.RAG for Text\nTo begin with, text generation is among the most important\nand widely deployed applications for RAG. Here we introduce\npopular works for seven tasks, respectively.\n1)Question Answering :Question Answering involves the\nprocess of providing responses to posed questions by drawing\nfrom a vast and comprehensive collection of textual sources.", "start_char_idx": 1127, "end_char_idx": 1617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3984af1d-e971-46af-8596-695647deb311": {"__data__": {"id_": "3984af1d-e971-46af-8596-695647deb311", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c090fe5-243a-41c0-8129-aae9e995378d", "node_type": "1", "metadata": {}, "hash": "95b544fb71173001db2cef997e59cd39d831c936b7e0e5c1bc86f4c93d491941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0abdbd95-fcbf-4913-89d8-f527efd2400e", "node_type": "1", "metadata": {}, "hash": "8a4500445f9cd24183393f9bc6371095490678d0e7fff6432a5052e2eb591e8e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}}, "text": "FiD [35] and REALM [33] identify the top-k most pertinent\narticle snippets based on the query and forward each snippet\nalong with the question to LLMs to generate k responses.\nThese responses are then synthesized into a final answer.\nToutanova et al. [222] substituted the text corpus in REALM\nwith subgraphs from a knowledge graph, yielding impressive\nresults. As shown in Fig. 5, RETRO [36] employs attention\nmechanisms to integrate the question with relevant retrieved\ndocuments within the model to produce the final answer.", "start_char_idx": 1618, "end_char_idx": 2145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0abdbd95-fcbf-4913-89d8-f527efd2400e": {"__data__": {"id_": "0abdbd95-fcbf-4913-89d8-f527efd2400e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3984af1d-e971-46af-8596-695647deb311", "node_type": "1", "metadata": {}, "hash": "813624b36a4b9f9d5ca140283c54afe415e12d6788b37cfd343417e9702315a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd", "node_type": "1", "metadata": {}, "hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "class_name": "RelatedNodeInfo"}}, "text": "SKR [215] observes that using RAG does not invariably\nbenefit Question Answering and thus explored guiding the\nmodel to evaluate its grasp of pertinent knowledge, subse-\nquently adapting its use of external resources for retrieval\nenhancement.", "start_char_idx": 2146, "end_char_idx": 2389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e6fced2-1c4c-44c4-8d7c-2f6fdb25a3a0": {"__data__": {"id_": "6e6fced2-1c4c-44c4-8d7c-2f6fdb25a3a0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9f5e60a-8be2-4228-8bfa-5d35ca444603", "node_type": "1", "metadata": {}, "hash": "504dcbdb20a5665898226c9c99137af0d19f8890826960d9803bc9ba1363b173", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}}, "text": "TOG [223] introduces an innovative knowledge\ngraph-augmented LLM framework, which excels by fostering\ninteractions between LLMs and the Knowledge Graph and\nby expanding the inference path space with beam search.\nNPM [162] pioneers the use of nonparametric data distribu-\ntions in lieu of the softmax layer, enabling models with fewer\nparameters to perform effectively. Self-RAG [126] improves\nanswer quality by learning to discern when to retrieve, as-\nsess the retrieved content\u2019s relevance, and evaluate the final\ngenerated results using four types of reflective tokens.", "start_char_idx": 0, "end_char_idx": 572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9f5e60a-8be2-4228-8bfa-5d35ca444603": {"__data__": {"id_": "f9f5e60a-8be2-4228-8bfa-5d35ca444603", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e6fced2-1c4c-44c4-8d7c-2f6fdb25a3a0", "node_type": "1", "metadata": {}, "hash": "1cd39309c519ca0bb144895b03e1a94d3ebd40904ed4fbf72e830e0d774387df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f04f21a-584f-4e6c-9df3-42e9a2f38966", "node_type": "1", "metadata": {}, "hash": "c0793e2ea7ea8fbc6a54769f1fc5a656745c62a2f75d8610db2cf81d8d69d9e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}}, "text": "CL-\nReLKT [224] employs a language-generalized encoder to\nbridge the gap between question-document pairs across lan-\nguages, thus better leveraging multilingual data. CORE [225]\nmitigates language resource disparities by introducing a novel\ndense passage retrieval algorithm and a multilingual autore-\ngressive generation model. Lastly, EAE [156] enhances answer\nquality by retrieving entity embeddings for query entities and\nintegrating these with hidden states for further processing. UR-\nC C A F FW T r a ns f ormer \nEnc o der \nR etrie v al \nd ataset", "start_char_idx": 573, "end_char_idx": 1126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f04f21a-584f-4e6c-9df3-42e9a2f38966": {"__data__": {"id_": "9f04f21a-584f-4e6c-9df3-42e9a2f38966", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9f5e60a-8be2-4228-8bfa-5d35ca444603", "node_type": "1", "metadata": {}, "hash": "504dcbdb20a5665898226c9c99137af0d19f8890826960d9803bc9ba1363b173", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f37a12ea-f7e2-4d3b-8761-660a17587794", "node_type": "1", "metadata": {}, "hash": "043521c99d65eafb211c8d0f1c6a14d092a9fe335c06bd269e35a2e14504f6e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}}, "text": "r a ns f ormer \nEnc o der \nR etrie v al \nd ataset \nFrozen kNN Retriever \nKV\nRETR O b lo c k ( x L ) N eig hb o ur s \nIn p ut \nt o k ens Ch unk ed cr oss-att en tion ( C C A ) \nB ER T B ER T \nCondition \nA tt ending c h unk s Enc o ded neig hb o ur s \nC A \nC A \nA T T N Q \nEMB REA D A tt end Enc o ded neig hb o ur s", "start_char_idx": 1077, "end_char_idx": 1391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f37a12ea-f7e2-4d3b-8761-660a17587794": {"__data__": {"id_": "f37a12ea-f7e2-4d3b-8761-660a17587794", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f04f21a-584f-4e6c-9df3-42e9a2f38966", "node_type": "1", "metadata": {}, "hash": "c0793e2ea7ea8fbc6a54769f1fc5a656745c62a2f75d8610db2cf81d8d69d9e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99734d75-ed29-4007-a8a8-dc4f79d9e8b6", "node_type": "1", "metadata": {}, "hash": "ce7095d7b065f243de29d6e4b7a1ab794598f777056463ae1af47a9f761c9e97", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}}, "text": "N Q \nEMB REA D A tt end Enc o ded neig hb o ur s \nC1 \nC2 \nC3 H1 \nH2 \nH3 \nHH1+ \nH2+ E1E2E1\nE2\nCA (H1+, E1) \nCA (H2+, E2) \nCCA (H, E) \nX\nFig. 5: Architecture of RETRO [36] model.", "start_char_idx": 1343, "end_char_idx": 1519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99734d75-ed29-4007-a8a8-dc4f79d9e8b6": {"__data__": {"id_": "99734d75-ed29-4007-a8a8-dc4f79d9e8b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f37a12ea-f7e2-4d3b-8761-660a17587794", "node_type": "1", "metadata": {}, "hash": "043521c99d65eafb211c8d0f1c6a14d092a9fe335c06bd269e35a2e14504f6e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "51582689-3bf2-4ba0-b051-10e732354bda", "node_type": "1", "metadata": {}, "hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "class_name": "RelatedNodeInfo"}}, "text": "E) \nX\nFig. 5: Architecture of RETRO [36] model.\nQA [226] found that when encountering unseen problems,\nretrieving QA pairs has a better final effect; When encoun-\ntering problems that have not been seen before, the retrieve12\nTABLE I: Taxonomy of RAG applications across various modalities.", "start_char_idx": 1472, "end_char_idx": 1762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab870226-7a14-42b6-9fee-e893c73c5cd4": {"__data__": {"id_": "ab870226-7a14-42b6-9fee-e893c73c5cd4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90c41dad-e5cd-4732-9250-448c021e4383", "node_type": "1", "metadata": {}, "hash": "b67465ae3f47b8bf0c6d2bca74caac653135d04ec00f72af68ba3ddb7e4c3336", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}}, "text": "RAG for Text\nQuestion Answering Human-Machine Conversation Neural Machine Translation Summarization Others\nREALM\u2021\u00a7TKEGEN\u00a7TOG\u2021\nSKR\u00a7\u00b6Self-RAG\u00a7\u00b6RIAG\u2021\nFiD\u2021\u00a7RETRO\u00a7NPM\u2021\u00a7CREA-ICL\u2020\u2021BlenderBot3\u2021\u00a7\nCEG\u2021\u2225Internet", "start_char_idx": 0, "end_char_idx": 200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90c41dad-e5cd-4732-9250-448c021e4383": {"__data__": {"id_": "90c41dad-e5cd-4732-9250-448c021e4383", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab870226-7a14-42b6-9fee-e893c73c5cd4", "node_type": "1", "metadata": {}, "hash": "78c170cea536e40744c67d3b9109c2452b044dff94ef0b21a9def3ab450354a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15ad2d42-ba2f-4e70-9b47-27a0ddbd9339", "node_type": "1", "metadata": {}, "hash": "b55021b3e46f6715b5b90741fad6a2b7215d07eeb6ad9db40a49c0d2723be1a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}}, "text": "3\u2021\u00a7\nCEG\u2021\u2225Internet-Augmented-DG\u2021\u00a7\nConceptFlow\u2021\u00a7Skeleton-to-Response\u2021\u00a7NMT-with-Monolingual-TM\u2020\u2021\u00a7\nTRIME\u2021\u00a7KNN-MT\u2021\u00a7COG\u2021RAMK", "start_char_idx": 183, "end_char_idx": 301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15ad2d42-ba2f-4e70-9b47-27a0ddbd9339": {"__data__": {"id_": "15ad2d42-ba2f-4e70-9b47-27a0ddbd9339", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90c41dad-e5cd-4732-9250-448c021e4383", "node_type": "1", "metadata": {}, "hash": "b67465ae3f47b8bf0c6d2bca74caac653135d04ec00f72af68ba3ddb7e4c3336", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47cdba86-01b2-4134-8ce2-89a9a558fc6f", "node_type": "1", "metadata": {}, "hash": "2c759c4e1d4bbf1f31cba44d9dfb074ac0aea35af15e734dd637449c7d92d5aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}}, "text": "\u00a7KNN-MT\u2021\u00a7COG\u2021RAMKG\u2021\u00a7RPRR\u2021RIGHT\u2021\u00a7\nUnlimiformer\u00a7CONCRETE\u2021\u00a7Atlas\u2021\u00a7\nKG-BART\u2021\u00a7R-GQA\u2021\u00a7\nRAG for Code\nCode Generation Code Summarization Code CompletionAutomatic\nProgram RepairText-to-SQL and Code\n-based Semantic ParsingOth", "start_char_idx": 284, "end_char_idx": 499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47cdba86-01b2-4134-8ce2-89a9a558fc6f": {"__data__": {"id_": "47cdba86-01b2-4134-8ce2-89a9a558fc6f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15ad2d42-ba2f-4e70-9b47-27a0ddbd9339", "node_type": "1", "metadata": {}, "hash": "b55021b3e46f6715b5b90741fad6a2b7215d07eeb6ad9db40a49c0d2723be1a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "803a9f00-6ef8-488e-b428-cda0027f4d4f", "node_type": "1", "metadata": {}, "hash": "df59a9100bbecd3559fd871a13502bbc4f7ff2933247da740fa2d2e10942fedf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}}, "text": "and Code\n-based Semantic ParsingOthers\nSKCODER\u00a7RRGCode\u2021\nARKS\u2020\u00b6RECODE\nKNN-TRANX\u2225Toolcoder\u00a7\u2225RACE\u2020BASHEXPLAINER\u2021\nREADSUM\u2225Rencos\u2021\nCoRec\u2021Tram\u00a7\nED", "start_char_idx": 464, "end_char_idx": 604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "803a9f00-6ef8-488e-b428-cda0027f4d4f": {"__data__": {"id_": "803a9f00-6ef8-488e-b428-cda0027f4d4f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47cdba86-01b2-4134-8ce2-89a9a558fc6f", "node_type": "1", "metadata": {}, "hash": "2c759c4e1d4bbf1f31cba44d9dfb074ac0aea35af15e734dd637449c7d92d5aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a7c4090-0eb8-406d-97aa-727111851701", "node_type": "1", "metadata": {}, "hash": "b8282c2ad2503a6110ab799e5ca9f170c077d49ebd84e55d2f696c5eefad8916", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}}, "text": "os\u2021\nCoRec\u2021Tram\u00a7\nEDITSUM\u2021ReACC\u2020\u2021RepoCoder\u2020\u00a7\u00b6\nDe-Hallucinator\u00b6REPOFUSE\u00a7\nRepoFusion\u00a7EDITAS\u00a7RING\u2225CEDAR\u00a7\nRAP-Gen\u2021\u00a7InferFix\u00a7\nSAR", "start_char_idx": 586, "end_char_idx": 708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a7c4090-0eb8-406d-97aa-727111851701": {"__data__": {"id_": "3a7c4090-0eb8-406d-97aa-727111851701", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "803a9f00-6ef8-488e-b428-cda0027f4d4f", "node_type": "1", "metadata": {}, "hash": "df59a9100bbecd3559fd871a13502bbc4f7ff2933247da740fa2d2e10942fedf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ae6e793-db56-4b3f-a68e-609335a5b269", "node_type": "1", "metadata": {}, "hash": "4cbbdb588e4bc63d20bacfa8b9f4b2e14b1c191853f00a6f282c33a9906b8050", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}}, "text": "-Gen\u2021\u00a7InferFix\u00a7\nSARGAM\u00a7RTLFixer\u2021\u00a7XRICL\u2021\u00a7SYNCHROMESH\u2021\u00a7\nRESDSQL\u00a7REFSQL\u2021\u00a7\nCodeICL\u00a7MURRE\u2225\u00b6De-fine\u2021\u2225Code4UIE\u00a7\nE&V", "start_char_idx": 689, "end_char_idx": 797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ae6e793-db56-4b3f-a68e-609335a5b269": {"__data__": {"id_": "2ae6e793-db56-4b3f-a68e-609335a5b269", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a7c4090-0eb8-406d-97aa-727111851701", "node_type": "1", "metadata": {}, "hash": "b8282c2ad2503a6110ab799e5ca9f170c077d49ebd84e55d2f696c5eefad8916", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cdd04060-a8e4-474c-a338-f65f89616e95", "node_type": "1", "metadata": {}, "hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "class_name": "RelatedNodeInfo"}}, "text": "ine\u2021\u2225Code4UIE\u00a7\nE&V StackSpotAI\u2021\u00a7\nImputBlaster\u00b6\nRAG for Knowledge RAG for 3D\nKnowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others", "start_char_idx": 779, "end_char_idx": 927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df0e1575-9ac0-4490-b8af-d43a8fa606ef": {"__data__": {"id_": "df0e1575-9ac0-4490-b8af-d43a8fa606ef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1409477-4e3c-4538-96b0-ad47279e810e", "node_type": "1", "metadata": {}, "hash": "43254f2cf24047a828eaa2bb62bc3f66ac4bfe60859c81894dacc2d90e40195d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}}, "text": "for 3D\nKnowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1409477-4e3c-4538-96b0-ad47279e810e": {"__data__": {"id_": "a1409477-4e3c-4538-96b0-ad47279e810e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df0e1575-9ac0-4490-b8af-d43a8fa606ef", "node_type": "1", "metadata": {}, "hash": "7716787fa54deb6c06c7d2a449a5b31aa8e5d0fbb5f83f9e61a9d25686773564", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3d3599d-8950-4295-8694-79a0ddfd1125", "node_type": "1", "metadata": {}, "hash": "163c8055f315b311ed6fc8826b9f1e630fd0e4ec87eef7dcbc2f13e736e84693", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}}, "text": "for 3D\nKnowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others Text-to-3D\nCBR-KBQA\u2021\u00a7\u2225TIARA\u2020\u2021\u00a7Keqing\u2020\u2021\u00a7\nRNG-KBQA\u2021\u2225ReTraCk\u00a7SKP\u2020\u2021\u00a7UniK-QA\u2020\u2021KG-FiD\u2021GRAPE\u2021\nSKURG\u2020\u2021KnowledGPT\u2021EFSUM\u00a7EfficientQA\u2021CORE\u00a7Convinse\u2020\u2021\nRINK\u2021\u00a7T-RAG\u2021\u00a7StructGPT\u2021GRetriever\u00a7SURGE\u00a7\nK-LaMP", "start_char_idx": 0, "end_char_idx": 266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3d3599d-8950-4295-8694-79a0ddfd1125": {"__data__": {"id_": "f3d3599d-8950-4295-8694-79a0ddfd1125", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1409477-4e3c-4538-96b0-ad47279e810e", "node_type": "1", "metadata": {}, "hash": "43254f2cf24047a828eaa2bb62bc3f66ac4bfe60859c81894dacc2d90e40195d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83ae85ec-a12a-4af1-8b75-888ba8a4b1a0", "node_type": "1", "metadata": {}, "hash": "339b3412669c8da9f7837d1486cb78c4dc2a3fef96f753fa62d8cfc9a67d09bf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}}, "text": "RHO\u2225ReMoDiffuse\u2020\u2021\nAMD\u2020\nRAG for Image RAG for Video\nImage Generation Image Captioning Others Video Captioning Video QA & Dialogue Others\nRetrieveGAN\u2021IC-GAN\u00a7Re-imagen\u00a7\nRDM", "start_char_idx": 267, "end_char_idx": 436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83ae85ec-a12a-4af1-8b75-888ba8a4b1a0": {"__data__": {"id_": "83ae85ec-a12a-4af1-8b75-888ba8a4b1a0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3d3599d-8950-4295-8694-79a0ddfd1125", "node_type": "1", "metadata": {}, "hash": "163c8055f315b311ed6fc8826b9f1e630fd0e4ec87eef7dcbc2f13e736e84693", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "180aee7f-e742-4903-bab5-cfce033587e9", "node_type": "1", "metadata": {}, "hash": "2a7534500517c93f62de8b2a9f286b9f1d1058af6de3e12d2684a169447aaf39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}}, "text": "QA & Dialogue Others\nRetrieveGAN\u2021IC-GAN\u00a7Re-imagen\u00a7\nRDM Retrieve&Fuse\u00a7KNN-DiffusionMA\u2225REVEAL\u2021SMALLCAP\u2020\nCRSR\u2020RA-TransformerPICa\u2225Maira\u2021\nKIF\u2021RA-VQA\u2021KaVD\u2021\u00a7R-ConvED\u2021\u00a7\nCARE\u00a7\nEgoInstructor\u2020\u2021\u00a7MA-DRNN\u2020\u2021R2A\u2021\nTvqa+\u00a7VGNMN\u2021VidIL\u2020\u2021RAG-Driver\u2021\nAnimate-A-Story\u2020\u00a7\nRAG", "start_char_idx": 382, "end_char_idx": 631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "180aee7f-e742-4903-bab5-cfce033587e9": {"__data__": {"id_": "180aee7f-e742-4903-bab5-cfce033587e9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83ae85ec-a12a-4af1-8b75-888ba8a4b1a0", "node_type": "1", "metadata": {}, "hash": "339b3412669c8da9f7837d1486cb78c4dc2a3fef96f753fa62d8cfc9a67d09bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51619bcb-760b-4af8-8b26-ad2d6635661d", "node_type": "1", "metadata": {}, "hash": "15683b26837fdf389ee09e1d95478661de799df79ec01184a2e3e4a963084234", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}}, "text": "for Science RAG for Audio\nDrug Discovery Biomedical Informatics Enhancement Math Applications Audio Generation Audio Captioning\nRetMol\u2020\u00a7PromptDiff\u2020\u2021PoET\u2021Chat-Orthopedist\u2020\u00a7BIOREADER\u2020MedWriter\u2021QARAG\u2020\u2021LeanDojo\u2021RAG-for-math-QA\u2020\u2021Re-AudioLDM\u00a7Make-An-Audio\u2020\u00a7RECAP\u2021\u00a7\nQuery-based Latent-based Logit-based Speculative\n Query+Latent\n Latent+Logit \u2020Input \u2021Retriever \u00a7Generator \u2225Output", "start_char_idx": 632, "end_char_idx": 1004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51619bcb-760b-4af8-8b26-ad2d6635661d": {"__data__": {"id_": "51619bcb-760b-4af8-8b26-ad2d6635661d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "180aee7f-e742-4903-bab5-cfce033587e9", "node_type": "1", "metadata": {}, "hash": "2a7534500517c93f62de8b2a9f286b9f1d1058af6de3e12d2684a169447aaf39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69604801-7765-4976-81e7-231241d2b2b1", "node_type": "1", "metadata": {}, "hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "class_name": "RelatedNodeInfo"}}, "text": "Latent+Logit \u2020Input \u2021Retriever \u00a7Generator \u2225Output \u00b6Pipeline\ntext chunk performs better. Therefore, it is proposed to simul-\ntaneously retrieve QA pairs and text chunks, and select the\nfinal answer by comparing the calibrated confidences.", "start_char_idx": 955, "end_char_idx": 1192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31f544b8-fc3e-4aab-aa56-68a1f20f7e14": {"__data__": {"id_": "31f544b8-fc3e-4aab-aa56-68a1f20f7e14", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1047af7a-6186-49ad-a0f0-9b3504bc95d4", "node_type": "1", "metadata": {}, "hash": "03ac4bdba0fccb257111d37163e87ab7e6738774bd3d9d68ee4000f7b4c9293f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}}, "text": "DISC-\nLawLLM [227] constructs a supervised fine-tuning dataset\nthrough a legal syllogism prompting strategy, enabling the\nmodel to receive support from the latest legal information.\nRAG-end2end [228] conducts simultaneous training of the\nretriever (DPR) and the generator (BART) to optimize per-\nformance for the end-to-end question-answering task and to\nfacilitate domain adaptation.", "start_char_idx": 0, "end_char_idx": 384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1047af7a-6186-49ad-a0f0-9b3504bc95d4": {"__data__": {"id_": "1047af7a-6186-49ad-a0f0-9b3504bc95d4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31f544b8-fc3e-4aab-aa56-68a1f20f7e14", "node_type": "1", "metadata": {}, "hash": "5ac33e121490e6ec757c4d927dd6662d4f814884a7df62c28d1831e088aae63b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbfb8209-388f-4094-a987-e33a058a0252", "node_type": "1", "metadata": {}, "hash": "7580ff928a48ce4f189bfcede1f0d1ef844838f4366a06d5cc23f477931b7b9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}}, "text": "MultiHop-RAG [229] is designed\nto extract pertinent information from a variety of distinct\ndocuments, aggregating this knowledge to equip the generator\nwith the necessary context for producing the definitive answer\nto the query.\n2)Fact Verification :Fact Verification involves assessing\nthe veracity of information, a critical function in disciplines\nsuch as Natural Language Processing (NLP), Information Re-\ntrieval, and Data Mining. In today\u2019s digital era, characterized\nby an exponential increase in data, particularly across social\nmedia and online news platforms, there is a rapid proliferation\nof unchecked information.", "start_char_idx": 385, "end_char_idx": 1011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbfb8209-388f-4094-a987-e33a058a0252": {"__data__": {"id_": "fbfb8209-388f-4094-a987-e33a058a0252", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1047af7a-6186-49ad-a0f0-9b3504bc95d4", "node_type": "1", "metadata": {}, "hash": "03ac4bdba0fccb257111d37163e87ab7e6738774bd3d9d68ee4000f7b4c9293f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16b516cf-f294-4811-84de-614fd3732235", "node_type": "1", "metadata": {}, "hash": "6e7007578125968a72a37271e7a6ceb5d3ce393a0f4f98dad3fffe1f2ab7e942", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}}, "text": "Fact verification plays an essential\nrole in countering the spread of fake news, deceptive content,\nand rumors, thereby preserving the integrity of the information\nlandscape and ensuring the public\u2019s access to accurate knowl-\nedge. Consequently, automated fact verification systems are of\nimmense importance, with broad applications and significant\npractical value. CONCRETE [230] leverages cross-lingual\nretrieval mechanisms to tap into a wealth of multilingual evi-\ndence, effectively bridging the gap in resources for languages\nthat are underrepresented in fact-checking datasets.", "start_char_idx": 1012, "end_char_idx": 1595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16b516cf-f294-4811-84de-614fd3732235": {"__data__": {"id_": "16b516cf-f294-4811-84de-614fd3732235", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbfb8209-388f-4094-a987-e33a058a0252", "node_type": "1", "metadata": {}, "hash": "7580ff928a48ce4f189bfcede1f0d1ef844838f4366a06d5cc23f477931b7b9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a664023c-ea54-4d03-805e-474d4063dd8b", "node_type": "1", "metadata": {}, "hash": "4bed89afca62165b8da4bb71e49f0f07a808396c31d8b0276ca3a736e1312d87", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}}, "text": "Hagstr \u00a8om\net al. [231] proved on LLaMA [4] and Atlas [30] that search\naugmentation is more beneficial for solving inconsistencyproblems than increasing model size. Atlas [30] shows that\nusing RAG to support LLMs in knowledge-intensive tasks\nmarkedly improves their few-shot learning performance.\n3)Commonsense Reasoning :Commonsense Reasoning\nentails the capability of machines to infer or make decisions on\nproblems or tasks in a human-like manner, drawing upon their\nacquired external knowledge and its application.", "start_char_idx": 1596, "end_char_idx": 2114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a664023c-ea54-4d03-805e-474d4063dd8b": {"__data__": {"id_": "a664023c-ea54-4d03-805e-474d4063dd8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16b516cf-f294-4811-84de-614fd3732235", "node_type": "1", "metadata": {}, "hash": "6e7007578125968a72a37271e7a6ceb5d3ce393a0f4f98dad3fffe1f2ab7e942", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56", "node_type": "1", "metadata": {}, "hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "class_name": "RelatedNodeInfo"}}, "text": "However, the\nvast scope of common sense knowledge and the intricacies of\nreasoning processes make Commonsense Reasoning a peren-\nnially challenging and prominent area of research within the\nfield of NLP.", "start_char_idx": 2115, "end_char_idx": 2318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "418fc8e2-aaf2-40b9-9c67-7e349f169503": {"__data__": {"id_": "418fc8e2-aaf2-40b9-9c67-7e349f169503", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de618738-3540-432b-9a94-c7d107d99047", "node_type": "1", "metadata": {}, "hash": "346cabd965f44a84879007ecb97d4e84c0ae3c6680bbc62578612e8dc42de8ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}}, "text": "KG-BART [232] expands the conceptual land-\nscape by incorporating intricate interrelations among diverse\nconcepts within a knowledge graph. It employs graph attention\nmechanisms to aid LLMs in crafting more nuanced and\nlogically coherent sentences. This approach not only improves\nthe models\u2019 generalization capabilities but also significantly\nbolsters their Commonsense Reasoning proficiency. Wan et\nal. [233] constructed the CONFLICTINGQA dataset, compris-\ning contentious questions and conflicting answer documents,\nto examine which textual features significantly influence LMs\u2019\nability to independently navigate controversial issues.", "start_char_idx": 0, "end_char_idx": 637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de618738-3540-432b-9a94-c7d107d99047": {"__data__": {"id_": "de618738-3540-432b-9a94-c7d107d99047", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "418fc8e2-aaf2-40b9-9c67-7e349f169503", "node_type": "1", "metadata": {}, "hash": "8bd99ac6d990dab535934da58469b133cb5cba145e84af7c874cf83adcf7d119", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9072dec7-70eb-4adf-a06d-8419b381e147", "node_type": "1", "metadata": {}, "hash": "c8e4004b6301fbcc0dde13bd4a98401cbece0a56d8f1f345bd9c28dacb19dd77", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}}, "text": "The\nfindings reveal that LMs often neglect the stylistic aspects of\ntext that are typically valued by people.\n4)Human-Machine Conversation :Human-Machine Con-\nversation encompasses the ability of machines to comprehend\nnatural language and adeptly employ this skill to engage with\nhumans seamlessly. This capability represents a significant\nchallenge within the realms of Artificial Intelligence and Natu-\nral Language Processing and offers a broad spectrum of practi-\ncal applications. As such, Human-Machine Conversation con-\ntinues to be a focal point of research for many scholars.", "start_char_idx": 638, "end_char_idx": 1223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9072dec7-70eb-4adf-a06d-8419b381e147": {"__data__": {"id_": "9072dec7-70eb-4adf-a06d-8419b381e147", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de618738-3540-432b-9a94-c7d107d99047", "node_type": "1", "metadata": {}, "hash": "346cabd965f44a84879007ecb97d4e84c0ae3c6680bbc62578612e8dc42de8ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4debacb-72ef-420b-80cb-f38869c8f88a", "node_type": "1", "metadata": {}, "hash": "abcdabb421f7aa40586529b8925a82e8e298ae6c056de319b7994e49ae8c6c07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}}, "text": "Con-\nceptFlow [234] leverages a commonsense knowledge graph to13\nstructure conversations, directing the flow of dialogue based on\nattention scores, and propelling the conversation forward. This\nmethod achieves commendable results even with a substantial\nreduction in model parameters. Cai et al. [235] reimagined the\ntext generation task as a cloze test by retrieving and distilling\nthe essence of past conversational history, leading to notable\noutcomes. Komeili et al.", "start_char_idx": 1224, "end_char_idx": 1694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4debacb-72ef-420b-80cb-f38869c8f88a": {"__data__": {"id_": "f4debacb-72ef-420b-80cb-f38869c8f88a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9072dec7-70eb-4adf-a06d-8419b381e147", "node_type": "1", "metadata": {}, "hash": "c8e4004b6301fbcc0dde13bd4a98401cbece0a56d8f1f345bd9c28dacb19dd77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8aa981c8-f72d-4180-b9f8-c1019163280e", "node_type": "1", "metadata": {}, "hash": "fc37b98df62da7972342b729efc3faec48860c91aac19ede52ac6c18a908c3d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}}, "text": "Komeili et al. [236] augmented dialogue generation\nquality by harnessing advanced search engine technologies to\nsource pertinent content from the internet. BlenderBot3 [237]\nbroadens its search horizon, not only mining relevant internet\ncontent but also local dialogue history, and employs entity\nextraction among other techniques to refine the quality of the\nresulting dialogue. Kim et al.", "start_char_idx": 1680, "end_char_idx": 2070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8aa981c8-f72d-4180-b9f8-c1019163280e": {"__data__": {"id_": "8aa981c8-f72d-4180-b9f8-c1019163280e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4debacb-72ef-420b-80cb-f38869c8f88a", "node_type": "1", "metadata": {}, "hash": "abcdabb421f7aa40586529b8925a82e8e298ae6c056de319b7994e49ae8c6c07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4a77345a-c539-4264-a387-adf9022090e0", "node_type": "1", "metadata": {}, "hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "class_name": "RelatedNodeInfo"}}, "text": "Kim et al. [238], PARC [239], and CREA-\nICL [240] improve the caliber of non-English conversations by\nincorporating cross-lingual knowledge, effectively addressing\nthe scarcity of non-English datasets and enhancing the quality\nof the generated dialogue.", "start_char_idx": 2060, "end_char_idx": 2313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72b96e4e-d8af-49f6-b47b-1a08f90868d5": {"__data__": {"id_": "72b96e4e-d8af-49f6-b47b-1a08f90868d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c4884c63-ab65-499b-a999-c681de0dfcc4", "node_type": "1", "metadata": {}, "hash": "46bd8a4fc3ed4d3290e7003f3407af8e4d2db97749eb64131085bf710c8ab18a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54b25020-b744-4a8e-b370-c1ebce85705e", "node_type": "1", "metadata": {}, "hash": "956ab1da27c305e031d2bf652135d4845fafe3fcc02ce354f743d93098361d67", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c4884c63-ab65-499b-a999-c681de0dfcc4", "node_type": "1", "metadata": {}, "hash": "46bd8a4fc3ed4d3290e7003f3407af8e4d2db97749eb64131085bf710c8ab18a", "class_name": "RelatedNodeInfo"}}, "text": "CEG [241] addresses hallucination\nissues through a post-processing mechanism, verifying LLM-\ngenerated answers through retrieval. If the answer is correct,\nthe retrieved document is added to the original answer as a\nreference; if the answer lacks reliable references, it guides the\nLLM to respond to the question anew.\n5)Neural Machine Translation :Neural Machine Transla-\ntion (NMT) is the automated process of translating text from\na source language to a target language [161], [242], [243].", "start_char_idx": 0, "end_char_idx": 493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54b25020-b744-4a8e-b370-c1ebce85705e": {"__data__": {"id_": "54b25020-b744-4a8e-b370-c1ebce85705e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c4884c63-ab65-499b-a999-c681de0dfcc4", "node_type": "1", "metadata": {}, "hash": "46bd8a4fc3ed4d3290e7003f3407af8e4d2db97749eb64131085bf710c8ab18a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72b96e4e-d8af-49f6-b47b-1a08f90868d5", "node_type": "1", "metadata": {}, "hash": "bc936310148fb4e9f57248298e2722518f33232cd3df7d593ebfa3209a9a9006", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c4884c63-ab65-499b-a999-c681de0dfcc4", "node_type": "1", "metadata": {}, "hash": "46bd8a4fc3ed4d3290e7003f3407af8e4d2db97749eb64131085bf710c8ab18a", "class_name": "RelatedNodeInfo"}}, "text": "It is a pivotal task in the domain of NLP and represents a\nsignificant objective in the pursuit of AI, boasting considerable\nscientific and practical significance. Cai et al.", "start_char_idx": 494, "end_char_idx": 668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9d7ac54-078c-45bd-b1d4-8dfa6dbf170a": {"__data__": {"id_": "b9d7ac54-078c-45bd-b1d4-8dfa6dbf170a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1e5279b-e384-4417-b2cb-d9c34616ccca", "node_type": "1", "metadata": {}, "hash": "0aee4a40af52680ca585b1560c0397b012a3d8dbab10a215160f651bfc48b9ab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}}, "text": "Cai et al. [242] proposed\nan innovative approach that utilizes monolingual corpora\nalongside multilingual learning techniques, challenging the\ntraditional dependency on bilingual corpora in Neural Machine\nTranslation. This approach ensures that the retrieval system\nprovides ample information while simultaneously optimizing\nboth the retrieval mechanism and the translation model, cul-\nminating in impressive performance. KNN-MT [243] executes\ntranslation tasks at the token level by computing vector space\ndistances.", "start_char_idx": 0, "end_char_idx": 517, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1e5279b-e384-4417-b2cb-d9c34616ccca": {"__data__": {"id_": "e1e5279b-e384-4417-b2cb-d9c34616ccca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9d7ac54-078c-45bd-b1d4-8dfa6dbf170a", "node_type": "1", "metadata": {}, "hash": "d9647231a697ca7bf65e464ed7daff9f0dd2cb2f8b7ba598fbac015a7d253747", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d1e4e84-60a7-432c-bca6-5189663bcf12", "node_type": "1", "metadata": {}, "hash": "62aa6443caf0f05b663198b701b18cfb9c192b06cf974c870856ec6d7ba4e841", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}}, "text": "TRIME [161] effectively minimizes the discrepancy\nbetween training and inference phases by jointly training the\nretrieval system and the generation model, thereby enhancing\nthe precision of translations.\n6)Event Extraction :Event Extraction is a specialized\ntask within Natural Language Processing (NLP) that focuses\non pinpointing and extracting instances of particular event\ntypes from unstructured textual data. An event is generally\ncharacterized by a central action or predicate and the related\nentities, which can include participants, temporal indicators,\nlocations, and other relevant attributes.", "start_char_idx": 518, "end_char_idx": 1122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d1e4e84-60a7-432c-bca6-5189663bcf12": {"__data__": {"id_": "2d1e4e84-60a7-432c-bca6-5189663bcf12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1e5279b-e384-4417-b2cb-d9c34616ccca", "node_type": "1", "metadata": {}, "hash": "0aee4a40af52680ca585b1560c0397b012a3d8dbab10a215160f651bfc48b9ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbbda80d-54fa-48fe-a793-c1fb0f178cd6", "node_type": "1", "metadata": {}, "hash": "1312862de7181e050df710916f0cd6a8bb576b5fc5baa7affd058ef99a3bcef9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}}, "text": "The objective of event\nextraction is to convert the nuanced details embedded within\ntext into a structured format, thereby facilitating advanced\nanalysis, efficient information retrieval, and practical down-\nstream applications. R-GQA [244] employs a retrieval-based\napproach to enhance the context of a given issue by identifying\nand utilizing the most closely aligned Question-Answer pair\nfrom a repository, thereby enriching the information available\nfor processing the current query.", "start_char_idx": 1123, "end_char_idx": 1610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbbda80d-54fa-48fe-a793-c1fb0f178cd6": {"__data__": {"id_": "bbbda80d-54fa-48fe-a793-c1fb0f178cd6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d1e4e84-60a7-432c-bca6-5189663bcf12", "node_type": "1", "metadata": {}, "hash": "62aa6443caf0f05b663198b701b18cfb9c192b06cf974c870856ec6d7ba4e841", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d05fe151-9ada-49db-8ce4-4086284632a5", "node_type": "1", "metadata": {}, "hash": "afde5d25f136883ae4af9cb82655fc3ab85d84e0468f47fed884c00dac997db9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}}, "text": "7)Summarization :In the realm of NLP, Summarization\nis a task aimed at distilling the essential information from\nlengthy texts and producing a concise, coherent summary thatencapsulates the primary themes. Summarization enables users\nto quickly grasp the essence of a text, thereby conserving\ntime that would otherwise be spent on reading extensive\nmaterial. There are two main approaches to Summarization:\nExtractive and Abstractive. Extractive Summarization involves\nthe automatic selection and compilation of key phrases directly\nfrom the source text.", "start_char_idx": 1611, "end_char_idx": 2165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d05fe151-9ada-49db-8ce4-4086284632a5": {"__data__": {"id_": "d05fe151-9ada-49db-8ce4-4086284632a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbbda80d-54fa-48fe-a793-c1fb0f178cd6", "node_type": "1", "metadata": {}, "hash": "1312862de7181e050df710916f0cd6a8bb576b5fc5baa7affd058ef99a3bcef9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc", "node_type": "1", "metadata": {}, "hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "class_name": "RelatedNodeInfo"}}, "text": "A key phrase succinctly captures the\nmain themes, content, or perspectives of the text and is\ntypically composed of one or several words. The generation\nof key phrases is instrumental for understanding, categorizing,\nretrieving, and organizing textual information. It is extensively\napplied in fields such as search engine optimization, aca-\ndemic research, text summarization, and more.", "start_char_idx": 2166, "end_char_idx": 2553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d2dd5cc-61b0-40b5-917f-d7371b591334": {"__data__": {"id_": "8d2dd5cc-61b0-40b5-917f-d7371b591334", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4876034-8e45-4364-b80d-3e066f5fe57f", "node_type": "1", "metadata": {}, "hash": "4356bb4631422f6bbb91bf37c5e6f44f5cc87bb6a1b05cdce5b76001a1cacb5d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}}, "text": "This technique\nrefrains from creating new sentences, instead repurposing\nsegments from the original text. Abstractive Summarization,\non the other hand, entails comprehending the original text\u2019s\nmeaning and reformulating it into new sentences [153], [245]\u2013\n[247]. This approach can convey the source\u2019s intent more\nfluidly but poses greater challenges in terms of implementation\ndue to its complexity. RAMKG [245] effectively leverages a\ncomprehensive English corpus to bolster the performance of\nKeyphrase Generation in non-English contexts.", "start_char_idx": 0, "end_char_idx": 540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4876034-8e45-4364-b80d-3e066f5fe57f": {"__data__": {"id_": "e4876034-8e45-4364-b80d-3e066f5fe57f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d2dd5cc-61b0-40b5-917f-d7371b591334", "node_type": "1", "metadata": {}, "hash": "2ce498d4d0e77d4733c9ea7455880dccf40e9cda6a38dc8b93680100a80a6b81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6fba84e-9aff-44df-8957-f463106d4046", "node_type": "1", "metadata": {}, "hash": "0f0d29e362cc918b3699de19f12832cc74568e11534b19f3573a7b67dc56c467", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}}, "text": "It does so by\nenhancing the alignment of keywords extracted from texts in\ndifferent languages that share similar subject matter. Unlimi-\nformer [153] addresses the issue of input length constraints in\ntransformer-based models by retrieving and utilizing the top-\nk most relevant hidden states, thereby extending the model\u2019s\ncapacity to handle longer inputs. RPRR [246] employs a\nRetrieve-Plan-Retrieve-Read approach to overcome the limited\ncontext window constraints faced by LLMs, utilizing retrieved\ninformation to generate high-quality Wikipedia documents for\nemerging events.", "start_char_idx": 541, "end_char_idx": 1120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6fba84e-9aff-44df-8957-f463106d4046": {"__data__": {"id_": "a6fba84e-9aff-44df-8957-f463106d4046", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4876034-8e45-4364-b80d-3e066f5fe57f", "node_type": "1", "metadata": {}, "hash": "4356bb4631422f6bbb91bf37c5e6f44f5cc87bb6a1b05cdce5b76001a1cacb5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cc459b3-d325-40e8-9cd2-2a80a7a7f523", "node_type": "1", "metadata": {}, "hash": "93c3fc9d2429ef0ab639ab0294b5355fc54830c4a7f05ac63c97601df9c9750c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}}, "text": "RIGHT [247] chooses to use different types\nof retrievers in different datasets to enhance the generator,\nwhich can effectively improve the quality of automatically\ngenerated labels in simple deployment.\nB.RAG for Code\nSeparate retrieval and generation approaches have histor-\nically been employed for code-related tasks. For retrieval,\nsimilar code snippets can be identified using Abstract Syntax\nTrees (AST) or text edit distance. For generation, sequence-\nto-sequence models are employed to generate code or natural\nlanguage. Recent RAG research combines both retrieval and\ngeneration techniques to enhance the overall performance.", "start_char_idx": 1121, "end_char_idx": 1755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cc459b3-d325-40e8-9cd2-2a80a7a7f523": {"__data__": {"id_": "1cc459b3-d325-40e8-9cd2-2a80a7a7f523", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6fba84e-9aff-44df-8957-f463106d4046", "node_type": "1", "metadata": {}, "hash": "0f0d29e362cc918b3699de19f12832cc74568e11534b19f3573a7b67dc56c467", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfabe07a-608f-4828-813d-97c83b376819", "node_type": "1", "metadata": {}, "hash": "a7ef90141bb9fa66607f9921cd3cfc59efd5174dc8ca34beb3eb67045ac9b304", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}}, "text": "Recent RAG research combines both retrieval and\ngeneration techniques to enhance the overall performance.\n1)Code Generation :The goal of code generation is to\ntransform natural language (NL) descriptions into code im-\nplementation, which can be seen a process of text-to-code.\nTherefore, LSTM and transformer models are widely used for\ngenerator. Whether to use code-specific retrieval or text-based\nretrieval depends on the contents to be searched.\nRetrieval-based prompt engineering is one of the most\nprevalent scenarios of RAG in code generation.", "start_char_idx": 1650, "end_char_idx": 2200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfabe07a-608f-4828-813d-97c83b376819": {"__data__": {"id_": "dfabe07a-608f-4828-813d-97c83b376819", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cc459b3-d325-40e8-9cd2-2a80a7a7f523", "node_type": "1", "metadata": {}, "hash": "93c3fc9d2429ef0ab639ab0294b5355fc54830c4a7f05ac63c97601df9c9750c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "node_type": "1", "metadata": {}, "hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "class_name": "RelatedNodeInfo"}}, "text": "In-context\nlearning includes training samples in prompts as the input for\nsequence-to-sequence generative models. Retrieval techniques\nare adopted to find similar training samples to the test input,\nso that the prompt can be more informative and related.", "start_char_idx": 2201, "end_char_idx": 2455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbc3a6be-152f-4360-a086-e317b1979737": {"__data__": {"id_": "dbc3a6be-152f-4360-a086-e317b1979737", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5206c7a8-aeeb-4e49-b845-ef739b76e3ce", "node_type": "1", "metadata": {}, "hash": "86f0e70f43fcd760c4dea692b88c797580efe42389a0cd517a13b26214324fb5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}}, "text": "REDCODER [40] retrieves similar NL descriptions using\ndense retriever CodeBert [25], then concatenates the NL texts,\ntheir paired codes, for downstream generator PLBART [41].14\nCodeT5Mix [248] adopts the paradigm of RECODER, propos-\ning a model that functions dually as both the retriever and\ngenerator. APICoder [129] first train a Bert-based deep re-\ntriever to align the embeddings of NL descriptions and API\ndocumentation; then, it retrieves relevant API information to\nbuild prompt for the generator CODEGEN-MONO [249].", "start_char_idx": 0, "end_char_idx": 524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5206c7a8-aeeb-4e49-b845-ef739b76e3ce": {"__data__": {"id_": "5206c7a8-aeeb-4e49-b845-ef739b76e3ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbc3a6be-152f-4360-a086-e317b1979737", "node_type": "1", "metadata": {}, "hash": "6f3a016faa77dd4691e65c98393271ca6ba86b60449aa726a2fc9cf818696ef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50e13e9b-86a6-4bb9-b538-3c314a67bc03", "node_type": "1", "metadata": {}, "hash": "60e8bc62a163e6eb837552784eafcfc5795233b38dd7113d28ab8c90aca2a87b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}}, "text": "The\nfollowing work [250] uses the same pipline, renaming the\nretriever module as APIFinder. COCOGEN [251] aims at com-\nmonsense reasoning, which generates code-based reasoning-\ngraphs with Codex [2] given NL inputs; it adds an evaluation\nsetting of dynamic prompt selection, which actually retrieves\nrelevant examples for prompts. In DocPrompting [42] given\nan NL intent, the retriver retrieves relevant documentations,\nthen the generator generates codes based on the NL and\nretrieved documents.", "start_char_idx": 525, "end_char_idx": 1020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50e13e9b-86a6-4bb9-b538-3c314a67bc03": {"__data__": {"id_": "50e13e9b-86a6-4bb9-b538-3c314a67bc03", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5206c7a8-aeeb-4e49-b845-ef739b76e3ce", "node_type": "1", "metadata": {}, "hash": "86f0e70f43fcd760c4dea692b88c797580efe42389a0cd517a13b26214324fb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a7ebacc-2913-49d8-9dd6-45bcfebe24f8", "node_type": "1", "metadata": {}, "hash": "01dbf15cc102e66fec9160d9dc1168ca57d55e49273ce0fc4bbfa91bf7735186", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}}, "text": "It evaluates both sparse retrievers and\ndense retrievers, and also tries different generators in ex-\nperiments. CodeT5+ [252] adopts the CodeT5 [106] model\nfor both the retriever and generator, leveraging only the\nencoder part in the retrieval process. AceCoder [188] fixes the\nretriever to BM25 [19], and tests several LLM generators for\ncode generation.", "start_char_idx": 1021, "end_char_idx": 1376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a7ebacc-2913-49d8-9dd6-45bcfebe24f8": {"__data__": {"id_": "3a7ebacc-2913-49d8-9dd6-45bcfebe24f8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50e13e9b-86a6-4bb9-b538-3c314a67bc03", "node_type": "1", "metadata": {}, "hash": "60e8bc62a163e6eb837552784eafcfc5795233b38dd7113d28ab8c90aca2a87b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92637d5c-b17b-47a1-9264-bf112dbb6030", "node_type": "1", "metadata": {}, "hash": "1322a0b65c4691879655abbe54e364ee6e6059e1025e15c8a8db3bc14e1477a6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}}, "text": "A3CodGen [253] extracts local information,\nretrieves relevant global functions using embeddings, and\nincorporates third-party library information to construct the\nprompt for LLM-based code generation. SKCODER [254]\nemploys BM25 to retrieve relevant code snippets, which are\nfurther processed to produce sketch template. The template and\nthe original description are concatenated for final generation.\nCodeGen4Libs [255] uses RAG for both import statements and\ncodes, employing BM25 as retriever and finetuned CodeT5\nas generator.", "start_char_idx": 1377, "end_char_idx": 1906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92637d5c-b17b-47a1-9264-bf112dbb6030": {"__data__": {"id_": "92637d5c-b17b-47a1-9264-bf112dbb6030", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a7ebacc-2913-49d8-9dd6-45bcfebe24f8", "node_type": "1", "metadata": {}, "hash": "01dbf15cc102e66fec9160d9dc1168ca57d55e49273ce0fc4bbfa91bf7735186", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "node_type": "1", "metadata": {}, "hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "class_name": "RelatedNodeInfo"}}, "text": "CODEAGENT [256] design agents to search\non web, retrieve relevant documentation, generate programs,\nand test correctness.", "start_char_idx": 1907, "end_char_idx": 2028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed5469d4-5c93-486d-81d8-c7f56922eb1b": {"__data__": {"id_": "ed5469d4-5c93-486d-81d8-c7f56922eb1b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "206281fa-552c-4bf2-93dd-014d41c7ff14", "node_type": "1", "metadata": {}, "hash": "fc31ca62bd3062edad05cc9cc9db407f9164beea8f54fd3ce681065c4fde5f4a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}}, "text": "RRGCode [257] retrieves relevant code\nsnippets using both sparse and dense retrieval, employs a\ncross-encoder to re-rank the retrieval results, then generates\ncode using the concatenation of the query and the retrieved\ncodes. A recent study [258] shows that retrieval-augmented\nframework for code suggestions, including code generation\nand code completion, can improve the performance by a\nlarge margin.", "start_char_idx": 0, "end_char_idx": 403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "206281fa-552c-4bf2-93dd-014d41c7ff14": {"__data__": {"id_": "206281fa-552c-4bf2-93dd-014d41c7ff14", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed5469d4-5c93-486d-81d8-c7f56922eb1b", "node_type": "1", "metadata": {}, "hash": "010731eb2a2146cf0b40c785666f86bbc1ff81fb0504fd2b4e274d1ae3b32008", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a7f8864-6fd6-44db-bca1-9b388792455b", "node_type": "1", "metadata": {}, "hash": "85a30c5a8222e21feeaa0fa966a1109023f1f7ba6e04f0b74644aca72475ea68", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}}, "text": "ARKS [259] steps further upon prompt-based\nRAG, incorporating iterative RAG to re-formulate queries and\nupdate knowledge soup (containing documentation, execution\nfeedback, generated code, etc.) for dense retrieval, improving\nfinal generation accuracy.\nRetrieval results can be applied during the generation pro-\ncess as well. RECODE [120] retrieves NL descriptions and\npaired codes using edit distance, then extracts n-gram action\nsubtrees from codes\u2019 ASTs.", "start_char_idx": 404, "end_char_idx": 862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a7f8864-6fd6-44db-bca1-9b388792455b": {"__data__": {"id_": "4a7f8864-6fd6-44db-bca1-9b388792455b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "206281fa-552c-4bf2-93dd-014d41c7ff14", "node_type": "1", "metadata": {}, "hash": "fc31ca62bd3062edad05cc9cc9db407f9164beea8f54fd3ce681065c4fde5f4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00ca55a6-99e0-4f3d-8c8d-cf2f350b020c", "node_type": "1", "metadata": {}, "hash": "a9b8c1396b5928d35ba73802312dd896c62fd1478d49dbaa2d59a80de63f8f0e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}}, "text": "During LSTM-based generation,\nthe patterns of processed subtrees are leveraged to increase the\ncorresponding word probability at each decoding step. kNN-\nTRANX [163] uses seq2tree model BertranX [260] to convert\nNL to code AST. It constructs a datastore for each code AST\nprefix and NL pair; i.e., for each NL-code pair, the context\nrepresentation of the i-th context is obtained by encoding\nNL and the i-th prefix of code\u2019s AST through the seq2tree\nmodel.", "start_char_idx": 863, "end_char_idx": 1319, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00ca55a6-99e0-4f3d-8c8d-cf2f350b020c": {"__data__": {"id_": "00ca55a6-99e0-4f3d-8c8d-cf2f350b020c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a7f8864-6fd6-44db-bca1-9b388792455b", "node_type": "1", "metadata": {}, "hash": "85a30c5a8222e21feeaa0fa966a1109023f1f7ba6e04f0b74644aca72475ea68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69d976e7-b806-444e-ba71-9e7eae0820b8", "node_type": "1", "metadata": {}, "hash": "2e5e42d92dbe93f4e08cfc902be01307824abc995499fa48db3ea761b070ca5d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}}, "text": "At each decoding step of the generation, the hidden\nrepresentations are searched within the datastore to form a\nnew probability, which is later combined with the seq2tree\nmodel\u2019s output through a confidence network.ToolCoder [261] performs normal code generation, and\nconducts online search or offline retrieval when encountering\nspecial < API > token. This paradigm makes the model learn\nto leverage API tools.\n2)Code Summarization :The goal of code summarization\nis to transform code into natural language (NL) descriptions,\nwhich is a process of code-to-text.", "start_char_idx": 1320, "end_char_idx": 1882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69d976e7-b806-444e-ba71-9e7eae0820b8": {"__data__": {"id_": "69d976e7-b806-444e-ba71-9e7eae0820b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00ca55a6-99e0-4f3d-8c8d-cf2f350b020c", "node_type": "1", "metadata": {}, "hash": "a9b8c1396b5928d35ba73802312dd896c62fd1478d49dbaa2d59a80de63f8f0e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "node_type": "1", "metadata": {}, "hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "class_name": "RelatedNodeInfo"}}, "text": "Same to code generation,\nmany sequence-to-sequence models are applied as generator.\nIn many research works, the retrieval results are processed\nby additional encoders.", "start_char_idx": 1883, "end_char_idx": 2050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f21507d8-c965-413c-939d-44f3e5f1c095": {"__data__": {"id_": "f21507d8-c965-413c-939d-44f3e5f1c095", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad848f9e-6e82-4e71-92f7-1a534a8a184b", "node_type": "1", "metadata": {}, "hash": "58b053e1b7137d45e355adb640ff6ae383be6bfb4865c6fe8d3ada623fe85b69", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad848f9e-6e82-4e71-92f7-1a534a8a184b", "node_type": "1", "metadata": {}, "hash": "58b053e1b7137d45e355adb640ff6ae383be6bfb4865c6fe8d3ada623fe85b69", "class_name": "RelatedNodeInfo"}}, "text": "In many research works, the retrieval results are processed\nby additional encoders. Re2Com [141] and EditSum [138]\nSource Code Repository\nExtract\nJava MethodsComments\nCommentCode\nCommentCode\nCommentCodeTraining Set\nTest SetValidation SetDivided by projectRetrieval Corpus / Training SetRetrieve Module\nRefine ModuleAttention MechanismInput Code RepresentationSimilar Code RepresentationExemplar RepresentationData PreprocessingTraining and Test\nEncodersEncodersEncodersDecoder\nFig.", "start_char_idx": 0, "end_char_idx": 481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "936de599-ea95-4d78-9c3f-1c249d80d80b": {"__data__": {"id_": "936de599-ea95-4d78-9c3f-1c249d80d80b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f30055d2-7f02-460a-8448-94843c6757af", "node_type": "1", "metadata": {}, "hash": "06077b5569c4638c3e429be712f1652ae498b9452d4bc00e72c91449b702f363", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}}, "text": "6: Architecture of Re2Com [141] model.\nboth retrieve similar code using sparse retrieval BM25 and\ngenerate summary using LSTM. As shown in Fig. 6, they\nseparately encode the input, the retrieved code, and the corre-\nsponding summary, then combine the middle representations\nor probabilities in the decoder. HGNN [262] instead uses\ncode edit distance for retrieval, and substitutes the encoders\nfor codes with hybrid GNN on their Code Property Graphs\n(CPG) [263]. RACE [142] aims at generating commit message\nfor code difference.", "start_char_idx": 0, "end_char_idx": 528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f30055d2-7f02-460a-8448-94843c6757af": {"__data__": {"id_": "f30055d2-7f02-460a-8448-94843c6757af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "936de599-ea95-4d78-9c3f-1c249d80d80b", "node_type": "1", "metadata": {}, "hash": "f40523a4df87bcb25e7c99f3d66a52d6b6523337c56997506af070d93515e489", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c99b4aba-fa2d-455d-9d84-77c3d20635f6", "node_type": "1", "metadata": {}, "hash": "b2edc0a803af4671e5d9360245a9965f3afe77298c5f40c04b5f5d7c2148ce57", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}}, "text": "RACE [142] aims at generating commit message\nfor code difference. It leverages dense retrieval for similar\nexamples and transformer model for generation. It also uses\nthree encoders for the input, the retrieved code difference,\nand corresponding commit message, then combines the results\nbefore feeding into the decoder. BASHEXPLAINER [139]\nshares the similar idea. Its dense retrieval module is based on\nCodeBert [25], and for generation, the output representations\nof the input and the similar code from CodeBert are directly\nfused for transformer-based decoder.", "start_char_idx": 463, "end_char_idx": 1027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c99b4aba-fa2d-455d-9d84-77c3d20635f6": {"__data__": {"id_": "c99b4aba-fa2d-455d-9d84-77c3d20635f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f30055d2-7f02-460a-8448-94843c6757af", "node_type": "1", "metadata": {}, "hash": "06077b5569c4638c3e429be712f1652ae498b9452d4bc00e72c91449b702f363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39fab97e-52f2-4e71-88f2-fe89f43d9e96", "node_type": "1", "metadata": {}, "hash": "b67bcf24635157a6caa9356e8f5d1f83a7f9ffdedeb93438a45bc398702179a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}}, "text": "READSUM [264] re-\ntrieves similar code using Levenshtein distance, applies code\nencoder and summary encoder to retrieved pairs, and generates\nsummary using a decoder where a fusion network combines\nthe information.\nRAG for in-context learning, which retrieves similar ex-\namples and build prompt for generation, also works in code\nsummary. REDCODER [40] works for both code generation\nand summary, and it replaces the retriever with GraphCode-\nBert [105] for code summary task.", "start_char_idx": 1028, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39fab97e-52f2-4e71-88f2-fe89f43d9e96": {"__data__": {"id_": "39fab97e-52f2-4e71-88f2-fe89f43d9e96", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c99b4aba-fa2d-455d-9d84-77c3d20635f6", "node_type": "1", "metadata": {}, "hash": "b2edc0a803af4671e5d9360245a9965f3afe77298c5f40c04b5f5d7c2148ce57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27497f07-deb3-4d74-bcb8-7865ae6f8fca", "node_type": "1", "metadata": {}, "hash": "a0bf98d1b8dde780e20ff462ae52525c4448603c69be2dbb296466b947aea8cb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}}, "text": "ASAP [203] retrieves\nsimilar code with BM25, and generates summary with GPT\nmodels. Similar to the paradigm described above, research\non pseudocode generation [265] employs the retrieved code\nas input for generation and subsequently replaces the results\nwith the original input. SCCLLM [266] retrieves relevant code\nsnippets by semantic, syntactic, and lexical-based retrieval,\nthen forms in-context prompts for smart contract comment\ngeneration via LLM. UniLog [267] retrieves similar code\nsnippets paired with their log statements to conduct in-context\nlearning for log statement generation.", "start_char_idx": 1506, "end_char_idx": 2099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27497f07-deb3-4d74-bcb8-7865ae6f8fca": {"__data__": {"id_": "27497f07-deb3-4d74-bcb8-7865ae6f8fca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39fab97e-52f2-4e71-88f2-fe89f43d9e96", "node_type": "1", "metadata": {}, "hash": "b67bcf24635157a6caa9356e8f5d1f83a7f9ffdedeb93438a45bc398702179a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "node_type": "1", "metadata": {}, "hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "class_name": "RelatedNodeInfo"}}, "text": "Logit-based RAG also prevails in code summarization.", "start_char_idx": 2100, "end_char_idx": 2152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "398b9c48-d6a9-41cb-b4c5-d800a081bd55": {"__data__": {"id_": "398b9c48-d6a9-41cb-b4c5-d800a081bd55", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "278e7c07-9191-417a-9fc5-452bfcb6b8ef", "node_type": "1", "metadata": {}, "hash": "f76ce2ced3a02f398c81c9ebc4e1796d9a0f80c824520b0a85560966303bf574", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}}, "text": "Logit-based RAG also prevails in code summarization.\nRencos [121] utilizes two different methods to retrieve similar15\ncode snippets, syntactic similarity between abstract syntax\ntrees (AST) and cosine similarity between dense representa-\ntions. For generator, it adopts attention-based LSTM. There are\nthree encoder-decoder models for the code input and two re-\ntrieval results respectively, and the probabilities are combined\nfor final generation. CoRec [268] generates commit message\ngiven code diff and retrieved similar code diff.", "start_char_idx": 0, "end_char_idx": 535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "278e7c07-9191-417a-9fc5-452bfcb6b8ef": {"__data__": {"id_": "278e7c07-9191-417a-9fc5-452bfcb6b8ef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "398b9c48-d6a9-41cb-b4c5-d800a081bd55", "node_type": "1", "metadata": {}, "hash": "c731288a38ae45222d36eddc88a86c59e6a38454421934bf6351586eea72e742", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "692f0364-6df0-4243-bd64-3a5b65ea337c", "node_type": "1", "metadata": {}, "hash": "d213eabab635020e7eadaef204ece34e38863722e9ce62a6f1c154f31c0d7d21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}}, "text": "CoRec [268] generates commit message\ngiven code diff and retrieved similar code diff. Multiple LSTM\ngenerators handle the input code diff and the retrieved code\ndiff, then adds the probabilities for final generation. kNN-\nTransformer [269] obtains context vector by feeding code into\nan encoder-decoder generator, then gets three parts of logits:\nthe first is from searching the vector, the second is from normal\nTransformer, and the third is the copy mechanism that reserve\nrare tokens from the input. Tram [270] involves retrieval in\nboth token-level and sentence-level.", "start_char_idx": 450, "end_char_idx": 1022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "692f0364-6df0-4243-bd64-3a5b65ea337c": {"__data__": {"id_": "692f0364-6df0-4243-bd64-3a5b65ea337c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "278e7c07-9191-417a-9fc5-452bfcb6b8ef", "node_type": "1", "metadata": {}, "hash": "f76ce2ced3a02f398c81c9ebc4e1796d9a0f80c824520b0a85560966303bf574", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acbc0f4c-614d-4494-b95f-1759fde42b04", "node_type": "1", "metadata": {}, "hash": "4ede8bb7e31ea7496e82bf828088b393a3677a8e8cf97be1439a1b5030ab6dd1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}}, "text": "Tram [270] involves retrieval in\nboth token-level and sentence-level. It encodes source code\nand corresponding AST into hidden states representations; for\ntoken-level, it retrieves similar representations in the database\nto form next-token prediction logits; for sentence-level, it uses\nsimilar code for autoregressive generation logits; it also add the\noriginal autoregressive generation logits. CMR-Sum [271] uses\nencoder-decoder model to generate summaries. It conducts\ncross attention between representations of retrieved summary\nand generated summary, and add the logits to the original\ntransformer logits for final distribution.", "start_char_idx": 953, "end_char_idx": 1587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acbc0f4c-614d-4494-b95f-1759fde42b04": {"__data__": {"id_": "acbc0f4c-614d-4494-b95f-1759fde42b04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "692f0364-6df0-4243-bd64-3a5b65ea337c", "node_type": "1", "metadata": {}, "hash": "d213eabab635020e7eadaef204ece34e38863722e9ce62a6f1c154f31c0d7d21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6847293d-9f0b-427d-b17d-187f2b533b44", "node_type": "1", "metadata": {}, "hash": "0fab70f128917d34f24978c7f836ffd9044fe784f8e6c736e4d706a8e14b57c6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}}, "text": "3)Code Completion :Code completion can be thought of\nas the coding equivalent of the \u201cnext sentence prediction\u201d task.\nEarly attempts on function completion [272] employs DPR to\nretrieve relevant template functions using function docstring,\nthen concatenate the information as the input to the code\ngenerator BART. ReACC [132] retrieves similar codes to build\nprompts for generation. For retrieval, it uses hybrid retriever,\nwhich combines sparse and dense retrieval; for generation, it\nuses CodeGPT-adapted [273].", "start_char_idx": 1588, "end_char_idx": 2101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6847293d-9f0b-427d-b17d-187f2b533b44": {"__data__": {"id_": "6847293d-9f0b-427d-b17d-187f2b533b44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acbc0f4c-614d-4494-b95f-1759fde42b04", "node_type": "1", "metadata": {}, "hash": "4ede8bb7e31ea7496e82bf828088b393a3677a8e8cf97be1439a1b5030ab6dd1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "node_type": "1", "metadata": {}, "hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "class_name": "RelatedNodeInfo"}}, "text": "RepoCoder [218] steps further\nto perform iterative retrieval and generation to bridge the gap\nbetween the retrieval context and the intended completion tar-\nget. In each iteration, for retrieval, the code query is augmented\nwith previously generated code; for generation, the prompt\nis formed by combining the newest retrieved codes with the\ncode query.", "start_char_idx": 2102, "end_char_idx": 2455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5a0007e-aa29-4be4-8b2c-177b5707175b": {"__data__": {"id_": "e5a0007e-aa29-4be4-8b2c-177b5707175b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27d32337-66ea-48ac-bc2d-dad3f1ad9f00", "node_type": "1", "metadata": {}, "hash": "3658d0b51ed1b9b7906452aa6727f38756893740f883f3b140a52217bb48ae8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}}, "text": "Other than combining retrieval results in prompts,\nthe retrieval-and-edit framework [140] first retrieves similar\ntraining examples using dense retrieval, then encodes the\ninput and the retrieved result separately, finally combine them\nthrough attention for later LSTM decoder. CoCoMic [274]\nbuilds a project context graph for the whole code project, and\nretrieves top-k neighbors of the input source code. It generates\nrepresentations of both source code and retrieved contexts,\nthen jointly processes the embeddings to complete the code.", "start_char_idx": 0, "end_char_idx": 539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27d32337-66ea-48ac-bc2d-dad3f1ad9f00": {"__data__": {"id_": "27d32337-66ea-48ac-bc2d-dad3f1ad9f00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5a0007e-aa29-4be4-8b2c-177b5707175b", "node_type": "1", "metadata": {}, "hash": "c8cfef6c2fbad2bddf7bd38fb415981a629d2fefca854437453d8c71ae9ddbda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a65a88d-bb2b-4c2d-9a66-95d2302cc0f2", "node_type": "1", "metadata": {}, "hash": "54fc47517fe02ff2c264738afb2321e73283adeae52bdaa206198ec276fb733d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}}, "text": "RepoFusion [275] follows the idea of Fusion-in-Decoder; it\nemploys multiple encoder to input the concatenation of the\nretrieved repo contexts and the unfinished code, them fuses\nthem and generates the results through a decoder. KNM-\nLM [276] uses the same model for retrieval encoding and\ngeneration, then combines the logits using bayes inference.\nEDITAS [277] aims at assertion generation.", "start_char_idx": 540, "end_char_idx": 931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a65a88d-bb2b-4c2d-9a66-95d2302cc0f2": {"__data__": {"id_": "5a65a88d-bb2b-4c2d-9a66-95d2302cc0f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27d32337-66ea-48ac-bc2d-dad3f1ad9f00", "node_type": "1", "metadata": {}, "hash": "3658d0b51ed1b9b7906452aa6727f38756893740f883f3b140a52217bb48ae8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dedc7f2c-f064-44fd-84bd-3f4c428591ad", "node_type": "1", "metadata": {}, "hash": "3b565b9e1b23086a38ba8f4cd58d7a29796157c1d10e6b7d03f56d0e78f2c4f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}}, "text": "EDITAS [277] aims at assertion generation. It retrieves similar\nqueries and their assertions, encodes the edit sequence (from\nthe retrieved query to the original query) and the retrieved\nassertion, then fuses the information for decoder generation.\nDe-Hallucinator [278] first generates code snippets withoutretrieval, then retrieves relevant API references given gener-\nated contents. In the second pass, retrieved API references are\ncombined in prompt for better generation. REPOFUSE [279]\nuses both rationale context and retrieved similar codes to\nconstruct prompt.", "start_char_idx": 889, "end_char_idx": 1457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dedc7f2c-f064-44fd-84bd-3f4c428591ad": {"__data__": {"id_": "dedc7f2c-f064-44fd-84bd-3f4c428591ad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a65a88d-bb2b-4c2d-9a66-95d2302cc0f2", "node_type": "1", "metadata": {}, "hash": "54fc47517fe02ff2c264738afb2321e73283adeae52bdaa206198ec276fb733d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3759a715-0879-495a-b493-c4b225e963b7", "node_type": "1", "metadata": {}, "hash": "ae651c280891e84d0034439418e38059bab9f5019aaa471f90185f615a0b7d99", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}}, "text": "To fit in the input length limit, it reserves\nthe contexts that are most relevant to the query.\n4)Automatic Program Repair :Buggy code can take a lot\nof effort to fix. Automatic program repair leverages generative\nmodels to output the correct version. Query-based RAG tech-\ninque is widely used in automatic program repair [130], [131],\n[182], [208], [209]. Among them, RING [209] retrieves similar\nerror messages based on both sparse and dense vectors, then\nbuilds prompt for the generator Codex [2].", "start_char_idx": 1458, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3759a715-0879-495a-b493-c4b225e963b7": {"__data__": {"id_": "3759a715-0879-495a-b493-c4b225e963b7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dedc7f2c-f064-44fd-84bd-3f4c428591ad", "node_type": "1", "metadata": {}, "hash": "3b565b9e1b23086a38ba8f4cd58d7a29796157c1d10e6b7d03f56d0e78f2c4f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "92c16379-d8bd-463f-9c60-ec9831a037e6", "node_type": "1", "metadata": {}, "hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "class_name": "RelatedNodeInfo"}}, "text": "CEDAR [130] ap-\nplies for both assertion generation and program repairs tasks; it\nuses sparse and dense retrieval to search for similar codes, then\nforms prompt for Codex to generate results. InferFix [131]\ncrafts a prompt carrying the bug type, location, relevant syntax\nhierarchies, and similar fixes through dense retrieval.", "start_char_idx": 1960, "end_char_idx": 2287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd1a0ed5-3007-43f1-98ad-4201cb85a042": {"__data__": {"id_": "bd1a0ed5-3007-43f1-98ad-4201cb85a042", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "133d75a8-9182-42a0-999e-0436128e7872", "node_type": "1", "metadata": {}, "hash": "f5f572420f155b551cd1fcd0753f4be4432c0ceed9ae756f9a35ae8f86c91d6f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}}, "text": "Then it\nalso uses Codex for generation. RAP-Gen [182] also retrieves\nsimilar buggy codes and corresponding fixes through hybrid\nretriever (including both sparse and dense retriever). It fine-\ntunes CodeT5 [106] with this RAG paradigm. SARGAM [208]\nsearches similar buggy code using dense retrieval, generates\npatches using augmented input, then applies another model to\nmodify the final result. These research works also involve error\nlocalization, which is not our focus.", "start_char_idx": 0, "end_char_idx": 472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "133d75a8-9182-42a0-999e-0436128e7872": {"__data__": {"id_": "133d75a8-9182-42a0-999e-0436128e7872", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd1a0ed5-3007-43f1-98ad-4201cb85a042", "node_type": "1", "metadata": {}, "hash": "083dcfee709cf6951da210ad73396a73a02c3db6c94f40be30735ae3b488fd2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bf2fb29-9f66-46f7-859a-6d90dfc99792", "node_type": "1", "metadata": {}, "hash": "0ba5dcc18772d6b6de4d405bd88bf7a16e923a6c80d83565dac6acb2773ebe8d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}}, "text": "These research works also involve error\nlocalization, which is not our focus. RTLFixer [280] leverages\nReAct and RAG to implement an agent fixing errors in Verilog\ncodes. It iteratively retrieves relevant errors and corresponding\nsolutions, and combines reasoning and action planning to form\nprompts for LLMs.\n5)Text-to-SQL and Code-based Semantic Parsing :Se-\nmantic parsing is the task of translating natural language\nutterances to unambiguous structured meaning representations,\nwhere code language is often leveraged to augment this pro-\ncess.", "start_char_idx": 395, "end_char_idx": 942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bf2fb29-9f66-46f7-859a-6d90dfc99792": {"__data__": {"id_": "3bf2fb29-9f66-46f7-859a-6d90dfc99792", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "133d75a8-9182-42a0-999e-0436128e7872", "node_type": "1", "metadata": {}, "hash": "f5f572420f155b551cd1fcd0753f4be4432c0ceed9ae756f9a35ae8f86c91d6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e20b5f0d-7df5-4c8e-932f-7d9935f2f2c2", "node_type": "1", "metadata": {}, "hash": "9a7004724a9337596c37aed41ddada2ee10ce166a5ef528d77f8d9fe3001edcb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}}, "text": "SQL is not only a programming language but can also be\nconsidered as a structured representation, so we place text-to-\nSQL (a special case of code generation) in this subsection. Re-\nlated research works all apply query-based RAG. XRICL [189]\nfocuses on the problem of translating non-English utterances\ninto SQL queries. It searches and reranks English utterance\nusing non-English ones by dense retrieval, then builds prompt\nfor Codex to generate SQL queries.", "start_char_idx": 943, "end_char_idx": 1403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e20b5f0d-7df5-4c8e-932f-7d9935f2f2c2": {"__data__": {"id_": "e20b5f0d-7df5-4c8e-932f-7d9935f2f2c2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf2fb29-9f66-46f7-859a-6d90dfc99792", "node_type": "1", "metadata": {}, "hash": "0ba5dcc18772d6b6de4d405bd88bf7a16e923a6c80d83565dac6acb2773ebe8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2fba76e-41e1-4e4e-ab13-1b470c8216da", "node_type": "1", "metadata": {}, "hash": "90a0ce2703139eda58f06d328f72f331e55a18123f827878b221fb0f76628050", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}}, "text": "SYNCHROMESH [122]\nproposes constrained semantic decoding to enforce rich syn-\ntactic and semantic constraints during generation of SQL or\nother languages. It uses the similarity between abstract syntax\ntrees (AST) to finetune the dense retriever S-Bert. During\ninference, similar NL and SQL are retrived to form the prompt\nof GPT-3. CodeICL [281] uses Python for semantic parsing,\nand augments prompts with a structured domain description\nfor GPT generation.", "start_char_idx": 1404, "end_char_idx": 1862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2fba76e-41e1-4e4e-ab13-1b470c8216da": {"__data__": {"id_": "c2fba76e-41e1-4e4e-ab13-1b470c8216da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e20b5f0d-7df5-4c8e-932f-7d9935f2f2c2", "node_type": "1", "metadata": {}, "hash": "9a7004724a9337596c37aed41ddada2ee10ce166a5ef528d77f8d9fe3001edcb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "node_type": "1", "metadata": {}, "hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "class_name": "RelatedNodeInfo"}}, "text": "In few-shot learning setting, it leverages\nBM25 sparse retrieval to find similar training examples. RES-\nDSQL [282] ranks schemas using cross-encoder, then includes\nranked schemas into prompts to generate SQL skeleton and\nSQL query. ReFSQL [283] retrieves relevant questions and\ncorresponding SQL to augment text-to-SQL generation.", "start_char_idx": 1863, "end_char_idx": 2194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a33b256d-abe8-4813-9b29-7f34c07e22b2": {"__data__": {"id_": "a33b256d-abe8-4813-9b29-7f34c07e22b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7b4af96-3f92-422b-a14e-63fbead4b498", "node_type": "1", "metadata": {}, "hash": "c94d5758c800353256e28de210bee124682beb04df926d46fc0e7f952804b1ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7b4af96-3f92-422b-a14e-63fbead4b498", "node_type": "1", "metadata": {}, "hash": "c94d5758c800353256e28de210bee124682beb04df926d46fc0e7f952804b1ee", "class_name": "RelatedNodeInfo"}}, "text": "It\ninvolves structure-enhanced retriever with schema linking, and\nMahalanobis contrastive learning to improve the retrieval per-\nformance.", "start_char_idx": 0, "end_char_idx": 138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a726854f-f8a9-431e-829b-30e4742f7d46": {"__data__": {"id_": "a726854f-f8a9-431e-829b-30e4742f7d46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "450ef87a-0957-4826-83a2-5d9d8e4773c7", "node_type": "1", "metadata": {}, "hash": "da8cb6a8784d136d0aad2cc17c8d2d18221184da47dc36c66ed3499bdc2b14e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}}, "text": "ODIS [284] retrieves in-domain and out-of-domain16\ndemonstrations using BM25, then includes them for in-context\nlearning to generate SQL. Another work [285] retrieves both\nsimilar and diverse demonstrations, and then builds prompt for\nin-context learning to generate SQL. MURRE [286] conducts\nmulti-hop retrieve-rewrite, where relevant tables are retrieved\nthrough dense retrieval and then re-writed to generate new\ntabularized question. A rank module at last integrate the\nretrieval results and select the top tables for the text-to-SQL\nmodule.", "start_char_idx": 0, "end_char_idx": 545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "450ef87a-0957-4826-83a2-5d9d8e4773c7": {"__data__": {"id_": "450ef87a-0957-4826-83a2-5d9d8e4773c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a726854f-f8a9-431e-829b-30e4742f7d46", "node_type": "1", "metadata": {}, "hash": "ea52ad687cb13a7ec54c3f74179630253570e15f8d148d494b7942356022b6e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b24883c-5a4c-4f3b-be38-9c962ce7bc1f", "node_type": "1", "metadata": {}, "hash": "42589916d045fdcf2f3694e659c92851f6f4addd375e78669898df82b6985dc5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}}, "text": "CodeS [287] retrieves relevant information from table\ndatabases in a coarse-to-fine manner, then includes retrieved\nvalues to build prompts for both finetuning and inference.\n6)Others :There are several other code-related tasks that\nadopt RAG paradigm. In [288] for numerical reasoning task,\nthe Chain-of-Thought is replaced with the programs as the\nintermediate reasoning step, and dense retrieval-based similar\nexamples are augmented in prompt for downstream LLMs.\nDe-fine [289] tries to resolve intricate tasks using programs.", "start_char_idx": 546, "end_char_idx": 1075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b24883c-5a4c-4f3b-be38-9c962ce7bc1f": {"__data__": {"id_": "9b24883c-5a4c-4f3b-be38-9c962ce7bc1f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "450ef87a-0957-4826-83a2-5d9d8e4773c7", "node_type": "1", "metadata": {}, "hash": "da8cb6a8784d136d0aad2cc17c8d2d18221184da47dc36c66ed3499bdc2b14e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e862db03-86f6-4080-81bd-d15c046aa835", "node_type": "1", "metadata": {}, "hash": "a9044711eb44f3f5c95ae885b8f5e837f354b88e3c7db9ace3d673be80d928ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}}, "text": "De-fine [289] tries to resolve intricate tasks using programs.\nIt follows the paradigm in SKCODER, retrieves relevant\npairs of query and code, then produces sketch template for\nreal program generation. After generation, it combines the\nfeedback to refine the answer with the same generator. The\nrefined programs, regarded as optimal results, are added back\nto the codebase for subsequent serving. E&V [290] leverages\nan LLM-based agent for program static analysis.", "start_char_idx": 1013, "end_char_idx": 1477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e862db03-86f6-4080-81bd-d15c046aa835": {"__data__": {"id_": "e862db03-86f6-4080-81bd-d15c046aa835", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b24883c-5a4c-4f3b-be38-9c962ce7bc1f", "node_type": "1", "metadata": {}, "hash": "42589916d045fdcf2f3694e659c92851f6f4addd375e78669898df82b6985dc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "843259ee-797d-4210-a4b1-8ec05c4e6bc7", "node_type": "1", "metadata": {}, "hash": "f1909e97b67db94e20e1fb205ed79114bc0462660e7f60a32fbd13a416248f32", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}}, "text": "E&V [290] leverages\nan LLM-based agent for program static analysis. The agent\nuses source code retrieval, pseudo-code execution, execution\nspecifications verification, and other tools to form interme-\ndiate results. The retrieval is implemented by AST pattern\nmatching. Code4UIE [291] leverages code representation for\ninformation extraction tasks. It retrieves relevant examples\nthrough dense retrieval, and constructs in-context learning\nprompt using the retrieved queries and their corresponding\ncodes. StackSpotAI [292] builds an AI coding assistant,\nwhich incorporates many code-related tasks.", "start_char_idx": 1410, "end_char_idx": 2008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "843259ee-797d-4210-a4b1-8ec05c4e6bc7": {"__data__": {"id_": "843259ee-797d-4210-a4b1-8ec05c4e6bc7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e862db03-86f6-4080-81bd-d15c046aa835", "node_type": "1", "metadata": {}, "hash": "a9044711eb44f3f5c95ae885b8f5e837f354b88e3c7db9ace3d673be80d928ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89", "node_type": "1", "metadata": {}, "hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "class_name": "RelatedNodeInfo"}}, "text": "StackSpotAI [292] builds an AI coding assistant,\nwhich incorporates many code-related tasks. It implements\nan RAG component, identifying the most relevant pieces of\ninformation which serve as the context for GPT generation.\nInputBlaster [293] aims to generate unusual text input that\ncould cause mobile app crash. It combines retrieved relevant\nbuggy text with valid input to form the prompt for generation.", "start_char_idx": 1916, "end_char_idx": 2323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f8e59fd-24fd-49fa-8626-d7723415d3a5": {"__data__": {"id_": "8f8e59fd-24fd-49fa-8626-d7723415d3a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c144f42f-9652-4ab8-b893-b42cbdcb8a98", "node_type": "1", "metadata": {}, "hash": "76dab4b41a2359b22b37552b5b44de99aac495c844fe1eb99086fe3fe3d92cfe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}}, "text": "It combines retrieved relevant\nbuggy text with valid input to form the prompt for generation.\nC.RAG for Knowledge\nStructured knowledge, including KGs (knowledge graph)\nand tables, is widely used in language-related tasks. It usually\nserves as the retrieval source to augment generation. In addi-\ntion to regular sparse and dense retrieval, NER (named-entity\nrecognition) technique and graph-aware neighbor retrieval are\napplied to identify and extract relevant entities and relations.", "start_char_idx": 0, "end_char_idx": 484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c144f42f-9652-4ab8-b893-b42cbdcb8a98": {"__data__": {"id_": "c144f42f-9652-4ab8-b893-b42cbdcb8a98", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f8e59fd-24fd-49fa-8626-d7723415d3a5", "node_type": "1", "metadata": {}, "hash": "594f9849e7e66a5d46a98c2863f93f4987dd2f10b40fb2c50aebc0ba97812114", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49acf9e1-1fa1-40e3-9801-a7f64e4a132d", "node_type": "1", "metadata": {}, "hash": "66f74b21899b27f7755248d31d26cc574bdc4de4507bb6722e71a35c77f51e25", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}}, "text": "1)Knowledge Base Question Answering :KBQA (knowl-\nedge base question answering) typically utilizes a knowledge\nbase to determine the correct answer to a question. Many\nsemantic parsing methods have been proposed, generating\nlogical forms (e.g. SPARQL) based on the question.\nQuery-based RAG is the mainstream approach. For a given\nquery, Unseen Entity Handling [53] retrieves topic entities\nthrough FreeBase [294], and concatenates the query with the\nentity for an encoder-decoder to generate SPARQL output.", "start_char_idx": 485, "end_char_idx": 992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49acf9e1-1fa1-40e3-9801-a7f64e4a132d": {"__data__": {"id_": "49acf9e1-1fa1-40e3-9801-a7f64e4a132d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c144f42f-9652-4ab8-b893-b42cbdcb8a98", "node_type": "1", "metadata": {}, "hash": "76dab4b41a2359b22b37552b5b44de99aac495c844fe1eb99086fe3fe3d92cfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d641491f-61ef-4359-b9fe-9dfa8bd9cb8d", "node_type": "1", "metadata": {}, "hash": "0593dff6bbb31422e64f41c87800c8af17bfe5d1f729322ce6795d4c07a67083", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}}, "text": "CBR-KBQA [54] retrieves relevant questions and correspond-\ning logical form answers with roberta-based deep retrieval,\nthen concatenates the question and the retrieved pairs for\nencoder-decoder transformer model. It also revises the finalgeneration result to align the generated relations with relations\npresent in the local neighborhood of the query entity in the\nknowledge graph. GMT-KBQA [52] first retrieves relevant\nentities and relations through bert-based deep retrieval, then\nconcatenates the information for T5 generation.", "start_char_idx": 993, "end_char_idx": 1524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d641491f-61ef-4359-b9fe-9dfa8bd9cb8d": {"__data__": {"id_": "d641491f-61ef-4359-b9fe-9dfa8bd9cb8d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49acf9e1-1fa1-40e3-9801-a7f64e4a132d", "node_type": "1", "metadata": {}, "hash": "66f74b21899b27f7755248d31d26cc574bdc4de4507bb6722e71a35c77f51e25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aff25138-c9f0-4611-bb45-175028a2ca57", "node_type": "1", "metadata": {}, "hash": "8a6480c1cff870f48f255d1523ae0c99c09e17426f89620ee48e2d9151705ec5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}}, "text": "To improve\nthe retrieval result, it leverages cross-encoder to rerank the\ncandidates, and uses the same T5 structure to conduct relation\nclassification and entity dismbiguation. RNG-KBQA [123]\nenumerates candidate logical forms in the knowledge graph,\nthen ranks the candidates and concatenates them with query to\nform the prompt input to generate final logical form through\na T5 model. Based on this idea, TIARA [124] also retrieve\nentity and schema besides logical forms, while a follow-\ning work [295] retrieves top-k questions with BM25.", "start_char_idx": 1525, "end_char_idx": 2066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aff25138-c9f0-4611-bb45-175028a2ca57": {"__data__": {"id_": "aff25138-c9f0-4611-bb45-175028a2ca57", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d641491f-61ef-4359-b9fe-9dfa8bd9cb8d", "node_type": "1", "metadata": {}, "hash": "0593dff6bbb31422e64f41c87800c8af17bfe5d1f729322ce6795d4c07a67083", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60", "node_type": "1", "metadata": {}, "hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "class_name": "RelatedNodeInfo"}}, "text": "Uni-\nParser [133] retrieves relevant entities from knowledge graph\nusing mention detection, cross-encoder ranker, and 2-hop paths\nextraction. It also considers enumerating tables and columns\nfrom databases.", "start_char_idx": 2067, "end_char_idx": 2273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "733f45d2-d38c-4127-a959-e19802493591": {"__data__": {"id_": "733f45d2-d38c-4127-a959-e19802493591", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "173fb9b0-9e9f-46a0-9829-c989e9918d6e", "node_type": "1", "metadata": {}, "hash": "f64b62db6925c0ea9576063430929b8dcdafe839a94656d2de46b03f260fc54d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}}, "text": "It also considers enumerating tables and columns\nfrom databases. On obtaining the relevant information, it\nconcatenates the top-k primitives with the query and generates\nlogical forms through T5. BLLM augmentation [135] uses\nTIARA as the retrieval for relevant knowledge base elements,\nthen performs in-context learning through black-box LLM\nsuch as GPT-4 for generating logical forms. ECBRF [134]\nfollows the case-based reasoning paradigm [296], retrieving\nsimilar triplet with dense retriever and constructing prompt in-\nput for BART or GPT-2 in-context learning.", "start_char_idx": 0, "end_char_idx": 565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "173fb9b0-9e9f-46a0-9829-c989e9918d6e": {"__data__": {"id_": "173fb9b0-9e9f-46a0-9829-c989e9918d6e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "733f45d2-d38c-4127-a959-e19802493591", "node_type": "1", "metadata": {}, "hash": "93d44df141bc660c8b690aeec3deb44ffab8d3cade1a8ef6a39403ae1c02fffc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "914f3071-6fb5-478c-acf8-1ec4f4529e08", "node_type": "1", "metadata": {}, "hash": "e8cf591ff745cb74efc940c41662648433104d08571c7f7ce8ffc561cff7e778", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}}, "text": "FC-KBQA [297]\nextracts relevant class, relation, and entity given a question.\nFor class and relation, it uses BM25 as retriever and a\nBert-based cross-encoder as re-ranker. For entity, it follows\nthe mention detection paradigm. To compose all the com-\nponent candidates, it applies T5 model to generate logical\nexpression. StructGPT [298] extracts relevant information,\nincluding triplets and nearest entities, to form prompt for\nsubsequent LLM.", "start_char_idx": 566, "end_char_idx": 1011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "914f3071-6fb5-478c-acf8-1ec4f4529e08": {"__data__": {"id_": "914f3071-6fb5-478c-acf8-1ec4f4529e08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "173fb9b0-9e9f-46a0-9829-c989e9918d6e", "node_type": "1", "metadata": {}, "hash": "f64b62db6925c0ea9576063430929b8dcdafe839a94656d2de46b03f260fc54d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14912092-810f-441d-9cae-bb9025dfafb1", "node_type": "1", "metadata": {}, "hash": "d514d5f2a086f1c7d5faadc6d13396b5aa734ac5938f086116e9d04a5658c280", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}}, "text": "KAPING [299] builds prompt including the\nuser query and the retrieved relevant facts (through entity\nmatching), then generates the answer through LLM. Another\nwork [300] also follows the retrieve-then-generate paradigm,\nreplacing the retrieval with a relation distribution generation\nmodel for weighted triplets. Retrieve-Rewrite-Answer [301]\nfirst retrieves subgraph using hop prediction, relation path\nprediction, and triplet sampling. It then performs KG-to-text\nand zero-shot generation with retrieved subgraphs as prompt.", "start_char_idx": 1012, "end_char_idx": 1538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14912092-810f-441d-9cae-bb9025dfafb1": {"__data__": {"id_": "14912092-810f-441d-9cae-bb9025dfafb1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "914f3071-6fb5-478c-acf8-1ec4f4529e08", "node_type": "1", "metadata": {}, "hash": "e8cf591ff745cb74efc940c41662648433104d08571c7f7ce8ffc561cff7e778", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3220a14f-bb5c-4a16-9a37-6c03ab5029db", "node_type": "1", "metadata": {}, "hash": "1989e998e491eae739ce7adc26758bcec8eb276fd825941ceda8b6bada38f3e6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}}, "text": "It then performs KG-to-text\nand zero-shot generation with retrieved subgraphs as prompt.\nKeqing [302] first decomposes a complext question into simple\nsub-questions through finetuned LLM, then retrieves similar\nsub-question template by dense retriever RoBERTa to extract\ncandidate entities from knowledge graph, and finally generates\nanswer through ChatGPT given relevant entities as context\ninput. To probe the deep understanding of natural language in\nLLMs with formal languages, a research work [303] explores\nthe capability of formal language understanding and formal\nlanguage generation.", "start_char_idx": 1450, "end_char_idx": 2042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3220a14f-bb5c-4a16-9a37-6c03ab5029db": {"__data__": {"id_": "3220a14f-bb5c-4a16-9a37-6c03ab5029db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14912092-810f-441d-9cae-bb9025dfafb1", "node_type": "1", "metadata": {}, "hash": "d514d5f2a086f1c7d5faadc6d13396b5aa734ac5938f086116e9d04a5658c280", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "da514732-a3cf-4a31-a994-8bc6710f22ec", "node_type": "1", "metadata": {}, "hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "class_name": "RelatedNodeInfo"}}, "text": "It leverages retrieved pairs to perform in-\ncontext learning. For understanding, it uses tree edit distance\nto retrieve similar logical forms, while for generation, it uses\nBM25 to retrieve similar natural language queries. Interactive-\nKBQA [304] treats LLM as agent and KG as environment.", "start_char_idx": 2043, "end_char_idx": 2333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3480fc6-2d39-40e2-be85-87c2c822cede": {"__data__": {"id_": "e3480fc6-2d39-40e2-be85-87c2c822cede", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8995d339-b032-4fa5-8f3b-43650975ed6e", "node_type": "1", "metadata": {}, "hash": "27a3d5ac32af29ad3d440f795997c9704675c56bdf7d226facd161bb46a46185", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}}, "text": "Interactive-\nKBQA [304] treats LLM as agent and KG as environment.\nIn each step, the LLM conducts entity-linking and one-hop\nretrieval on KG, then generates current thought and action,\nuntil obtaining the final answer.17\nLatent representation-based RAG is also employed in\nKBQA. ReTraCk [305] links entities using mention detection,\nand retrieves schema items using dense retriever Bert. It\nthen generates logical forms by LSTM, incorporating re-\ntrieved items through knowledge-specific rules.", "start_char_idx": 0, "end_char_idx": 494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8995d339-b032-4fa5-8f3b-43650975ed6e": {"__data__": {"id_": "8995d339-b032-4fa5-8f3b-43650975ed6e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3480fc6-2d39-40e2-be85-87c2c822cede", "node_type": "1", "metadata": {}, "hash": "0b251a06b01867da1a9cb13084131026715e08ea78acb957a5f1d094b244c6a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "661a03b4-e664-4d71-88ee-a259b4da4183", "node_type": "1", "metadata": {}, "hash": "fddabeaa9b428e2ebaf58c18a03c64a2f68180cab1c991467f2439f9df3a12c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}}, "text": "SKP [145]\nconcatenates triplets with the query and uses fusion-in-decoder\ntechnique in inference. It adds a pretraining stage with a\nknowledge-aware MLM loss on triplets and knowledge con-\nstrastive loss with respect to the retrieved items. DECAF [144]\nforms Resource Description Format knowledge base triplets\ninto sentences, then concatenates sentences with the same\nhead entity into documents.", "start_char_idx": 495, "end_char_idx": 891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "661a03b4-e664-4d71-88ee-a259b4da4183": {"__data__": {"id_": "661a03b4-e664-4d71-88ee-a259b4da4183", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8995d339-b032-4fa5-8f3b-43650975ed6e", "node_type": "1", "metadata": {}, "hash": "27a3d5ac32af29ad3d440f795997c9704675c56bdf7d226facd161bb46a46185", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2988ad8e-3321-476a-a8f9-5a2bbc50646f", "node_type": "1", "metadata": {}, "hash": "ca8349ec39b705fcdc18dcf074d49bcb4e2dc1955b85fa60909ec2c4ef9f949b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}}, "text": "It retrieves relevant documents\nusing BM25 sparse retrieval or Bert dense retrieval, then\nleverages Fusion-in-Decoder technique to generate logical\nform and direct answer given each (query, document) pair as\ninput. It combines the output to obtain the final answer. KD-\nCoT [146] uses the same dense retriever and fusion-in-decoder\ngenerator as DECAF. It follows a Chain-of-Thought paradigm,\niteratively performing retrieval, generation, and verification\nuntil the CoT is finished.", "start_char_idx": 892, "end_char_idx": 1373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2988ad8e-3321-476a-a8f9-5a2bbc50646f": {"__data__": {"id_": "2988ad8e-3321-476a-a8f9-5a2bbc50646f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "661a03b4-e664-4d71-88ee-a259b4da4183", "node_type": "1", "metadata": {}, "hash": "fddabeaa9b428e2ebaf58c18a03c64a2f68180cab1c991467f2439f9df3a12c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f", "node_type": "1", "metadata": {}, "hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "class_name": "RelatedNodeInfo"}}, "text": "2)Knowledge-augmented Open-domain Question An-\nswering :Structured knowledge is often leveraged to augment\nODQA (open-domain question answering).\nThe use of latent representation-based RAG, particularly\nthe fusion-in-decoder technique, is prevalent for knowledge-\naugmented open-domain question answering. UniK-QA [143]\nconcatenates the text forms of the components in a triplet\nand build document pool for retrieval. For a given question,\nit leverages dense retriever for relevant documents, then per-\nforms fusion-in-decoder technique to incorporate the infor-\nmation for answer generation.", "start_char_idx": 1374, "end_char_idx": 1966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b83c4fe-27e8-4f03-bc18-81e067a18526": {"__data__": {"id_": "3b83c4fe-27e8-4f03-bc18-81e067a18526", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93b3e0da-3e52-4b92-8a34-ab241bfe5838", "node_type": "1", "metadata": {}, "hash": "551bee74a9305e1db91f96c09d76ce2a6f35623064d56a1b30e728dcd2e4f134", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}}, "text": "KG-FiD [306] conducts the\nEncoder\nL\n1\n \nLayers \nText \nKnowledge \nSource\nDPR \nRetriever\nP1\nP3\nP5\nP6\nP2\nP4\nP7\nP8\nKG\nDecoder\nInput \nQuestion\nEncoder\nL\n1\n \nLayers \nEncoder\nL-L\n1\n \nLayers \nEncoder\nL\n1\n \nLayers \nEncoder\nL\n1\n \nLayers \nConcatenation\nOutput\nAnswer\nP1\nP3\nP5\nP2\nP7\nQuestion \n+", "start_char_idx": 0, "end_char_idx": 282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93b3e0da-3e52-4b92-8a34-ab241bfe5838": {"__data__": {"id_": "93b3e0da-3e52-4b92-8a34-ab241bfe5838", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b83c4fe-27e8-4f03-bc18-81e067a18526", "node_type": "1", "metadata": {}, "hash": "b57fbc2ebf590b52b8cb0bbdd48c42d44502010f5dd7ec9f73b1b12e7e91f597", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89fb7b72-777a-4cc7-837e-9f5b47dabb85", "node_type": "1", "metadata": {}, "hash": "b82674335f03284752135f4dc8b3acd2ee1cfe7a17372db8afc4a1d5f82cea1b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}}, "text": "+ \nP1\nQuestion \n+ \nP2\nQuestion \n+ \nP3\nQuestion \n+ \nP5\nEncoder\nL\n1\n \nLayers \nQuestion \n+ \nP7\nEncoder\nL-L\n1\n \nLayers \nRetrieved\nPassages \n& \nEmbeddings\nStage-1 \nReranking\nReading \nModule\nWhen \ndid \nthe \nYankees \nmove \nto \nNew \nYork?", "start_char_idx": 281, "end_char_idx": 511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89fb7b72-777a-4cc7-837e-9f5b47dabb85": {"__data__": {"id_": "89fb7b72-777a-4cc7-837e-9f5b47dabb85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93b3e0da-3e52-4b92-8a34-ab241bfe5838", "node_type": "1", "metadata": {}, "hash": "551bee74a9305e1db91f96c09d76ce2a6f35623064d56a1b30e728dcd2e4f134", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcc25b7c-9372-44a8-94ca-19e7319a9be9", "node_type": "1", "metadata": {}, "hash": "9865d3e13b72010893102e1184658ada8baeba91690c655a84fea99758db9bc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}}, "text": "did \nthe \nYankees \nmove \nto \nNew \nYork?\n1903\nNew \nYork \nYankees\nYankee \nStadium\nStaten \nIsland \nYankees\nNew \nYork \nYankees\nOperator\nYankee \nStadium\nNew \nYork \nYankees\nParent\nClub\nStaten \nIsland \nYankees\n......\nN\n0\n \nPassages\nN\n1\n \nPassages\nN\n2\n \nPassages\nStage-2 \nReranking\nFig.", "start_char_idx": 472, "end_char_idx": 750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcc25b7c-9372-44a8-94ca-19e7319a9be9": {"__data__": {"id_": "bcc25b7c-9372-44a8-94ca-19e7319a9be9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89fb7b72-777a-4cc7-837e-9f5b47dabb85", "node_type": "1", "metadata": {}, "hash": "b82674335f03284752135f4dc8b3acd2ee1cfe7a17372db8afc4a1d5f82cea1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ab130ed-3f3d-4917-b41e-406b1094c588", "node_type": "1", "metadata": {}, "hash": "049fe44b6242213c14f99937de30e224b60860bde52cdee87f6e47c834c60590", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}}, "text": "7: Architecture of KG-FiD [306] model.\nretrieval and generation as normal FiD. It add re-ranking in\ntwo ways: the first is to use a graph attention network on the\ngraph formed by retrieved documents; the second is to use\nthe hidden states in the generator. OREOLM [307] empowers\nLLM with knowledge reasoning paths.", "start_char_idx": 751, "end_char_idx": 1065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ab130ed-3f3d-4917-b41e-406b1094c588": {"__data__": {"id_": "9ab130ed-3f3d-4917-b41e-406b1094c588", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcc25b7c-9372-44a8-94ca-19e7319a9be9", "node_type": "1", "metadata": {}, "hash": "9865d3e13b72010893102e1184658ada8baeba91690c655a84fea99758db9bc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "20062614-22e0-4695-9dae-521fdff26f79", "node_type": "1", "metadata": {}, "hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "class_name": "RelatedNodeInfo"}}, "text": "OREOLM [307] empowers\nLLM with knowledge reasoning paths. Concretely, it uses\nentity linking to determine the initial state, then conducts\ncontextualized random walk on KG to get reasoning paths,\nwhose entity value memory are combined into the hidden\nstates of LLM for better generation. GRAPE [308] constructs\nbipartite graph for each pair of question and retrieved passage,\nthen builds bipartite graph on entity for fusing knowledge.It leverages FiD as backbone model and generate answers.", "start_char_idx": 1008, "end_char_idx": 1499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70f5ba7c-7a51-4706-a4cf-fd4b7aadcaa6": {"__data__": {"id_": "70f5ba7c-7a51-4706-a4cf-fd4b7aadcaa6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47039706-e2d3-4000-ab1b-9d20e70132ee", "node_type": "1", "metadata": {}, "hash": "45bda953d0f366db5c776f2f66a8bc0463baf8344d27f63e79d9ed353c3dc27c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}}, "text": "SKURG [309] forms a knowledge graph using text and image\ndata, then updates each source\u2019s representation with their\nrelevant sources. It then uses a specially designed decoder\nto iteratively retrieve and generate. It conducts cross-attention\nwith all the sources, then retrieves the source with highest\nscore and concatenates to the original input embedding; if a\ngate score does not exceed the threshold, it starts to generate\nthe real answer, otherwise the retrieval stage re-starts.\nWith the rapid development of LLMs, query-based RAG is\nemerging as a new standard.", "start_char_idx": 0, "end_char_idx": 568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47039706-e2d3-4000-ab1b-9d20e70132ee": {"__data__": {"id_": "47039706-e2d3-4000-ab1b-9d20e70132ee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70f5ba7c-7a51-4706-a4cf-fd4b7aadcaa6", "node_type": "1", "metadata": {}, "hash": "015e5673068b5f833b52db799479cab5b46df495ef645d4d06e4de3f63ee94c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "976714c0-e4d5-43a0-ba7e-0fab9a51bcff", "node_type": "1", "metadata": {}, "hash": "350ff98415c71c002b25af89b10d0688c29d77d932f3dd9d65ab07666bfb47d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}}, "text": "DIVKNOWQA [310] develops\na retrieval-augmentation tool, including sparse retrieval on\nstructured knowledge, dense retrieval on texts, and sparql\ngeneration on KB. Through CoT-based LLM, it retrieves\nand re-ranks in each step, and generates the final answer.\nKnowledGPT [311] uses generated code to retrieve from\nboth public and personal knowledge bases, so as to build\nprompt for LLM question answering.", "start_char_idx": 569, "end_char_idx": 972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "976714c0-e4d5-43a0-ba7e-0fab9a51bcff": {"__data__": {"id_": "976714c0-e4d5-43a0-ba7e-0fab9a51bcff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47039706-e2d3-4000-ab1b-9d20e70132ee", "node_type": "1", "metadata": {}, "hash": "45bda953d0f366db5c776f2f66a8bc0463baf8344d27f63e79d9ed353c3dc27c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "075a15af-6fe7-4aa3-ad2e-93674b13240c", "node_type": "1", "metadata": {}, "hash": "21533d9f05f52b65109bc408bdf8bbaf25b507920bd699dd0c55640e0c18bf4a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}}, "text": "EFSUM [312] generates\nevidence-focused summary with retrieved relevant facts, then\noptimizes the summary to align the QA-specific preference\nwith another generator and the filters for helpfulness and\nfaithfulness. GenTKGQA [313] conducts subgraph retrieval\nthrough relation ranking and time mining, then employs GNN\nto incorporate structural and temporal information into virtual\ntoken representations for subsequent LLM generation.", "start_char_idx": 973, "end_char_idx": 1405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "075a15af-6fe7-4aa3-ad2e-93674b13240c": {"__data__": {"id_": "075a15af-6fe7-4aa3-ad2e-93674b13240c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "976714c0-e4d5-43a0-ba7e-0fab9a51bcff", "node_type": "1", "metadata": {}, "hash": "350ff98415c71c002b25af89b10d0688c29d77d932f3dd9d65ab07666bfb47d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68a5291a-3462-4b62-a5ec-ebe9bde1e764", "node_type": "1", "metadata": {}, "hash": "06b14469e41df83541f8ec8aebc3be24ae5b01b90b4a6864858eb7358cdc148c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}}, "text": "Knowl-\nedgeNavigator [314] analyzes complex questions and performs\nretrieval on knowledge graph through iterative filtering of\nrelations with respect to core entities, so as to obtain relevant\ntriplets. It then includes the triplets into prompt for generation.\n3)Table for Question Answering :Tables, as another form\nof structured knowledge, also facilitates question answering.\nFusion-in-decoder [35] style RAG is often used for table\nQA.", "start_char_idx": 1406, "end_char_idx": 1845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68a5291a-3462-4b62-a5ec-ebe9bde1e764": {"__data__": {"id_": "68a5291a-3462-4b62-a5ec-ebe9bde1e764", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "075a15af-6fe7-4aa3-ad2e-93674b13240c", "node_type": "1", "metadata": {}, "hash": "21533d9f05f52b65109bc408bdf8bbaf25b507920bd699dd0c55640e0c18bf4a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f056fd99-2019-42e5-9b42-718302e68fe2", "node_type": "1", "metadata": {}, "hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "class_name": "RelatedNodeInfo"}}, "text": "Fusion-in-decoder [35] style RAG is often used for table\nQA. EfficientQA [315], a competition held in NeurIPS 2020,\nwitnessed the proposal of numerous retrieval-reader systems\nthat rely on textual and tabular data. Dual Reader-Parser [316]\nre-ranks the retrieved textual and tabular data for generation.\nConvinse [317] retrieves information from heterogeneous re-\nsources (including knowledge bases, tables, and texts) after\nquestion understanding.", "start_char_idx": 1785, "end_char_idx": 2233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90051f7a-8d7b-40ad-87e1-292a767de344": {"__data__": {"id_": "90051f7a-8d7b-40ad-87e1-292a767de344", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b489fbfc-5190-4afb-a1df-d181d1dfe24a", "node_type": "1", "metadata": {}, "hash": "70d6cbb5f0ddbc2c67b4bed7ab5feae2c924188236f7747ac329686d4f5404a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}}, "text": "CORE [318] retrieves relevant tables\nand passages through dense representation, then re-ranks the\nretrieved results using query-generation score from a T0 linker.\nIt uses FiD for final generation. RINK [319] follows the\nretriever-reranker-reader paradigm for table-augmented ques-\ntion answering. It designs a set-level reader-inherited reranker\nto get the relevance score of blocks (table segments).", "start_char_idx": 0, "end_char_idx": 400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b489fbfc-5190-4afb-a1df-d181d1dfe24a": {"__data__": {"id_": "b489fbfc-5190-4afb-a1df-d181d1dfe24a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90051f7a-8d7b-40ad-87e1-292a767de344", "node_type": "1", "metadata": {}, "hash": "07aec4a3479cee064cd203ac5f764888cb18ebbc174ab3eddedcc410c956a70d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5813aad4-2cc6-4ea7-8a51-b5b7c512d119", "node_type": "1", "metadata": {}, "hash": "dfe9511d7a7d0489935b1517df9db707251218badbe7a8893f6220eb9df43622", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}}, "text": "TAG-\nQA [320] retrieves from both tables and texts: for tables, it\nconverts tables to graphs then performs GNN to select relevant\ncontents; for texts, it uses BM25 for retrieval. It then leverages\nFiD for answer generation.\nTables can be incorporated into prompts for query-based\nRAG. T-RAG [321] retrieves relevant tables given a user\nquery, then concatenates the query with the tables as a prompt\nto generate the answer through BART.", "start_char_idx": 401, "end_char_idx": 836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5813aad4-2cc6-4ea7-8a51-b5b7c512d119": {"__data__": {"id_": "5813aad4-2cc6-4ea7-8a51-b5b7c512d119", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b489fbfc-5190-4afb-a1df-d181d1dfe24a", "node_type": "1", "metadata": {}, "hash": "70d6cbb5f0ddbc2c67b4bed7ab5feae2c924188236f7747ac329686d4f5404a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da33621c-3be9-444b-8b72-86b42cadccd9", "node_type": "1", "metadata": {}, "hash": "af55e4ebfc1a3ccfe3e179906cb05169564745cbbfd578a4e5bb6f3206fd4d72", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}}, "text": "OmniTab [322] con-\nducts multi-task training to improve the performance of table\nquestion answering model. It retrieves relevant tables given a\nquery, then concatenates them for masked-token pre-training.\nCARP [323] retrieves relevant tables and passages using\nentity linking, then extracts hybrid chain of retrieved knowl-18\nedge, which is later used to construct prompt for generation.\nStructGPT [298] extracts relevant information from knowledge\ngraph, table, or database, to form prompt and generate answers\nthrough LLMs.", "start_char_idx": 837, "end_char_idx": 1362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da33621c-3be9-444b-8b72-86b42cadccd9": {"__data__": {"id_": "da33621c-3be9-444b-8b72-86b42cadccd9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5813aad4-2cc6-4ea7-8a51-b5b7c512d119", "node_type": "1", "metadata": {}, "hash": "dfe9511d7a7d0489935b1517df9db707251218badbe7a8893f6220eb9df43622", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d88553b-27a7-4aec-9b20-866a54191319", "node_type": "1", "metadata": {}, "hash": "728721a1408e46e2475b6aade3b1e409f904a943cdae18e75c1b8a532aef53f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}}, "text": "cTBLS [324] retrieves relevant tables through\ndense retrieval, then for each query, it forms prompt with\nranked tables for answer generation. A recent work [325]\nfirst uses table-to-text techniques to integrate tabular data into\ncorpora, then conducts experiments on both finetuning and\nRAG for question answering.\n4)Others :Prototype-KRG [326] retrieves knowledge facts\nand dialogue prototypes, then integrates them into the\nGRU-based generator by both hidden states and logits.", "start_char_idx": 1363, "end_char_idx": 1842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d88553b-27a7-4aec-9b20-866a54191319": {"__data__": {"id_": "8d88553b-27a7-4aec-9b20-866a54191319", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da33621c-3be9-444b-8b72-86b42cadccd9", "node_type": "1", "metadata": {}, "hash": "af55e4ebfc1a3ccfe3e179906cb05169564745cbbfd578a4e5bb6f3206fd4d72", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "32e72675-2960-49e4-afd6-5052ce1b0802", "node_type": "1", "metadata": {}, "hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "class_name": "RelatedNodeInfo"}}, "text": "SURGE [327] retrieves relevant subgraphs using dense re-\ntrieval, then adds them into the input of the generator for\ndialogue generation. RHO [328] fuses KG embedding of\nrelevant entities and relations into textual embeddings during\nthe generation of open-domain dialogue. K-LaMP [329] re-\ntrieves entity in history queries to construct prompt for query\nsuggestion.", "start_char_idx": 1843, "end_char_idx": 2208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddd5a5e0-ea32-47c9-8629-7b251984223f": {"__data__": {"id_": "ddd5a5e0-ea32-47c9-8629-7b251984223f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49fb7aa6-8e4c-40de-b2f3-cc2a83b89252", "node_type": "1", "metadata": {}, "hash": "2fa5ad0999650b12a5ee597b9f587280bafce0c57a515c53f7cb764b209cd64d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}}, "text": "ReSKGC [147] linearizes all training triplets into\ntext by concatenation, then retrieves relevant triplets using\nBM25, and generates completed triplet using T5 with fusion-\nin-decoder. G-Retriever [330] retrieves relevant nodes and\nedges from graph-based data, then constructs subgraph and\nperforms graph prompt tuning for question answering based\non textual graphs.\nD.RAG for Image\n1)Image Generation :Image Generation refers to the\nprocess of creating new images, typically using algorithms in\nthe field of artificial intelligence and machine learning.", "start_char_idx": 0, "end_char_idx": 554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49fb7aa6-8e4c-40de-b2f3-cc2a83b89252": {"__data__": {"id_": "49fb7aa6-8e4c-40de-b2f3-cc2a83b89252", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddd5a5e0-ea32-47c9-8629-7b251984223f", "node_type": "1", "metadata": {}, "hash": "649f3e3f6eae647dc2c7cccb9a5c26373cc7764f54537472af0fd7698596f9b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44b33897-061e-42bb-aa09-288cb0daa2c4", "node_type": "1", "metadata": {}, "hash": "7aa4aafade0a30f65b28e4167949bd05a7901f2ee7b4ffb647e5204be6a73c39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}}, "text": "The retrieval process can not only help yield high-quality\nimages even for rare or unseen subjects, but also reduces\nthe parameter count and computational expense [45], [137],\n[148]\u2013[152], [331]. For GAN-based model, RetrieveGAN [45]\nemploys a differentiable retrieval process to select compatible\nimage patches for generation, with Gumbel-softmax trick and\nend-to-end training.", "start_char_idx": 555, "end_char_idx": 933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44b33897-061e-42bb-aa09-288cb0daa2c4": {"__data__": {"id_": "44b33897-061e-42bb-aa09-288cb0daa2c4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49fb7aa6-8e4c-40de-b2f3-cc2a83b89252", "node_type": "1", "metadata": {}, "hash": "2fa5ad0999650b12a5ee597b9f587280bafce0c57a515c53f7cb764b209cd64d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90470b79-c441-4938-a09b-0bed43975300", "node_type": "1", "metadata": {}, "hash": "d97cafae7b1bca2b682ff10bc186a04ba59ffd5d249a66097681d7788f5a55ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}}, "text": "IC-GAN [137] models data as a mix\nof conditional distributions around each training instance,\nconditioning both the generator and discriminator on these\ninstances, and can control the semantics and style by swap-\nping class labels or conditional instances. Recently, diffusion\nmodels beat GANs on image generation [332]. KNN-Diffusion\n[149] trains a text-to-image diffusion model without text data,\nby conditions the model on CLIP joint embedding of the\ninstance and k-nearest neighbors from image database.", "start_char_idx": 934, "end_char_idx": 1441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90470b79-c441-4938-a09b-0bed43975300": {"__data__": {"id_": "90470b79-c441-4938-a09b-0bed43975300", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44b33897-061e-42bb-aa09-288cb0daa2c4", "node_type": "1", "metadata": {}, "hash": "7aa4aafade0a30f65b28e4167949bd05a7901f2ee7b4ffb647e5204be6a73c39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a06ca537-930e-469e-8b48-e7a30a705046", "node_type": "1", "metadata": {}, "hash": "92c1d81820d166fdd215c7feb82cb56d5d68ae6ba36a3bf5378615625ca190b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}}, "text": "These\nk-NN embeddings bridge the text-image distribution gap and\nallow image generation from different domains by swapping\nthe database. Similarly, RDM [150] conditions diffusion or\nautoregressive models on CLIP embeddings of external image\ndatabases. It enables post-hoc conditioning on class labels,\ntext prompts and zero-shot stylization [151]. Beyond retrieving\nonly images, Re-imagen [148] conditions on both text prompts\nand retrieved image-text pairs for text-to-image generation.\nInterleaved classifier-free guidance is also proposed to balance\nthe alignment between text prompts and retrieval conditions.", "start_char_idx": 1442, "end_char_idx": 2055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a06ca537-930e-469e-8b48-e7a30a705046": {"__data__": {"id_": "a06ca537-930e-469e-8b48-e7a30a705046", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90470b79-c441-4938-a09b-0bed43975300", "node_type": "1", "metadata": {}, "hash": "d97cafae7b1bca2b682ff10bc186a04ba59ffd5d249a66097681d7788f5a55ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dc06d373-fc7c-4d14-b2cb-95d390096739", "node_type": "1", "metadata": {}, "hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "class_name": "RelatedNodeInfo"}}, "text": "To avoid information loss of CLIP embeddings and access\nto all visual condition, Retrieve&Fuse [331] concatenates\nthe retrieved conditional image and the noised image before\neach attention block in U-Net, and allowing interaction viaself-attention.", "start_char_idx": 2056, "end_char_idx": 2304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "397e8007-44af-4d39-827a-d0c027215c52": {"__data__": {"id_": "397e8007-44af-4d39-827a-d0c027215c52", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca", "node_type": "1", "metadata": {}, "hash": "c60ae4ed8fdcfe51dafce5c4af7675cbd46ee8f0bb30e601660346acd0633957", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca", "node_type": "1", "metadata": {}, "hash": "c60ae4ed8fdcfe51dafce5c4af7675cbd46ee8f0bb30e601660346acd0633957", "class_name": "RelatedNodeInfo"}}, "text": "RPG [79] retrieves representative images to\nconstruct informative in-context examples (i.e., image-region\npairs), and utilizes multi-modal chain-of-thought reasoning\n[333] to plan out complementary subregions for compositional\ntext-to-image diffusion.", "start_char_idx": 0, "end_char_idx": 251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2b2e28f-36c4-44d8-8e1d-f200916d2b17": {"__data__": {"id_": "f2b2e28f-36c4-44d8-8e1d-f200916d2b17", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "302545ac-ad76-44da-813d-2c45ea69dcb2", "node_type": "1", "metadata": {}, "hash": "bc383378ddced11c32265c187bc61c78aab6c5e9dd79b0cc874b0b212abcee0d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}}, "text": "2)Image Captioning :Image Captioning is the process of\ngenerating a textual description of an image.\nRetrieval-augmented image captioning typically synthesises\ndescription with a collection of retrieved captions, instead\ndepending only on the input image. MA [164] augments via\na memory bank, built with historical context and target word\nof image-text training set, and queried during inference with\nthe current context. V ocabulary distribution is computed based\non retrieved entries, and interpolated with the original predic-\ntion.", "start_char_idx": 0, "end_char_idx": 535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "302545ac-ad76-44da-813d-2c45ea69dcb2": {"__data__": {"id_": "302545ac-ad76-44da-813d-2c45ea69dcb2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2b2e28f-36c4-44d8-8e1d-f200916d2b17", "node_type": "1", "metadata": {}, "hash": "2d87addbd47e1638b3ab677261388b22c8684d16547131fa64cdde00c05114f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f3cf145-b04e-4360-857c-a098a8f9d4e9", "node_type": "1", "metadata": {}, "hash": "12bd7949a060e9cfc6589be7b308f0cf831db18ea708fee3dc06c8fb35cba6dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}}, "text": "In adversarial training, RAMP [334] employs retrieved\ncaptions as reference for discriminator training, prompting\nthe generator to make full use of retrieved captions. The\nmemory-augmented attention and copying mechanism are also\nexploited to better use. The RA-Transformer [46] and EXTRA\n[335], both retrieval-augmented transformer-based captioning\nmodels, utilize cross-attention over encoded retrieved captions.\nEXTRA, as depicted in Fig. 8, jointly process the image and\naman slopeSEP.\n.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f3cf145-b04e-4360-857c-a098a8f9d4e9": {"__data__": {"id_": "4f3cf145-b04e-4360-857c-a098a8f9d4e9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "302545ac-ad76-44da-813d-2c45ea69dcb2", "node_type": "1", "metadata": {}, "hash": "bc383378ddced11c32265c187bc61c78aab6c5e9dd79b0cc874b0b212abcee0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "205bf3a5-3e9b-4f95-a13b-506a843bbb2e", "node_type": "1", "metadata": {}, "hash": "2f7bf8a234dca68d77c97b1c2d3bdcf225399f814949b7d74df778417ac25acf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}}, "text": "8, jointly process the image and\naman slopeSEP.\n. aa\nBOSEOS\nskierskierheads\nmountainsmountains\nthe.\n.\nv1 w1wM v2 vN-1vN CLS.\n\"a man riding skis\ndown a snow covered slope\"Autoregressive\nLanguage DecoderVision-and-Language\nEncoder\nCross-Attention\nCurrent Image Retrieved Caption\n\"a man riding skis\ndown a snow covered slope\". .\"a couple of people with\nski 's standing in the snow\"DISTANCESINPUT\nIMAGE CAPTIONDatastore\n215.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "205bf3a5-3e9b-4f95-a13b-506a843bbb2e": {"__data__": {"id_": "205bf3a5-3e9b-4f95-a13b-506a843bbb2e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f3cf145-b04e-4360-857c-a098a8f9d4e9", "node_type": "1", "metadata": {}, "hash": "12bd7949a060e9cfc6589be7b308f0cf831db18ea708fee3dc06c8fb35cba6dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd6e7113-d963-43a0-a7f5-6ad2a59683af", "node_type": "1", "metadata": {}, "hash": "71f625ce05e2377efb7c4d573d190af8262c140bbe59ec678fe52230757616b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}}, "text": "hv1hv2 hvN-1hvNhw1hw2hwM-1hwM.\nFig. 8: Architecture of EXTRA [335] model.\nretrieved captions with V&L encoder, such that the decoder\nattends to both visual and linguistic contexts. Beyond retrieved\ncaptions, REVEAL [336] uniformly encodes and retrieves\nmulti-modal world knowledge, including image-text pairs,\nquestion answering pairs, and knowledge graph triplets, which\nis then integrated with image features by retrieval score-\naware attention module.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd6e7113-d963-43a0-a7f5-6ad2a59683af": {"__data__": {"id_": "fd6e7113-d963-43a0-a7f5-6ad2a59683af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "205bf3a5-3e9b-4f95-a13b-506a843bbb2e", "node_type": "1", "metadata": {}, "hash": "2f7bf8a234dca68d77c97b1c2d3bdcf225399f814949b7d74df778417ac25acf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b646a538-109b-47c0-ad7a-ac2283530d47", "node_type": "1", "metadata": {}, "hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "class_name": "RelatedNodeInfo"}}, "text": "Straightforwardly, SMALLCAP [47]\nemploys a CLIP vision encoder and a LLM decoder, linked\nby trainable cross-attention layers, where retrieved captions\nserve as input-specific in-context examples for prompt for en-\nhancement.", "start_char_idx": 1881, "end_char_idx": 2105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a227faa-e6f0-439d-86e9-94aa2b4ba4c7": {"__data__": {"id_": "7a227faa-e6f0-439d-86e9-94aa2b4ba4c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5216234a-e278-4358-b80c-1309c790e54e", "node_type": "1", "metadata": {}, "hash": "72b02ec6981491005b3f5c95435a9f1171570e1eda6e058df6221fe6d1d05170", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}}, "text": "For remote sensing image, CRSR [337] enhances\nretrieved captions with semantic refinement, i.e. filters out\nmisleading details and emphasizes visually salient content.\nBesides, the visual features is also enriched by transformer\nnetwork with learnable queries, capturing more intricate details\nwithin the images19\n3)Others :There also exist many retrieval augmented\nworks for other image-related tasks.", "start_char_idx": 0, "end_char_idx": 402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5216234a-e278-4358-b80c-1309c790e54e": {"__data__": {"id_": "5216234a-e278-4358-b80c-1309c790e54e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a227faa-e6f0-439d-86e9-94aa2b4ba4c7", "node_type": "1", "metadata": {}, "hash": "ffb23b90835b4e19169323e10e9bafa8967ace6f061bc963630ab0ac6974e036", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7cc3307-ed1b-4e3c-93ac-40875c2d3796", "node_type": "1", "metadata": {}, "hash": "c860231e3bea18303a5cd2c93b08d2a6e30193260c92536f247ed7ed2a70a2d4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}}, "text": "For visual question\nanswering (VQA), PICa [338] leverages GPT-3\u2019s implicit\nknowledge retrieval and reasoning capabilities, which con-\nverts images into textual descriptions, then prompts GPT-3\nto predict answers based on these descriptions and the ques-\ntion, finally ensembles multi-query results. RA-VQA [339]\nidentifies a limitation that the retrieval in previous work is\ntrained separately from answer generation, and propose a joint\ntraining scheme that integrates differentiable retrieval with\nanswer generation, enabling end-to-end training.", "start_char_idx": 403, "end_char_idx": 951, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7cc3307-ed1b-4e3c-93ac-40875c2d3796": {"__data__": {"id_": "a7cc3307-ed1b-4e3c-93ac-40875c2d3796", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5216234a-e278-4358-b80c-1309c790e54e", "node_type": "1", "metadata": {}, "hash": "72b02ec6981491005b3f5c95435a9f1171570e1eda6e058df6221fe6d1d05170", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5e16604-7e38-4ab2-8e19-a62413f43be8", "node_type": "1", "metadata": {}, "hash": "5ee101198383d6ca2370f84ed48e4be0ffe6a2da74daf79a511608317e5893d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}}, "text": "For visually\ngrounded dialogue, KNN-based Information Fetching (KIF)\n[340] enhances generative Transformer for dialog modeling.\nEach KIF module learns a read operation to access fixed\nexternal knowledge. Maria [341], a neural conversation agent,\nenhances dialog generation by leveraging visual experiences\nretrieved from a large-scale image index as extra context.", "start_char_idx": 952, "end_char_idx": 1316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5e16604-7e38-4ab2-8e19-a62413f43be8": {"__data__": {"id_": "b5e16604-7e38-4ab2-8e19-a62413f43be8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7cc3307-ed1b-4e3c-93ac-40875c2d3796", "node_type": "1", "metadata": {}, "hash": "c860231e3bea18303a5cd2c93b08d2a6e30193260c92536f247ed7ed2a70a2d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "659bcfea-85b8-403c-bc6f-60b89fc4ffcd", "node_type": "1", "metadata": {}, "hash": "1c354d09d92bef3ad875a3d1d68361a7d38e4183407b042a1eafe7eb29ad208f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}}, "text": "For multi-modal machine translation, which aims to improve\nNMT with multi-modal information, [342] incorporates visual\ninformation at the phrase level to address the sparsity of paired\nsentence-image, employing a conditional V AE to filters out\nredundant visual information from sentence-image datasets.\nE.RAG for Video\n1)Video Captioning :Video captioning is to describe the\nvisual content with a descriptive utterance. KaVD [343]\ngenerates news video caption with background knowledge\nmined from topically related documents such as named entities\nand events.", "start_char_idx": 1317, "end_char_idx": 1877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "659bcfea-85b8-403c-bc6f-60b89fc4ffcd": {"__data__": {"id_": "659bcfea-85b8-403c-bc6f-60b89fc4ffcd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5e16604-7e38-4ab2-8e19-a62413f43be8", "node_type": "1", "metadata": {}, "hash": "5ee101198383d6ca2370f84ed48e4be0ffe6a2da74daf79a511608317e5893d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc", "node_type": "1", "metadata": {}, "hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "class_name": "RelatedNodeInfo"}}, "text": "R-ConvED [48] introduces retrieval-augmented\nmechanism to facilitate the word prediction. It uses Dual\nEncoding [109] for video-text retrieval, and proposes a convo-\nlutional encoder-decoder network for generation. For a given\ninput video, R-ConvED first retrieves top-k relevant sentences\nand their corresponding video from training set, then feeds\nthese pairs and the input video into the generator separately.", "start_char_idx": 1878, "end_char_idx": 2290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "777cf26e-c814-4055-ad5e-732d45dba164": {"__data__": {"id_": "777cf26e-c814-4055-ad5e-732d45dba164", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab6eb898-4f1a-4cb8-8ea8-e3b33547d7e3", "node_type": "1", "metadata": {}, "hash": "ddc6cfcc0005f3fb6ec0474ccb6d775fb1a6e0525e02508ce376f9727deea40c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}}, "text": "The obtained decoder hidden states are combined through\nattention-like read operation, so that the target word can be\npredicted using the final representation. CARE [160] utilizes\nvisual encoder, audio encoder, and text encoder for frame,\naudio, and retrieved texts, respectively. It uses CLIP as re-\ntriever, and transformer decoder as generator. The embeddings\nof the three modalities are combined to augment the decoder,\nproducing global semantic guidance which attends the input\nembedding, and local semantic guidance which attends the\nattention layer.", "start_char_idx": 0, "end_char_idx": 556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab6eb898-4f1a-4cb8-8ea8-e3b33547d7e3": {"__data__": {"id_": "ab6eb898-4f1a-4cb8-8ea8-e3b33547d7e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "777cf26e-c814-4055-ad5e-732d45dba164", "node_type": "1", "metadata": {}, "hash": "acc09292f676e1ffea0794cb34400731fd86968bc22364173d15a53ecdd65676", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdcb691e-6311-4440-a5cd-ae83b3f5a07a", "node_type": "1", "metadata": {}, "hash": "a36b46800ce88f00acdc7d1391739a35a19452fd9aca6b16667c158dc2fb244a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}}, "text": "EgoInstructor [49] generates captions for first-\nperson view videos. It retrieves relevant exocentric videos\nand corresponding texts via dense retrieval, then encodes the\ninput egocentric video and the retrieved exocentric videos\nthrough a CLIP-based visual encoder and a transformer-\ndecoder-based bottleneck module. Then it generates captions\nthrough decoder-based LLM which takes the retrieved texts\nas input and interacts with encoded videos in gated cross-\nattention layer.\n2)Video QA&Dialogue :Video QA&Dialogue generates\nsingle or multiple-round responses in alignment with video\ncontent.", "start_char_idx": 557, "end_char_idx": 1152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdcb691e-6311-4440-a5cd-ae83b3f5a07a": {"__data__": {"id_": "fdcb691e-6311-4440-a5cd-ae83b3f5a07a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab6eb898-4f1a-4cb8-8ea8-e3b33547d7e3", "node_type": "1", "metadata": {}, "hash": "ddc6cfcc0005f3fb6ec0474ccb6d775fb1a6e0525e02508ce376f9727deea40c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67eb05b7-2887-4807-a2d6-8703d4e2af90", "node_type": "1", "metadata": {}, "hash": "90db2db81321c29a3625ac42af3f5ab5e5eece66320a591113f02c2ad815e3b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}}, "text": "For video question answering (VideoQA), MA-DRNN\n[344] leverages the differentiable neural computer (DNC) with\nan external memory, for storing and retrieving useful infor-\nmation in questions and videos, and modeling the long-termvisual-textual dependence. Given the video input, R2A [345]\nretrieves semantically similar texts by multi-modal model, e.g.\nCLIP, and query LLM with both the question and the retrieved\ntexts.", "start_char_idx": 1153, "end_char_idx": 1573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67eb05b7-2887-4807-a2d6-8703d4e2af90": {"__data__": {"id_": "67eb05b7-2887-4807-a2d6-8703d4e2af90", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdcb691e-6311-4440-a5cd-ae83b3f5a07a", "node_type": "1", "metadata": {}, "hash": "a36b46800ce88f00acdc7d1391739a35a19452fd9aca6b16667c158dc2fb244a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d886886d-ff01-4653-bb1a-adaea32bfb9d", "node_type": "1", "metadata": {}, "hash": "1e6d2cd3af9d14b626de8fcfeae80e420e8d121517c3b3e6303acb294e0c57ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}}, "text": "CLIP, and query LLM with both the question and the retrieved\ntexts. For video-grounded dialogue, [346] proposes TVQA+\ndataset which enables to retrieve relevant moments and visual\nconcepts to answer questions about videos, and also proposes\nSpatio-Temporal Answerer with Grounded Evidence (STAGE)\nto exploit it. VGNMN [347] also extracts visual cues from\nvideos, while the retrieval is carried out using neural module\nnetworks parameterized by entities and actions in previous\ndialogues.", "start_char_idx": 1506, "end_char_idx": 1993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d886886d-ff01-4653-bb1a-adaea32bfb9d": {"__data__": {"id_": "d886886d-ff01-4653-bb1a-adaea32bfb9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67eb05b7-2887-4807-a2d6-8703d4e2af90", "node_type": "1", "metadata": {}, "hash": "90db2db81321c29a3625ac42af3f5ab5e5eece66320a591113f02c2ad815e3b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fc9fe157-ab09-4904-b323-e8019ece8204", "node_type": "1", "metadata": {}, "hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "class_name": "RelatedNodeInfo"}}, "text": "3)Others :There also exist many retrieval augmented\nworks for other video-related tasks. VidIL [348] exploits\nimage-language models to translate video content into\ntemporal-aware prompts with few-shot examples, for vari-\nous video-language tasks including video captioning, video\nquestion answering, video caption retrieval, and video fu-\nture event prediction.", "start_char_idx": 1994, "end_char_idx": 2355, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a5ec0e0-ea1e-478b-9d66-97f0fb1cfca3": {"__data__": {"id_": "9a5ec0e0-ea1e-478b-9d66-97f0fb1cfca3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14288937-76a6-415e-b127-6be38d179ae5", "node_type": "1", "metadata": {}, "hash": "f2e85e1f011008644bd6e77d8b4dbed46579f2a38148f980d78a15a853e342d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}}, "text": "Notably, for trustworthy autonomous\ndriving, RAG-Driver [349] grounds the MLLM in relevant\nexpert demonstrations from a memory database, to produce\ndriving action explanations, justifications, and control signal\nprediction. Text-to-video generation is to generate video given\nnatural language descriptions. As shown in Fig. 9, Animate-\nPlot 1Motion structure retrieval\nStoryboard descriptionStructure-guidedtext-to-video synthesisText promptsVideo databaseText queriesPlot iPlot n\u22ef\u22ef\n\u22ef\u22efStory script\nFig.", "start_char_idx": 0, "end_char_idx": 502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14288937-76a6-415e-b127-6be38d179ae5": {"__data__": {"id_": "14288937-76a6-415e-b127-6be38d179ae5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a5ec0e0-ea1e-478b-9d66-97f0fb1cfca3", "node_type": "1", "metadata": {}, "hash": "a8dc18a87cea9bdda30c0a9e0da96ab38cc14062c117652010447a571dbdb7dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed353fdf-d90b-4d26-9c0f-227c3236f79d", "node_type": "1", "metadata": {}, "hash": "62fdaf3c02618b87c732fe7e851354a9a4e77666b856cdb7f27174241fec6e8b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}}, "text": "9: Architecture of Animate-A-Story [206] model.\nA-Story [206] develops a framework which can generate high-\nquality storytelling videos based on texts. It first separates the\ntext into individual plots, and decorates the description using\nLLM. It then retrieves relevant videos for each plot through\na dense retriever [110]. It generates videos through a latent\ndiffusion model, consisting of two branches: a text encoder20\nCLIP, and a structure encoder which takes the estimated depth\nof the retrieved videos as structure control.", "start_char_idx": 503, "end_char_idx": 1034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed353fdf-d90b-4d26-9c0f-227c3236f79d": {"__data__": {"id_": "ed353fdf-d90b-4d26-9c0f-227c3236f79d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14288937-76a6-415e-b127-6be38d179ae5", "node_type": "1", "metadata": {}, "hash": "f2e85e1f011008644bd6e77d8b4dbed46579f2a38148f980d78a15a853e342d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8633af80-72a5-42dd-8ad4-362e972bfc14", "node_type": "1", "metadata": {}, "hash": "5020e1dd3eaf177794475d9b17e8cb34a6ff91027da1d83c4ee2afe0a500a62e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}}, "text": "F .RAG for Audio\n1)Audio Generation :The goal of audio generation is to\ngenerate audio with natural language input.\n\u201cA bottle of champagne is popped and then poured into a glass\u201dInputprompt\nOutputWaveform\nCLAPEncoder\nDatabaseAudio FeatureLanguageFeature\u201cSome water pure into the glass\u201d\n\u201cWater pure into the glass\u201d\n\u201cA champagne is popped while a man talks\u201d\nVAEDecoderHiFi-GANRetrievalAudioMAET5Audio & Language FeatureLDMCrossAttention\nFig. 10: Architecture of Re-AudioLDM [159] model.", "start_char_idx": 1035, "end_char_idx": 1519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8633af80-72a5-42dd-8ad4-362e972bfc14": {"__data__": {"id_": "8633af80-72a5-42dd-8ad4-362e972bfc14", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed353fdf-d90b-4d26-9c0f-227c3236f79d", "node_type": "1", "metadata": {}, "hash": "62fdaf3c02618b87c732fe7e851354a9a4e77666b856cdb7f27174241fec6e8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c488c930-29d8-4119-800f-e2a195f83bee", "node_type": "1", "metadata": {}, "hash": "ed387b66755b48b0ded2e5c36528bfedebcb85ee626460ebe25911f1aa82c2c1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}}, "text": "10: Architecture of Re-AudioLDM [159] model.\nRe-AudioLDM [159] adopts dense retriever CLAP [26] to\nretrieve similar caption-audio pairs given input prompt. As\nshown in Fig. 10, the generator, latent diffusion model and\nV AE-based decoder, take the representations of input text and\nretrieved pairs as input and generate output audio. Make-An-\nAudio [44] uses dense retriever CLAP [26] to augment data,\nretrieving related audio given natural language text.", "start_char_idx": 1475, "end_char_idx": 1930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c488c930-29d8-4119-800f-e2a195f83bee": {"__data__": {"id_": "c488c930-29d8-4119-800f-e2a195f83bee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8633af80-72a5-42dd-8ad4-362e972bfc14", "node_type": "1", "metadata": {}, "hash": "5020e1dd3eaf177794475d9b17e8cb34a6ff91027da1d83c4ee2afe0a500a62e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d4055d84-e057-42ea-901f-7ba1753df337", "node_type": "1", "metadata": {}, "hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "class_name": "RelatedNodeInfo"}}, "text": "It then\nconstructs pseudo prompts for diffusion-based text-to-audio\nmodel training.", "start_char_idx": 1931, "end_char_idx": 2014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8a28e70-6de3-4ea3-b8bb-68c535a48178": {"__data__": {"id_": "b8a28e70-6de3-4ea3-b8bb-68c535a48178", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c98e98ee-03b5-4bf9-a652-999317a0b671", "node_type": "1", "metadata": {}, "hash": "70828131a4e089a5fd70194bf07328bef70c911b67ea89eae6f21470b4185c0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c98e98ee-03b5-4bf9-a652-999317a0b671", "node_type": "1", "metadata": {}, "hash": "70828131a4e089a5fd70194bf07328bef70c911b67ea89eae6f21470b4185c0c", "class_name": "RelatedNodeInfo"}}, "text": "It then\nconstructs pseudo prompts for diffusion-based text-to-audio\nmodel training.\n2)Audio Captioning :The goal of audio captioning is\nto generate natural language data with audio data, which is\nbasically a sequence-to-sequence task. RECAP [350] leverages\nCLAP [26] to retrieve related captions given audio data.", "start_char_idx": 0, "end_char_idx": 313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92be1258-6cac-48f4-9fc8-c2e56846fb94": {"__data__": {"id_": "92be1258-6cac-48f4-9fc8-c2e56846fb94", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdc7067e-8cb5-4467-8e3e-90737e05f7f2", "node_type": "1", "metadata": {}, "hash": "713d2e9a5a11988662306a934bd30ed4098a19a154b88b70a1a20c14e14b3c1c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}}, "text": "The\nretrieved captions are then included into the prompt input for\nGPT-2 model, which interacts with audio embeddings through\ncross attention. In [43], dense retriver VGGish [107] is adopted\nto produce dense embedding of audio data, and GPT-2 is\nadopted to generate representations of the retrieved captions\nwhich are paired with similar audios. After obtaining the\nrepresentations of audios and captions, an extra multi-head\nattention block and a linear layer fuses all the information\nand generates the output.", "start_char_idx": 0, "end_char_idx": 512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdc7067e-8cb5-4467-8e3e-90737e05f7f2": {"__data__": {"id_": "bdc7067e-8cb5-4467-8e3e-90737e05f7f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92be1258-6cac-48f4-9fc8-c2e56846fb94", "node_type": "1", "metadata": {}, "hash": "8cd0f612e6794f2d275bc30e52ef6cffdd6bb76955978871e9a35d5489de5274", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04338204-b1ff-477a-b016-8e7de89b26d5", "node_type": "1", "metadata": {}, "hash": "74ae8f85b1714060d332c2f5818befb7fc9b486f415fb86caa66924ff5e6cf2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}}, "text": "Some research studies transform\naudio modality to text, in order to leverage advancements\nin LLMs [351]\u2013[353]. They take advantage of deep retrieval\nmodels, aligning the modalities into the same latent space for\ndownstream text generation.\nG.RAG for 3D\n1)Text-to-3D :Retrieval can be applied to augment the\ngeneration of 3D contents. ReMoDiffuse [51] aims at gener-\nating motions using diffusion models.", "start_char_idx": 513, "end_char_idx": 916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04338204-b1ff-477a-b016-8e7de89b26d5": {"__data__": {"id_": "04338204-b1ff-477a-b016-8e7de89b26d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdc7067e-8cb5-4467-8e3e-90737e05f7f2", "node_type": "1", "metadata": {}, "hash": "713d2e9a5a11988662306a934bd30ed4098a19a154b88b70a1a20c14e14b3c1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67cd8374-8b19-4e27-b88d-659eaf35c055", "node_type": "1", "metadata": {}, "hash": "f99d525051e1ed451734b0e9fd9683f4232e9362bf8a92182a8c84a4a1e843c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}}, "text": "ReMoDiffuse [51] aims at gener-\nating motions using diffusion models. It first retrieves relevant\nmotion entites through CLIP given text input, then leverages\nthe information of the text and the retrieved entities through\na semantic-modulated attention layer.\nAMD [158] designs two branches of motion diffusion for\nfidelity and diversity. As shown in Fig. 11, the first branch\ninputs the original prompt text for diffusion; the second branch\ndecomposes the input text into anatomical scripts and retrieve\nsimilar reference motions for diffusion.", "start_char_idx": 847, "end_char_idx": 1392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67cd8374-8b19-4e27-b88d-659eaf35c055": {"__data__": {"id_": "67cd8374-8b19-4e27-b88d-659eaf35c055", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04338204-b1ff-477a-b016-8e7de89b26d5", "node_type": "1", "metadata": {}, "hash": "74ae8f85b1714060d332c2f5818befb7fc9b486f415fb86caa66924ff5e6cf2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25335f95-389e-4dfd-a165-a8c49e4ad787", "node_type": "1", "metadata": {}, "hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "class_name": "RelatedNodeInfo"}}, "text": "A transformer-based\nfusion module is further applied to adaptively balance the\nresult from two branches. RetDream [50] targets general 3D\ngeneration, using retrieved 3D assets to augment the process\n...A man is pretending to \nbe a chicken , constantly \npecking at the ground \nand waving his arms like a \nchicken.\n1)Amanlowers his head     \ntowards the ground.\n2) ... opens and closes \nhis mouth rapidly.\n3) ... moves his head up and \ndown, mimicking a \npecking motion .", "start_char_idx": 1393, "end_char_idx": 1862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7aec402f-4bb8-4eb0-b627-ed7989264302": {"__data__": {"id_": "7aec402f-4bb8-4eb0-b627-ed7989264302", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b429fff9-0da0-4633-af64-90ad7e215920", "node_type": "1", "metadata": {}, "hash": "6b10c3b746dfdafe48deb59742440fbfac0a9cf84231f58ca8932e71467db6a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}}, "text": "3) ... moves his head up and \ndown, mimicking a \npecking motion .\n4) . flaps his arms up \nand down ,imitating a \nchicken's wings .\ud835\udc61 MLPCross\nAttentionLinearFusion Block\nTransformer", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b429fff9-0da0-4633-af64-90ad7e215920": {"__data__": {"id_": "b429fff9-0da0-4633-af64-90ad7e215920", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7aec402f-4bb8-4eb0-b627-ed7989264302", "node_type": "1", "metadata": {}, "hash": "92c6cc55abd2f8ec86af9cbbe0df8a64e1825894463c00afcc99ef6099a65437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13274157-7229-45b0-8997-6fdb2dbae214", "node_type": "1", "metadata": {}, "hash": "ea76a66ba11bdba8a29eb6070d2f0fe53c943a6e0d1574c027b5ec4549f496e9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc61 MLPCross\nAttentionLinearFusion Block\nTransformer  EncoderCLIP\nText\nCLIP\nText\nLinear\ud835\udc65\ud835\udc611\ud835\udc65\ud835\udc612\ud835\udc65\ud835\udc613\ud835\udc65\ud835\udc61\ud835\udc41\n\ud835\udc5a1\ud835\udc5a2\ud835\udc67\ud835\udc61\ud835\udc58\ud835\udc60\ud835\udc67\ud835\udc61\ud835\udc58\ud835\udc59\n\ud835\udc5a3\ud835\udc5a\ud835\udc45", "start_char_idx": 132, "end_char_idx": 247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13274157-7229-45b0-8997-6fdb2dbae214": {"__data__": {"id_": "13274157-7229-45b0-8997-6fdb2dbae214", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b429fff9-0da0-4633-af64-90ad7e215920", "node_type": "1", "metadata": {}, "hash": "6b10c3b746dfdafe48deb59742440fbfac0a9cf84231f58ca8932e71467db6a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f567d5c9-2dc1-47a3-9193-897d26055ee0", "node_type": "1", "metadata": {}, "hash": "2bf5d5f4d1bbd8192e7da5f07ad81a58d462885c505647410b4bed13178563d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}}, "text": "\u2295\u0ddc\ud835\udc6501\n\u0ddc\ud835\udc6502\n\u0ddc\ud835\udc6503\n\u0ddc\ud835\udc650\ud835\udc41\u22ef\u22ef\n\u22ef\n\u22ef\n\u22ef\u22efLinear\nLinearTransformer  EncoderFusion Block\u2295Origin  Motion Diffusion\nFeature FusionText Decomposition\nfine -tuned\nSelf \nAttention\nDropout\nLayer Norm\nLinear\nGELU\nLayer NormDropout\nLinear\nFusion Block", "start_char_idx": 248, "end_char_idx": 477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f567d5c9-2dc1-47a3-9193-897d26055ee0": {"__data__": {"id_": "f567d5c9-2dc1-47a3-9193-897d26055ee0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13274157-7229-45b0-8997-6fdb2dbae214", "node_type": "1", "metadata": {}, "hash": "ea76a66ba11bdba8a29eb6070d2f0fe53c943a6e0d1574c027b5ec4549f496e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a396e262-669d-4c9d-9a1c-17da560b9118", "node_type": "1", "metadata": {}, "hash": "bbb63ca1a3b8ff529d8b39f3fa9fabcc5e69078985e1e96210a64788ba7c000e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}}, "text": "Norm\nLinear\nGELU\nLayer NormDropout\nLinear\nFusion Block optional\u2131\ud835\udc592\n\u2131\ud835\udc60\u2131\ud835\udc61\ud835\udc9e\ud835\udc59\n\ud835\udc9e\ud835\udc60\nHybrid Retrieval\u2130\ud835\udc60\u2130\ud835\udc59\u2131\ud835\udc591\ud835\udc5d\ud835\udf031\nSearch with Anatomical TextBest Match\nDataset\ud835\udc5a1:\ud835\udc45Reference Action \nTokensRandom Select\nComplex and Decomposed\nText Features\nDiffused Motion and \nReference Action Features", "start_char_idx": 423, "end_char_idx": 697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a396e262-669d-4c9d-9a1c-17da560b9118": {"__data__": {"id_": "a396e262-669d-4c9d-9a1c-17da560b9118", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f567d5c9-2dc1-47a3-9193-897d26055ee0", "node_type": "1", "metadata": {}, "hash": "2bf5d5f4d1bbd8192e7da5f07ad81a58d462885c505647410b4bed13178563d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1817d0b-a1d0-4e69-8562-0a506d10f3e7", "node_type": "1", "metadata": {}, "hash": "baf0d0e5f9874e48d3e8a0ae8d89ce07bc6512843bd06bcdbb077fa21e0cb509", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}}, "text": "Select\nComplex and Decomposed\nText Features\nDiffused Motion and \nReference Action Features \ud835\udc65\ud835\udc611:\ud835\udc41Diffused Motion Data\ud835\udc58,\ud835\udc63\n\ud835\udc5e\nReference Motion Diffusion\ud835\udc5d\ud835\udf032Fig. 11: Architecture of AMD [158] model.\nof variational score distillation from 2D diffusion models.\nGiven an input query, it retrieves relevant 3D assets through\nCLIP, then utilizes the retrieved assets to provide geometric\nprior and adapted 2D prior.", "start_char_idx": 607, "end_char_idx": 1011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1817d0b-a1d0-4e69-8562-0a506d10f3e7": {"__data__": {"id_": "a1817d0b-a1d0-4e69-8562-0a506d10f3e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a396e262-669d-4c9d-9a1c-17da560b9118", "node_type": "1", "metadata": {}, "hash": "bbb63ca1a3b8ff529d8b39f3fa9fabcc5e69078985e1e96210a64788ba7c000e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa", "node_type": "1", "metadata": {}, "hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "class_name": "RelatedNodeInfo"}}, "text": "Concretely, retrieved assets not\nonly impose an additional velocity on particles for distribution\ninitialization, but also help optimize the 2D diffusion model\nthrough low-rank adaptation.", "start_char_idx": 1012, "end_char_idx": 1200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6760dec7-64fb-4236-8b3a-0d596faa42fa": {"__data__": {"id_": "6760dec7-64fb-4236-8b3a-0d596faa42fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d643c3b-afb5-4c96-be4e-f5d35e44f22b", "node_type": "1", "metadata": {}, "hash": "e259dae028f671e24a833c32bc349fd5bc0d5edb563ae13cb79f4be8228ad506", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}}, "text": "H.RAG for Science\nRAG has also emerged as a promising research direction\nfor many interdisciplinary applications, such as molecular\ngeneration, medical tasks and computational research.\n1)Drug Discovery :The goal of drug discovery is to\ngenerate molecules that concurrently fulfill diverse properties.", "start_char_idx": 0, "end_char_idx": 301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d643c3b-afb5-4c96-be4e-f5d35e44f22b": {"__data__": {"id_": "4d643c3b-afb5-4c96-be4e-f5d35e44f22b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6760dec7-64fb-4236-8b3a-0d596faa42fa", "node_type": "1", "metadata": {}, "hash": "565a1fc0ca9660b9e84cef013466a7b26ee224161c074daadb84e05b26ebaf59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "139b6acd-1363-4fe5-9b28-dc397c0a70e1", "node_type": "1", "metadata": {}, "hash": "19b5312e829790b1abc9d8bcdcfde1dcf125d3ef387cc0ac56dd9bce31a90502", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}}, "text": "RetMol [55] integrates a lightweight retrieval mechanism and\n-4.9 kcal/molRetrieval databaseRetrieverInformation fusionDecoderEncoder\n-8.4 kcal/mol-10.3 kcal/mol-10.9 kcal/molEncoderShared  weightsInput molecule\nRetrieved exemplar moleculesInput embedding\nRetrieved embeddingsFused embedding-8.4 kcal/mol\nOutput molecule\nRetrieval modulePre-trained module\nFig. 12: Architecture of RetMol [55] model.", "start_char_idx": 302, "end_char_idx": 701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "139b6acd-1363-4fe5-9b28-dc397c0a70e1": {"__data__": {"id_": "139b6acd-1363-4fe5-9b28-dc397c0a70e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d643c3b-afb5-4c96-be4e-f5d35e44f22b", "node_type": "1", "metadata": {}, "hash": "e259dae028f671e24a833c32bc349fd5bc0d5edb563ae13cb79f4be8228ad506", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79244573-93ef-474d-b526-9fad3bb61d58", "node_type": "1", "metadata": {}, "hash": "e8e5317db72c541c28e57aefa4f6752d9b642b49b1edbe4954300336dcc96209", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}}, "text": "12: Architecture of RetMol [55] model.\nmolecular strings into a pre-trained encoder-decoder gener-\native model to retrieve and fuse exemplar molecules with\nthe input. PromptDiff [354] introduces an interaction-based,\nretrieval-augmented 3D molecular diffusion model that re-\ntrieves a curated set of ligand references to guide the synthesis\nof ligands meeting specific design criteria.", "start_char_idx": 663, "end_char_idx": 1048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79244573-93ef-474d-b526-9fad3bb61d58": {"__data__": {"id_": "79244573-93ef-474d-b526-9fad3bb61d58", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "139b6acd-1363-4fe5-9b28-dc397c0a70e1", "node_type": "1", "metadata": {}, "hash": "19b5312e829790b1abc9d8bcdcfde1dcf125d3ef387cc0ac56dd9bce31a90502", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50351b61-7460-48e7-ab37-0b865bdee1f3", "node_type": "1", "metadata": {}, "hash": "31ae0c605b4b880a0307a579f48a4c43f2a409d9d941e4d47bb63e6070c2b4ca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}}, "text": "2)Biomedical Informatics Enhancement :Several re-\ncent studies have improved the expressiveness of LLM\nby retrieving information from biomedical domain-specific\ndatabases, thereby augmenting the model\u2019s capabilities to\nprovide valuable guidance for tasks in the medical field.\nPoET [355] is an autoregressive generative model based on\na variant of Transformer that integrates a retrieval mech-\nanism to enable prompt augmentation, thereby expediting\nthe prediction of fitness properties for protein variants.", "start_char_idx": 1049, "end_char_idx": 1557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50351b61-7460-48e7-ab37-0b865bdee1f3": {"__data__": {"id_": "50351b61-7460-48e7-ab37-0b865bdee1f3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79244573-93ef-474d-b526-9fad3bb61d58", "node_type": "1", "metadata": {}, "hash": "e8e5317db72c541c28e57aefa4f6752d9b642b49b1edbe4954300336dcc96209", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2bb9ba0-e566-4971-82e0-7da09e2666fe", "node_type": "1", "metadata": {}, "hash": "1b1c45fae9e01893f0142260fb929b4e7d65dc64e53a288037414d4cd0e9e9f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}}, "text": "Chat-Orthopedist [136] enhances ChatGPT with a retrieval-\naugmented mechanism focused on adolescent idiopathic sco-\nliosis (AIS), utilizing an external knowledge base for precise\nresponses. BIOREADER [356] is the first retrieval-enhanced\ntext-to-text transformer-based model for biomedical natural21\nlanguage processing, incorporating the retrieved literature\nevidence into the model using a chunked-cross attention\nmechanism.", "start_char_idx": 1558, "end_char_idx": 1984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2bb9ba0-e566-4971-82e0-7da09e2666fe": {"__data__": {"id_": "c2bb9ba0-e566-4971-82e0-7da09e2666fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50351b61-7460-48e7-ab37-0b865bdee1f3", "node_type": "1", "metadata": {}, "hash": "31ae0c605b4b880a0307a579f48a4c43f2a409d9d941e4d47bb63e6070c2b4ca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f", "node_type": "1", "metadata": {}, "hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "class_name": "RelatedNodeInfo"}}, "text": "MedWriter [357] employs a hierarchical retrieval-\naugmented generation method that combines report-level and\nsentence-level templates to produce coherent and clinically\naccurate medical reports from images. QA-RAG [358] em-\nploys a dual-track RAG strategy to enhance pharmaceutical\ncompliance by effectively retrieving and integrating regula-\ntory guidelines based on language model responses and user\nqueries.", "start_char_idx": 1985, "end_char_idx": 2395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12baea84-1c2e-406e-9fe6-d6c5ae40afe7": {"__data__": {"id_": "12baea84-1c2e-406e-9fe6-d6c5ae40afe7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2b02493-0913-4aa1-a942-76a5693942e6", "node_type": "1", "metadata": {}, "hash": "19e1a97de4bd3b6cd38a3d4a6a06edae0fddbbfe16a0808a0c15ec04f3174ed6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}}, "text": "3)Math Applications :Retrieval-augmented generation\ntechnology in mathematics streamlines problem-solving,\nboosts research innovation, and refines educational strategies.\nLeanDojo [359] boosts theorem proving by using retrieval-\naugmented methods to choose relevant premises from exten-\nsive mathematical libraries, improving automation and theo-\nrem generalization. RAG-for-math-QA [360] improves math\nquestion-answering by integrating a high-quality math text-\nbook with retrieval-augmented generation, enhancing LLM-\ngenerated responses for middle-school algebra and geometry.", "start_char_idx": 0, "end_char_idx": 579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2b02493-0913-4aa1-a942-76a5693942e6": {"__data__": {"id_": "a2b02493-0913-4aa1-a942-76a5693942e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12baea84-1c2e-406e-9fe6-d6c5ae40afe7", "node_type": "1", "metadata": {}, "hash": "2aeede78d038ba29f40fcaaf9ce6e9204b6a52cd0f6c0d514fa4736fb60543a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2d1d2c9-03fc-4370-8b49-bf0a6482fc52", "node_type": "1", "metadata": {}, "hash": "31968708e17aa7e3e46b6f80cbc31b8100b1dfece13d693db0719b73693f8245", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}}, "text": "V. B ENCHMARK\nGiven the increasing research interests and applications of\nRAG, there have also been several benchmarks assessing RAG\nfrom certain aspects.\nChen et al. [361] proposed an RAG benchmark, which\nevaluates RAG from four aspects: Noise Robustness, Neg-\native Rejection, Information Integration, and Counterfactual\nRobustness, respectively. Noise Robustness evaluates whether\nLLMs could extract the necessary information from documents\ncontaining noisy information. The noisy information is rele-\nvant to the input query but useless for answering it.", "start_char_idx": 580, "end_char_idx": 1138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2d1d2c9-03fc-4370-8b49-bf0a6482fc52": {"__data__": {"id_": "b2d1d2c9-03fc-4370-8b49-bf0a6482fc52", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2b02493-0913-4aa1-a942-76a5693942e6", "node_type": "1", "metadata": {}, "hash": "19e1a97de4bd3b6cd38a3d4a6a06edae0fddbbfe16a0808a0c15ec04f3174ed6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20beb164-b808-4fab-bdb1-2cab9205485b", "node_type": "1", "metadata": {}, "hash": "84f799769391b9be09a02d6763ab27bae4d2948e81d675f40d5c82ffc66e2cf2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}}, "text": "The noisy information is rele-\nvant to the input query but useless for answering it. Negative\nRejection measures whether LLMs would reject to respond the\nquery when the retrieved content is not enough. Information\nIntegration assesses whether LLMs could acquire knowledge\nand make responses by integrating multiple retrieved contents.\nCounterfactual Robustness refers to the ability of LLMs to\nidentify counterfactual errors in the retrieved content.", "start_char_idx": 1054, "end_char_idx": 1504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20beb164-b808-4fab-bdb1-2cab9205485b": {"__data__": {"id_": "20beb164-b808-4fab-bdb1-2cab9205485b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2d1d2c9-03fc-4370-8b49-bf0a6482fc52", "node_type": "1", "metadata": {}, "hash": "31968708e17aa7e3e46b6f80cbc31b8100b1dfece13d693db0719b73693f8245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45e7963b-3f3f-4503-beab-9d52b2e33541", "node_type": "1", "metadata": {}, "hash": "e09994106885dd8c04c785edf18c9728e3b00eb7174a10204da1d43b740edbc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}}, "text": "Another three benchmarks, RAGAS [362], ARES [363] and\nTruLens [364], consider three different aspects: Faithfulness,\nAnswer Relevance, and Context Relevance, respectively. Faith-\nfulness focuses on the factual errors in the results when the\ncorrect answers can be inferred from the retrieved contents.\nAnswer Relevance measures whether the generated results\nactually address the problems (i.e., queries) or not. Context\nRelevance judges whether the retrieved contents contain as\nmuch knowledge as possible to answer the queries, and as\nlittle irrelevant information as possible.", "start_char_idx": 1505, "end_char_idx": 2083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45e7963b-3f3f-4503-beab-9d52b2e33541": {"__data__": {"id_": "45e7963b-3f3f-4503-beab-9d52b2e33541", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20beb164-b808-4fab-bdb1-2cab9205485b", "node_type": "1", "metadata": {}, "hash": "84f799769391b9be09a02d6763ab27bae4d2948e81d675f40d5c82ffc66e2cf2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "668623f4-21a5-4b59-b6ac-e78759e6b403", "node_type": "1", "metadata": {}, "hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "class_name": "RelatedNodeInfo"}}, "text": "CRUD-RAG [365] divides all RAG tasks into four cate-\ngories, which are Create, Read, Update, and Delete, respec-\ntively, and also evaluates each category using text continuation,\nquestion answering (with single- and multi-document ques-\ntions), hallucination modification, and open-domain multi-\ndocument summary.", "start_char_idx": 2084, "end_char_idx": 2397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d99a087-25e4-4697-aa87-74b61e82a6ad": {"__data__": {"id_": "0d99a087-25e4-4697-aa87-74b61e82a6ad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26f6c18-927a-4e11-b542-6fa85387b872", "node_type": "1", "metadata": {}, "hash": "d4de0904dc60501c95e3fd2e514611431f3b100c9259692bc819ded096879298", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c26f6c18-927a-4e11-b542-6fa85387b872", "node_type": "1", "metadata": {}, "hash": "d4de0904dc60501c95e3fd2e514611431f3b100c9259692bc819ded096879298", "class_name": "RelatedNodeInfo"}}, "text": "MIRAGE [366] is a benchmark designed\nfor assessing the application of RAG in the medical domain,\nfocusing on the comparison and optimization of medical\nquestion-answering systems\u2019 performance. KILT [367] is an-\nother benchmark focuses on ensuring information accuracyand reliability by aligning Wikipedia pages with specific snap-\nshots and pinpointing the most pertinent text ranges through\nBLEU score evaluations.", "start_char_idx": 0, "end_char_idx": 415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb13df0d-57d8-48be-9f81-b00f3aa17fe0": {"__data__": {"id_": "bb13df0d-57d8-48be-9f81-b00f3aa17fe0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a77cb3bb-d25e-43d3-a9c0-d530dd22739b", "node_type": "1", "metadata": {}, "hash": "9050ce00c526f93dcc82f7a1cce2aab4283c09c0b54e26bb876ee24dbd113811", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}}, "text": "It filters out lower-quality data to\nmaintain a high standard of information mapping, offering a\nvariety of retrieval system options like TF-IDF, DPR, RAG,\nand BLINK + flair to support evidence-based predictions or\ncitations according to task requirements.\nVI. D ISCUSSION\nA.Limitations\nDespite the widespread adoption of RAG, it suffers from\nseveral limitations by nature. In this paper, we provide a sum-\nmary of the limitations and engage in an in-depth discussion.", "start_char_idx": 0, "end_char_idx": 468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a77cb3bb-d25e-43d3-a9c0-d530dd22739b": {"__data__": {"id_": "a77cb3bb-d25e-43d3-a9c0-d530dd22739b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb13df0d-57d8-48be-9f81-b00f3aa17fe0", "node_type": "1", "metadata": {}, "hash": "863a26731ac86bfa63a1b2efcc3bf8dcfd08e3d777e5926cf01add9e54be611f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3194219b-ebcd-4142-b47c-5fa24070e90f", "node_type": "1", "metadata": {}, "hash": "72a840308d31a0302150fddd5aaae9f145064ee2ea3630351c964b80e3584a86", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}}, "text": "1)Noises in Retrieval Results :Information retrieval can-\nnot yield perfect results because information loss appears in\nrepresentations generated by encoder models. Additionally,\nANN search can also provide approximate results rather than\nexact ones. Consequently, certain degree of noise is inevitable\nin retrieval results, manifesting as irrelevant objects or mis-\nleading information, which may cause failure points in RAG\nsystems [368].", "start_char_idx": 469, "end_char_idx": 909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3194219b-ebcd-4142-b47c-5fa24070e90f": {"__data__": {"id_": "3194219b-ebcd-4142-b47c-5fa24070e90f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a77cb3bb-d25e-43d3-a9c0-d530dd22739b", "node_type": "1", "metadata": {}, "hash": "9050ce00c526f93dcc82f7a1cce2aab4283c09c0b54e26bb876ee24dbd113811", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31fd32b6-a023-49c1-8fa4-a4c977281d19", "node_type": "1", "metadata": {}, "hash": "fb4e74c749db844bc1f37627d035eb64ddaa9c5f30e41156ea390d254a625259", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}}, "text": "Though the common sense is that increasing\nthe accuracy of retrieval will contribute to the effectiveness of\nRAG, a recent study surprisingly shows that noisy retrieval\nresults may conversely help improve the generation qual-\nity [369]. A possible explanation is that diversity in retrieval\nresults may also be necessary for prompt construction [370].\nAs a result, the impact of noise in retrieval results remains\nuncertain, leading to confusion in practical uses regarding\nwhich metric to employ for retrieval and how to facilitate the\ninteraction between the retriever and the generator.", "start_char_idx": 910, "end_char_idx": 1499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31fd32b6-a023-49c1-8fa4-a4c977281d19": {"__data__": {"id_": "31fd32b6-a023-49c1-8fa4-a4c977281d19", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3194219b-ebcd-4142-b47c-5fa24070e90f", "node_type": "1", "metadata": {}, "hash": "72a840308d31a0302150fddd5aaae9f145064ee2ea3630351c964b80e3584a86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f967a8e1-0805-45a8-8ffd-8652afc40ca8", "node_type": "1", "metadata": {}, "hash": "d87ba2f9649be406707cf8943eeac7c5add458702613a4203be3394a750c290f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}}, "text": "2)Extra Overhead :While retrieval can help mitigate the\ncosts of generation in certain cases [30]\u2013[32], the incorporation\nof retrieval sometimes introduces non-negligible overhead.\nConsidering that RAG is primarily employed to improve the\nperformance of existing generative models, the inclusion of\nadditional retrieval and interaction processes leads to increased\nlatency. Worse still, when combined with complex enhance-\nment methods, such as recursive retrieval [371] and iterative\nRAG [218], the extra overhead will become even more sig-\nnificant.", "start_char_idx": 1500, "end_char_idx": 2051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f967a8e1-0805-45a8-8ffd-8652afc40ca8": {"__data__": {"id_": "f967a8e1-0805-45a8-8ffd-8652afc40ca8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31fd32b6-a023-49c1-8fa4-a4c977281d19", "node_type": "1", "metadata": {}, "hash": "fb4e74c749db844bc1f37627d035eb64ddaa9c5f30e41156ea390d254a625259", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05", "node_type": "1", "metadata": {}, "hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "class_name": "RelatedNodeInfo"}}, "text": "Furthermore, as the scale of retrieval expands, the\nstorage and access complexity associated with data sources\nwill also increase. In presence, RAG systems exhibit a trade-\noff between costs and benefits. Looking ahead, we anticipate\nfurther optimization to alleviate the associated overhead [372].\n3)The Gap between Retrievers and Generators :Seam-\nlessly integrating retrieval and generation components requires\nmeticulous design and optimization.", "start_char_idx": 2052, "end_char_idx": 2501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e34bb962-915a-4424-974d-2c772aee08b6": {"__data__": {"id_": "e34bb962-915a-4424-974d-2c772aee08b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc7c7c5e-5606-49c5-abc4-f7ae526fab8f", "node_type": "1", "metadata": {}, "hash": "a6ab9ece106b1dc1605f98b3b224a29614a04b2f3bc5dce77d703282ee06d460", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}}, "text": "Since the objectives of\nretrievers and generators may not align, and their latent spaces\nmight differ, designing their interaction poses challenges. As\nintroduced in Section III, numerous approaches have been\nproposed to enable effective RAG, and these approaches\neither disentangle the retrieval and generation processes or\nintegrate them at an intermediate stage. While the former is\nmore modularized, the latter could potentially benefit from\njoint training.", "start_char_idx": 0, "end_char_idx": 461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc7c7c5e-5606-49c5-abc4-f7ae526fab8f": {"__data__": {"id_": "fc7c7c5e-5606-49c5-abc4-f7ae526fab8f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e34bb962-915a-4424-974d-2c772aee08b6", "node_type": "1", "metadata": {}, "hash": "53372c64133ba60545e97b15510a4af2a1d8d4a65762fde1e3683b0360d36d95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1613544-b38c-4ed0-9a78-59133c3529f4", "node_type": "1", "metadata": {}, "hash": "901e70e1c2f5f93fbab7ee3e5ec525e86ff1532cafc6771162a4d1d598a5e261", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}}, "text": "While the former is\nmore modularized, the latter could potentially benefit from\njoint training. Till not, there lacks a sufficient comparison of\ndifferent ways of interaction across various scenarios.22\n4)Increased System Complexity :The introduction of re-\ntrieval unavoidably increases the system complexity and the\nnumber of hyper-parameters to tune. For instance, a recent\nstudy on the trade-off between attribution and fluency in\nprompt-augmentation-style RAG demonstrates that using top-\nk retrieval for generation improves attribution, but hurts flu-\nency in turns [373].", "start_char_idx": 366, "end_char_idx": 944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1613544-b38c-4ed0-9a78-59133c3529f4": {"__data__": {"id_": "e1613544-b38c-4ed0-9a78-59133c3529f4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc7c7c5e-5606-49c5-abc4-f7ae526fab8f", "node_type": "1", "metadata": {}, "hash": "a6ab9ece106b1dc1605f98b3b224a29614a04b2f3bc5dce77d703282ee06d460", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a866526-497b-49f3-a6da-5c3d580b7c85", "node_type": "1", "metadata": {}, "hash": "5c847c5b287e488f1ad3805947b76e9c6eca2d21dbb0b4c34e3f5fda46a44898", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}}, "text": "The counter effects of different aspects\nin RAG, such as metric selection, are still under explored.\nTherefore, further refinement of RAG systems, both in terms\nof algorithms, and deployment, is necessary to fully unlock\ntheir potentials.\n5)Lengthy Context :One of the primary shortcomings of\nRAG, in particular the query-based RAG, is that it lengthens\nthe context tremendously, making it infeasible for generators\nwith limited context length. In addition, the lengthened context\nalso slows down the generation process generally.", "start_char_idx": 945, "end_char_idx": 1475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a866526-497b-49f3-a6da-5c3d580b7c85": {"__data__": {"id_": "1a866526-497b-49f3-a6da-5c3d580b7c85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1613544-b38c-4ed0-9a78-59133c3529f4", "node_type": "1", "metadata": {}, "hash": "901e70e1c2f5f93fbab7ee3e5ec525e86ff1532cafc6771162a4d1d598a5e261", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "217c9a59-07ca-468b-8329-6c3f7031478d", "node_type": "1", "metadata": {}, "hash": "1c7ceaa85c9ae8fbd26360435168711ac8984197cf93af74c0167f8e2e3a65ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}}, "text": "In addition, the lengthened context\nalso slows down the generation process generally. The research\nadvancements in prompt compression [202] and long-context\nsupport [374] have partially mitigated these challenges, albeit\nwith a slight trade-off in accuracy or costs.\nB.Potential Future Directions\nLastly, we wish to outline several potential directions for\nfuture RAG research and applications.\n1)More Advanced Research on RAG Methodologies, En-\nhancements, and Applications :A straight-forward research\ndirection is to develop more advanced methodologies, en-\nhancements, and applications of RAG.", "start_char_idx": 1390, "end_char_idx": 1987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "217c9a59-07ca-468b-8329-6c3f7031478d": {"__data__": {"id_": "217c9a59-07ca-468b-8329-6c3f7031478d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a866526-497b-49f3-a6da-5c3d580b7c85", "node_type": "1", "metadata": {}, "hash": "5c847c5b287e488f1ad3805947b76e9c6eca2d21dbb0b4c34e3f5fda46a44898", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "node_type": "1", "metadata": {}, "hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "class_name": "RelatedNodeInfo"}}, "text": "As introduced in Section III-A, existing works have ex-\nplored various interaction patterns between retrievers and\ngenerators. However, since the optimization target of these two\ncomponents are distinct, the practical augmentation process\nhas a large impact on the final generation results. Investigation\nof more advanced foundations for augmentation holds promise\nfor fully unleashing the potential of RAG.", "start_char_idx": 1988, "end_char_idx": 2395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8767cf3-fa04-46e0-85e9-f36df1a1f878": {"__data__": {"id_": "a8767cf3-fa04-46e0-85e9-f36df1a1f878", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c8c9fd2-3b8c-4a7a-ad1e-a1b74840dc1a", "node_type": "1", "metadata": {}, "hash": "a443d2d5b71c1e86d5d172e72934343995e0e01e41569ec475b45c0ebf15ee1b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}}, "text": "Based on a constructed RAG system, enhancements are\nhelpful to improve the effectiveness of certain components\nor the entire pipeline. Given the inherent complexity of the\nsystem, there exists significant potential for RAG to improve,\nnecessitating proper tuning and careful engineering. We look\nforward to further experimental analysis and in-depth explo-\nration that will contribute to the development of more effective\nand more robust RAG systems.\nAs introduced in Section IV, RAG is a general technique\nthat has been applied across diverse modalities and tasks.", "start_char_idx": 0, "end_char_idx": 565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c8c9fd2-3b8c-4a7a-ad1e-a1b74840dc1a": {"__data__": {"id_": "4c8c9fd2-3b8c-4a7a-ad1e-a1b74840dc1a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8767cf3-fa04-46e0-85e9-f36df1a1f878", "node_type": "1", "metadata": {}, "hash": "c61aa576e78e57206b42dba50fa8fb45dfd10de9ffa4e9cbe50df835d0cca675", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00eafba3-f472-4835-ab19-ac519d4aa3cb", "node_type": "1", "metadata": {}, "hash": "3430023d3b35f1cc958713d1ccbfe291475be1a641b6e3db62775d6aadc033ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}}, "text": "Yet\nmost of existing works straightforwardly integrate external\nknowledge with the specific generation tasks, without thor-\noughly taking into account the key characteristics of the target\ndomains. Therefore, for generation tasks that do not fully\nleverage the power of RAG, we are confident that designing\nproper RAG system will be beneficial.\n2)Efficient Deployment and Processing :Currently, sev-\neral deployment solutions of query-based RAG for LLMs\nhave been proposed, such as LangChain [375], LLAMA-\nIndex [175], and PipeRAG [376].", "start_char_idx": 566, "end_char_idx": 1103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00eafba3-f472-4835-ab19-ac519d4aa3cb": {"__data__": {"id_": "00eafba3-f472-4835-ab19-ac519d4aa3cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c8c9fd2-3b8c-4a7a-ad1e-a1b74840dc1a", "node_type": "1", "metadata": {}, "hash": "a443d2d5b71c1e86d5d172e72934343995e0e01e41569ec475b45c0ebf15ee1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2117da92-d682-475d-aba8-a71d376f683a", "node_type": "1", "metadata": {}, "hash": "064cedaf76dcf26370c23f963fe33543c5c057a71bd040a92e67a796cc05af52", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}}, "text": "However, for other founda-\ntions of RAG and/or generation tasks, there lacks a plug-and-\nplay solution. In addition, given the extra overhead introduced\nby retrieval, and considering that the complexities of both\nthe retriever and generator will continue to grow, achievingefficient processing in RAG remains a challenge, necessitating\ntargeted system optimization.\n3)Incorporating Long-tail and Real-time Knowledge :\nWhile a key motivation of RAG is to harness real-time and\nlong-tail knowledge, few studies have explored the pipeline\nfor knowledge updating and expansion.", "start_char_idx": 1104, "end_char_idx": 1677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2117da92-d682-475d-aba8-a71d376f683a": {"__data__": {"id_": "2117da92-d682-475d-aba8-a71d376f683a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00eafba3-f472-4835-ab19-ac519d4aa3cb", "node_type": "1", "metadata": {}, "hash": "3430023d3b35f1cc958713d1ccbfe291475be1a641b6e3db62775d6aadc033ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce7a1637-31c5-4726-9393-835a332f13f2", "node_type": "1", "metadata": {}, "hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "class_name": "RelatedNodeInfo"}}, "text": "Many existing works\nmake up the retrieval sources with merely the training data\nof generators, thereby neglecting the dynamic and flexible\ninformation advantages that could have been offered by re-\ntrieval. As a consequence, designing a useful RAG system with\ncontinuously updated knowledge and/or flexible knowledge\nsources, along with corresponding system-level optimizations,\nis a growing research direction. With the capability of utilizing\nlong-tail knowledge, we also expect RAG to leverage person-\nalized information and features, so as to adapt to today\u2019s web\nservice.", "start_char_idx": 1678, "end_char_idx": 2254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b311141-51f6-48e8-85f4-19dcb6789636": {"__data__": {"id_": "7b311141-51f6-48e8-85f4-19dcb6789636", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2b30c75-d798-43b7-a3dc-3876e3820cef", "node_type": "1", "metadata": {}, "hash": "265a59a04791bb7a2f0fa9ed01900df3453d0573b5a2babd96baeb2cbcb62c4f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}}, "text": "4)Combined with Other Techniques :In essential, RAG\nis orthogonal to other techniques that share the goal of\nimproving AIGC effectiveness, including fine tuning, rein-\nforcement learning, chain-of-thought, agent-based generation,\nand other potential optimizations. However, the exploration of\nsimultaneously applying these techniques is still in its early\nstages, calling for further research to delve into algorithm\ndesign and fully leverage their potential. It is worthy to note\nthat a recent notion appears \u201clong-context models like Gemini\n1.5 will replace RAG\u201d.", "start_char_idx": 0, "end_char_idx": 565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2b30c75-d798-43b7-a3dc-3876e3820cef": {"__data__": {"id_": "f2b30c75-d798-43b7-a3dc-3876e3820cef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b311141-51f6-48e8-85f4-19dcb6789636", "node_type": "1", "metadata": {}, "hash": "baca1b715cc6c66d857813ffd6ba3f8808089c4ec5417ca5b7d2e9c711a56bfc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0906a26a-d932-4a45-b2fd-6b5d80b92412", "node_type": "1", "metadata": {}, "hash": "80bfe8682c1596fcb512c2f3c981f3bede057b1967012b8cfc59bd46143485e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}}, "text": "Nevertheless, this assertion does not\nhold true \u2014 RAG exhibits greater flexibility in managing\ndynamic information, encompassing both up-to-date and long-\ntail knowledge [377]. We believe that RAG in the future will\ntake advantage of long context generation to achieve even\nbetter performance, rather than simply being weeded out by\nit.\nVII. C ONCLUSION\nIn this paper, we conducted a thorough and comprehensive\nsurvey on RAG within the context of AIGC, with a particular\nfocus on augmentation foundations, enhancements, and ap-\nplications.", "start_char_idx": 566, "end_char_idx": 1105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0906a26a-d932-4a45-b2fd-6b5d80b92412": {"__data__": {"id_": "0906a26a-d932-4a45-b2fd-6b5d80b92412", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2b30c75-d798-43b7-a3dc-3876e3820cef", "node_type": "1", "metadata": {}, "hash": "265a59a04791bb7a2f0fa9ed01900df3453d0573b5a2babd96baeb2cbcb62c4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcd8e83f-99ae-4dab-adea-b18713a1e027", "node_type": "1", "metadata": {}, "hash": "0f9ad2e506a7c97760c83bc3fec86edebf45e91e9c5957b9ec270b479b915bce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}}, "text": "We first systematically organized and summarize\nthe foundation paradigms in RAG, providing insights into\nthe interaction between retrievers and generators. Then, we\nreviewed the enhancements that further improve the effective-\nness of RAG, including the enhancements on each component\nor the entire pipeline. To facilitate researchers across diverse\ndomains, we showcased practical applications of RAG in a\nrange of modalities and tasks. Finally, we also presented\nexisting benchmarks for RAG, discussed current limitations\nof RAG, and shed light on promising future directions.", "start_char_idx": 1106, "end_char_idx": 1684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcd8e83f-99ae-4dab-adea-b18713a1e027": {"__data__": {"id_": "dcd8e83f-99ae-4dab-adea-b18713a1e027", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0906a26a-d932-4a45-b2fd-6b5d80b92412", "node_type": "1", "metadata": {}, "hash": "80bfe8682c1596fcb512c2f3c981f3bede057b1967012b8cfc59bd46143485e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5093a13c-9d13-46ed-9d22-057236d90bde", "node_type": "1", "metadata": {}, "hash": "7ba01220e68a02efb03eacc795a92bc17df1b0fb16d45fa9d45e2cb3906f3d58", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}}, "text": "REFERENCES\n[1] T. B. Brown, B. Mann etal., \u201cLanguage models are few-shot learners,\u201d\ninNeurIPS, 2020.\n[2] M. Chen, J. Tworek etal., \u201cEvaluating large language models trained\non code,\u201d arXiv:2107.03374, 2021.\n[3] OpenAI, \u201cGPT-4 technical report,\u201d arXiv:2303.08774, 2023.", "start_char_idx": 1685, "end_char_idx": 1953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5093a13c-9d13-46ed-9d22-057236d90bde": {"__data__": {"id_": "5093a13c-9d13-46ed-9d22-057236d90bde", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcd8e83f-99ae-4dab-adea-b18713a1e027", "node_type": "1", "metadata": {}, "hash": "0f9ad2e506a7c97760c83bc3fec86edebf45e91e9c5957b9ec270b479b915bce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "001f02ad-43a4-4105-9aab-1eca82b49991", "node_type": "1", "metadata": {}, "hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "class_name": "RelatedNodeInfo"}}, "text": "[4] H. Touvron, T. Lavril etal., \u201cLlama: Open and efficient foundation\nlanguage models,\u201d arXiv:2302.13971, 2023.", "start_char_idx": 1954, "end_char_idx": 2066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "479df2ba-adbd-48c0-912c-45351d9583ca": {"__data__": {"id_": "479df2ba-adbd-48c0-912c-45351d9583ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45593217-9f9a-4741-a8d5-1545ac7e429e", "node_type": "1", "metadata": {}, "hash": "c97aee1c760250dd596a785c4de320feadd0c4f892ad3cd7d5518a1cd165e1d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45593217-9f9a-4741-a8d5-1545ac7e429e", "node_type": "1", "metadata": {}, "hash": "c97aee1c760250dd596a785c4de320feadd0c4f892ad3cd7d5518a1cd165e1d0", "class_name": "RelatedNodeInfo"}}, "text": "[5] H. Touvron, L. Martin etal., \u201cLlama 2: Open foundation and fine-tuned\nchat models,\u201d arXiv:2307.09288, 2023.", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30edca9e-602b-4751-8f1e-8ceea3fa7144": {"__data__": {"id_": "30edca9e-602b-4751-8f1e-8ceea3fa7144", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9ac09f7-abbd-4291-8e1b-59826aa8e9fb", "node_type": "1", "metadata": {}, "hash": "ec7bb8e20dd6cf16abe9a86cd42b285ccf6d240cdbcf8c8c396ddb7ffa225bc3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}}, "text": "[6] B. Rozi `ere, J. Gehring etal., \u201cCode llama: Open foundation models\nfor code,\u201d arXiv:2308.12950, 2023.", "start_char_idx": 0, "end_char_idx": 106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9ac09f7-abbd-4291-8e1b-59826aa8e9fb": {"__data__": {"id_": "e9ac09f7-abbd-4291-8e1b-59826aa8e9fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30edca9e-602b-4751-8f1e-8ceea3fa7144", "node_type": "1", "metadata": {}, "hash": "1aa5d5997c67fcf1db567ade1d71c118c3ef1d05ad54d88093e4dae616699bb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f025190-0e23-42a3-8fa2-bad898a7321e", "node_type": "1", "metadata": {}, "hash": "5d60c2d112691791a8f931895499dfecafe42faabb05fa5597eff5922bb5e13b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}}, "text": "[7] A. Ramesh, M. Pavlov, G. Goh etal., \u201cZero-shot text-to-image gener-\nation,\u201d in ICML, 2021.23\n[8] A. Ramesh, P. Dhariwal, A. Nichol etal., \u201cHierarchical text-conditional\nimage generation with CLIP latents,\u201d arXiv:2204.06125, 2022.\n[9] J. Betker, G. Goh, L. Jing etal., \u201cImproving image generation with\nbetter captions,\u201d Computer Science, vol.", "start_char_idx": 107, "end_char_idx": 452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f025190-0e23-42a3-8fa2-bad898a7321e": {"__data__": {"id_": "9f025190-0e23-42a3-8fa2-bad898a7321e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9ac09f7-abbd-4291-8e1b-59826aa8e9fb", "node_type": "1", "metadata": {}, "hash": "ec7bb8e20dd6cf16abe9a86cd42b285ccf6d240cdbcf8c8c396ddb7ffa225bc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6e7f471-d1a0-48aa-93cb-990c54902330", "node_type": "1", "metadata": {}, "hash": "4c006ac775ec69130023d6533d2961725059ebc8f6c4cdee2be43a96acfc8e87", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}}, "text": "2, no. 3, p. 8, 2023.\n[10] R. Rombach, A. Blattmann, D. Lorenz etal., \u201cHigh-resolution image\nsynthesis with latent diffusion models,\u201d in IEEE/CVF, 2022.\n[11] OpenAI, \u201cVideo generation models as world simulators,\u201d https://openai.\ncom/research/video-generation-models-as-world-simulators, 2024.\n[12] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\nComput., vol.", "start_char_idx": 453, "end_char_idx": 831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6e7f471-d1a0-48aa-93cb-990c54902330": {"__data__": {"id_": "d6e7f471-d1a0-48aa-93cb-990c54902330", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f025190-0e23-42a3-8fa2-bad898a7321e", "node_type": "1", "metadata": {}, "hash": "5d60c2d112691791a8f931895499dfecafe42faabb05fa5597eff5922bb5e13b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7f52bb1-c615-4d48-a1fa-281c34570180", "node_type": "1", "metadata": {}, "hash": "1f7f77ee4a3d8dd691b67d237409073e092f2232018e33d4ecfc2966fc161df8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}}, "text": "9, no. 8, pp. 1735\u20131780, 1997.\n[13] A. Vaswani, N. Shazeer, N. Parmar etal., \u201cAttention is all you need,\u201d\ninNeurIPS, 2017.\n[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza etal., \u201cGenerative adver-\nsarial networks,\u201d CACM, vol. 63, no. 11, pp. 139\u2013144, 2020.", "start_char_idx": 832, "end_char_idx": 1087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7f52bb1-c615-4d48-a1fa-281c34570180": {"__data__": {"id_": "a7f52bb1-c615-4d48-a1fa-281c34570180", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6e7f471-d1a0-48aa-93cb-990c54902330", "node_type": "1", "metadata": {}, "hash": "4c006ac775ec69130023d6533d2961725059ebc8f6c4cdee2be43a96acfc8e87", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19780472-c500-4db8-9fe0-a3572c15efe9", "node_type": "1", "metadata": {}, "hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "class_name": "RelatedNodeInfo"}}, "text": "63, no. 11, pp. 139\u2013144, 2020.\n[15] J. Devlin, M. Chang etal., \u201cBERT: pre-training of deep bidirectional\ntransformers for language understanding,\u201d in NAACL-HLT, 2019.\n[16] C. Raffel, N. Shazeer, A. Roberts etal., \u201cExploring the limits of transfer\nlearning with a unified text-to-text transformer,\u201d JMLR, vol. 21, pp.", "start_char_idx": 1057, "end_char_idx": 1373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d643aa0b-a69d-41b5-8731-a0b676b53177": {"__data__": {"id_": "d643aa0b-a69d-41b5-8731-a0b676b53177", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52d9581f-cd10-4219-850b-7cfa60631540", "node_type": "1", "metadata": {}, "hash": "6305023dd1bb84b1de043dc2641644b37cbaf17b5a10b8c2c8593776e5f5f0a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}}, "text": "21, pp.\n140:1\u2013140:67, 2020.\n[17] W. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling to\ntrillion parameter models with simple and efficient sparsity,\u201d JMLR,\nvol. 23, no. 120, pp. 1\u201339, 2022.\n[18] J. Kaplan, S. McCandlish, T. Henighan etal., \u201cScaling laws for neural\nlanguage models,\u201d 2020.", "start_char_idx": 0, "end_char_idx": 302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52d9581f-cd10-4219-850b-7cfa60631540": {"__data__": {"id_": "52d9581f-cd10-4219-850b-7cfa60631540", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d643aa0b-a69d-41b5-8731-a0b676b53177", "node_type": "1", "metadata": {}, "hash": "ce2d048fa36bdbb619690152be1d24e33df0e2dfa0b71eb5ab2906c0f4066e95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51acae86-bf53-4591-8137-987b5f3a0c6c", "node_type": "1", "metadata": {}, "hash": "504194cd9029ef71a80737bf125a2b63236236f7291b8f4812148675703bbb52", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}}, "text": "[19] S. E. Robertson and H. Zaragoza, \u201cThe probabilistic relevance frame-\nwork: BM25 and beyond,\u201d FTIR, vol. 3, no. 4, pp. 333\u2013389, 2009.\n[20] V . Karpukhin, B. Oguz, S. Min etal., \u201cDense passage retrieval for\nopen-domain question answering,\u201d in EMNLP, 2020.", "start_char_idx": 303, "end_char_idx": 561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51acae86-bf53-4591-8137-987b5f3a0c6c": {"__data__": {"id_": "51acae86-bf53-4591-8137-987b5f3a0c6c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52d9581f-cd10-4219-850b-7cfa60631540", "node_type": "1", "metadata": {}, "hash": "6305023dd1bb84b1de043dc2641644b37cbaf17b5a10b8c2c8593776e5f5f0a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f75ff35e-1ac0-488e-829c-f62aa23907e3", "node_type": "1", "metadata": {}, "hash": "83dda942aed08acfee5b36e535c3e1384fc635ad9355b22311195cc2ed168277", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}}, "text": "[21] J. Johnson, M. Douze, and H. J \u00b4egou, \u201cBillion-scale similarity search\nwith gpus,\u201d IEEE Trans. BigData, vol. 7, no. 3, pp. 535\u2013547, 2021.\n[22] Q. Chen, B. Zhao, H. Wang etal., \u201cSPANN: highly-efficient billion-\nscale approximate nearest neighborhood search,\u201d in NeurIPS, 2021.", "start_char_idx": 562, "end_char_idx": 842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f75ff35e-1ac0-488e-829c-f62aa23907e3": {"__data__": {"id_": "f75ff35e-1ac0-488e-829c-f62aa23907e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51acae86-bf53-4591-8137-987b5f3a0c6c", "node_type": "1", "metadata": {}, "hash": "504194cd9029ef71a80737bf125a2b63236236f7291b8f4812148675703bbb52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a91d0b66-8351-440d-b6f8-5bce59073802", "node_type": "1", "metadata": {}, "hash": "aeffa4332e0d98db2bc2c5bcf56cf6c9f9c6a959a6bf4db76a13f9c99d5cddd2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}}, "text": "[23] R. Datta, D. Joshi, J. Li etal., \u201cImage retrieval: Ideas, influences, and\ntrends of the new age,\u201d CSUR, vol. 40, no. 2, pp. 5:1\u20135:60, 2008.\n[24] A. Radford, J. W. Kim, C. Hallacy etal., \u201cLearning transferable visual\nmodels from natural language supervision,\u201d in ICML, 2021.", "start_char_idx": 843, "end_char_idx": 1121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a91d0b66-8351-440d-b6f8-5bce59073802": {"__data__": {"id_": "a91d0b66-8351-440d-b6f8-5bce59073802", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f75ff35e-1ac0-488e-829c-f62aa23907e3", "node_type": "1", "metadata": {}, "hash": "83dda942aed08acfee5b36e535c3e1384fc635ad9355b22311195cc2ed168277", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "node_type": "1", "metadata": {}, "hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "class_name": "RelatedNodeInfo"}}, "text": "[25] Z. Feng, D. Guo etal., \u201cCodebert: A pre-trained model for program-\nming and natural languages,\u201d in EMNLP Findings, 2020.\n[26] Y . Wu, K. Chen, T. Zhang etal., \u201cLarge-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-caption augmen-\ntation,\u201d in ICASSP, 2023.", "start_char_idx": 1122, "end_char_idx": 1415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5bc3b1a-4bf1-4261-a969-6e7883ff5c4a": {"__data__": {"id_": "b5bc3b1a-4bf1-4261-a969-6e7883ff5c4a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a23e8c3e-903b-41c7-a0a0-652cbcebb4a1", "node_type": "1", "metadata": {}, "hash": "fa6387ddcae8c4dd5120c6c3930dacd954fa1745791c842498ef46e449e066b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}}, "text": "[27] A. Mallen, A. Asai, V . Zhong etal., \u201cWhen not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric\nmemories,\u201d in ACL, 2023.\n[28] N. Carlini, F. Tram `eretal., \u201cExtracting training data from large\nlanguage models,\u201d in USENIX, 2021.", "start_char_idx": 0, "end_char_idx": 270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a23e8c3e-903b-41c7-a0a0-652cbcebb4a1": {"__data__": {"id_": "a23e8c3e-903b-41c7-a0a0-652cbcebb4a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5bc3b1a-4bf1-4261-a969-6e7883ff5c4a", "node_type": "1", "metadata": {}, "hash": "289b69fbd9473307f8b274e9d118151bcc73fe65a2a2ba8ef626f4d27b4ad996", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4081b4f-d08e-416d-bde6-65a05eccbc1d", "node_type": "1", "metadata": {}, "hash": "75bac725f262aae6b18ffaef66cdee7b0acc1d743322dbb112bd29e25d335f0d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}}, "text": "[29] M. Kang, N. M. G \u00a8urel etal., \u201cC-RAG: certified generation risks for\nretrieval-augmented language models,\u201d arXiv:2402.03181, 2024.\n[30] G. Izacard, P. Lewis, M. Lomeli etal., \u201cAtlas: Few-shot learning with\nretrieval augmented language models,\u201d arXiv:2208.03299, 2022.\n[31] Y .", "start_char_idx": 271, "end_char_idx": 552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4081b4f-d08e-416d-bde6-65a05eccbc1d": {"__data__": {"id_": "a4081b4f-d08e-416d-bde6-65a05eccbc1d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a23e8c3e-903b-41c7-a0a0-652cbcebb4a1", "node_type": "1", "metadata": {}, "hash": "fa6387ddcae8c4dd5120c6c3930dacd954fa1745791c842498ef46e449e066b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "451e2b54-d25f-4063-948f-01bd7cebaab7", "node_type": "1", "metadata": {}, "hash": "f7fb578fd52e3cb144aae359abe069f89aa3dbfe4a7fbb3bcf9c013fdd5c7129", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}}, "text": "[31] Y . Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, \u201cMemorizing\ntransformers,\u201d in ICLR, 2022.\n[32] Z. He, Z. Zhong, T. Cai etal., \u201cREST: retrieval-based speculative\ndecoding,\u201d arxiv:2311.08252, 2023.\n[33] K. Guu, K. Lee, Z. Tung etal., \u201cREALM: retrieval-augmented language\nmodel pre-training,\u201d ICML, 2020.", "start_char_idx": 544, "end_char_idx": 851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "451e2b54-d25f-4063-948f-01bd7cebaab7": {"__data__": {"id_": "451e2b54-d25f-4063-948f-01bd7cebaab7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4081b4f-d08e-416d-bde6-65a05eccbc1d", "node_type": "1", "metadata": {}, "hash": "75bac725f262aae6b18ffaef66cdee7b0acc1d743322dbb112bd29e25d335f0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "138049a2-5244-4799-b421-bab9d1feada8", "node_type": "1", "metadata": {}, "hash": "1480e5cf29e0125b1ed50a523a6671aa7cca4f2b09a5cce7e82742e9137b15dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}}, "text": "[34] P. S. H. Lewis, E. Perez, A. Piktus etal., \u201cRetrieval-augmented\ngeneration for knowledge-intensive NLP tasks,\u201d in NeurIPS, 2020.\n[35] G. Izacard and E. Grave, \u201cLeveraging passage retrieval with generative\nmodels for open domain question answering,\u201d in EACL, 2021.\n[36] S. Borgeaud, A. Mensch etal., \u201cImproving language models by\nretrieving from trillions of tokens,\u201d in ICML, 2022.", "start_char_idx": 852, "end_char_idx": 1238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "138049a2-5244-4799-b421-bab9d1feada8": {"__data__": {"id_": "138049a2-5244-4799-b421-bab9d1feada8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "451e2b54-d25f-4063-948f-01bd7cebaab7", "node_type": "1", "metadata": {}, "hash": "f7fb578fd52e3cb144aae359abe069f89aa3dbfe4a7fbb3bcf9c013fdd5c7129", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "17e36111-0937-4893-91bf-173e818746db", "node_type": "1", "metadata": {}, "hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "class_name": "RelatedNodeInfo"}}, "text": "[37] U. Khandelwal, O. Levy, D. Jurafsky etal., \u201cGeneralization through\nmemorization: Nearest neighbor language models,\u201d in ICLR, 2020.", "start_char_idx": 1239, "end_char_idx": 1374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1686d5fe-b11e-433d-b2ae-ae0d0ac166d6": {"__data__": {"id_": "1686d5fe-b11e-433d-b2ae-ae0d0ac166d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed2d2138-7296-4372-9bbf-171bee1866c2", "node_type": "1", "metadata": {}, "hash": "5a7f0b35a6e18fdd8d16de1e6ae8ab7dc202871d3bf5e21bf87d34520c59d4ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}}, "text": "[38] J. He, G. Neubig, and T. Berg-Kirkpatrick, \u201cEfficient nearest neighbor\nlanguage models,\u201d in EMNLP, 2021.\n[39] zilliztech. (2023) Gptcache. [Online]. Available: https://github.com/\nzilliztech/GPTCache\n[40] M. R. Parvez, W. U. Ahmad etal., \u201cRetrieval augmented code gener-\nation and summarization,\u201d in EMNLP Findings, 2021.", "start_char_idx": 0, "end_char_idx": 326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed2d2138-7296-4372-9bbf-171bee1866c2": {"__data__": {"id_": "ed2d2138-7296-4372-9bbf-171bee1866c2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1686d5fe-b11e-433d-b2ae-ae0d0ac166d6", "node_type": "1", "metadata": {}, "hash": "49ed764b19c7b75918ac9854e42c9af38bf8f5a91de6c84936998c0c8edd6510", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2b13420-35ca-40d2-9df7-87435b80f27f", "node_type": "1", "metadata": {}, "hash": "74ef3c302d5b60735e4860d4a66b8605a663f1e378f1ff254f411e9ae9076276", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}}, "text": "[41] W. U. Ahmad, S. Chakraborty, B. Ray etal., \u201cUnified pre-training for\nprogram understanding and generation,\u201d in NAACL-HLT, 2021.\n[42] S. Zhou, U. Alon, F. F. Xu etal., \u201cDocprompting: Generating code by\nretrieving the docs,\u201d in ICLR, 2023.\n[43] Y . Koizumi, Y .", "start_char_idx": 327, "end_char_idx": 591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2b13420-35ca-40d2-9df7-87435b80f27f": {"__data__": {"id_": "a2b13420-35ca-40d2-9df7-87435b80f27f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed2d2138-7296-4372-9bbf-171bee1866c2", "node_type": "1", "metadata": {}, "hash": "5a7f0b35a6e18fdd8d16de1e6ae8ab7dc202871d3bf5e21bf87d34520c59d4ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d05a8f34-087a-44e4-ad06-8a128b226335", "node_type": "1", "metadata": {}, "hash": "c898d8cbf75ec59da38a8d33d028a1cdfc531750ed9e011deee04be3aa8c0f3b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}}, "text": "[43] Y . Koizumi, Y . Ohishi etal., \u201cAudio captioning using pre-trained large-\nscale language model guided by audio-based similar caption retrieval,\u201d\narXiv:2012.07331, 2020.[44] R. Huang, J. Huang, D. Yang etal., \u201cMake-an-audio: Text-to-audio\ngeneration with prompt-enhanced diffusion models,\u201d in ICML, 2023.\n[45] H.-Y . Tseng, H.-Y .", "start_char_idx": 570, "end_char_idx": 904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d05a8f34-087a-44e4-ad06-8a128b226335": {"__data__": {"id_": "d05a8f34-087a-44e4-ad06-8a128b226335", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2b13420-35ca-40d2-9df7-87435b80f27f", "node_type": "1", "metadata": {}, "hash": "74ef3c302d5b60735e4860d4a66b8605a663f1e378f1ff254f411e9ae9076276", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4452e0c4-d1eb-470e-8439-f7cdfd816f13", "node_type": "1", "metadata": {}, "hash": "0f7be290d0887dc9fb9a71e161ab04749b91195a6a6db670d2a470a7dd087fa3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}}, "text": "[45] H.-Y . Tseng, H.-Y . Lee etal., \u201cRetrievegan: Image synthesis via\ndifferentiable patch retrieval,\u201d in ECCV, 2020.\n[46] S. Sarto, M. Cornia, L. Baraldi, and R. Cucchiara, \u201cRetrieval-\naugmented transformer for image captioning,\u201d in CBMI, 2022.\n[47] R. Ramos, B. Martins etal., \u201cSmallcap: lightweight image captioning\nprompted with retrieval augmentation,\u201d in CVPR, 2023.", "start_char_idx": 879, "end_char_idx": 1252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4452e0c4-d1eb-470e-8439-f7cdfd816f13": {"__data__": {"id_": "4452e0c4-d1eb-470e-8439-f7cdfd816f13", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d05a8f34-087a-44e4-ad06-8a128b226335", "node_type": "1", "metadata": {}, "hash": "c898d8cbf75ec59da38a8d33d028a1cdfc531750ed9e011deee04be3aa8c0f3b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b9d1983-3c1b-423b-b132-1100f72674af", "node_type": "1", "metadata": {}, "hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "class_name": "RelatedNodeInfo"}}, "text": "[48] J. Chen, Y . Pan, Y . Li etal., \u201cRetrieval augmented convolutional\nencoder-decoder networks for video captioning,\u201d TOMCCAP, vol. 19,\nno. 1s, pp. 48:1\u201348:24, 2023.\n[49] J. Xu, Y .", "start_char_idx": 1253, "end_char_idx": 1436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e02bfa91-da6a-4d9d-9f4e-2347f1e94a0f": {"__data__": {"id_": "e02bfa91-da6a-4d9d-9f4e-2347f1e94a0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e209caf1-ead6-467f-9555-1a2e27ae1c9f", "node_type": "1", "metadata": {}, "hash": "e5f3c064f008e09ee6687542919075f8b54044aab99ed4a359a26a7af4ea6f39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e209caf1-ead6-467f-9555-1a2e27ae1c9f", "node_type": "1", "metadata": {}, "hash": "e5f3c064f008e09ee6687542919075f8b54044aab99ed4a359a26a7af4ea6f39", "class_name": "RelatedNodeInfo"}}, "text": "[49] J. Xu, Y . Huang, J. Hou etal., \u201cRetrieval-augmented egocentric video\ncaptioning,\u201d arXiv:2401.00789, 2024.", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6a2f908-211f-415d-81d3-09169bfb6813": {"__data__": {"id_": "e6a2f908-211f-415d-81d3-09169bfb6813", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e243190-6938-47b3-8006-d15ada0261cc", "node_type": "1", "metadata": {}, "hash": "c30a7ec0dd443d67dc1637e359654d89346b4ef7f16e37b8beb995cbb5a53abe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}}, "text": "[50] J. Seo, S. Hong etal., \u201cRetrieval-augmented score distillation for text-\nto-3d generation,\u201d arXiv:2402.02972, 2024.\n[51] M. Zhang, X. Guo, L. Pan etal., \u201cRemodiffuse: Retrieval-augmented\nmotion diffusion model,\u201d in ICCV, 2023.\n[52] X. Hu, X. Wu, Y . Shu, and Y .", "start_char_idx": 0, "end_char_idx": 267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e243190-6938-47b3-8006-d15ada0261cc": {"__data__": {"id_": "4e243190-6938-47b3-8006-d15ada0261cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6a2f908-211f-415d-81d3-09169bfb6813", "node_type": "1", "metadata": {}, "hash": "eb97d5aaae1c9e28e4d03c9bb640716c99b6bf98136f2e020dfc0c8a8023ab8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c514d792-de01-46d1-87ca-abf33eda5d0e", "node_type": "1", "metadata": {}, "hash": "d90631eb36dc84c8523b1e5398103b24dde746c1a8f2755ec083c5b6f35afbe8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}}, "text": "Shu, and Y . Qu, \u201cLogical form generation via multi-\ntask learning for complex question answering over knowledge bases,\u201d\ninCOLING, 2022.\n[53] X. Huang, J. Kim, and B. Zou, \u201cUnseen entity handling in complex\nquestion answering over knowledge base via language generation,\u201d in\nEMNLP Findings, 2021.\n[54] R. Das, M. Zaheer, D. Thai etal., \u201cCase-based reasoning for natural\nlanguage queries over knowledge bases,\u201d in EMNLP, 2021.", "start_char_idx": 255, "end_char_idx": 680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c514d792-de01-46d1-87ca-abf33eda5d0e": {"__data__": {"id_": "c514d792-de01-46d1-87ca-abf33eda5d0e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e243190-6938-47b3-8006-d15ada0261cc", "node_type": "1", "metadata": {}, "hash": "c30a7ec0dd443d67dc1637e359654d89346b4ef7f16e37b8beb995cbb5a53abe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0992dd4-0c0d-498e-a846-4f7f7b68f59f", "node_type": "1", "metadata": {}, "hash": "753cd5a5e55d964c4bcef72ce1ae5071592ae9a5dae07cef279d37843b465b8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}}, "text": "[55] Z. Wang, W. Nie, Z. Qiao etal., \u201cRetrieval-based controllable molecule\ngeneration,\u201d in ICLR, 2022.\n[56] Q. Jin, Y . Yang, Q. Chen, and Z. Lu, \u201cGenegpt: Augmenting large\nlanguage models with domain tools for improved access to biomedical\ninformation,\u201d Bioinformatics, vol. 40, no. 2, p. btae075, 2024.\n[57] H. Li, Y .", "start_char_idx": 681, "end_char_idx": 1002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0992dd4-0c0d-498e-a846-4f7f7b68f59f": {"__data__": {"id_": "e0992dd4-0c0d-498e-a846-4f7f7b68f59f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c514d792-de01-46d1-87ca-abf33eda5d0e", "node_type": "1", "metadata": {}, "hash": "d90631eb36dc84c8523b1e5398103b24dde746c1a8f2755ec083c5b6f35afbe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5c3898d-8380-4d44-bfa2-269be17f7145", "node_type": "1", "metadata": {}, "hash": "dc1ccf44a6c6fe428aa18abb96ebc067dabf366a6b268661ffe61a5da184f600", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}}, "text": "[57] H. Li, Y . Su, D. Cai etal., \u201cA survey on retrieval-augmented text\ngeneration,\u201d arxiv:2202.01110, 2022.\n[58] A. Asai, S. Min, Z. Zhong, and D. Chen, \u201cAcl 2023 tutorial: Retrieval-\nbased language models and applications,\u201d ACL 2023, 2023.\n[59] Y . Gao, Y .", "start_char_idx": 987, "end_char_idx": 1246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5c3898d-8380-4d44-bfa2-269be17f7145": {"__data__": {"id_": "e5c3898d-8380-4d44-bfa2-269be17f7145", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0992dd4-0c0d-498e-a846-4f7f7b68f59f", "node_type": "1", "metadata": {}, "hash": "753cd5a5e55d964c4bcef72ce1ae5071592ae9a5dae07cef279d37843b465b8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb", "node_type": "1", "metadata": {}, "hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "class_name": "RelatedNodeInfo"}}, "text": "[59] Y . Gao, Y . Xiong etal., \u201cRetrieval-augmented generation for large\nlanguage models: A survey,\u201d arxiv:2312.10997, 2023.\n[60] R. Zhao, H. Chen etal., \u201cRetrieving multimodal information for\naugmented generation: A survey,\u201d in EMNLP, 2023.", "start_char_idx": 1229, "end_char_idx": 1470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bff354a3-9750-4889-a0dd-11699b472edd": {"__data__": {"id_": "bff354a3-9750-4889-a0dd-11699b472edd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e895353f-be88-42b4-934b-393a825705db", "node_type": "1", "metadata": {}, "hash": "99a6434b29246bcfbef8dd3840bd8bbaf891b077d8466cb6e17e1bee332b9f6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}}, "text": "[61] J. Chen, H. Guo, K. Yi etal., \u201cVisualgpt: Data-efficient adaptation of\npretrained language models for image captioning,\u201d in CVPR, 2022.\n[62] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, \u201cEfficient transformers:\nA survey,\u201d CSUR, vol. 55, no. 6, pp. 109:1\u2013109:28, 2023.\n[63] G. V .", "start_char_idx": 0, "end_char_idx": 287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e895353f-be88-42b4-934b-393a825705db": {"__data__": {"id_": "e895353f-be88-42b4-934b-393a825705db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bff354a3-9750-4889-a0dd-11699b472edd", "node_type": "1", "metadata": {}, "hash": "a090b4b62ad158c3d1eff259bd59dc8e13e83c4660da5ec4ceb8349a9d702ac4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dba9662-1f9c-4b25-8f7a-cd783c2ae84c", "node_type": "1", "metadata": {}, "hash": "33efce2539999b18ca35a5ab149bb5b9a5874cad5595a96a87c250c2c760379e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}}, "text": "109:1\u2013109:28, 2023.\n[63] G. V . Houdt etal., \u201cA review on the long short-term memory model,\u201d\nArtif. Intell. Rev., vol. 53, no. 8, pp. 5929\u20135955, 2020.\n[64] L. Yang, Z. Zhang etal., \u201cDiffusion models: A comprehensive survey\nof methods and applications,\u201d CSUR, vol. 56, no. 4, pp. 1\u201339, 2023.", "start_char_idx": 256, "end_char_idx": 546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dba9662-1f9c-4b25-8f7a-cd783c2ae84c": {"__data__": {"id_": "3dba9662-1f9c-4b25-8f7a-cd783c2ae84c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e895353f-be88-42b4-934b-393a825705db", "node_type": "1", "metadata": {}, "hash": "99a6434b29246bcfbef8dd3840bd8bbaf891b077d8466cb6e17e1bee332b9f6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c2e66f1-7761-4419-a5df-cd1f3ea06672", "node_type": "1", "metadata": {}, "hash": "2826c44898df37a4742b621df8f67423537a661ad09aaa96ccdc91d0f5a0f669", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}}, "text": "56, no. 4, pp. 1\u201339, 2023.\n[65] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d\ninICML, 2015.\n[66] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d in NeurIPS, 2020.", "start_char_idx": 520, "end_char_idx": 794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c2e66f1-7761-4419-a5df-cd1f3ea06672": {"__data__": {"id_": "9c2e66f1-7761-4419-a5df-cd1f3ea06672", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dba9662-1f9c-4b25-8f7a-cd783c2ae84c", "node_type": "1", "metadata": {}, "hash": "33efce2539999b18ca35a5ab149bb5b9a5874cad5595a96a87c250c2c760379e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7089157f-0053-48fd-a59b-f1648da859b3", "node_type": "1", "metadata": {}, "hash": "867d8cd5a95f027f7c856bf49abab2ac13e43610e40f1bb579ad7a67a1f5372a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}}, "text": "[67] A. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffusion proba-\nbilistic models,\u201d in ICML, 2021.\n[68] Y . Song and S. Ermon, \u201cGenerative modeling by estimating gradients\nof the data distribution,\u201d in NeurIPS, 2019.\n[69] \u2014\u2014, \u201cImproved techniques for training score-based generative mod-\nels,\u201d in NeurIPS, 2020.\n[70] Y .", "start_char_idx": 795, "end_char_idx": 1121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7089157f-0053-48fd-a59b-f1648da859b3": {"__data__": {"id_": "7089157f-0053-48fd-a59b-f1648da859b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c2e66f1-7761-4419-a5df-cd1f3ea06672", "node_type": "1", "metadata": {}, "hash": "2826c44898df37a4742b621df8f67423537a661ad09aaa96ccdc91d0f5a0f669", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "node_type": "1", "metadata": {}, "hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "class_name": "RelatedNodeInfo"}}, "text": "[70] Y . Song, J. Sohl-Dickstein, D. P. Kingma etal., \u201cScore-based generative\nmodeling through stochastic differential equations,\u201d in ICLR, 2021.\n[71] Y . Song, C. Durkan, I. Murray, and S. Ermon, \u201cMaximum likelihood\ntraining of score-based diffusion models,\u201d in NeurIPS, 2021.", "start_char_idx": 1113, "end_char_idx": 1390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "745f7e7c-d8e7-4889-9eb5-618fcad78057": {"__data__": {"id_": "745f7e7c-d8e7-4889-9eb5-618fcad78057", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1e8f74d-fa3f-413c-8b62-dd170422eaff", "node_type": "1", "metadata": {}, "hash": "e956286d16e8242e1a80157a7936c287286453d76c2cd5f424873c3060d9189f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}}, "text": "[72] L. Yang, H. Qian, Z. Zhang etal., \u201cStructure-guided adversarial training\nof diffusion models,\u201d in CVPR, 2024.\n[73] X. Zhang, L. Yang, Y . Cai etal., \u201cRealcompo: Dynamic equilibrium\nbetween realism and compositionality improves text-to-image diffusion\nmodels,\u201d arXiv:2402.12908, 2024.", "start_char_idx": 0, "end_char_idx": 288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1e8f74d-fa3f-413c-8b62-dd170422eaff": {"__data__": {"id_": "a1e8f74d-fa3f-413c-8b62-dd170422eaff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "745f7e7c-d8e7-4889-9eb5-618fcad78057", "node_type": "1", "metadata": {}, "hash": "ecea6b6fcd3f012d1896fabc7c08adc74e40a9fcf5ad790037736e49a09d34ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1800ede4-c7b2-4de9-b55d-2b34066e14ae", "node_type": "1", "metadata": {}, "hash": "625e626d11766e2f788af3cc6c6fc2cbff06939a111f51df981f8e0afae50807", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}}, "text": "[74] R. Rombach, A. Blattmann, D. Lorenz etal., \u201cHigh-resolution image\nsynthesis with latent diffusion models,\u201d in CVPR, 2022.\n[75] A. Ramesh, P. Dhariwal, A. Nichol etal., \u201cHierarchical text-conditional\nimage generation with clip latents,\u201d arXiv:2204.06125, 2022.\n[76] H. Li, Y .", "start_char_idx": 289, "end_char_idx": 569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1800ede4-c7b2-4de9-b55d-2b34066e14ae": {"__data__": {"id_": "1800ede4-c7b2-4de9-b55d-2b34066e14ae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1e8f74d-fa3f-413c-8b62-dd170422eaff", "node_type": "1", "metadata": {}, "hash": "e956286d16e8242e1a80157a7936c287286453d76c2cd5f424873c3060d9189f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abde9574-d6b3-4840-bcc0-e33de5bd3070", "node_type": "1", "metadata": {}, "hash": "8e0eaaa16361aebfbf01015d519265747ac47f753454a3641f90fe706c402a06", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}}, "text": "[76] H. Li, Y . Yang, M. Chang etal., \u201cSrdiff: Single image super-resolution\nwith diffusion probabilistic models,\u201d Neurocomputing, vol. 479, pp.\n47\u201359, 2022.\n[77] J. Ho, C. Saharia, W. Chan etal., \u201cCascaded diffusion models for high\nfidelity image generation,\u201d JMLR, vol. 23, no. 1, pp. 2249\u20132281, 2022.", "start_char_idx": 554, "end_char_idx": 857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abde9574-d6b3-4840-bcc0-e33de5bd3070": {"__data__": {"id_": "abde9574-d6b3-4840-bcc0-e33de5bd3070", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1800ede4-c7b2-4de9-b55d-2b34066e14ae", "node_type": "1", "metadata": {}, "hash": "625e626d11766e2f788af3cc6c6fc2cbff06939a111f51df981f8e0afae50807", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9c2b5a6-8b09-40ba-ac0f-b2a6873f5f23", "node_type": "1", "metadata": {}, "hash": "3be0b42916ed68b31f592385e2c5b9aaf74b99ab37a70e501f53c53b16664b1f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}}, "text": "23, no. 1, pp. 2249\u20132281, 2022.\n[78] L. Yang, J. Liu, S. Hong etal., \u201cImproving diffusion-based image\nsynthesis with context prediction,\u201d in NeurIPS, 2024.24\n[79] L. Yang, Z. Yu, C. Meng etal., \u201cMastering text-to-image diffu-\nsion: Recaptioning, planning, and generating with multimodal llms,\u201d\narXiv:2401.11708, 2024.", "start_char_idx": 826, "end_char_idx": 1143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9c2b5a6-8b09-40ba-ac0f-b2a6873f5f23": {"__data__": {"id_": "c9c2b5a6-8b09-40ba-ac0f-b2a6873f5f23", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abde9574-d6b3-4840-bcc0-e33de5bd3070", "node_type": "1", "metadata": {}, "hash": "8e0eaaa16361aebfbf01015d519265747ac47f753454a3641f90fe706c402a06", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "381ee8e8-4d6a-4558-b143-621cfd55728c", "node_type": "1", "metadata": {}, "hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "class_name": "RelatedNodeInfo"}}, "text": "[80] S. Gong, M. Li, J. Feng etal., \u201cDiffuseq: Sequence to sequence text\ngeneration with diffusion models,\u201d in ICLR, 2023.\n[81] X. Li, J. Thickstun, I. Gulrajani etal., \u201cDiffusion-lm improves control-\nlable text generation,\u201d in NeurIPS, 2022.", "start_char_idx": 1144, "end_char_idx": 1386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e58987c4-bd55-4bf0-bb09-1698c797a463": {"__data__": {"id_": "e58987c4-bd55-4bf0-bb09-1698c797a463", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e90a33d-f58e-4365-a8a9-4ffa41e09c95", "node_type": "1", "metadata": {}, "hash": "c284589570f68a43f745abde60a0ebdac3cfbb5bf6b619af0d3f4f8b64928d6a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}}, "text": "[82] J. Austin, D. D. Johnson, J. Ho etal., \u201cStructured denoising diffusion\nmodels in discrete state-spaces,\u201d in NeurIPS, 2021.\n[83] T. Chen, R. Zhang, and G. Hinton, \u201cAnalog bits: Generating discrete\ndata using diffusion models with self-conditioning,\u201d in ICLR, 2023.", "start_char_idx": 0, "end_char_idx": 268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e90a33d-f58e-4365-a8a9-4ffa41e09c95": {"__data__": {"id_": "0e90a33d-f58e-4365-a8a9-4ffa41e09c95", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e58987c4-bd55-4bf0-bb09-1698c797a463", "node_type": "1", "metadata": {}, "hash": "d3b2f988a6ede2eb558eab53553d2703ee3e43096808515c01d041e841a36687", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5bbbc6e-ba7f-4ffd-a27f-c7606dca596e", "node_type": "1", "metadata": {}, "hash": "b40023c0fe1a3fd176e7b8872af8036701a46b6d7294d7d2ee00c9cc1be41bbb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}}, "text": "[84] J. Ho, W. Chan, C. Saharia etal., \u201cImagen video: High definition video\ngeneration with diffusion models,\u201d arXiv:2210.02303, 2022.\n[85] W. Harvey, S. Naderiparizi, V . Masrani etal., \u201cFlexible diffusion\nmodeling of long videos,\u201d in NeurIPS, 2022.\n[86] R. Yang, P. Srivastava, and S. Mandt, \u201cDiffusion probabilistic modeling\nfor video generation,\u201d Entropy, vol.", "start_char_idx": 269, "end_char_idx": 633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5bbbc6e-ba7f-4ffd-a27f-c7606dca596e": {"__data__": {"id_": "e5bbbc6e-ba7f-4ffd-a27f-c7606dca596e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e90a33d-f58e-4365-a8a9-4ffa41e09c95", "node_type": "1", "metadata": {}, "hash": "c284589570f68a43f745abde60a0ebdac3cfbb5bf6b619af0d3f4f8b64928d6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77e00ea8-8fb5-4044-91fd-7b108537e696", "node_type": "1", "metadata": {}, "hash": "bf3777df857681a33c3915572d2ba0608a557201169a931fa5351735775bc0b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}}, "text": "25, no. 10, p. 1469, 2023.\n[87] M. Zhang, Z. Cai, L. Pan etal., \u201cMotiondiffuse: Text-driven human\nmotion generation with diffusion model,\u201d TPAMI, 2024.\n[88] L. Yang, Z. Zhang, Z. Yu etal., \u201cCross-modal contextualized diffusion\nmodels for text-guided visual generation and editing,\u201d in ICLR, 2024.", "start_char_idx": 634, "end_char_idx": 930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77e00ea8-8fb5-4044-91fd-7b108537e696": {"__data__": {"id_": "77e00ea8-8fb5-4044-91fd-7b108537e696", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5bbbc6e-ba7f-4ffd-a27f-c7606dca596e", "node_type": "1", "metadata": {}, "hash": "b40023c0fe1a3fd176e7b8872af8036701a46b6d7294d7d2ee00c9cc1be41bbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afeebcea-17f8-4375-bf26-5a290876fe4b", "node_type": "1", "metadata": {}, "hash": "e48d2e31b94aaf985dde26b9365c57ea82b6f086ca79bf9968b94305633f7803", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}}, "text": "[89] N. Anand and T. Achim, \u201cProtein structure and sequence gen-\neration with equivariant denoising diffusion probabilistic models,\u201d\narXiv:2205.15019, 2022.\n[90] M. Xu, L. Yu, Y . Song etal., \u201cGeodiff: A geometric diffusion model\nfor molecular conformation generation,\u201d in ICLR, 2022.\n[91] E. Hoogeboom, V .", "start_char_idx": 931, "end_char_idx": 1238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afeebcea-17f8-4375-bf26-5a290876fe4b": {"__data__": {"id_": "afeebcea-17f8-4375-bf26-5a290876fe4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77e00ea8-8fb5-4044-91fd-7b108537e696", "node_type": "1", "metadata": {}, "hash": "bf3777df857681a33c3915572d2ba0608a557201169a931fa5351735775bc0b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "node_type": "1", "metadata": {}, "hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "class_name": "RelatedNodeInfo"}}, "text": "[91] E. Hoogeboom, V . G. Satorras, C. Vignac, and M. Welling, \u201cEquivari-\nant diffusion for molecule generation in 3d,\u201d in ICML, 2022.\n[92] B. Jing, G. Corso, J. Chang etal., \u201cTorsional diffusion for molecular\nconformer generation,\u201d in NeurIPS, 2022.", "start_char_idx": 1216, "end_char_idx": 1466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbc3690b-efdf-4a99-943f-660533e7b5f9": {"__data__": {"id_": "cbc3690b-efdf-4a99-943f-660533e7b5f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d4afe67-524b-4449-a53a-5b1d6ae6c70f", "node_type": "1", "metadata": {}, "hash": "8edc8e65283541f8c15f5ecce593f0b9e2b2228dce4d420f53679e94054d3b16", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4d4afe67-524b-4449-a53a-5b1d6ae6c70f", "node_type": "1", "metadata": {}, "hash": "8edc8e65283541f8c15f5ecce593f0b9e2b2228dce4d420f53679e94054d3b16", "class_name": "RelatedNodeInfo"}}, "text": "[93] Z. Huang, L. Yang, X. Zhou etal., \u201cProtein-ligand interaction prior for\nbinding-aware 3d molecule diffusion models,\u201d in ICLR, 2024.", "start_char_idx": 0, "end_char_idx": 136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32985451-0f54-4e07-80c7-552d7245cb21": {"__data__": {"id_": "32985451-0f54-4e07-80c7-552d7245cb21", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82a40930-4bf1-484d-8e80-27b16b533ce9", "node_type": "1", "metadata": {}, "hash": "c3fab7c99fcaa1e6924d82710f438cb1a5b46b08f11ddd7c8353261eafaa9e7c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}}, "text": "[94] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit mod-\nels,\u201d in ICLR, 2021.\n[95] X. Liu, C. Gong, and Q. Liu, \u201cFlow straight and fast: Learning to\ngenerate and transfer data with rectified flow,\u201d 2023.\n[96] Y . Song, P. Dhariwal, M. Chen, and I. Sutskever, \u201cConsistency models,\u201d\ninICML, 2023.\n[97] J. Gui, Z. Sun, Y .", "start_char_idx": 0, "end_char_idx": 334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82a40930-4bf1-484d-8e80-27b16b533ce9": {"__data__": {"id_": "82a40930-4bf1-484d-8e80-27b16b533ce9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32985451-0f54-4e07-80c7-552d7245cb21", "node_type": "1", "metadata": {}, "hash": "6e1a5a210dc7d842d97c68742a1cdfb9ee154be44ce2d8b8d5d9f15e7e2f84b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbb1b4f2-8ae6-42b3-b5a5-32c0fd1a45bc", "node_type": "1", "metadata": {}, "hash": "4c977108c3535d2913c7e9f1c7b448f2d8043b89fc8b530eac08f82f82ba2c71", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}}, "text": "[97] J. Gui, Z. Sun, Y . Wen etal., \u201cA review on generative adversarial\nnetworks: Algorithms, theory, and applications,\u201d TKDE, vol. 35, no. 4,\npp. 3313\u20133332, 2023.\n[98] S. E. Robertson and S. Walker, \u201cOn relevance weights with little\nrelevance information,\u201d in SIGIR, 1997.", "start_char_idx": 310, "end_char_idx": 583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbb1b4f2-8ae6-42b3-b5a5-32c0fd1a45bc": {"__data__": {"id_": "cbb1b4f2-8ae6-42b3-b5a5-32c0fd1a45bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82a40930-4bf1-484d-8e80-27b16b533ce9", "node_type": "1", "metadata": {}, "hash": "c3fab7c99fcaa1e6924d82710f438cb1a5b46b08f11ddd7c8353261eafaa9e7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "401146da-c25d-4ba5-ad60-1e09269f229f", "node_type": "1", "metadata": {}, "hash": "ad0a048b31dfa982b05f70d4998785d5e24708a8bedb3a284200f1244c6b2c1b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}}, "text": "[99] J. D. Lafferty and C. Zhai, \u201cDocument language models, query models,\nand risk minimization for information retrieval,\u201d in SIGIR, 2001.\n[100] Y . Liu, M. Ott etal., \u201cRoberta: A robustly optimized BERT pretraining\napproach,\u201d arxiv:1907.11692, 2019.\n[101] L. Xiong, C. Xiong, Y .", "start_char_idx": 584, "end_char_idx": 865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "401146da-c25d-4ba5-ad60-1e09269f229f": {"__data__": {"id_": "401146da-c25d-4ba5-ad60-1e09269f229f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbb1b4f2-8ae6-42b3-b5a5-32c0fd1a45bc", "node_type": "1", "metadata": {}, "hash": "4c977108c3535d2913c7e9f1c7b448f2d8043b89fc8b530eac08f82f82ba2c71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf048c23-77df-48f8-b313-a9669d8d1d1c", "node_type": "1", "metadata": {}, "hash": "a8320f9c6c05fb94f30817a491c529d5fc38ddcea3f49ffca8a3bd6fc1e5c2d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}}, "text": "[101] L. Xiong, C. Xiong, Y . Li etal., \u201cApproximate nearest neighbor\nnegative contrastive learning for dense text retrieval,\u201d in ICLR, 2021.\n[102] H. Zhang, Y . Gong, Y . Shen etal., \u201cAdversarial retriever-ranker for\ndense text retrieval,\u201d in ICLR, 2022.\n[103] Y . Qu, Y .", "start_char_idx": 836, "end_char_idx": 1109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf048c23-77df-48f8-b313-a9669d8d1d1c": {"__data__": {"id_": "cf048c23-77df-48f8-b313-a9669d8d1d1c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "401146da-c25d-4ba5-ad60-1e09269f229f", "node_type": "1", "metadata": {}, "hash": "ad0a048b31dfa982b05f70d4998785d5e24708a8bedb3a284200f1244c6b2c1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1691a577-07ca-4fb1-ae78-7496bf7c34f2", "node_type": "1", "metadata": {}, "hash": "5152f74bb3173eb01ee428cb6017b0d6fd1cdc9e2995df6a3d7df4421d5a47a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}}, "text": "[103] Y . Qu, Y . Ding, J. Liu etal., \u201cRocketqa: An optimized training approach\nto dense passage retrieval for open-domain question answering,\u201d in\nNAACL-HLT, 2021.\n[104] L. Gao and J. Callan, \u201cCondenser: a pre-training architecture for dense\nretrieval,\u201d in EMNLP, 2021.\n[105] D. Guo, S. Ren etal., \u201cGraphcodebert: Pre-training code representa-\ntions with data flow,\u201d in ICLR, 2021.", "start_char_idx": 1092, "end_char_idx": 1473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1691a577-07ca-4fb1-ae78-7496bf7c34f2": {"__data__": {"id_": "1691a577-07ca-4fb1-ae78-7496bf7c34f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf048c23-77df-48f8-b313-a9669d8d1d1c", "node_type": "1", "metadata": {}, "hash": "a8320f9c6c05fb94f30817a491c529d5fc38ddcea3f49ffca8a3bd6fc1e5c2d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c8034349-37aa-434a-baba-a1780c471578", "node_type": "1", "metadata": {}, "hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "class_name": "RelatedNodeInfo"}}, "text": "[106] Y .", "start_char_idx": 1474, "end_char_idx": 1483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ec3eea0-5965-4197-86c4-c6de0f49eb59": {"__data__": {"id_": "0ec3eea0-5965-4197-86c4-c6de0f49eb59", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4d029c3-4be1-41fb-b582-9b251b13ce44", "node_type": "1", "metadata": {}, "hash": "bb607afb8ab9e4c79a6f9644d555f42617a7b6173500270a2801506da8dc44be", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}}, "text": "[106] Y . Wang, W. Wang, S. R. Joty, and S. C. H. Hoi, \u201cCodet5: Identifier-\naware unified pre-trained encoder-decoder models for code understand-\ning and generation,\u201d in EMNLP, 2021.\n[107] S. Hershey, S. Chaudhuri etal., \u201cCNN architectures for large-scale\naudio classification,\u201d in ICASSP, 2017.", "start_char_idx": 0, "end_char_idx": 295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4d029c3-4be1-41fb-b582-9b251b13ce44": {"__data__": {"id_": "b4d029c3-4be1-41fb-b582-9b251b13ce44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ec3eea0-5965-4197-86c4-c6de0f49eb59", "node_type": "1", "metadata": {}, "hash": "016fa8a704ebeaa45b595a36bbe1b9d94b0ffe29f004eafa485e71a61313cd8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ed89d36-44f5-49b6-b641-ec69f55c4c45", "node_type": "1", "metadata": {}, "hash": "a2912157e1bc157e274d9de5f00b6f9f1bb535d826099043061d1adce90e4b0a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}}, "text": "[108] X. Yuan, Z. Lin, J. Kuen etal., \u201cMultimodal contrastive training for\nvisual representation learning,\u201d in CVPR, 2021.\n[109] J. Dong, X. Li, C. Xu etal., \u201cDual encoding for zero-example video\nretrieval,\u201d in CVPR, 2019.\n[110] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, \u201cFrozen in time:\nA joint video and image encoder for end-to-end retrieval,\u201d in ICCV,\n2021.", "start_char_idx": 296, "end_char_idx": 664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ed89d36-44f5-49b6-b641-ec69f55c4c45": {"__data__": {"id_": "3ed89d36-44f5-49b6-b641-ec69f55c4c45", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4d029c3-4be1-41fb-b582-9b251b13ce44", "node_type": "1", "metadata": {}, "hash": "bb607afb8ab9e4c79a6f9644d555f42617a7b6173500270a2801506da8dc44be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3afa8dc-ece3-4538-859d-2395de671669", "node_type": "1", "metadata": {}, "hash": "fb7bb6e405e2dac3f190c757accbfe4e8de75e9d4fc5b225c5e1fe7d625e20ab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}}, "text": "[111] J. Zhan, J. Mao, Y . Liu etal., \u201cOptimizing dense retrieval model\ntraining with hard negatives,\u201d in SIGIR, 2021.\n[112] J. L. Bentley, \u201cMultidimensional binary search trees used for associa-\ntive searching,\u201d CACM, vol. 18, no. 9, pp. 509\u2013517, 1975.", "start_char_idx": 665, "end_char_idx": 918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3afa8dc-ece3-4538-859d-2395de671669": {"__data__": {"id_": "d3afa8dc-ece3-4538-859d-2395de671669", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ed89d36-44f5-49b6-b641-ec69f55c4c45", "node_type": "1", "metadata": {}, "hash": "a2912157e1bc157e274d9de5f00b6f9f1bb535d826099043061d1adce90e4b0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "284e6c20-913d-45df-943d-2415e097f426", "node_type": "1", "metadata": {}, "hash": "61c247edd1304705944f140830d9a1d9e9bea189fff7831e14ce337d565ef1fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}}, "text": "18, no. 9, pp. 509\u2013517, 1975.\n[113] W. Li, C. Feng, D. Lian etal., \u201cLearning balanced tree indexes for\nlarge-scale vector retrieval,\u201d in SIGKDDg, 2023.[114] M. Datar, N. Immorlica, P. Indyk etal., \u201cLocality-sensitive hashing\nscheme based on p-stable distributions,\u201d in SCG, 2004.\n[115] Y .", "start_char_idx": 889, "end_char_idx": 1178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "284e6c20-913d-45df-943d-2415e097f426": {"__data__": {"id_": "284e6c20-913d-45df-943d-2415e097f426", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3afa8dc-ece3-4538-859d-2395de671669", "node_type": "1", "metadata": {}, "hash": "fb7bb6e405e2dac3f190c757accbfe4e8de75e9d4fc5b225c5e1fe7d625e20ab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e", "node_type": "1", "metadata": {}, "hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "class_name": "RelatedNodeInfo"}}, "text": "[115] Y . A. Malkov and D. A. Yashunin, \u201cEfficient and robust approxi-\nmate nearest neighbor search using hierarchical navigable small world\ngraphs,\u201d TPAMI, vol. 42, no. 4, pp. 824\u2013836, 2018.\n[116] S. Jayaram Subramanya, F. Devvrit etal., \u201cDiskann: Fast accurate\nbillion-point nearest neighbor search on a single node,\u201d NeurIPS, 2019.", "start_char_idx": 1169, "end_char_idx": 1503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb895d59-7721-489d-8f8f-a2433ddff395": {"__data__": {"id_": "fb895d59-7721-489d-8f8f-a2433ddff395", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e905510b-f520-4148-ac32-cfde6e07b550", "node_type": "1", "metadata": {}, "hash": "6671f6f4a810ac84da61d0269bb1e30d5229215a50a58043608b430e970fc719", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}}, "text": "[117] J. Ren, M. Zhang, and D. Li, \u201cHm-ann: Efficient billion-point nearest\nneighbor search on heterogeneous memory,\u201d NeurIPS, 2020.\n[118] Y . Wang, Y . Hou, H. Wang etal., \u201cA neural corpus indexer for\ndocument retrieval,\u201d in NeurIPS, 2022.\n[119] H. Zhang, Y . Wang, Q. Chen etal., \u201cModel-enhanced vector index,\u201d\ninNeurIPS, 2023.", "start_char_idx": 0, "end_char_idx": 329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e905510b-f520-4148-ac32-cfde6e07b550": {"__data__": {"id_": "e905510b-f520-4148-ac32-cfde6e07b550", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb895d59-7721-489d-8f8f-a2433ddff395", "node_type": "1", "metadata": {}, "hash": "3a87a652a6f86075d377ce9afc2ccce3b28bdfcaffbe872dce1c347d018b12f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7fdee8d-fb8b-4b70-88d7-28387deff320", "node_type": "1", "metadata": {}, "hash": "4d7e1a39e009e85483a05336a17d42c29f88e56145e0203d53721033c59a77b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}}, "text": "[120] S. A. Hayati, R. Olivier, P. Avvaru etal., \u201cRetrieval-based neural code\ngeneration,\u201d in EMNLP, 2018.\n[121] J. Zhang, X. Wang, H. Zhang etal., \u201cRetrieval-based neural source\ncode summarization,\u201d in ICSE, 2020.\n[122] G. Poesia, A. Polozov, V . Le etal., \u201cSynchromesh: Reliable code\ngeneration from pre-trained language models,\u201d in ICLR, 2022.", "start_char_idx": 330, "end_char_idx": 676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7fdee8d-fb8b-4b70-88d7-28387deff320": {"__data__": {"id_": "e7fdee8d-fb8b-4b70-88d7-28387deff320", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e905510b-f520-4148-ac32-cfde6e07b550", "node_type": "1", "metadata": {}, "hash": "6671f6f4a810ac84da61d0269bb1e30d5229215a50a58043608b430e970fc719", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d97b4cae-b4b0-4f06-a7be-1069f141ac10", "node_type": "1", "metadata": {}, "hash": "a1b1aaf1104de8f0bfa389a154d097aec46b0cfa48570c87403bc96b16c6acf2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}}, "text": "[123] X. Ye, S. Yavuz etal., \u201cRNG-KBQA: generation augmented iterative\nranking for knowledge base question answering,\u201d in ACL, 2022.\n[124] Y . Shu etal., \u201cTIARA: multi-grained retrieval for robust question\nanswering over large knowledge bases,\u201d arXiv:2210.12925, 2022.\n[125] X. V .", "start_char_idx": 677, "end_char_idx": 958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d97b4cae-b4b0-4f06-a7be-1069f141ac10": {"__data__": {"id_": "d97b4cae-b4b0-4f06-a7be-1069f141ac10", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7fdee8d-fb8b-4b70-88d7-28387deff320", "node_type": "1", "metadata": {}, "hash": "4d7e1a39e009e85483a05336a17d42c29f88e56145e0203d53721033c59a77b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dec47915-f50b-44a2-be1d-c858cfbae020", "node_type": "1", "metadata": {}, "hash": "c3171852d81b620af7e61e3eeb8c6fa8ddafc17b0f9dcfbee952350af953a8f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}}, "text": "[125] X. V . Lin, R. Socher etal., \u201cBridging textual and tabular data for\ncross-domain text-to-sql semantic parsing,\u201d arXiv:2012.12627, 2020.\n[126] A. Asai, Z. Wu, Y . Wang etal., \u201cSelf-rag: Learning to retrieve, generate,\nand critique through self-reflection,\u201d arxiv:2310.11511, 2023.", "start_char_idx": 946, "end_char_idx": 1231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dec47915-f50b-44a2-be1d-c858cfbae020": {"__data__": {"id_": "dec47915-f50b-44a2-be1d-c858cfbae020", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d97b4cae-b4b0-4f06-a7be-1069f141ac10", "node_type": "1", "metadata": {}, "hash": "a1b1aaf1104de8f0bfa389a154d097aec46b0cfa48570c87403bc96b16c6acf2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60", "node_type": "1", "metadata": {}, "hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "class_name": "RelatedNodeInfo"}}, "text": "[127] W. Shi, S. Min, M. Yasunaga etal., \u201cReplug: Retrieval-augmented\nblack-box language models,\u201d arXiv:2301.12652, 2023.\n[128] O. Ram, Y .", "start_char_idx": 1232, "end_char_idx": 1371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "476c0ba3-36ff-4975-a779-221211e47d3b": {"__data__": {"id_": "476c0ba3-36ff-4975-a779-221211e47d3b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6448425b-2559-4546-8595-c3b65dd0e3e4", "node_type": "1", "metadata": {}, "hash": "0286e9e3d50cd3d87b69996feff4938ccd8fdfcdff7d4511d1eb04f082addb5c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}}, "text": "[128] O. Ram, Y . Levine, I. Dalmedigos etal., \u201cIn-context retrieval-\naugmented language models,\u201d arXiv:2302.00083, 2023.\n[129] D. Zan, B. Chen, Z. Lin etal., \u201cWhen language model meets private\nlibrary,\u201d in EMNLP Findings, 2022.\n[130] N. Nashid, M. Sintaha, and A. Mesbah, \u201cRetrieval-based prompt\nselection for code-related few-shot learning,\u201d in ICSE, 2023.", "start_char_idx": 0, "end_char_idx": 358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6448425b-2559-4546-8595-c3b65dd0e3e4": {"__data__": {"id_": "6448425b-2559-4546-8595-c3b65dd0e3e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "476c0ba3-36ff-4975-a779-221211e47d3b", "node_type": "1", "metadata": {}, "hash": "7fc2dfa101bea461f9d42ba6b5081e56374181aa78e977c8627aaf7190dab339", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a35ffa7-9401-4a09-98af-479d127abace", "node_type": "1", "metadata": {}, "hash": "3ddd875a82ae87b6dacee134dfac0046dc0fc56803b10cda69076fb4a54f5115", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}}, "text": "[131] M. Jin, S. Shahriar, M. Tufano etal., \u201cInferfix: End-to-end program\nrepair with llms,\u201d in ESEC/FSE, 2023.\n[132] S. Lu, N. Duan, H. Han etal., \u201cReacc: A retrieval-augmented code\ncompletion framework,\u201d in ACL, 2022.\n[133] Y . Liu etal., \u201cUni-parser: Unified semantic parser for question answer-\ning on knowledge base and database,\u201d in EMNLP, 2022.", "start_char_idx": 359, "end_char_idx": 710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a35ffa7-9401-4a09-98af-479d127abace": {"__data__": {"id_": "7a35ffa7-9401-4a09-98af-479d127abace", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6448425b-2559-4546-8595-c3b65dd0e3e4", "node_type": "1", "metadata": {}, "hash": "0286e9e3d50cd3d87b69996feff4938ccd8fdfcdff7d4511d1eb04f082addb5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1814bc3-fec5-4b96-82b3-676e2b6b0347", "node_type": "1", "metadata": {}, "hash": "af2299a56891f2f862fccb94236f7866a0f5c59ef49ba60e0f5f0b1b84490e4b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}}, "text": "[134] Z. Yang, X. Du, E. Cambria etal., \u201cEnd-to-end case-based reasoning\nfor commonsense knowledge base completion,\u201d in EACL, 2023.\n[135] M. Patidar, A. K. Singh, R. Sawhney etal., \u201cCombining transfer\nlearning with in-context learning using blackbox llms for zero-shot\nknowledge base question answering,\u201d arXiv:2311.08894, 2023.\n[136] W. Shi, Y . Zhuang, Y .", "start_char_idx": 711, "end_char_idx": 1069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1814bc3-fec5-4b96-82b3-676e2b6b0347": {"__data__": {"id_": "c1814bc3-fec5-4b96-82b3-676e2b6b0347", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a35ffa7-9401-4a09-98af-479d127abace", "node_type": "1", "metadata": {}, "hash": "3ddd875a82ae87b6dacee134dfac0046dc0fc56803b10cda69076fb4a54f5115", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7191792-a0cf-4120-a368-bbd2841d09a2", "node_type": "1", "metadata": {}, "hash": "83ca15596edab9d4a4e4fa43e7bcc948adb4d49d2e4f199086788cd0f6913299", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}}, "text": "[136] W. Shi, Y . Zhuang, Y . Zhu etal., \u201cRetrieval-augmented large language\nmodels for adolescent idiopathic scoliosis patients in shared decision-\nmaking,\u201d in ACM-BCB, 2023.\n[137] A. Casanova, M. Careil, J. Verbeek etal., \u201cInstance-conditioned gan,\u201d\ninNeurIPS, 2021.\n[138] J. Li, Y .", "start_char_idx": 1040, "end_char_idx": 1325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7191792-a0cf-4120-a368-bbd2841d09a2": {"__data__": {"id_": "c7191792-a0cf-4120-a368-bbd2841d09a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1814bc3-fec5-4b96-82b3-676e2b6b0347", "node_type": "1", "metadata": {}, "hash": "af2299a56891f2f862fccb94236f7866a0f5c59ef49ba60e0f5f0b1b84490e4b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "node_type": "1", "metadata": {}, "hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "class_name": "RelatedNodeInfo"}}, "text": "[138] J. Li, Y . Li, G. Li etal., \u201cEditsum: A retrieve-and-edit framework for\nsource code summarization,\u201d in ASE, 2021.", "start_char_idx": 1309, "end_char_idx": 1428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f91f59a-b476-412b-9245-f889cbc210fd": {"__data__": {"id_": "0f91f59a-b476-412b-9245-f889cbc210fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be6792a4-da55-47a2-b289-7b1b5869811d", "node_type": "1", "metadata": {}, "hash": "f96435e0fc6aced74c8216eab27599c4dced159add03bf04dae32d2430393afc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "be6792a4-da55-47a2-b289-7b1b5869811d", "node_type": "1", "metadata": {}, "hash": "f96435e0fc6aced74c8216eab27599c4dced159add03bf04dae32d2430393afc", "class_name": "RelatedNodeInfo"}}, "text": "[139] C. Yu, G. Yang, X. Chen etal., \u201cBashexplainer: Retrieval-augmented\nbash code comment generation based on fine-tuned codebert,\u201d in\nICSME, 2022.\n[140] T. B. Hashimoto, K. Guu, Y .", "start_char_idx": 0, "end_char_idx": 183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60d49ff3-1b7b-4a96-8e1f-afabc2cee86c": {"__data__": {"id_": "60d49ff3-1b7b-4a96-8e1f-afabc2cee86c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02c2a2a6-761e-4211-bfd5-86c926d63d22", "node_type": "1", "metadata": {}, "hash": "dbdd689ba75bfdd2280cc0d59256b3918b8eb1e8240b75a88b9ea05f218205b7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}}, "text": "[140] T. B. Hashimoto, K. Guu, Y . Oren, and P. Liang, \u201cA retrieve-and-edit\nframework for predicting structured outputs,\u201d in NeurIPS, 2018.\n[141] B. Wei, Y . Li, G. Li etal., \u201cRetrieve and refine: Exemplar-based neural\ncomment generation,\u201d in ASE, 2020.\n[142] E. Shi, Y . Wang, W. Tao etal., \u201cRACE: retrieval-augmented commit\nmessage generation,\u201d in EMNLP, 2022.", "start_char_idx": 0, "end_char_idx": 362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02c2a2a6-761e-4211-bfd5-86c926d63d22": {"__data__": {"id_": "02c2a2a6-761e-4211-bfd5-86c926d63d22", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60d49ff3-1b7b-4a96-8e1f-afabc2cee86c", "node_type": "1", "metadata": {}, "hash": "242fa1918ffb0da90b5b82992a5e4878ed0839be50c891420bee4863cbe09834", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "749657c9-194f-478d-b477-5fa7311abb91", "node_type": "1", "metadata": {}, "hash": "8edefe3efa2b413f716f0df12a5a3daa56a1c34812b2309e6bf3e96a2178e828", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}}, "text": "[143] B. Oguz, X. Chen, V . Karpukhin etal., \u201cUnik-qa: Unified repre-\nsentations of structured and unstructured knowledge for open-domain\nquestion answering,\u201d in NAACL Findings, 2022.\n[144] D. Yu, S. Zhang etal., \u201cDecaf: Joint decoding of answers and logical\nforms for question answering over knowledge bases,\u201d in ICLR, 2023.", "start_char_idx": 363, "end_char_idx": 688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "749657c9-194f-478d-b477-5fa7311abb91": {"__data__": {"id_": "749657c9-194f-478d-b477-5fa7311abb91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02c2a2a6-761e-4211-bfd5-86c926d63d22", "node_type": "1", "metadata": {}, "hash": "dbdd689ba75bfdd2280cc0d59256b3918b8eb1e8240b75a88b9ea05f218205b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02ca04f3-521e-43e5-8af5-2acd94eed1c6", "node_type": "1", "metadata": {}, "hash": "09815c57fa0b93b03b3d2afa08a681a76e740d5cbbd73ac4a269f79f02a8fea3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}}, "text": "[145] G. Dong, R. Li, S. Wang etal., \u201cBridging the kb-text gap: Leveraging\nstructured knowledge-aware pre-training for KBQA,\u201d in CIKM, 2023.\n[146] K. Wang, F. Duan, S. Wang etal., \u201cKnowledge-driven cot: Exploring\nfaithful reasoning in llms for knowledge-intensive question answering,\u201d\narXiv:2308.13259, 2023.\n[147] D. Yu and Y .", "start_char_idx": 689, "end_char_idx": 1017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02ca04f3-521e-43e5-8af5-2acd94eed1c6": {"__data__": {"id_": "02ca04f3-521e-43e5-8af5-2acd94eed1c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "749657c9-194f-478d-b477-5fa7311abb91", "node_type": "1", "metadata": {}, "hash": "8edefe3efa2b413f716f0df12a5a3daa56a1c34812b2309e6bf3e96a2178e828", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c4d1346-cb20-48bd-8b99-723773cb2c04", "node_type": "1", "metadata": {}, "hash": "464640c70193231b16ee89b72e5fbd7f8e746a1f99bc02e0b9ec8c04a71dde62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}}, "text": "[147] D. Yu and Y . Yang, \u201cRetrieval-enhanced generative model for large-\nscale knowledge graph completion,\u201d in SIGIR, 2023.", "start_char_idx": 998, "end_char_idx": 1122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c4d1346-cb20-48bd-8b99-723773cb2c04": {"__data__": {"id_": "0c4d1346-cb20-48bd-8b99-723773cb2c04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02ca04f3-521e-43e5-8af5-2acd94eed1c6", "node_type": "1", "metadata": {}, "hash": "09815c57fa0b93b03b3d2afa08a681a76e740d5cbbd73ac4a269f79f02a8fea3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "280e9ac2-9291-48a8-9bf0-2cbb0074a5b2", "node_type": "1", "metadata": {}, "hash": "3b39c5e6ce412a9abca8fc16f6cb3b40899ca3563cd916833c688215bc969554", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}}, "text": "[148] W. Chen, H. Hu, C. Saharia, and W. W. Cohen, \u201cRe-imagen: Retrieval-\naugmented text-to-image generator,\u201d in ICLR, 2023.25\n[149] S. Sheynin, O. Ashual, A. Polyak etal., \u201cKnn-diffusion: Image gener-\nation via large-scale retrieval,\u201d in ICLR, 2023.", "start_char_idx": 1123, "end_char_idx": 1373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "280e9ac2-9291-48a8-9bf0-2cbb0074a5b2": {"__data__": {"id_": "280e9ac2-9291-48a8-9bf0-2cbb0074a5b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c4d1346-cb20-48bd-8b99-723773cb2c04", "node_type": "1", "metadata": {}, "hash": "464640c70193231b16ee89b72e5fbd7f8e746a1f99bc02e0b9ec8c04a71dde62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "99895f25-3030-456c-a22c-ea588744446b", "node_type": "1", "metadata": {}, "hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "class_name": "RelatedNodeInfo"}}, "text": "[150] A. Blattmann, R. Rombach, K. Oktay etal., \u201cRetrieval-augmented\ndiffusion models,\u201d in NeurIPS, 2022.", "start_char_idx": 1374, "end_char_idx": 1479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f60617e-1a2a-40e2-a93c-1dc9b6ab3bb4": {"__data__": {"id_": "9f60617e-1a2a-40e2-a93c-1dc9b6ab3bb4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dc9760a-cda7-4939-8472-c521a7f46e66", "node_type": "1", "metadata": {}, "hash": "fcd46a50fcef1ddbfdd3e2b562f84c6a8692fbfea8bac39ea1ebfdf67970e8db", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}}, "text": "[151] R. Rombach, A. Blattmann, and B. Ommer, \u201cText-guided synthe-\nsis of artistic images with retrieval-augmented diffusion models,\u201d\narXiv:2207.13038, 2022.\n[152] B. Li, P. H. Torr, and T. Lukasiewicz, \u201cMemory-driven text-to-image\ngeneration,\u201d arXiv:2208.07022, 2022.", "start_char_idx": 0, "end_char_idx": 268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dc9760a-cda7-4939-8472-c521a7f46e66": {"__data__": {"id_": "3dc9760a-cda7-4939-8472-c521a7f46e66", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f60617e-1a2a-40e2-a93c-1dc9b6ab3bb4", "node_type": "1", "metadata": {}, "hash": "e4be48d349161dfcfb7e9ddb57c73f23fbe67cdf8afc366a4df412f0d7e81b65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "894a1219-36f1-4b31-986c-03bc089e1262", "node_type": "1", "metadata": {}, "hash": "de1d4b99c10db9aa551edd6112c6ea4ef5cd7a3bd8089bf226f53f9f7bdfdeb8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}}, "text": "[153] A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley, \u201cUnlimiformer:\nLong-range transformers with unlimited length input,\u201d 2023.\n[154] Y . Kuratov, A. Bulatov etal., \u201cIn search of needles in a 10m haystack:\nRecurrent memory finds what llms miss,\u201d arXiv:2402.10790, 2024.", "start_char_idx": 269, "end_char_idx": 542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "894a1219-36f1-4b31-986c-03bc089e1262": {"__data__": {"id_": "894a1219-36f1-4b31-986c-03bc089e1262", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dc9760a-cda7-4939-8472-c521a7f46e66", "node_type": "1", "metadata": {}, "hash": "fcd46a50fcef1ddbfdd3e2b562f84c6a8692fbfea8bac39ea1ebfdf67970e8db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33afe81c-7321-4870-b761-ee193aa81ad2", "node_type": "1", "metadata": {}, "hash": "2b43f7fd4bbc49c5c11bce548c27d32b4e777aac016348e4a783bccf50e0bc91", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}}, "text": "[155] N. F. Liu, K. Lin, J. Hewitt etal., \u201cLost in the middle: How language\nmodels use long contexts,\u201d arxiv:2307.03172, 2023.\n[156] T. F \u00b4evry, L. B. Soares etal., \u201cEntities as experts: Sparse memory access\nwith entity supervision,\u201d in EMNLP, 2020.\n[157] M. de Jong, Y .", "start_char_idx": 543, "end_char_idx": 814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33afe81c-7321-4870-b761-ee193aa81ad2": {"__data__": {"id_": "33afe81c-7321-4870-b761-ee193aa81ad2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "894a1219-36f1-4b31-986c-03bc089e1262", "node_type": "1", "metadata": {}, "hash": "de1d4b99c10db9aa551edd6112c6ea4ef5cd7a3bd8089bf226f53f9f7bdfdeb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d1760c6-4e1b-40df-9be3-f2f77ecd10fd", "node_type": "1", "metadata": {}, "hash": "50e827384787f4f73008f65caef654179164d6ecb748091579c0581712b234af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}}, "text": "[157] M. de Jong, Y . Zemlyanskiy, N. FitzGerald etal., \u201cMention memory:\nincorporating textual knowledge into transformers through entity men-\ntion attention,\u201d in ICLR, 2021.\n[158] B. Jing, Y . Zhang, Z. Song etal., \u201cAmd: Anatomical motion diffusion\nwith interpretable motion decomposition and fusion,\u201d in AAAI, 2024.\n[159] Y .", "start_char_idx": 793, "end_char_idx": 1120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d1760c6-4e1b-40df-9be3-f2f77ecd10fd": {"__data__": {"id_": "6d1760c6-4e1b-40df-9be3-f2f77ecd10fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33afe81c-7321-4870-b761-ee193aa81ad2", "node_type": "1", "metadata": {}, "hash": "2b43f7fd4bbc49c5c11bce548c27d32b4e777aac016348e4a783bccf50e0bc91", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "node_type": "1", "metadata": {}, "hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "class_name": "RelatedNodeInfo"}}, "text": "[159] Y . Yuan, H. Liu, X. Liu etal., \u201cRetrieval-augmented text-to-audio\ngeneration,\u201d in ICASSP, 2024.\n[160] B. Yang, M. Cao, and Y . Zou, \u201cConcept-aware video captioning:\nDescribing videos with effective prior information,\u201d TIP, vol. 32, pp.\n5366\u20135378, 2023.", "start_char_idx": 1111, "end_char_idx": 1370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9b944ae-7d4c-4001-9ede-131a7bae71ed": {"__data__": {"id_": "d9b944ae-7d4c-4001-9ede-131a7bae71ed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6ec21f9-4ef7-4c2f-9981-ebb4ea136d25", "node_type": "1", "metadata": {}, "hash": "c87f7538e91a66d29f4e65c5c400e389214e34bcc169fb018b5051b089013b82", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}}, "text": "32, pp.\n5366\u20135378, 2023.\n[161] Z. Zhong, T. Lei, and D. Chen, \u201cTraining language models with\nmemory augmentation,\u201d in EMNLP, 2022.\n[162] S. Min, W. Shi, M. Lewis etal., \u201cNonparametric masked language\nmodeling,\u201d in ACL Findings, 2023.\n[163] X. Zhang, Y . Zhou, G. Yang, and T. Chen, \u201cSyntax-aware retrieval\naugmented code generation,\u201d in EMNLP Findings, 2023.", "start_char_idx": 0, "end_char_idx": 358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6ec21f9-4ef7-4c2f-9981-ebb4ea136d25": {"__data__": {"id_": "e6ec21f9-4ef7-4c2f-9981-ebb4ea136d25", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9b944ae-7d4c-4001-9ede-131a7bae71ed", "node_type": "1", "metadata": {}, "hash": "e14a8fc720d36aaf7ebaf31e04a2330924feb61e5ec30599b093090550d214a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e300f2d2-8346-4926-9e45-958579e50006", "node_type": "1", "metadata": {}, "hash": "6a7c02ced3fd4ac1205faed566ef63c127c203b47ba1cec9c0dc3a954355a87c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}}, "text": "[164] Z. Fei, \u201cMemory-augmented image captioning,\u201d in AAAI, 2021.\n[165] Y . Leviathan, M. Kalman, and Y . Matias, \u201cFast inference from trans-\nformers via speculative decoding,\u201d in ICML, 2023.\n[166] T. Lan, D. Cai, Y . Wang etal., \u201cCopy is all you need,\u201d in ICLR, 2023.", "start_char_idx": 359, "end_char_idx": 627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e300f2d2-8346-4926-9e45-958579e50006": {"__data__": {"id_": "e300f2d2-8346-4926-9e45-958579e50006", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6ec21f9-4ef7-4c2f-9981-ebb4ea136d25", "node_type": "1", "metadata": {}, "hash": "c87f7538e91a66d29f4e65c5c400e389214e34bcc169fb018b5051b089013b82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61eeeea0-f16d-4f99-861b-ab4e26d8f559", "node_type": "1", "metadata": {}, "hash": "e2e7fcd8947c9df3c2ea51aab17d9f5d0bd7ce6b71d0d262a20b1ea65e57c59a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}}, "text": "Wang etal., \u201cCopy is all you need,\u201d in ICLR, 2023.\n[167] B. Cao, D. Cai, L. Cui etal., \u201cRetrieval is accurate generation,\u201d\narXiv:2402.17532, 2024.\n[168] L. Wang, N. Yang, and F. Wei, \u201cQuery2doc: Query expansion with\nlarge language models,\u201d in EMNLP, 2023.", "start_char_idx": 577, "end_char_idx": 832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61eeeea0-f16d-4f99-861b-ab4e26d8f559": {"__data__": {"id_": "61eeeea0-f16d-4f99-861b-ab4e26d8f559", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e300f2d2-8346-4926-9e45-958579e50006", "node_type": "1", "metadata": {}, "hash": "6a7c02ced3fd4ac1205faed566ef63c127c203b47ba1cec9c0dc3a954355a87c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf9e4349-a22c-4b71-b1f4-43964496d78d", "node_type": "1", "metadata": {}, "hash": "ea193ec17f467678656fd450d8273954166a37c7dff016e229440b011ec23f24", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}}, "text": "[169] L. Gao, X. Ma, J. Lin, and J. Callan, \u201cPrecise zero-shot dense retrieval\nwithout relevance labels,\u201d in ACL, 2023.\n[170] G. Kim, S. Kim, B. Jeon etal., \u201cTree of clarifications: Answering\nambiguous questions with retrieval-augmented large language models,\u201d\ninEMNLP, 2023.", "start_char_idx": 833, "end_char_idx": 1108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf9e4349-a22c-4b71-b1f4-43964496d78d": {"__data__": {"id_": "cf9e4349-a22c-4b71-b1f4-43964496d78d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61eeeea0-f16d-4f99-861b-ab4e26d8f559", "node_type": "1", "metadata": {}, "hash": "e2e7fcd8947c9df3c2ea51aab17d9f5d0bd7ce6b71d0d262a20b1ea65e57c59a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e2690e2d-178c-4921-a734-f9334530f2e6", "node_type": "1", "metadata": {}, "hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "class_name": "RelatedNodeInfo"}}, "text": "[171] M. Xia, S. Malladi, S. Gururangan etal., \u201cLESS: selecting influential\ndata for targeted instruction tuning,\u201d arXiv:2402.04333, 2024.\n[172] S. Yao, J. Zhao, D. Yu etal., \u201cReact: Synergizing reasoning and acting\nin language models,\u201d in ICLR, 2023.", "start_char_idx": 1109, "end_char_idx": 1360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92eb3717-2b38-491d-baf3-6c0ff9b342e7": {"__data__": {"id_": "92eb3717-2b38-491d-baf3-6c0ff9b342e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d25af56-930f-403f-8f20-f02c3a197bc7", "node_type": "1", "metadata": {}, "hash": "eef2f65a892389cd1e9b956738993356488fc8ad644dd1fc6ea75dce8ef8ff24", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}}, "text": "[173] J. Wei, X. Wang, D. Schuurmans etal., \u201cChain-of-thought prompting\nelicits reasoning in large language models,\u201d in NeurIPS, 2022.\n[174] T. Pouplin, H. Sun, S. Holt, and M. Van der Schaar, \u201cRetrieval-\naugmented thought process as sequential decision making,\u201d\narXiv:2402.07812, 2024.\n[175] J. Liu, \u201cLlamaIndex,\u201d 11 2022. [Online]. Available: https://github.", "start_char_idx": 0, "end_char_idx": 360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d25af56-930f-403f-8f20-f02c3a197bc7": {"__data__": {"id_": "3d25af56-930f-403f-8f20-f02c3a197bc7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92eb3717-2b38-491d-baf3-6c0ff9b342e7", "node_type": "1", "metadata": {}, "hash": "b8e0c472b81098d79b399af1a212347f5ae96ad64c2652b5dee776368aeec2c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5414bd03-37a1-473a-858d-a56a90cd023b", "node_type": "1", "metadata": {}, "hash": "379e713ebe54fdbd426cd7c7ee6304a56209e04313ba11b9c925945aa7c8ca07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}}, "text": "[Online]. Available: https://github.\ncom/jerryjliu/llama index\n[176] P. Sarthi, S. Abdullah, A. Tuli etal., \u201cRaptor: Recursive abstractive\nprocessing for tree-organized retrieval,\u201d in ICLR, 2023.\n[177] S. Xiao, Z. Liu, P. Zhang etal., \u201cC-pack: Packaged resources to advance\ngeneral chinese embedding,\u201d arxiv:2309.07597, 2023.", "start_char_idx": 324, "end_char_idx": 649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5414bd03-37a1-473a-858d-a56a90cd023b": {"__data__": {"id_": "5414bd03-37a1-473a-858d-a56a90cd023b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d25af56-930f-403f-8f20-f02c3a197bc7", "node_type": "1", "metadata": {}, "hash": "eef2f65a892389cd1e9b956738993356488fc8ad644dd1fc6ea75dce8ef8ff24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e7379bb-e063-4df2-8867-a18129fcda65", "node_type": "1", "metadata": {}, "hash": "be9119a02e9d2969b0135681db778caa41f5b3a7f4deb0e4e4e11118aaee5598", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}}, "text": "[178] J. Chen, S. Xiao, P. Zhang etal., \u201cBge m3-embedding: Multi-lingual,\nmulti-functionality, multi-granularity text embeddings through self-\nknowledge distillation,\u201d arxiv:2309.07597, 2023.\n[179] S. Xiao, Z. Liu, P. Zhang, and X. Xing, \u201cLm-cocktail: Resilient tuning\nof language models via model merging,\u201d arxiv:2311.13534, 2023.", "start_char_idx": 650, "end_char_idx": 981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e7379bb-e063-4df2-8867-a18129fcda65": {"__data__": {"id_": "8e7379bb-e063-4df2-8867-a18129fcda65", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5414bd03-37a1-473a-858d-a56a90cd023b", "node_type": "1", "metadata": {}, "hash": "379e713ebe54fdbd426cd7c7ee6304a56209e04313ba11b9c925945aa7c8ca07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf97515-a241-4826-aae1-39007c84f619", "node_type": "1", "metadata": {}, "hash": "756e1017767c48066deec1d3e8e016d8f66895d088a09215e924c76dba96e7f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}}, "text": "[180] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, \u201cRetrieve anything\nto augment large language models,\u201d arxiv:2310.07554, 2023.\n[181] M. Kulkarni, P. Tangarajan, K. Kim etal., \u201cReinforcement learning for\noptimizing RAG for domain chatbots,\u201d arXiv:2401.06800, 2024.\n[182] W. Wang, Y .", "start_char_idx": 982, "end_char_idx": 1272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf97515-a241-4826-aae1-39007c84f619": {"__data__": {"id_": "abf97515-a241-4826-aae1-39007c84f619", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e7379bb-e063-4df2-8867-a18129fcda65", "node_type": "1", "metadata": {}, "hash": "be9119a02e9d2969b0135681db778caa41f5b3a7f4deb0e4e4e11118aaee5598", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d88f6d7f-5944-408b-bd56-1536e37a36df", "node_type": "1", "metadata": {}, "hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "class_name": "RelatedNodeInfo"}}, "text": "[182] W. Wang, Y . Wang etal., \u201cRap-gen: Retrieval-augmented patch gener-\nation with codet5 for automatic program repair,\u201d in ESEC/FSE, 2023.\n[183] S.-Q. Yan, J.-C. Gu, Y .", "start_char_idx": 1254, "end_char_idx": 1426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4acb61f4-1b83-4081-9d70-8affb4aa5e0c": {"__data__": {"id_": "4acb61f4-1b83-4081-9d70-8affb4aa5e0c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3592c0a4-8fb2-4091-8a28-67a42ea67b94", "node_type": "1", "metadata": {}, "hash": "d363c37c0208a96b75cdf2c8c5083629ba0e987c7fb6517541f69fbf03d4867c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3592c0a4-8fb2-4091-8a28-67a42ea67b94", "node_type": "1", "metadata": {}, "hash": "d363c37c0208a96b75cdf2c8c5083629ba0e987c7fb6517541f69fbf03d4867c", "class_name": "RelatedNodeInfo"}}, "text": "Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, \u201cCorrective retrieval\naugmented generation,\u201d arXiv:2401.15884, 2024.\n[184] W. Huang, M. Lapata, P. V ougiouklis etal., \u201cRetrieval augmented\ngeneration with rich answer encoding,\u201d in IJCNLP-AACL, 2023.", "start_char_idx": 0, "end_char_idx": 240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4530412d-0461-4aec-8c2e-94451c75012d": {"__data__": {"id_": "4530412d-0461-4aec-8c2e-94451c75012d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e1b6ac2-7a00-4b97-981b-5fc244c31c5e", "node_type": "1", "metadata": {}, "hash": "796cc5569f7b79f17e863cb81c44dd080744263907b9b900251d4c9fff9c50c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}}, "text": "[185] H. Wang, W. Huang, Y . Deng etal., \u201cUnims-rag: A unified multi-source\nretrieval-augmented generation for personalized dialogue systems,\u201d\narXiv:2401.13256, 2024.\n[186] M. R. Glass, G. Rossiello, M. F. M. Chowdhury etal., \u201cRe2g: Retrieve,\nrerank, generate,\u201d in NAACL, 2022.", "start_char_idx": 0, "end_char_idx": 277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e1b6ac2-7a00-4b97-981b-5fc244c31c5e": {"__data__": {"id_": "0e1b6ac2-7a00-4b97-981b-5fc244c31c5e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4530412d-0461-4aec-8c2e-94451c75012d", "node_type": "1", "metadata": {}, "hash": "ae2cb352b8a9a5b6a37bb9a45fdbd374d533da31d0634b37b84c984be158a440", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "045dbdf7-80fc-4477-9cdd-fe8ba3dffe30", "node_type": "1", "metadata": {}, "hash": "db6d4dbab74a08a5e05a2caf0c64a41251f0051f7446415202216221934bce42", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}}, "text": "[187] R. F. Nogueira and K. Cho, \u201cPassage re-ranking with BERT,\u201d\narxiv:1901.04085, 2019.\n[188] J. Li, Y . Zhao, Y . Li etal., \u201cAcecoder: Utilizing existing code to\nenhance code generation,\u201d arXiv:2303.17780, 2023.", "start_char_idx": 278, "end_char_idx": 491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "045dbdf7-80fc-4477-9cdd-fe8ba3dffe30": {"__data__": {"id_": "045dbdf7-80fc-4477-9cdd-fe8ba3dffe30", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e1b6ac2-7a00-4b97-981b-5fc244c31c5e", "node_type": "1", "metadata": {}, "hash": "796cc5569f7b79f17e863cb81c44dd080744263907b9b900251d4c9fff9c50c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9735bdd-5304-4a97-9c17-ebaf054f6327", "node_type": "1", "metadata": {}, "hash": "4a6ccc1997816d156334f654f25e0bd57f793ede736e3f22d03a2d5844eb3b0e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}}, "text": "[189] P. Shi, R. Zhang, H. Bai, and J. Lin, \u201cXRICL: cross-lingual retrieval-\naugmented in-context learning for cross-lingual text-to-sql semantic\nparsing,\u201d in EMNLP Findings, 2022.\n[190] K. Rangan and Y . Yin, \u201cA fine-tuning enhanced rag system with\nquantized influence measure as ai judge,\u201d arXiv:2402.17081, 2024.", "start_char_idx": 492, "end_char_idx": 807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9735bdd-5304-4a97-9c17-ebaf054f6327": {"__data__": {"id_": "f9735bdd-5304-4a97-9c17-ebaf054f6327", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "045dbdf7-80fc-4477-9cdd-fe8ba3dffe30", "node_type": "1", "metadata": {}, "hash": "db6d4dbab74a08a5e05a2caf0c64a41251f0051f7446415202216221934bce42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19526a39-41ed-4d66-bb7a-142aa63cc92d", "node_type": "1", "metadata": {}, "hash": "040830d10fababd13698456acb44eb487bca10f3b354605bd93448d378206a39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}}, "text": "[191] J. Saad-Falcon, O. Khattab, K. Santhanam etal., \u201cUdapdr: Unsu-\npervised domain adaptation via llm prompting and distillation of\nrerankers,\u201d in EMNLP, 2023.\n[192] L. Wang, N. Yang, and F. Wei, \u201cLearning to retrieve in-context\nexamples for large language models,\u201d arXiv:2307.07164, 2023.", "start_char_idx": 808, "end_char_idx": 1099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19526a39-41ed-4d66-bb7a-142aa63cc92d": {"__data__": {"id_": "19526a39-41ed-4d66-bb7a-142aa63cc92d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9735bdd-5304-4a97-9c17-ebaf054f6327", "node_type": "1", "metadata": {}, "hash": "4a6ccc1997816d156334f654f25e0bd57f793ede736e3f22d03a2d5844eb3b0e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "node_type": "1", "metadata": {}, "hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "class_name": "RelatedNodeInfo"}}, "text": "[193] Z. Wang, J. Araki, Z. Jiang etal., \u201cLearning to filter context for\nretrieval-augmented generation,\u201d arxiv:2311.08377, 2023.\n[194] S. Hofst \u00a8atter, J. Chen, K. Raman, and H. Zamani, \u201cFid-light: Efficient\nand effective retrieval-augmented text generation,\u201d in SIGIR, 2023.", "start_char_idx": 1100, "end_char_idx": 1376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b9fbb45-d937-4b73-a820-43f0c42e1d9d": {"__data__": {"id_": "7b9fbb45-d937-4b73-a820-43f0c42e1d9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5ab0743-bfe3-4e21-9e1b-3113313cb509", "node_type": "1", "metadata": {}, "hash": "7ddafac6cdd173978a7f489d4d9bd4dc8ca2c4955cc3746a23219de709851f9e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}}, "text": "[195] D. Arora, A. Kini, S. R. Chowdhury etal., \u201cGar-meets-rag paradigm\nfor zero-shot information retrieval,\u201d arXiv:2310.20158, 2023.\n[196] https://www.pinecone.io.\n[197] W. Yu, D. Iter etal., \u201cGenerate rather than retrieve: Large language\nmodels are strong context generators,\u201d arXiv:2209.10063, 2022.", "start_char_idx": 0, "end_char_idx": 302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5ab0743-bfe3-4e21-9e1b-3113313cb509": {"__data__": {"id_": "e5ab0743-bfe3-4e21-9e1b-3113313cb509", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b9fbb45-d937-4b73-a820-43f0c42e1d9d", "node_type": "1", "metadata": {}, "hash": "abe78ff677d5d2c4973ee48b761aa71102ea35ec9a4edb501adcd551f115bc81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4041d27a-1bdd-45ff-a06e-af9f1e2f8b98", "node_type": "1", "metadata": {}, "hash": "d8cbe2d91a7c82a65608b245827df267ceb113705fb9d98880f941550e9a03c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}}, "text": "[198] A. Abdallah and A. Jatowt, \u201cGenerator-retriever-generator: A novel ap-\nproach to open-domain question answering,\u201d arXiv:2307.11278, 2023.\n[199] E. Saravia, \u201cPrompt Engineering Guide,\u201d\nhttps://github.com/dair-ai/Prompt-Engineering-Guide, 12 2022.", "start_char_idx": 303, "end_char_idx": 554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4041d27a-1bdd-45ff-a06e-af9f1e2f8b98": {"__data__": {"id_": "4041d27a-1bdd-45ff-a06e-af9f1e2f8b98", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5ab0743-bfe3-4e21-9e1b-3113313cb509", "node_type": "1", "metadata": {}, "hash": "7ddafac6cdd173978a7f489d4d9bd4dc8ca2c4955cc3746a23219de709851f9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cefee77b-cebd-48e7-a40a-9c1f7c770a2b", "node_type": "1", "metadata": {}, "hash": "995f0a9f1316a9dc1420e1b4e77e03e8c616a3ad556862778514c819d2edadc0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}}, "text": "[200] H. S. Zheng, S. Mishra etal., \u201cTake a step back: Evoking reasoning\nvia abstraction in large language models,\u201d arxiv:2310.06117, 2023.\n[201] S. Diao, P. Wang, Y . Lin, and T. Zhang, \u201cActive prompting with chain-\nof-thought for large language models,\u201d arxiv:2302.12246, 2023.", "start_char_idx": 555, "end_char_idx": 834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cefee77b-cebd-48e7-a40a-9c1f7c770a2b": {"__data__": {"id_": "cefee77b-cebd-48e7-a40a-9c1f7c770a2b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4041d27a-1bdd-45ff-a06e-af9f1e2f8b98", "node_type": "1", "metadata": {}, "hash": "d8cbe2d91a7c82a65608b245827df267ceb113705fb9d98880f941550e9a03c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50ce162f-34f2-4eda-811c-936d27b9fb31", "node_type": "1", "metadata": {}, "hash": "b3168d27d763c71b013c14ba72d78326d639ad460b2f2afc750b798b102db7bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}}, "text": "[202] H. Jiang, Q. Wu, C. Lin etal., \u201cLlmlingua: Compressing prompts for\naccelerated inference of large language models,\u201d in EMNLP, 2023.\n[203] T. Ahmed, K. S. Pai, P. Devanbu, and E. T. Barr, \u201cAutomatic semantic\naugmentation of language model prompts (for code summarization),\u201d\narXiv:2304.06815, 2024.\n[204] Z. Xu, Z. Liu, Y .", "start_char_idx": 835, "end_char_idx": 1162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50ce162f-34f2-4eda-811c-936d27b9fb31": {"__data__": {"id_": "50ce162f-34f2-4eda-811c-936d27b9fb31", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cefee77b-cebd-48e7-a40a-9c1f7c770a2b", "node_type": "1", "metadata": {}, "hash": "995f0a9f1316a9dc1420e1b4e77e03e8c616a3ad556862778514c819d2edadc0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "node_type": "1", "metadata": {}, "hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "class_name": "RelatedNodeInfo"}}, "text": "[204] Z. Xu, Z. Liu, Y . Liu etal., \u201cActiverag: Revealing the treasures of\nknowledge via active learning,\u201d arXiv:2402.13547, 2024.\n[205] E. Nijkamp, B. Pang, H. Hayashi etal., \u201cA conversational paradigm\nfor program synthesis,\u201d arxiv:2203.13474, 2022.\n[206] Y .", "start_char_idx": 1138, "end_char_idx": 1398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a369f9b-0d6f-4bbe-9c0b-fc461b2629a8": {"__data__": {"id_": "1a369f9b-0d6f-4bbe-9c0b-fc461b2629a8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22e7c4c1-23b8-4bb4-ae60-589ba5c3d7a3", "node_type": "1", "metadata": {}, "hash": "7db6672cf96271b26bad2242b171ec987ed22abb86de121d54cac6d6b82f4e23", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}}, "text": "[206] Y . He, M. Xia, H. Chen etal., \u201cAnimate-a-story: Storytelling with\nretrieval-augmented video generation,\u201d arXiv:2307.06940, 2023.\n[207] E. J. Hu, Y . Shen, P. Wallis etal., \u201cLora: Low-rank adaptation of large\nlanguage models,\u201d in ICLR, 2022.\n[208] C. Liu, P. C \u00b8 etin, Y .", "start_char_idx": 0, "end_char_idx": 278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22e7c4c1-23b8-4bb4-ae60-589ba5c3d7a3": {"__data__": {"id_": "22e7c4c1-23b8-4bb4-ae60-589ba5c3d7a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a369f9b-0d6f-4bbe-9c0b-fc461b2629a8", "node_type": "1", "metadata": {}, "hash": "2241eb8ba9fbe7c1fc140f0461669da8bd0095626d26d246ccb691a6df0e5244", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "691ba182-53b9-4313-9afc-5c68f3bfb72a", "node_type": "1", "metadata": {}, "hash": "b81ddc7e1d9920321aaf31e9b029618e53314c01d589f49270cf601f0cb98cd3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}}, "text": "[208] C. Liu, P. C \u00b8 etin, Y . Patodia etal., \u201cAutomated code editing with search-\ngenerate-modify,\u201d arXiv:2306.06490, 2023.\n[209] H. Joshi, J. P. C. S \u00b4anchez, S. Gulwani etal., \u201cRepair is nearly\ngeneration: Multilingual program repair with llms,\u201d in AAAI, 2023.", "start_char_idx": 248, "end_char_idx": 511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "691ba182-53b9-4313-9afc-5c68f3bfb72a": {"__data__": {"id_": "691ba182-53b9-4313-9afc-5c68f3bfb72a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22e7c4c1-23b8-4bb4-ae60-589ba5c3d7a3", "node_type": "1", "metadata": {}, "hash": "7db6672cf96271b26bad2242b171ec987ed22abb86de121d54cac6d6b82f4e23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0567f877-e780-4e95-a4a9-238de74ab8fa", "node_type": "1", "metadata": {}, "hash": "2ab0ba8a9e45669097f5dbaec7306fcbdf937cfca7513d37cdddbff8d8b45e89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}}, "text": "[210] Z. Jiang, F. F. Xu, L. Gao etal., \u201cActive retrieval augmented genera-\ntion,\u201d arXiv:2305.06983, 2023.\n[211] A. Mallen, A. Asai, V . Zhong etal., \u201cWhen not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric\nmemories,\u201d in ACL, 2023.\n[212] Z. Jiang, J. Araki, H. Ding, and G. Neubig, \u201cHow can we know When\nlanguage models know?", "start_char_idx": 512, "end_char_idx": 877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0567f877-e780-4e95-a4a9-238de74ab8fa": {"__data__": {"id_": "0567f877-e780-4e95-a4a9-238de74ab8fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "691ba182-53b9-4313-9afc-5c68f3bfb72a", "node_type": "1", "metadata": {}, "hash": "b81ddc7e1d9920321aaf31e9b029618e53314c01d589f49270cf601f0cb98cd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe3b96dc-a5d8-4db6-9f81-0c2bab67df04", "node_type": "1", "metadata": {}, "hash": "e4fc90625dc3578202c399fca6ea56894a2d8d85bf0fa1c8ab71f41e30d5250a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}}, "text": "on the calibration of language models for\nquestion answering,\u201d TACL, 2021.\n[213] N. Kandpal, H. Deng, A. Roberts etal., \u201cLarge language models\nstruggle to learn long-tail knowledge,\u201d in ICML, 2023.\n[214] R. Ren, Y . Wang, Y . Qu etal., \u201cInvestigating the factual knowledge\nboundary of large language models with retrieval augmentation,\u201d\narxiv:2307.11019, 2023.\n[215] Y .", "start_char_idx": 878, "end_char_idx": 1248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe3b96dc-a5d8-4db6-9f81-0c2bab67df04": {"__data__": {"id_": "fe3b96dc-a5d8-4db6-9f81-0c2bab67df04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0567f877-e780-4e95-a4a9-238de74ab8fa", "node_type": "1", "metadata": {}, "hash": "2ab0ba8a9e45669097f5dbaec7306fcbdf937cfca7513d37cdddbff8d8b45e89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eb544f90-4fa9-4e51-9b79-d85553445142", "node_type": "1", "metadata": {}, "hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "class_name": "RelatedNodeInfo"}}, "text": "[215] Y . Wang, P. Li, M. Sun, and Y . Liu, \u201cSelf-knowledge guided retrieval\naugmentation for large language models,\u201d in EMNLP Findings, 2023.", "start_char_idx": 1239, "end_char_idx": 1381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dbfd7da-3238-4854-8260-ae26e9f8190d": {"__data__": {"id_": "8dbfd7da-3238-4854-8260-ae26e9f8190d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83b74235-3739-4666-9e38-f63ed2c59679", "node_type": "1", "metadata": {}, "hash": "fc75bc8f6cbd4323e33271d315807750b35481ba08a9c0eb577694f585121312", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}}, "text": "[216] H. Ding, L. Pang, Z. Wei etal., \u201cRetrieve only when it needs: Adaptive\nretrieval augmentation for hallucination mitigation in large language\nmodels,\u201d arXiv:2402.10612, 2024.\n[217] S. Jeong, J. Baek, S. Cho etal., \u201cAdaptive-rag: Learning to adapt\nretrieval-augmented large language models through question complex-\nity,\u201d arXiv:2403.14403, 2024.", "start_char_idx": 0, "end_char_idx": 349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83b74235-3739-4666-9e38-f63ed2c59679": {"__data__": {"id_": "83b74235-3739-4666-9e38-f63ed2c59679", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dbfd7da-3238-4854-8260-ae26e9f8190d", "node_type": "1", "metadata": {}, "hash": "6cdf8def1341f4a14a5bfef8a35884e5690195447b399c386f528728919c0864", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e688435b-aaaf-4141-87e8-f1e22fecb5a6", "node_type": "1", "metadata": {}, "hash": "d20b7023b89841ef24f0bec946084949fa971a3bd849d37250dbceb014a45811", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}}, "text": "[218] F. Zhang, B. Chen etal., \u201cRepocoder: Repository-level code completion\nthrough iterative retrieval and generation,\u201d in EMNLP, 2023.26\n[219] Z. Shao, Y . Gong, Y . Shen etal., \u201cEnhancing retrieval-augmented\nlarge language models with iterative retrieval-generation synergy,\u201d in\nEMNLP Findings, 2023.", "start_char_idx": 350, "end_char_idx": 653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e688435b-aaaf-4141-87e8-f1e22fecb5a6": {"__data__": {"id_": "e688435b-aaaf-4141-87e8-f1e22fecb5a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83b74235-3739-4666-9e38-f63ed2c59679", "node_type": "1", "metadata": {}, "hash": "fc75bc8f6cbd4323e33271d315807750b35481ba08a9c0eb577694f585121312", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac07f681-2700-4b32-af6a-a4597a472e52", "node_type": "1", "metadata": {}, "hash": "5e839c61ab73b9c5265881ee99630c477ca003ccbf54646d98f82db2fa0dd7ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}}, "text": "[220] X. Cheng, D. Luo, X. Chen etal., \u201cLift yourself up: Retrieval-\naugmented text generation with self-memory,\u201d in NeurIPS, 2023.\n[221] Z. Wang, A. Liu, H. Lin etal., \u201cRat: Retrieval augmented\nthoughts elicit context-aware reasoning in long-horizon generation,\u201d\narXiv:2403.05313, 2024.", "start_char_idx": 654, "end_char_idx": 941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac07f681-2700-4b32-af6a-a4597a472e52": {"__data__": {"id_": "ac07f681-2700-4b32-af6a-a4597a472e52", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e688435b-aaaf-4141-87e8-f1e22fecb5a6", "node_type": "1", "metadata": {}, "hash": "d20b7023b89841ef24f0bec946084949fa971a3bd849d37250dbceb014a45811", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e12552e-e877-4bb0-84ae-6c51230f6ad8", "node_type": "1", "metadata": {}, "hash": "921fd551544d041ad3c293aea7109b6ae8f1c7ef39bc70ab8e194cbe80fe0271", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}}, "text": "[222] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, \u201cKnowledge graph\nbased synthetic corpus generation for knowledge-enhanced language\nmodel pre-training,\u201d in NAACL-HLT, 2021.\n[223] J. Sun, C. Xu, L. Tang etal., \u201cThink-on-graph: Deep and respon-\nsible reasoning of large language model with knowledge graph,\u201d\narXiv:2307.07697, 2023.", "start_char_idx": 942, "end_char_idx": 1275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e12552e-e877-4bb0-84ae-6c51230f6ad8": {"__data__": {"id_": "9e12552e-e877-4bb0-84ae-6c51230f6ad8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac07f681-2700-4b32-af6a-a4597a472e52", "node_type": "1", "metadata": {}, "hash": "5e839c61ab73b9c5265881ee99630c477ca003ccbf54646d98f82db2fa0dd7ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "node_type": "1", "metadata": {}, "hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "class_name": "RelatedNodeInfo"}}, "text": "[224] P. Limkonchotiwat, W. Ponwitayarat, C. Udomcharoenchaikit etal.,\n\u201cCl-relkt: Cross-lingual language knowledge transfer for multilingual\nretrieval question answering,\u201d in NAACL Findings, 2022.", "start_char_idx": 1276, "end_char_idx": 1472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0de97f25-2e9e-4277-9c7a-25200581dd02": {"__data__": {"id_": "0de97f25-2e9e-4277-9c7a-25200581dd02", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "87de77de-8c9a-4dc5-987b-0dc007f852a7", "node_type": "1", "metadata": {}, "hash": "1cea245af87ea4c2c0f46e0709808b42ed0bfb92ef38349f3939fecce04411e0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "87de77de-8c9a-4dc5-987b-0dc007f852a7", "node_type": "1", "metadata": {}, "hash": "1cea245af87ea4c2c0f46e0709808b42ed0bfb92ef38349f3939fecce04411e0", "class_name": "RelatedNodeInfo"}}, "text": "[225] A. Asai, X. Yu, J. Kasai, and H. Hajishirzi, \u201cOne question answering\nmodel for many languages with cross-lingual dense passage retrieval,\u201d\ninNeurIPS, 2021.\n[226] K. Lee, S. Han etal., \u201cWhen to read documents or QA history: On\nunified and selective open-domain QA,\u201d in ACL Findings, 2023.", "start_char_idx": 0, "end_char_idx": 293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "710a03ff-d5ec-48fc-bb24-240df35a4ac8": {"__data__": {"id_": "710a03ff-d5ec-48fc-bb24-240df35a4ac8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25839326-3a8d-4ccc-bfd8-f565f47279db", "node_type": "1", "metadata": {}, "hash": "a36bbdda813e718fbde1f001e6b5bfd3f5ab2e5d1ccc70c92aa998800e5c543e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}}, "text": "[227] S. Yue, W. Chen etal., \u201cDisc-lawllm: Fine-tuning large language\nmodels for intelligent legal services,\u201d arXiv:2309.11325, 2023.\n[228] S. Siriwardhana, R. Weerasekera, T. Kaluarachchi etal., \u201cImproving\nthe domain adaptation of retrieval augmented generation (RAG) models\nfor open domain question answering,\u201d TACL, vol. 11, pp. 1\u201317, 2023.\n[229] Y . Tang and Y .", "start_char_idx": 0, "end_char_idx": 366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25839326-3a8d-4ccc-bfd8-f565f47279db": {"__data__": {"id_": "25839326-3a8d-4ccc-bfd8-f565f47279db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "710a03ff-d5ec-48fc-bb24-240df35a4ac8", "node_type": "1", "metadata": {}, "hash": "8682a7d99b5b1ed95823e92a26c87cdb76b67db2ede71a6adb89122de519e5b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3551135e-b305-4d1f-ad3f-740b6e77e337", "node_type": "1", "metadata": {}, "hash": "7bf987c4b8fb0a7d480d24cf0c53f4fc31d506fe7713a833ca31712cd59d305c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}}, "text": "1\u201317, 2023.\n[229] Y . Tang and Y . Yang, \u201cMultihop-rag: Benchmarking retrieval-\naugmented generation for multi-hop queries,\u201d arXiv:2401.15391, 2024.\n[230] K. Huang, C. Zhai, and H. Ji, \u201cCONCRETE: improving cross-lingual\nfact-checking with cross-lingual retrieval,\u201d in COLING, 2022.", "start_char_idx": 332, "end_char_idx": 613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3551135e-b305-4d1f-ad3f-740b6e77e337": {"__data__": {"id_": "3551135e-b305-4d1f-ad3f-740b6e77e337", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25839326-3a8d-4ccc-bfd8-f565f47279db", "node_type": "1", "metadata": {}, "hash": "a36bbdda813e718fbde1f001e6b5bfd3f5ab2e5d1ccc70c92aa998800e5c543e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca24b290-c3e9-4f9d-912a-914981fc368f", "node_type": "1", "metadata": {}, "hash": "c128cbfca05c7ca28b713879b0a9b437579fd2b75867d0ffabd4973579c544e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}}, "text": "[231] L. Hagstr \u00a8om, D. Saynova, T. Norlund etal., \u201cThe effect of scaling,\nretrieval augmentation and form on the factual consistency of language\nmodels,\u201d arXiv:2311.01307, 2023.\n[232] Y . Liu, Y . Wan etal., \u201cKG-BART: knowledge graph-augmented BART\nfor generative commonsense reasoning,\u201d in AAAI, 2021.", "start_char_idx": 614, "end_char_idx": 917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca24b290-c3e9-4f9d-912a-914981fc368f": {"__data__": {"id_": "ca24b290-c3e9-4f9d-912a-914981fc368f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3551135e-b305-4d1f-ad3f-740b6e77e337", "node_type": "1", "metadata": {}, "hash": "7bf987c4b8fb0a7d480d24cf0c53f4fc31d506fe7713a833ca31712cd59d305c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c091822f-cf84-43c4-912a-f283e9175500", "node_type": "1", "metadata": {}, "hash": "b6c6136f6c1d3b5b265e2c1e0ea846eb144283e68f089251b111209ff0ec99f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}}, "text": "[233] A. Wan, E. Wallace, and D. Klein, \u201cWhat evidence do language models\nfind convincing?\u201d arXiv:2402.11782, 2024.\n[234] H. Zhang, Z. Liu etal., \u201cGrounded conversation generation as guided\ntraverses in commonsense knowledge graphs,\u201d in ACL, 2020.\n[235] D. Cai, Y . Wang, W. Bi etal., \u201cSkeleton-to-response: Dialogue gener-\nation guided by retrieval memory,\u201d in NAACL-HLT, 2019.", "start_char_idx": 918, "end_char_idx": 1296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c091822f-cf84-43c4-912a-f283e9175500": {"__data__": {"id_": "c091822f-cf84-43c4-912a-f283e9175500", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca24b290-c3e9-4f9d-912a-914981fc368f", "node_type": "1", "metadata": {}, "hash": "c128cbfca05c7ca28b713879b0a9b437579fd2b75867d0ffabd4973579c544e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "node_type": "1", "metadata": {}, "hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "class_name": "RelatedNodeInfo"}}, "text": "[236] M. Komeili, K. Shuster, and J. Weston, \u201cInternet-augmented dialogue\ngeneration,\u201d in ACL, 2022.\n[237] K. Shuster, J. Xu etal., \u201cBlenderbot 3: a deployed conversational agent\nthat continually learns to responsibly engage,\u201d arXiv:2208.03188, 2022.", "start_char_idx": 1297, "end_char_idx": 1547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "978a5215-1623-4a61-8feb-9255f630d968": {"__data__": {"id_": "978a5215-1623-4a61-8feb-9255f630d968", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f126c652-bc73-4eae-90a8-9e0fdf8e0aa7", "node_type": "1", "metadata": {}, "hash": "c3839d871f54de06c2a76bc8a3eb9cc4526d9b321cce19a8654890a5f4a60ced", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}}, "text": "[238] S. Kim, J. Y . Jang, M. Jung, and S. Shin, \u201cA model of cross-lingual\nknowledge-grounded response generation for open-domain dialogue\nsystems,\u201d in EMNLP Findings, 2021.\n[239] E. Nie, S. Liang, H. Schmid, and H. Sch \u00a8utze, \u201cCross-lingual retrieval\naugmented prompt for low-resource languages,\u201d in ACL, 2023.", "start_char_idx": 0, "end_char_idx": 311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f126c652-bc73-4eae-90a8-9e0fdf8e0aa7": {"__data__": {"id_": "f126c652-bc73-4eae-90a8-9e0fdf8e0aa7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "978a5215-1623-4a61-8feb-9255f630d968", "node_type": "1", "metadata": {}, "hash": "128ace82b6ae2563c7a381efeb4fbb6624ccacb97311a365bce02e073c1f601f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d4e9a1a-35c4-4795-96f8-97258f6ae872", "node_type": "1", "metadata": {}, "hash": "c383037de9ecff9b72bb6cddc18dd274e506c81fcd081b5919146741709e6520", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}}, "text": "[240] X. Li, E. Nie, and S. Liang, \u201cFrom classification to generation: Insights\ninto crosslingual retrieval augmented icl,\u201d in NeurIPS, 2023.\n[241] W. Li, J. Li, W. Ma, and Y . Liu, \u201cCitation-enhanced generation for\nllm-based chatbot,\u201d arXiv:2402.16063, 2024.\n[242] D. Cai, Y .", "start_char_idx": 312, "end_char_idx": 589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d4e9a1a-35c4-4795-96f8-97258f6ae872": {"__data__": {"id_": "7d4e9a1a-35c4-4795-96f8-97258f6ae872", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f126c652-bc73-4eae-90a8-9e0fdf8e0aa7", "node_type": "1", "metadata": {}, "hash": "c3839d871f54de06c2a76bc8a3eb9cc4526d9b321cce19a8654890a5f4a60ced", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0932b200-84c6-4488-bfc5-2396cd5c9b95", "node_type": "1", "metadata": {}, "hash": "cf9c85b69747bc4b53da10ddab70e413839c435a1ab4dd5df6f68768c47472b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}}, "text": "[242] D. Cai, Y . Wang, H. Li etal., \u201cNeural machine translation with\nmonolingual translation memory,\u201d in ACL/IJCNLP, 2021.\n[243] U. Khandelwal, A. Fan, D. Jurafsky etal., \u201cNearest neighbor machine\ntranslation,\u201d in ICLR, 2021.\n[244] X. Du and H. Ji, \u201cRetrieval-augmented generative question answering\nfor event argument extraction,\u201d in EMNLP, 2022.\n[245] Y .", "start_char_idx": 572, "end_char_idx": 930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0932b200-84c6-4488-bfc5-2396cd5c9b95": {"__data__": {"id_": "0932b200-84c6-4488-bfc5-2396cd5c9b95", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d4e9a1a-35c4-4795-96f8-97258f6ae872", "node_type": "1", "metadata": {}, "hash": "c383037de9ecff9b72bb6cddc18dd274e506c81fcd081b5919146741709e6520", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "759f6c8c-8c8c-4cad-a6b3-a5984bcc99d5", "node_type": "1", "metadata": {}, "hash": "2136eb08e48e73e21fe2062daf9e91b8273cc994dbc7f57b7759fada6378125e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}}, "text": "[245] Y . Gao, Q. Yin, Z. Li etal., \u201cRetrieval-augmented multilingual\nkeyphrase generation with retriever-generator iterative training,\u201d in\nNAACL Findings, 2022.\n[246] J. Zhang, E. J. Yu, Q. Chen etal., \u201cRetrieval-based full-length wikipedia\ngeneration for emergent events,\u201d arXiv:2402.18264, 2024.\n[247] R. Fan, Y .", "start_char_idx": 921, "end_char_idx": 1237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "759f6c8c-8c8c-4cad-a6b3-a5984bcc99d5": {"__data__": {"id_": "759f6c8c-8c8c-4cad-a6b3-a5984bcc99d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0932b200-84c6-4488-bfc5-2396cd5c9b95", "node_type": "1", "metadata": {}, "hash": "cf9c85b69747bc4b53da10ddab70e413839c435a1ab4dd5df6f68768c47472b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "node_type": "1", "metadata": {}, "hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "class_name": "RelatedNodeInfo"}}, "text": "[247] R. Fan, Y . Fan, J. Chen etal., \u201cRIGHT: retrieval-augmented generation\nfor mainstream hashtag recommendation,\u201d arxiv:2312.10466, 2023.\n[248] Y . Wang, H. Le, A. D. Gotmare etal., \u201cCodet5mix: A pretrained\nmixture of encoder-decoder transformers for code understanding and\ngeneration,\u201d 2022.", "start_char_idx": 1220, "end_char_idx": 1515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a043749e-7ef5-4b9e-aeb1-6890d7db8c8d": {"__data__": {"id_": "a043749e-7ef5-4b9e-aeb1-6890d7db8c8d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb6f45f0-2d5b-41ac-9bfd-8950cde6e539", "node_type": "1", "metadata": {}, "hash": "86e4f89610c7e3535b341ebae1ad370be8bf7ce6a888f135c323c73ea208fd29", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}}, "text": "[249] E. Nijkamp, B. Pang, H. Hayashi etal., \u201cA conversational paradigm\nfor program synthesis,\u201d arXiv:2203.13474, 2022.\n[250] D. Zan, B. Chen, Y . Gong etal., \u201cPrivate-library-oriented code gener-\nation with large language models,\u201d arXiv:2307.15370, 2023.", "start_char_idx": 0, "end_char_idx": 255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb6f45f0-2d5b-41ac-9bfd-8950cde6e539": {"__data__": {"id_": "fb6f45f0-2d5b-41ac-9bfd-8950cde6e539", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a043749e-7ef5-4b9e-aeb1-6890d7db8c8d", "node_type": "1", "metadata": {}, "hash": "fbbeb9a212c3f4f49364acb3a9eba0c5bbd72e63a8fb61d3324a6916da91bb0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3077a316-a55b-413f-9eb6-138dbe78451a", "node_type": "1", "metadata": {}, "hash": "2c71fd75f5b6d894bbe32eab16472a1741facda3bc0bc5e60a16c91c6d83c10d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}}, "text": "[251] A. Madaan, S. Zhou, U. Alon etal., \u201cLanguage models of code are\nfew-shot commonsense learners,\u201d in EMNLP, 2022.[252] Y . Wang, H. Le, A. Gotmare etal., \u201cCodet5+: Open code large language\nmodels for code understanding and generation,\u201d in EMNLP, 2023.", "start_char_idx": 256, "end_char_idx": 511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3077a316-a55b-413f-9eb6-138dbe78451a": {"__data__": {"id_": "3077a316-a55b-413f-9eb6-138dbe78451a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb6f45f0-2d5b-41ac-9bfd-8950cde6e539", "node_type": "1", "metadata": {}, "hash": "86e4f89610c7e3535b341ebae1ad370be8bf7ce6a888f135c323c73ea208fd29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03f038cc-c3c7-4d93-91da-42d3daa5a276", "node_type": "1", "metadata": {}, "hash": "06dd0c1be81a8bfbab95f733151d92c790302e982e3bab37cb514ae9bea4098e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}}, "text": "[253] D. Liao, S. Pan, Q. Huang etal., \u201cContext-aware code generation\nframework for code repositories: Local, global, and third-party library\nawareness,\u201d arXiv:2312.05772, 2023.\n[254] J. Li, Y . Li, G. Li etal., \u201cSkcoder: A sketch-based approach for\nautomatic code generation,\u201d in ICSE, 2023.\n[255] M. Liu, T. Yang, Y .", "start_char_idx": 512, "end_char_idx": 831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03f038cc-c3c7-4d93-91da-42d3daa5a276": {"__data__": {"id_": "03f038cc-c3c7-4d93-91da-42d3daa5a276", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3077a316-a55b-413f-9eb6-138dbe78451a", "node_type": "1", "metadata": {}, "hash": "2c71fd75f5b6d894bbe32eab16472a1741facda3bc0bc5e60a16c91c6d83c10d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6575adcf-4350-4942-834d-e5fa50c56dd1", "node_type": "1", "metadata": {}, "hash": "5a6039f86ea79bced90d965f8685dc525aad4dd7a9c2eedeaafd5f97495220f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}}, "text": "[255] M. Liu, T. Yang, Y . Lou etal., \u201cCodegen4libs: A two-stage approach\nfor library-oriented code generation,\u201d in ASE, 2023.\n[256] K. Zhang, J. Li, G. Li etal., \u201cCodeagent: Enhancing code generation\nwith tool-integrated agent systems for real-world repo-level coding\nchallenges,\u201d arXiv:2401.07339, 2024.\n[257] Q. Gou, Y . Dong, Y .", "start_char_idx": 805, "end_char_idx": 1138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6575adcf-4350-4942-834d-e5fa50c56dd1": {"__data__": {"id_": "6575adcf-4350-4942-834d-e5fa50c56dd1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03f038cc-c3c7-4d93-91da-42d3daa5a276", "node_type": "1", "metadata": {}, "hash": "06dd0c1be81a8bfbab95f733151d92c790302e982e3bab37cb514ae9bea4098e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1ceb9831-1954-4b8d-a153-1a613db8408a", "node_type": "1", "metadata": {}, "hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "class_name": "RelatedNodeInfo"}}, "text": "[257] Q. Gou, Y . Dong, Y . Wu, and Q. Ke, \u201cRrgcode: Deep hierarchical\nsearch-based code generation,\u201d Journal ofSystems andSoftware, vol.\n211, p. 111982, 2024.\n[258] J. Chen, X. Hu, Z. Li etal., \u201cCode search is all you need? improving\ncode suggestions with code search,\u201d in ICSE, 2024.\n[259] H. Su, S. Jiang, Y .", "start_char_idx": 1111, "end_char_idx": 1423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fbf824b-c772-4db5-b5fa-ef25a6ab47fa": {"__data__": {"id_": "0fbf824b-c772-4db5-b5fa-ef25a6ab47fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cec0d187-467b-4b39-9224-7ca575532cbb", "node_type": "1", "metadata": {}, "hash": "9a9344b675485242253c35109b798d6986e8a79712271ea270e2495f86921f06", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}}, "text": "[259] H. Su, S. Jiang, Y . Lai etal., \u201cArks: Active retrieval in knowledge soup\nfor code generation,\u201d arXiv:2402.12317, 2024.\n[260] N. Beau and B. Crabb \u00b4e, \u201cThe impact of lexical and grammatical pro-\ncessing on generating code from natural language,\u201d in ACL Findings,\n2022.", "start_char_idx": 0, "end_char_idx": 274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cec0d187-467b-4b39-9224-7ca575532cbb": {"__data__": {"id_": "cec0d187-467b-4b39-9224-7ca575532cbb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fbf824b-c772-4db5-b5fa-ef25a6ab47fa", "node_type": "1", "metadata": {}, "hash": "af3a2449cf55f4b962a86b0e5675b18594cef35a79afe06b94acc77f291806bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2f6ade5-eaca-44b1-af47-fb893e56fa1d", "node_type": "1", "metadata": {}, "hash": "225257a78780b4879ed5be2d5ddec36375a986deff3558cd3c9e5afe06806331", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}}, "text": "[261] K. Zhang, G. Li, J. Li etal., \u201cToolcoder: Teach code generation models\nto use API search tools,\u201d arXiv:2305.04032, 2023.\n[262] S. Liu, Y . Chen, X. Xie etal., \u201cRetrieval-augmented generation for\ncode summarization via hybrid GNN,\u201d in ICLR, 2021.", "start_char_idx": 275, "end_char_idx": 526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2f6ade5-eaca-44b1-af47-fb893e56fa1d": {"__data__": {"id_": "b2f6ade5-eaca-44b1-af47-fb893e56fa1d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cec0d187-467b-4b39-9224-7ca575532cbb", "node_type": "1", "metadata": {}, "hash": "9a9344b675485242253c35109b798d6986e8a79712271ea270e2495f86921f06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ea88205-e0b2-44cf-9548-75623465ac25", "node_type": "1", "metadata": {}, "hash": "888afc592a16b6180226fa6e42d716f2662fe7d039489c45f3becc4e656666e9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}}, "text": "[263] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck, \u201cModeling and\ndiscovering vulnerabilities with code property graphs,\u201d in S&P, 2014.\n[264] Y . Choi, C. Na etal., \u201cReadsum: Retrieval-augmented adaptive trans-\nformer for source code summarization,\u201d IEEE Access, 2023.\n[265] A. Alokla, W. Gad, W. Nazih etal., \u201cRetrieval-based transformer\npseudocode generation,\u201d Mathematics, vol.", "start_char_idx": 527, "end_char_idx": 907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ea88205-e0b2-44cf-9548-75623465ac25": {"__data__": {"id_": "6ea88205-e0b2-44cf-9548-75623465ac25", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2f6ade5-eaca-44b1-af47-fb893e56fa1d", "node_type": "1", "metadata": {}, "hash": "225257a78780b4879ed5be2d5ddec36375a986deff3558cd3c9e5afe06806331", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "113fc76d-16b2-4b81-ba17-6abb5a94f511", "node_type": "1", "metadata": {}, "hash": "257e8d2008a1e29564a58175d585c69ab6279e4f4cfce81ba8f838a13e99344f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}}, "text": "10, no. 4, p. 604, 2022.\n[266] J. Zhao, X. Chen, G. Yang, and Y . Shen, \u201cAutomatic smart contract\ncomment generation via large language models and in-context learn-\ning,\u201d IST, vol. 168, p. 107405, 2024.\n[267] J. Xu, Z. Cui etal., \u201cUnilog: Automatic logging via LLM and in-\ncontext learning,\u201d in ICSE, 2024.", "start_char_idx": 908, "end_char_idx": 1214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "113fc76d-16b2-4b81-ba17-6abb5a94f511": {"__data__": {"id_": "113fc76d-16b2-4b81-ba17-6abb5a94f511", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ea88205-e0b2-44cf-9548-75623465ac25", "node_type": "1", "metadata": {}, "hash": "888afc592a16b6180226fa6e42d716f2662fe7d039489c45f3becc4e656666e9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9c62f556-ced5-4c6a-8278-780ade4f4643", "node_type": "1", "metadata": {}, "hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "class_name": "RelatedNodeInfo"}}, "text": "[268] H. Wang, X. Xia etal., \u201cContext-aware retrieval-based deep commit\nmessage generation,\u201d TOSEM, vol. 30, no. 4, pp. 56:1\u201356:30, 2021.\n[269] X. Zhu, C. Sha, and J. Niu, \u201cA simple retrieval-based method for code\ncomment generation,\u201d in SANER, 2022.", "start_char_idx": 1215, "end_char_idx": 1465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e687371a-384d-4866-bbed-8676f1b0a469": {"__data__": {"id_": "e687371a-384d-4866-bbed-8676f1b0a469", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd6520ae-784e-4cca-9dbf-a7a4daa3831b", "node_type": "1", "metadata": {}, "hash": "87af8aef80571944f8d4745d45cbb599ca7c0d86c1a2433dbaa098d509a8d94a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}}, "text": "[270] T. Ye, L. Wu, T. Ma etal., \u201cTram: A token-level retrieval-augmented\nmechanism for source code summarization,\u201d arXiv:2305.11074, 2023.\n[271] L. Li, B. Liang, L. Chen, and X. Zhang, \u201cCross-modal retrieval-\nenhanced code summarization based on joint learning for retrieval and\ngeneration,\u201d Available atSSRN 4724884.", "start_char_idx": 0, "end_char_idx": 318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd6520ae-784e-4cca-9dbf-a7a4daa3831b": {"__data__": {"id_": "dd6520ae-784e-4cca-9dbf-a7a4daa3831b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e687371a-384d-4866-bbed-8676f1b0a469", "node_type": "1", "metadata": {}, "hash": "03c166355cfcabed5a64a146b3e7ef2187f51a7bb239fe34b196ef1cdfcf55e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ced47e8-c50b-41d1-9e66-05758ec081c7", "node_type": "1", "metadata": {}, "hash": "82d340f379882ea34926b5b83ac223777c323ab548df5bb41f3d93e9dae525d7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}}, "text": "[272] D. Drain, C. Hu, C. Wu etal., \u201cGenerating code with the\nhelp of retrieved template functions and stack overflow answers,\u201d\narXiv:2104.05310, 2021.\n[273] S. Lu, D. Guo etal., \u201cCodexglue: A machine learning benchmark\ndataset for code understanding and generation,\u201d in NeurIPS Datasets\nandBenchmarks, 2021.\n[274] Y .", "start_char_idx": 319, "end_char_idx": 637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ced47e8-c50b-41d1-9e66-05758ec081c7": {"__data__": {"id_": "7ced47e8-c50b-41d1-9e66-05758ec081c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd6520ae-784e-4cca-9dbf-a7a4daa3831b", "node_type": "1", "metadata": {}, "hash": "87af8aef80571944f8d4745d45cbb599ca7c0d86c1a2433dbaa098d509a8d94a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5478d272-b834-4aea-b564-51d14663a9e3", "node_type": "1", "metadata": {}, "hash": "91899be3da3e19910bbc91cadb966b3d5486e4f05c54dc2f536e2efcfd2994be", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}}, "text": "[274] Y . Ding, Z. Wang etal., \u201cCocomic: Code completion by jointly\nmodeling in-file and cross-file context,\u201d arXiv:2212.10007, 2022.\n[275] D. Shrivastava, D. Kocetkov etal., \u201cRepofusion: Training code models\nto understand your repository,\u201d arXiv:2306.10998, 2023.", "start_char_idx": 628, "end_char_idx": 892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5478d272-b834-4aea-b564-51d14663a9e3": {"__data__": {"id_": "5478d272-b834-4aea-b564-51d14663a9e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ced47e8-c50b-41d1-9e66-05758ec081c7", "node_type": "1", "metadata": {}, "hash": "82d340f379882ea34926b5b83ac223777c323ab548df5bb41f3d93e9dae525d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e51a6da-ad2d-453a-bfe9-5c2c4fc099da", "node_type": "1", "metadata": {}, "hash": "1593eff9db65962ae3c58ff20c4d01235f1b4d597bb3fea27990036c3ccfceda", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}}, "text": "[276] Z. Tang, J. Ge, S. Liu etal., \u201cDomain adaptive code completion via\nlanguage models and decoupled domain databases,\u201d in ASE, 2023.\n[277] W. Sun, H. Li, M. Yan etal., \u201cRevisiting and improving retrieval-\naugmented deep assertion generation,\u201d in ASE, 2023.\n[278] A. Eghbali and M. Pradel, \u201cDe-hallucinator: Iterative grounding for\nllm-based code completion,\u201d arXiv:2401.01701, 2024.", "start_char_idx": 893, "end_char_idx": 1278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e51a6da-ad2d-453a-bfe9-5c2c4fc099da": {"__data__": {"id_": "2e51a6da-ad2d-453a-bfe9-5c2c4fc099da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5478d272-b834-4aea-b564-51d14663a9e3", "node_type": "1", "metadata": {}, "hash": "91899be3da3e19910bbc91cadb966b3d5486e4f05c54dc2f536e2efcfd2994be", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "40459286-87b0-4707-b53e-47333e6c46d3", "node_type": "1", "metadata": {}, "hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "class_name": "RelatedNodeInfo"}}, "text": "[279] M. Liang, X. Xie, G. Zhang etal., \u201cRepofuse: Repository-level code\ncompletion with fused dual context,\u201d arXiv:2402.14323, 2024.\n[280] Y .", "start_char_idx": 1279, "end_char_idx": 1422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68e6cf96-535c-4e35-ac82-1e87ef0e7c77": {"__data__": {"id_": "68e6cf96-535c-4e35-ac82-1e87ef0e7c77", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04b6beaf-8804-4a39-9022-0b734e37e3a8", "node_type": "1", "metadata": {}, "hash": "5cefee2f7197dc42816f47d743c8cebb3b40797cf4ca13fe97833f58feb6cd8b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}}, "text": "[280] Y . Tsai, M. Liu, and H. Ren, \u201cRtlfixer: Automatically fixing RTL syntax\nerrors with large language models,\u201d arXiv:2311.16543, 2023.\n[281] B. Bogin, S. Gupta, P. Clark etal., \u201cLeveraging code to improve in-\ncontext learning for semantic parsing,\u201d arXiv:2311.09519, 2023.", "start_char_idx": 0, "end_char_idx": 276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04b6beaf-8804-4a39-9022-0b734e37e3a8": {"__data__": {"id_": "04b6beaf-8804-4a39-9022-0b734e37e3a8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68e6cf96-535c-4e35-ac82-1e87ef0e7c77", "node_type": "1", "metadata": {}, "hash": "895087b36c2e9880905458ebe4e6847e198c7f48851cbcdec837dd8cb700cec9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fedcb3ca-1430-4371-ad76-fc95ec58436f", "node_type": "1", "metadata": {}, "hash": "10d2e443e04e8b7f2b7380c20f3121ebb8cedd466df183dd8693f52b6a8db9e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}}, "text": "[282] H. Li, J. Zhang, C. Li, and H. Chen, \u201cResdsql: Decoupling schema\nlinking and skeleton parsing for text-to-sql,\u201d in AAAI, 2023.\n[283] K. Zhang, X. Lin, Y . Wang etal., \u201cRefsql: A retrieval-augmentation\nframework for text-to-sql generation,\u201d in EMNLP Findings, 2023.", "start_char_idx": 277, "end_char_idx": 547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fedcb3ca-1430-4371-ad76-fc95ec58436f": {"__data__": {"id_": "fedcb3ca-1430-4371-ad76-fc95ec58436f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04b6beaf-8804-4a39-9022-0b734e37e3a8", "node_type": "1", "metadata": {}, "hash": "5cefee2f7197dc42816f47d743c8cebb3b40797cf4ca13fe97833f58feb6cd8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f374abd-ef51-4f8f-a1da-f098442c5477", "node_type": "1", "metadata": {}, "hash": "9e92bd3b9337b987ffa729df8aef5a0b7d38d316e2bf222405e8f24e8ed41b11", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}}, "text": "[284] S. Chang and E. Fosler-Lussier, \u201cSelective demonstrations for cross-\ndomain text-to-sql,\u201d arXiv:2310.06302, 2023.\n[285] L. Nan, Y .", "start_char_idx": 548, "end_char_idx": 685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f374abd-ef51-4f8f-a1da-f098442c5477": {"__data__": {"id_": "6f374abd-ef51-4f8f-a1da-f098442c5477", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fedcb3ca-1430-4371-ad76-fc95ec58436f", "node_type": "1", "metadata": {}, "hash": "10d2e443e04e8b7f2b7380c20f3121ebb8cedd466df183dd8693f52b6a8db9e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "981cb88b-4244-44d6-a938-3aa5646afe1d", "node_type": "1", "metadata": {}, "hash": "b7098b5232963024be15a4d4b1a4e59e97678966cad9ef2227bca4a44454177e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}}, "text": "[285] L. Nan, Y . Zhao, W. Zou etal., \u201cEnhancing text-to-sql capabilities\nof large language models: A study on prompt design strategies,\u201d in\nEMNLP Findings, 2023.27\n[286] X. Zhang, D. Wang, L. Dou etal., \u201cMulti-hop table retrieval for open-\ndomain text-to-sql,\u201d arXiv:2402.10666, 2024.", "start_char_idx": 668, "end_char_idx": 953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "981cb88b-4244-44d6-a938-3aa5646afe1d": {"__data__": {"id_": "981cb88b-4244-44d6-a938-3aa5646afe1d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f374abd-ef51-4f8f-a1da-f098442c5477", "node_type": "1", "metadata": {}, "hash": "9e92bd3b9337b987ffa729df8aef5a0b7d38d316e2bf222405e8f24e8ed41b11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfd31fdd-e9a9-4dab-a171-abb51b1b01e3", "node_type": "1", "metadata": {}, "hash": "dc8aaa2901a56043184176caa0df80ec6aa48c90174ce09e8c13da928d1b34c0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}}, "text": "[287] H. Li, J. Zhang, H. Liu etal., \u201cCodes: Towards building open-source\nlanguage models for text-to-sql,\u201d arXiv:2402.16347, 2024.\n[288] Z. Jie and W. Lu, \u201cLeveraging training data in few-shot prompting for\nnumerical reasoning,\u201d arXiv:2305.18170, 2023.", "start_char_idx": 954, "end_char_idx": 1207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfd31fdd-e9a9-4dab-a171-abb51b1b01e3": {"__data__": {"id_": "dfd31fdd-e9a9-4dab-a171-abb51b1b01e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "981cb88b-4244-44d6-a938-3aa5646afe1d", "node_type": "1", "metadata": {}, "hash": "b7098b5232963024be15a4d4b1a4e59e97678966cad9ef2227bca4a44454177e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de", "node_type": "1", "metadata": {}, "hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "class_name": "RelatedNodeInfo"}}, "text": "[289] M. Gao, J. Li, H. Fei etal., \u201cDe-fine: Decomposing and refining visual\nprograms with auto-feedback,\u201d arXiv:2311.12890, 2023.\n[290] Y .", "start_char_idx": 1208, "end_char_idx": 1348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0b5557e-d64e-44ba-a140-95adb1f5dfbd": {"__data__": {"id_": "f0b5557e-d64e-44ba-a140-95adb1f5dfbd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "528e2c6f-bd10-4799-8f30-9da620bfa474", "node_type": "1", "metadata": {}, "hash": "a39560e3eb4e7d1e0c3e78ce1542cb9290915fef8e27df389806183b9ff531ca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}}, "text": "[290] Y . Hao, W. Chen, Z. Zhou, and W. Cui, \u201cE&v: Prompting large\nlanguage models to perform static analysis by pseudo-code execution\nand verification,\u201d arXiv:2312.08477, 2023.\n[291] Y . Guo, Z. Li etal., \u201cRetrieval-augmented code generation for universal\ninformation extraction,\u201d arXiv:2311.02962, 2023.", "start_char_idx": 0, "end_char_idx": 305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "528e2c6f-bd10-4799-8f30-9da620bfa474": {"__data__": {"id_": "528e2c6f-bd10-4799-8f30-9da620bfa474", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0b5557e-d64e-44ba-a140-95adb1f5dfbd", "node_type": "1", "metadata": {}, "hash": "2f0d5bb64538c3dc0d9d82db5eb56041b9af6b28ff96ea525a9193f102e0ec8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8901e7d9-2c64-4f66-b5e7-84bee0d8cbab", "node_type": "1", "metadata": {}, "hash": "72b8ca19f09ee16ba22b64ec55eae35eec56c9d2cef23c5318923e6b6b83b06e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}}, "text": "[292] G. Pinto, C. de Souza etal., \u201cLessons from building stackspot ai: A\ncontextualized ai coding assistant,\u201d arXiv:2311.18450, 2024.\n[293] Z. Liu, C. Chen, J. Wang etal., \u201cTesting the limits: Unusual text inputs\ngeneration for mobile app crash detection with large language model,\u201d\narXiv:2310.15657, 2023.", "start_char_idx": 306, "end_char_idx": 613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8901e7d9-2c64-4f66-b5e7-84bee0d8cbab": {"__data__": {"id_": "8901e7d9-2c64-4f66-b5e7-84bee0d8cbab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "528e2c6f-bd10-4799-8f30-9da620bfa474", "node_type": "1", "metadata": {}, "hash": "a39560e3eb4e7d1e0c3e78ce1542cb9290915fef8e27df389806183b9ff531ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c245c70-d293-4329-bf96-74de97202104", "node_type": "1", "metadata": {}, "hash": "fffb6554c2c675722446ebc493afebfde76e754f47831d2786ca3d97665692e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}}, "text": "[294] K. D. Bollacker, C. Evans etal., \u201cFreebase: a collaboratively created\ngraph database for structuring human knowledge,\u201d in SIGMOD, 2008.\n[295] Y . Shu and Z. Yu, \u201cData distribution bottlenecks in grounding language\nmodels to knowledge bases,\u201d arXiv:2309.08345, 2023.\n[296] D. Leake and D. J. Crandall, \u201cOn bringing case-based reasoning\nmethodology to deep learning,\u201d in ICCBR, 2020.", "start_char_idx": 614, "end_char_idx": 1001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c245c70-d293-4329-bf96-74de97202104": {"__data__": {"id_": "8c245c70-d293-4329-bf96-74de97202104", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8901e7d9-2c64-4f66-b5e7-84bee0d8cbab", "node_type": "1", "metadata": {}, "hash": "72b8ca19f09ee16ba22b64ec55eae35eec56c9d2cef23c5318923e6b6b83b06e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "488f77e7-8395-4fc0-b72f-91c002ec5a4f", "node_type": "1", "metadata": {}, "hash": "cc7d985edcd477bcdbb0ebd21f1d2800106095479c22a6c6833961e52b24f030", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}}, "text": "[297] L. Zhang, J. Zhang etal., \u201cFC-KBQA: A fine-to-coarse composition\nframework for knowledge base question answering,\u201d in ACL, 2023.\n[298] J. Jiang, K. Zhou etal., \u201cStructgpt: A general framework for large\nlanguage model to reason over structured data,\u201d in EMNLP, 2023.", "start_char_idx": 1002, "end_char_idx": 1273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "488f77e7-8395-4fc0-b72f-91c002ec5a4f": {"__data__": {"id_": "488f77e7-8395-4fc0-b72f-91c002ec5a4f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c245c70-d293-4329-bf96-74de97202104", "node_type": "1", "metadata": {}, "hash": "fffb6554c2c675722446ebc493afebfde76e754f47831d2786ca3d97665692e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23", "node_type": "1", "metadata": {}, "hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "class_name": "RelatedNodeInfo"}}, "text": "[299] J. Baek, A. F. Aji, and A. Saffari, \u201cKnowledge-augmented language\nmodel prompting for zero-shot knowledge graph question answering,\u201d\narXiv:2306.04136, 2023.\n[300] P. Sen, S. Mavadia, and A. Saffari, \u201cKnowledge graph-augmented\nlanguage models for complex question answering,\u201d in NLRSE, 2023.\n[301] Y .", "start_char_idx": 1274, "end_char_idx": 1580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e703c2b-dba4-46cf-8ded-6734778d1d5a": {"__data__": {"id_": "1e703c2b-dba4-46cf-8ded-6734778d1d5a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ac57fb1-b698-4d77-b606-aa7ec2dde7bc", "node_type": "1", "metadata": {}, "hash": "55cb36b23880a53e722c7e524a501507fab326f9e42e4fa40dde0b2c89ed33da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}}, "text": "[301] Y . Wu, N. Hu, S. Bi etal., \u201cRetrieve-rewrite-answer: A kg-to-text\nenhanced llms framework for knowledge graph question answering,\u201d\narXiv:2309.11206, 2023.\n[302] C. Wang, Y . Xu, Z. Peng etal., \u201ckeqing: knowledge-based ques-\ntion answering is a nature chain-of-thought mentor of LLM,\u201d\narXiv:2401.00426, 2024.", "start_char_idx": 0, "end_char_idx": 314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ac57fb1-b698-4d77-b606-aa7ec2dde7bc": {"__data__": {"id_": "3ac57fb1-b698-4d77-b606-aa7ec2dde7bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e703c2b-dba4-46cf-8ded-6734778d1d5a", "node_type": "1", "metadata": {}, "hash": "d844c544b26f303eed76930fbe0f0d359d0469eede8e1f3fb98e92e4109569f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5221a53c-2c0c-44db-8829-db95e656070e", "node_type": "1", "metadata": {}, "hash": "b080948b3ede0ac5e0e92dabee796077f7c390758f9948c28fd0a3dce7dbf31c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}}, "text": "[303] J. Liu, S. Cao, J. Shi etal., \u201cProbing structured semantics under-\nstanding and generation of language models via question answering,\u201d\narXiv:2401.05777, 2024.\n[304] G. Xiong, J. Bao, and W. Zhao, \u201cInteractive-kbqa: Multi-turn inter-\nactions for knowledge base question answering with large language\nmodels,\u201d arXiv:2402.15131, 2024.", "start_char_idx": 315, "end_char_idx": 652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5221a53c-2c0c-44db-8829-db95e656070e": {"__data__": {"id_": "5221a53c-2c0c-44db-8829-db95e656070e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ac57fb1-b698-4d77-b606-aa7ec2dde7bc", "node_type": "1", "metadata": {}, "hash": "55cb36b23880a53e722c7e524a501507fab326f9e42e4fa40dde0b2c89ed33da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc43bea9-8e24-4e6d-9241-e795f49eb960", "node_type": "1", "metadata": {}, "hash": "f3832e65463c8ed6b6d493856bf09835dd6fa7e6f864ece74cba14172708f6d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}}, "text": "[305] S. Chen, Q. Liu, Z. Yu etal., \u201cRetrack: A flexible and efficient\nframework for knowledge base question answering,\u201d in ACL, 2021.\n[306] D. Yu, C. Zhu, Y . Fang etal., \u201cKg-fid: Infusing knowledge graph in\nfusion-in-decoder for open-domain question answering,\u201d in ACL, 2022.\n[307] Z. Hu, Y .", "start_char_idx": 653, "end_char_idx": 947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc43bea9-8e24-4e6d-9241-e795f49eb960": {"__data__": {"id_": "cc43bea9-8e24-4e6d-9241-e795f49eb960", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5221a53c-2c0c-44db-8829-db95e656070e", "node_type": "1", "metadata": {}, "hash": "b080948b3ede0ac5e0e92dabee796077f7c390758f9948c28fd0a3dce7dbf31c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6947143-c9bc-4abe-9c9d-79a5fe1e7f8b", "node_type": "1", "metadata": {}, "hash": "f03ead6bf70d24c0db8c9a736e0b53c2b0276168cb84342e26ab62593d4db33d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}}, "text": "[307] Z. Hu, Y . Xu, W. Yu etal., \u201cEmpowering language models with\nknowledge graph reasoning for open-domain question answering,\u201d in\nEMNLP, 2022.\n[308] M. Ju, W. Yu, T. Zhao etal., \u201cGrape: Knowledge graph enhanced\npassage reader for open-domain question answering,\u201d in EMNLP\nFindings, 2022.", "start_char_idx": 931, "end_char_idx": 1221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6947143-c9bc-4abe-9c9d-79a5fe1e7f8b": {"__data__": {"id_": "e6947143-c9bc-4abe-9c9d-79a5fe1e7f8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc43bea9-8e24-4e6d-9241-e795f49eb960", "node_type": "1", "metadata": {}, "hash": "f3832e65463c8ed6b6d493856bf09835dd6fa7e6f864ece74cba14172708f6d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "node_type": "1", "metadata": {}, "hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "class_name": "RelatedNodeInfo"}}, "text": "[309] Q. Yang, Q. Chen, W. Wang etal., \u201cEnhancing multi-modal multi-\nhop question answering via structured knowledge and unified retrieval-\ngeneration,\u201d in MM, 2023.\n[310] W. Zhao, Y . Liu, T. Niu etal., \u201cDIVKNOWQA: assessing the reasoning\nability of llms via open-domain question answering over knowledge\nbase and text,\u201d arXiv:2310.20170, 2023.", "start_char_idx": 1222, "end_char_idx": 1567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7dec7ad-180d-4584-aba7-1440a17e4926": {"__data__": {"id_": "a7dec7ad-180d-4584-aba7-1440a17e4926", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2b79e9f-0df5-46f7-b817-a49c288a6411", "node_type": "1", "metadata": {}, "hash": "1c7c217317296557c05da625f77cc0c9529a58c4230c59a18a99f88ff08fe221", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b2b79e9f-0df5-46f7-b817-a49c288a6411", "node_type": "1", "metadata": {}, "hash": "1c7c217317296557c05da625f77cc0c9529a58c4230c59a18a99f88ff08fe221", "class_name": "RelatedNodeInfo"}}, "text": "[311] X. Wang, Q. Yang, Y . Qiu etal., \u201cKnowledgpt: Enhancing large\nlanguage models with retrieval and storage access on knowledge bases,\u201d\narXiv:2308.11761, 2023.", "start_char_idx": 0, "end_char_idx": 162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f3a2651-8e67-4f13-b519-4fb05133d539": {"__data__": {"id_": "1f3a2651-8e67-4f13-b519-4fb05133d539", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ab974a1-c2d5-4f5b-ba7f-7cb8895494fb", "node_type": "1", "metadata": {}, "hash": "2521929a6f40b4bdb1fe61250b278fac3f35e880df83ab5e0f092eeb0c504945", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}}, "text": "[312] S. Ko, H. Cho, H. Chae etal., \u201cEvidence-focused fact summa-\nrization for knowledge-augmented zero-shot question answering,\u201d\narXiv:2403.02966, 2024.\n[313] Y . Gao, L. Qiao, Z. Kan etal., \u201cTwo-stage generative question\nanswering on temporal knowledge graph using large language models,\u201d\narXiv:2402.16568, 2024.", "start_char_idx": 0, "end_char_idx": 314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ab974a1-c2d5-4f5b-ba7f-7cb8895494fb": {"__data__": {"id_": "7ab974a1-c2d5-4f5b-ba7f-7cb8895494fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f3a2651-8e67-4f13-b519-4fb05133d539", "node_type": "1", "metadata": {}, "hash": "df56259fc0d40d1b77dc1eac26c524023327b96d67363bf07004bb33e60d14a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3daf5567-08fc-4895-9fc4-a5ddd2814134", "node_type": "1", "metadata": {}, "hash": "da05fb640e2428ef73a5f2dcf8e208b19e24f5f5f39778e6010d087e1cd8fd91", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}}, "text": "[314] T. Guo, Q. Yang, C. Wang etal., \u201cKnowledgenavigator: Leveraging\nlarge language models for enhanced reasoning over knowledge graph,\u201d\narXiv:2312.15880, 2023.\n[315] S. Min, J. Boyd-Graber, C. Alberti etal., \u201cNeurips 2020 efficientqa\ncompetition: Systems, analyses and lessons learned,\u201d in NeurIPS 2020\nCompetition andDemonstration Track, 2021.", "start_char_idx": 315, "end_char_idx": 661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3daf5567-08fc-4895-9fc4-a5ddd2814134": {"__data__": {"id_": "3daf5567-08fc-4895-9fc4-a5ddd2814134", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ab974a1-c2d5-4f5b-ba7f-7cb8895494fb", "node_type": "1", "metadata": {}, "hash": "2521929a6f40b4bdb1fe61250b278fac3f35e880df83ab5e0f092eeb0c504945", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "515b74fb-2324-44c3-bd65-4fb244f6e74d", "node_type": "1", "metadata": {}, "hash": "75f2d308eac87f9e29d3d3a24a6aa8be868a72e30cf3b1a30cadf415e7f25997", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}}, "text": "[316] A. H. Li, P. Ng, P. Xu etal., \u201cDual reader-parser on hybrid tex-\ntual and tabular evidence for open domain question answering,\u201d in\nACL/IJCNLP, 2021.\n[317] P. Christmann, R. S. Roy, and G. Weikum, \u201cConversational question\nanswering on heterogeneous sources,\u201d in SIGIR, 2022.", "start_char_idx": 661, "end_char_idx": 940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "515b74fb-2324-44c3-bd65-4fb244f6e74d": {"__data__": {"id_": "515b74fb-2324-44c3-bd65-4fb244f6e74d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3daf5567-08fc-4895-9fc4-a5ddd2814134", "node_type": "1", "metadata": {}, "hash": "da05fb640e2428ef73a5f2dcf8e208b19e24f5f5f39778e6010d087e1cd8fd91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4af6706-eb29-45ef-b74d-d7de98e6c324", "node_type": "1", "metadata": {}, "hash": "f5356c305bd9ede44480cbf292b58f3594d28989d3823c65fe9b8a26a1c9f0a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}}, "text": "[318] K. Ma, H. Cheng, X. Liu etal., \u201cOpen-domain question answering\nvia chain of reasoning over heterogeneous knowledge,\u201d in EMNLP\nFindings, 2022.\n[319] E. Park, S.-M. Lee etal., \u201cRink: reader-inherited evidence reranker for\ntable-and-text open domain question answering,\u201d in AAAI, 2023.\n[320] W. Zhao, Y . Liu, Y .", "start_char_idx": 941, "end_char_idx": 1257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4af6706-eb29-45ef-b74d-d7de98e6c324": {"__data__": {"id_": "a4af6706-eb29-45ef-b74d-d7de98e6c324", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "515b74fb-2324-44c3-bd65-4fb244f6e74d", "node_type": "1", "metadata": {}, "hash": "75f2d308eac87f9e29d3d3a24a6aa8be868a72e30cf3b1a30cadf415e7f25997", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b862e92-4358-47af-950d-ebbb46d47f6d", "node_type": "1", "metadata": {}, "hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "class_name": "RelatedNodeInfo"}}, "text": "[320] W. Zhao, Y . Liu, Y . Wan etal., \u201cLocalize, retrieve and fuse: A\ngeneralized framework for free-form question answering over tables,\u201d\narXiv:2309.11049, 2023.\n[321] F. Pan, M. Canim etal., \u201cEnd-to-end table question answering via\nretrieval-augmented generation,\u201d arXiv:2203.16714, 2022.\n[322] Z. Jiang, Y .", "start_char_idx": 1230, "end_char_idx": 1541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f4221be-6049-4901-8efb-e694d6bf02d3": {"__data__": {"id_": "2f4221be-6049-4901-8efb-e694d6bf02d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce5eeb5c-6acf-4e7b-930a-a01bbad620b5", "node_type": "1", "metadata": {}, "hash": "3648978ac2cd3e3d1f759bcfa6ced7d418797432a9bbdc3b867201c3a78e4da4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}}, "text": "[322] Z. Jiang, Y . Mao, P. He etal., \u201cOmnitab: Pretraining with natural\nand synthetic data for few-shot table-based question answering,\u201d in\nNAACL, 2022.\n[323] W. Zhong, J. Huang, Q. Liu etal., \u201cReasoning over hybrid chain for\ntable-and-text open domain question answering,\u201d in IJCAI, 2022.", "start_char_idx": 0, "end_char_idx": 290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce5eeb5c-6acf-4e7b-930a-a01bbad620b5": {"__data__": {"id_": "ce5eeb5c-6acf-4e7b-930a-a01bbad620b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f4221be-6049-4901-8efb-e694d6bf02d3", "node_type": "1", "metadata": {}, "hash": "5ec09b093f3b6b6ab1d21c44c2cc76e7e59ee729cdeb4d1e8e52c5eb351196a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb6869a9-dce1-4e01-b5f5-ab824ac0c734", "node_type": "1", "metadata": {}, "hash": "0c3eedac0486cfabe18c592fef3f7b353a44feec5286614ab7a0b58824261e93", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}}, "text": "[324] A. S. Sundar and L. Heck, \u201cctbl: Augmenting large language models\nfor conversational tables,\u201d arXiv:2303.12024, 2023.\n[325] D. Min, N. Hu, R. Jin etal., \u201cExploring the impact of table-to-text\nmethods on augmenting llm-based question answering with domain\nhybrid data,\u201d arXiv:2402.12869, 2024.\n[326] S. Wu, Y .", "start_char_idx": 291, "end_char_idx": 606, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb6869a9-dce1-4e01-b5f5-ab824ac0c734": {"__data__": {"id_": "cb6869a9-dce1-4e01-b5f5-ab824ac0c734", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce5eeb5c-6acf-4e7b-930a-a01bbad620b5", "node_type": "1", "metadata": {}, "hash": "3648978ac2cd3e3d1f759bcfa6ced7d418797432a9bbdc3b867201c3a78e4da4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4eed92e-1087-49a4-a506-5e1c481ca07c", "node_type": "1", "metadata": {}, "hash": "938e50e1d287bff1ee90f2f42c7099ba96aecbf80240efaaff4af1d05db3ba20", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}}, "text": "[326] S. Wu, Y . Li, D. Zhang, and Z. Wu, \u201cImproving knowledge-aware\ndialogue response generation by using human-written prototype dia-\nlogues,\u201d in EMNLP Findings, 2020.\n[327] M. Kang, J. M. Kwak etal., \u201cKnowledge-consistent dialogue genera-\ntion with knowledge graphs,\u201d in ICML Workshop, 2022.", "start_char_idx": 590, "end_char_idx": 884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4eed92e-1087-49a4-a506-5e1c481ca07c": {"__data__": {"id_": "d4eed92e-1087-49a4-a506-5e1c481ca07c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb6869a9-dce1-4e01-b5f5-ab824ac0c734", "node_type": "1", "metadata": {}, "hash": "0c3eedac0486cfabe18c592fef3f7b353a44feec5286614ab7a0b58824261e93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6078340-927d-4d92-b7c9-fd480fb21ce4", "node_type": "1", "metadata": {}, "hash": "4b195bc9bf2fd8e04dfb0e3b5f7ceddf9fbb40deffcbf83af2b677f720e01433", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}}, "text": "[328] Z. Ji, Z. Liu, N. Lee etal., \u201cRHO: reducing hallucination in open-\ndomain dialogues with knowledge grounding,\u201d in ACL Findings, 2023.\n[329] J. Baek, N. Chandrasekaran, S. Cucerzan etal., \u201cKnowledge-\naugmented large language models for personalized contextual query\nsuggestion,\u201d arXiv:2311.06318, 2023.\n[330] X. He, Y . Tian, Y .", "start_char_idx": 885, "end_char_idx": 1219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6078340-927d-4d92-b7c9-fd480fb21ce4": {"__data__": {"id_": "d6078340-927d-4d92-b7c9-fd480fb21ce4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4eed92e-1087-49a4-a506-5e1c481ca07c", "node_type": "1", "metadata": {}, "hash": "938e50e1d287bff1ee90f2f42c7099ba96aecbf80240efaaff4af1d05db3ba20", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69", "node_type": "1", "metadata": {}, "hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "class_name": "RelatedNodeInfo"}}, "text": "[330] X. He, Y . Tian, Y . Sun etal., \u201cG-retriever: Retrieval-augmented\ngeneration for textual graph understanding and question answering,\u201d\narXiv:2402.07630, 2024.\n[331] Y . Kirstain, O. Levy, and A. Polyak, \u201cX&fuse: Fusing visual informa-\ntion in text-to-image generation,\u201d arXiv:2303.01000, 2023.", "start_char_idx": 1193, "end_char_idx": 1491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5d3ba2f-8b66-4e08-8aaf-5ece6ba7cf19": {"__data__": {"id_": "d5d3ba2f-8b66-4e08-8aaf-5ece6ba7cf19", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4317992-1e40-424e-8538-d9ad7e83c557", "node_type": "1", "metadata": {}, "hash": "f5b2d4b4be2c9562849c991a94f84bcb1c3fca254e320b22e3526cb8a2ad023a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}}, "text": "[332] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d NeurIPS, 2021.\n[333] Z. Zhang, A. Zhang, M. Li etal., \u201cMultimodal chain-of-thought\nreasoning in language models,\u201d arXiv:2302.00923, 2023.", "start_char_idx": 0, "end_char_idx": 219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4317992-1e40-424e-8538-d9ad7e83c557": {"__data__": {"id_": "f4317992-1e40-424e-8538-d9ad7e83c557", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5d3ba2f-8b66-4e08-8aaf-5ece6ba7cf19", "node_type": "1", "metadata": {}, "hash": "8b714012ffb895a7b06bb6e0345c6c61df4fe702a3c7a24b89209d1b957abf7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7ea3364-fb19-4d7e-baa9-ea9b311062e0", "node_type": "1", "metadata": {}, "hash": "b97b335fef6dd815fbff2f8cbd6bf8b4d73a2fb1c65116096af0d3012d06e40e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}}, "text": "[334] C. Xu, M. Yang, X. Ao etal., \u201cRetrieval-enhanced adversarial train-\ning with dynamic memory-augmented attention for image paragraph\ncaptioning,\u201d Knowledge-Based Systems, vol. 214, p. 106730, 2021.\n[335] R. Ramos, D. Elliott, and B. Martins, \u201cRetrieval-augmented image\ncaptioning,\u201d in EACL, 2023.", "start_char_idx": 220, "end_char_idx": 521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7ea3364-fb19-4d7e-baa9-ea9b311062e0": {"__data__": {"id_": "e7ea3364-fb19-4d7e-baa9-ea9b311062e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4317992-1e40-424e-8538-d9ad7e83c557", "node_type": "1", "metadata": {}, "hash": "f5b2d4b4be2c9562849c991a94f84bcb1c3fca254e320b22e3526cb8a2ad023a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a07698f-dd77-4a84-a079-4546508d342a", "node_type": "1", "metadata": {}, "hash": "2dda67a7b890fa906707af3141ecb84314bb10db8a59b929b909f5a7f0be1851", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}}, "text": "[336] Z. Hu, A. Iscen, C. Sun etal., \u201cReveal: Retrieval-augmented visual-\nlanguage pre-training with multi-source multimodal knowledge mem-\nory,\u201d in CVPR, 2023.\n[337] Z. Li, W. Zhao, X. Du etal., \u201cCross-modal retrieval and semantic\nrefinement for remote sensing image captioning,\u201d Remote Sensing,\nvol. 16, no. 1, p. 196, 2024.", "start_char_idx": 522, "end_char_idx": 848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a07698f-dd77-4a84-a079-4546508d342a": {"__data__": {"id_": "0a07698f-dd77-4a84-a079-4546508d342a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7ea3364-fb19-4d7e-baa9-ea9b311062e0", "node_type": "1", "metadata": {}, "hash": "b97b335fef6dd815fbff2f8cbd6bf8b4d73a2fb1c65116096af0d3012d06e40e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "848f1cde-102a-4379-ada5-a556f70b4b2b", "node_type": "1", "metadata": {}, "hash": "6d1114011a1106eceded9c900877c93a6a1d3cc3a1797fa194a358940c91b055", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}}, "text": "16, no. 1, p. 196, 2024.\n[338] Z. Yang, Z. Gan, J. Wang etal., \u201cAn empirical study of gpt-3 for\nfew-shot knowledge-based vqa,\u201d in AAAI, 2022.\n[339] W. Lin and B. Byrne, \u201cRetrieval augmented visual question answering\nwith outside knowledge,\u201d in EMNLP, 2022.", "start_char_idx": 824, "end_char_idx": 1080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "848f1cde-102a-4379-ada5-a556f70b4b2b": {"__data__": {"id_": "848f1cde-102a-4379-ada5-a556f70b4b2b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a07698f-dd77-4a84-a079-4546508d342a", "node_type": "1", "metadata": {}, "hash": "2dda67a7b890fa906707af3141ecb84314bb10db8a59b929b909f5a7f0be1851", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1157e1f-ad11-4ca4-9184-987e74252bae", "node_type": "1", "metadata": {}, "hash": "ee4e1b792d4171e086d0235b7b2ffc7ea240936beb813429c737c7f2ffcc6cf6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}}, "text": "[340] A. Fan, C. Gardent, C. Braud, and A. Bordes, \u201cAugmenting transform-\ners with knn-based composite memory for dialog,\u201d TACL, vol. 9, pp.\n82\u201399, 2021.\n[341] Z. Liang, H. Hu, C. Xu etal., \u201cMaria: A visual experience powered\nconversational agent,\u201d in ACL-IJCNLP, 2021.\n[342] Q. Fang and Y .", "start_char_idx": 1081, "end_char_idx": 1372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1157e1f-ad11-4ca4-9184-987e74252bae": {"__data__": {"id_": "a1157e1f-ad11-4ca4-9184-987e74252bae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "848f1cde-102a-4379-ada5-a556f70b4b2b", "node_type": "1", "metadata": {}, "hash": "6d1114011a1106eceded9c900877c93a6a1d3cc3a1797fa194a358940c91b055", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "node_type": "1", "metadata": {}, "hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "class_name": "RelatedNodeInfo"}}, "text": "[342] Q. Fang and Y . Feng, \u201cNeural machine translation with phrase-level\nuniversal visual representations,\u201d in ACL, 2022.", "start_char_idx": 1351, "end_char_idx": 1473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "317486b6-b619-44da-b962-cbee9a56f424": {"__data__": {"id_": "317486b6-b619-44da-b962-cbee9a56f424", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2b58adf-a680-4275-ae91-7be5c85e20a7", "node_type": "1", "metadata": {}, "hash": "b25ea2969b66a8fd75bb1ceed26c4f1cce59f24f62a1e89e7d15ffe28ac7a2f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}}, "text": "[343] S. Whitehead, H. Ji, M. Bansal etal., \u201cIncorporating background\nknowledge into video description generation,\u201d in EMNLP, 2018.\n[344] C. Yin, J. Tang, Z. Xu, and Y . Wang, \u201cMemory augmented deep\nrecurrent neural network for video question answering,\u201d TNNLS,\nvol. 31, no. 9, pp. 3159\u20133167, 2019.\n[345] J. Pan, Z. Lin, Y .", "start_char_idx": 0, "end_char_idx": 324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2b58adf-a680-4275-ae91-7be5c85e20a7": {"__data__": {"id_": "b2b58adf-a680-4275-ae91-7be5c85e20a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "317486b6-b619-44da-b962-cbee9a56f424", "node_type": "1", "metadata": {}, "hash": "76590c98284586dee98cf2ad0f5a22ade960808cf462ab763c3de0211a6b5a7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "236a59e2-54bc-4212-a555-5ed4d2bce5c0", "node_type": "1", "metadata": {}, "hash": "1183292c66a8eb663f72dddc27f2a0b9f766082cfcd86b3ccb422b6ee8daf1d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}}, "text": "[345] J. Pan, Z. Lin, Y . Ge etal., \u201cRetrieving-to-answer: Zero-shot video\nquestion answering with frozen large language models,\u201d in ICCV, 2023.\n[346] J. Lei, L. Yu, T. L. Berg, and M. Bansal, \u201cTvqa+: Spatio-temporal\ngrounding for video question answering,\u201d in ACL, 2020.", "start_char_idx": 299, "end_char_idx": 570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "236a59e2-54bc-4212-a555-5ed4d2bce5c0": {"__data__": {"id_": "236a59e2-54bc-4212-a555-5ed4d2bce5c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2b58adf-a680-4275-ae91-7be5c85e20a7", "node_type": "1", "metadata": {}, "hash": "b25ea2969b66a8fd75bb1ceed26c4f1cce59f24f62a1e89e7d15ffe28ac7a2f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "787786c1-3692-41ff-aaba-cf5b6b1c3e57", "node_type": "1", "metadata": {}, "hash": "327c353a260d80583864df84f0830ed5bfa4b5d7a76e8e616b49b5f657ba6535", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}}, "text": "[347] H. Le, N. Chen, and S. Hoi, \u201cVgnmn: Video-grounded neural module\nnetworks for video-grounded dialogue systems,\u201d in NAACL, 2022.28\n[348] Z. Wang, M. Li, R. Xu etal., \u201cLanguage models with image descriptors\nare strong few-shot video-language learners,\u201d in NeurIPS, 2022.", "start_char_idx": 571, "end_char_idx": 845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "787786c1-3692-41ff-aaba-cf5b6b1c3e57": {"__data__": {"id_": "787786c1-3692-41ff-aaba-cf5b6b1c3e57", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "236a59e2-54bc-4212-a555-5ed4d2bce5c0", "node_type": "1", "metadata": {}, "hash": "1183292c66a8eb663f72dddc27f2a0b9f766082cfcd86b3ccb422b6ee8daf1d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "089dcf6d-5b10-43cf-9ea5-1dea702c7438", "node_type": "1", "metadata": {}, "hash": "f2a6a931615fc6906432bbe4c85bbdee305a3ea8aef2b7471bce942f95f4518a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}}, "text": "[349] J. Yuan, S. Sun, D. Omeiza etal., \u201cRag-driver: Generalisable driving\nexplanations with retrieval-augmented in-context learning in multi-\nmodal large language model,\u201d arXiv:2402.10828, 2024.\n[350] S. Ghosh, S. Kumar, C. K. R. Evuru etal., \u201cRecap: retrieval-augmented\naudio captioning,\u201d in ICASSP, 2024.", "start_char_idx": 846, "end_char_idx": 1153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "089dcf6d-5b10-43cf-9ea5-1dea702c7438": {"__data__": {"id_": "089dcf6d-5b10-43cf-9ea5-1dea702c7438", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "787786c1-3692-41ff-aaba-cf5b6b1c3e57", "node_type": "1", "metadata": {}, "hash": "327c353a260d80583864df84f0830ed5bfa4b5d7a76e8e616b49b5f657ba6535", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "db514a78-d261-471f-8efb-8514117e6f4e", "node_type": "1", "metadata": {}, "hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "class_name": "RelatedNodeInfo"}}, "text": "[351] B. Elizalde, S. Deshmukh, and H. Wang, \u201cNatural language supervision\nfor general-purpose audio representations,\u201d in ICASSP, 2024.\n[352] T. Kouzelis and V . Katsouros, \u201cWeakly-supervised automated audio\ncaptioning via text only training,\u201d in DCASE Workshop, 2023.", "start_char_idx": 1154, "end_char_idx": 1422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94d116e3-498d-4027-89f8-a5254567289c": {"__data__": {"id_": "94d116e3-498d-4027-89f8-a5254567289c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9278baa9-0cba-447f-b50c-77148e46b013", "node_type": "1", "metadata": {}, "hash": "29be3e881568d726fbf2459b3f5c8063aaf55426eea9a659531cae58f1f57c27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9278baa9-0cba-447f-b50c-77148e46b013", "node_type": "1", "metadata": {}, "hash": "29be3e881568d726fbf2459b3f5c8063aaf55426eea9a659531cae58f1f57c27", "class_name": "RelatedNodeInfo"}}, "text": "[353] S. Deshmukh, B. Elizalde, D. Emmanouilidou etal., \u201cTraining audio\ncaptioning models without audio,\u201d in ICASSP, 2024.\n[354] L. Yang, Z. Huang, X. Zhou etal., \u201cPrompt-based 3d molecular\ndiffusion models for structure-based drug design,\u201d 2023.", "start_char_idx": 0, "end_char_idx": 246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78085d7d-4ea8-45e2-9aee-16f04562ec6e": {"__data__": {"id_": "78085d7d-4ea8-45e2-9aee-16f04562ec6e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b46f14d-21db-4664-98ba-8af0ed85d3ee", "node_type": "1", "metadata": {}, "hash": "0e3b0c278f9b142b890269cb5fd2d3e6d2d522d1387c3b904022f52fb0e2e3db", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}}, "text": "[355] T. Truong Jr and T. Bepler, \u201cPoet: A generative model of protein\nfamilies as sequences-of-sequences,\u201d NeurIPS, 2024.\n[356] G. Frisoni, M. Mizutani, G. Moro, and L. Valgimigli, \u201cBioreader: a\nretrieval-enhanced text-to-text transformer for biomedical literature,\u201d\ninEMNLP, 2022.", "start_char_idx": 0, "end_char_idx": 282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b46f14d-21db-4664-98ba-8af0ed85d3ee": {"__data__": {"id_": "4b46f14d-21db-4664-98ba-8af0ed85d3ee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78085d7d-4ea8-45e2-9aee-16f04562ec6e", "node_type": "1", "metadata": {}, "hash": "f765b393ec1faf2e6cf4a0fa1053401bde53029b79c42ea1efff2b6c3bbaee73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4930c5a3-29a2-47bb-bd40-8bae48133f38", "node_type": "1", "metadata": {}, "hash": "87e54c77c26f8671e26ce5feab1e894a573ee11e436db9488a51e71f8d5f7700", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}}, "text": "[357] X. Yang, M. Ye, Q. You etal., \u201cWriting by memorizing: Hierarchical\nretrieval-based medical report generation,\u201d arXiv:2106.06471, 2021.\n[358] J. Kim and M. Min, \u201cFrom rag to qa-rag: Integrating generative ai\nfor pharmaceutical regulatory compliance process,\u201d arXiv:2402.01717,\n2024.", "start_char_idx": 283, "end_char_idx": 570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4930c5a3-29a2-47bb-bd40-8bae48133f38": {"__data__": {"id_": "4930c5a3-29a2-47bb-bd40-8bae48133f38", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b46f14d-21db-4664-98ba-8af0ed85d3ee", "node_type": "1", "metadata": {}, "hash": "0e3b0c278f9b142b890269cb5fd2d3e6d2d522d1387c3b904022f52fb0e2e3db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1034634c-de88-4479-b5e0-5222c05e3d8b", "node_type": "1", "metadata": {}, "hash": "ac0162af9ac43490214d70974394e0fd821b517f64ed5a95ec81fd40c47cba1f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}}, "text": "[359] K. Yang, A. Swope etal., \u201cLeandojo: Theorem proving with retrieval-\naugmented language models,\u201d in NeurIPS, 2024.\n[360] Z. Levonian, C. Li, W. Zhu etal., \u201cRetrieval-augmented generation to\nimprove math question-answering: Trade-offs between groundedness\nand human preference,\u201d arXiv:2310.03184, 2023.", "start_char_idx": 571, "end_char_idx": 877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1034634c-de88-4479-b5e0-5222c05e3d8b": {"__data__": {"id_": "1034634c-de88-4479-b5e0-5222c05e3d8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4930c5a3-29a2-47bb-bd40-8bae48133f38", "node_type": "1", "metadata": {}, "hash": "87e54c77c26f8671e26ce5feab1e894a573ee11e436db9488a51e71f8d5f7700", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a76ebce-b70f-40d6-ab38-599e67907623", "node_type": "1", "metadata": {}, "hash": "5ef82369c213bb59ca1ce528702c523e591cdf7564fe7dc68076942b7010ef1f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}}, "text": "[361] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large language\nmodels in retrieval-augmented generation,\u201d arxiv:2309.01431, 2023.\n[362] S. ES, J. James, L. E. Anke, and S. Schockaert, \u201cRAGAS: automated\nevaluation of retrieval augmented generation,\u201d arxiv:2309.15217, 2023.", "start_char_idx": 878, "end_char_idx": 1159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a76ebce-b70f-40d6-ab38-599e67907623": {"__data__": {"id_": "8a76ebce-b70f-40d6-ab38-599e67907623", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1034634c-de88-4479-b5e0-5222c05e3d8b", "node_type": "1", "metadata": {}, "hash": "ac0162af9ac43490214d70974394e0fd821b517f64ed5a95ec81fd40c47cba1f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8", "node_type": "1", "metadata": {}, "hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "class_name": "RelatedNodeInfo"}}, "text": "[363] J. Saad-Falcon, O. Khattab, C. Potts etal., \u201cARES: an automated\nevaluation framework for retrieval-augmented generation systems,\u201d\narxiv:2311.09476, 2023.\n[364] https://github.com/truera/trulens.\n[365] Y .", "start_char_idx": 1160, "end_char_idx": 1370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be0e2e87-03db-4fa1-8995-760715058d4c": {"__data__": {"id_": "be0e2e87-03db-4fa1-8995-760715058d4c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83de8f90-ecb7-4af1-889c-436add074d8c", "node_type": "1", "metadata": {}, "hash": "2d62cf3862b30951c57ac1d83c0e2b8371a5de387c8f8fcd17baa5b6f27c9e99", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}}, "text": "[364] https://github.com/truera/trulens.\n[365] Y . Lyu, Z. Li, S. Niu etal., \u201cCRUD-RAG: A comprehensive chinese\nbenchmark for retrieval-augmented generation of large language mod-\nels,\u201d arxiv:2401.17043, 2024.\n[366] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, \u201cBenchmarking retrieval-\naugmented generation for medicine,\u201d arXiv:2402.13178, 2024.", "start_char_idx": 0, "end_char_idx": 339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83de8f90-ecb7-4af1-889c-436add074d8c": {"__data__": {"id_": "83de8f90-ecb7-4af1-889c-436add074d8c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be0e2e87-03db-4fa1-8995-760715058d4c", "node_type": "1", "metadata": {}, "hash": "068dc30bc98aa568bf15ee519f219abb956711d691e7c3f8ddb632a39405530e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b8bb825-14cb-4e09-83a1-e561d198f0db", "node_type": "1", "metadata": {}, "hash": "b0964e6394cad3acb35ab2e1c357ddaefeafb92228ea5e0f9c110039ee57b831", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}}, "text": "[367] F. Petroni, A. Piktus etal., \u201cKilt: a benchmark for knowledge intensive\nlanguage tasks,\u201d in NAACL-HLT, 2021.\n[368] S. Barnett, S. Kurniawan, S. Thudumu etal., \u201cSeven failure\npoints when engineering a retrieval augmented generation system,\u201d\narXiv:2401.05856, 2024.", "start_char_idx": 340, "end_char_idx": 609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b8bb825-14cb-4e09-83a1-e561d198f0db": {"__data__": {"id_": "2b8bb825-14cb-4e09-83a1-e561d198f0db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83de8f90-ecb7-4af1-889c-436add074d8c", "node_type": "1", "metadata": {}, "hash": "2d62cf3862b30951c57ac1d83c0e2b8371a5de387c8f8fcd17baa5b6f27c9e99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eea9192d-fb03-4210-8a8b-4f4422ced3b3", "node_type": "1", "metadata": {}, "hash": "ef934910724b5ea4ca55e31e62ff250182d6b12bc5eb6e635a319767c6f46b2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}}, "text": "[369] F. Cuconasu, G. Trappolini, F. Siciliano etal., \u201cThe power of noise:\nRedefining retrieval for RAG systems,\u201d arXiv:2401.14887, 2024.\n[370] L. Qiu, P. Shaw, P. Pasupat etal., \u201cEvaluating the impact of\nmodel scale for compositional generalization in semantic parsing,\u201d\narXiv:2205.12253, 2022.", "start_char_idx": 610, "end_char_idx": 905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eea9192d-fb03-4210-8a8b-4f4422ced3b3": {"__data__": {"id_": "eea9192d-fb03-4210-8a8b-4f4422ced3b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b8bb825-14cb-4e09-83a1-e561d198f0db", "node_type": "1", "metadata": {}, "hash": "b0964e6394cad3acb35ab2e1c357ddaefeafb92228ea5e0f9c110039ee57b831", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9c00693-e096-43b0-9234-78e0bb1e380e", "node_type": "1", "metadata": {}, "hash": "001ae1d1b6916cf1d6b95233f50099b663812c53fac4d7b7a594c640f7083fbb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}}, "text": "[371] R. Jagerman, H. Zhuang, Z. Qin etal., \u201cQuery expansion by prompting\nlarge language models,\u201d arxiv:2305.03653, 2023.\n[372] H. Zhang, P. Zhao, X. Miao etal., \u201cExperimental analysis of large-scale\nlearnable vector storage compression,\u201d VLDB, 2023.", "start_char_idx": 906, "end_char_idx": 1156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9c00693-e096-43b0-9234-78e0bb1e380e": {"__data__": {"id_": "d9c00693-e096-43b0-9234-78e0bb1e380e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eea9192d-fb03-4210-8a8b-4f4422ced3b3", "node_type": "1", "metadata": {}, "hash": "ef934910724b5ea4ca55e31e62ff250182d6b12bc5eb6e635a319767c6f46b2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "node_type": "1", "metadata": {}, "hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "class_name": "RelatedNodeInfo"}}, "text": "[373] R. Aksitov, C. Chang, D. Reitter etal., \u201cCharacterizing attribution\nand fluency tradeoffs for retrieval-augmented large language models,\u201d\narXiv:2302.05578, 2023.\n[374] C. Han, Q. Wang, W. Xiong etal., \u201cLm-infinite: Simple on-the-fly\nlength generalization for large language models,\u201d arXiv:2308.16137,\n2023.", "start_char_idx": 1157, "end_char_idx": 1469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e31af5c-b9e7-4a58-a655-31047102064b": {"__data__": {"id_": "6e31af5c-b9e7-4a58-a655-31047102064b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea3280e2-9061-4d36-86ab-2bc4d70fce08", "node_type": "1", "metadata": {}, "hash": "d84a4fa0aa5d5dfaf3467309165bb22c19101c2ed6dec85cb948cdc96c82bf34", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ea3280e2-9061-4d36-86ab-2bc4d70fce08", "node_type": "1", "metadata": {}, "hash": "d84a4fa0aa5d5dfaf3467309165bb22c19101c2ed6dec85cb948cdc96c82bf34", "class_name": "RelatedNodeInfo"}}, "text": "[375] H. Chase, \u201cLangchain,\u201d https://github.com/langchain-ai/langchain,\n2022.\n[376] W. Jiang, S. Zhang, B. Han etal., \u201cPiperag: Fast retrieval-augmented\ngeneration via algorithm-system co-design,\u201d arXiv:2403.05676, 2024.\n[377] S. Jindal, \u201cDid google gemini 1.5 really kill rag?\u201d https://\nanalyticsindiamag.com/did-google-gemini-1-5-really-kill-rag/, 2024.", "start_char_idx": 0, "end_char_idx": 355, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"a4213a0c-8fd1-46eb-aafa-d22b7215bb4a": {"doc_hash": "ccce130458468eb38c051e2a2b25d383353515cb34bc1be157a2071fc99b5d87", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "78b629a5-d948-4752-953c-92ec8f2abd64": {"doc_hash": "4fbd19cdfcc2f3774745e412fd24f2147c4e7cc05fd091927c97215642821341", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "4961f468-f454-4ff5-8b6c-07d1984842c8": {"doc_hash": "a6769c1e85cf5398ae5115b74d67b705770557ec9162d22cf98b18968c24d9d7", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "801c3840-c975-4417-9efd-ba90e44414fc": {"doc_hash": "0599a883f50d8a31d3950fa61938a8bcc62397ea72ed582c5c0f22c9a81ab5ee", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "2c96feaa-3fa4-4003-80fa-069f0af17fb7": {"doc_hash": "866149bc2e510a0957bed951bb9dae5b5fcee1cff95d162b1daa53521910cbf9", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "0eb84439-aaea-4a8a-b8d2-504a38ad5eef": {"doc_hash": "75e8fd4d814824b8a7ece8eec4e9d8a7daaee5ef1029f1c9b458d096c0285dee", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "f170389f-7074-4cd1-a5c2-dd67b644a58f": {"doc_hash": "669901d783ff6d275dd817ae8f9254cf331935f99fbd544c5be156237191b312", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "d2915bcf-d475-4bab-9409-6cf973c97c17": {"doc_hash": "cee5cb9c0828bbe664d0333ee5cfe2fe280498a90f1cc2b8520248a55c13c8f8", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "d55f2bc4-1980-41a0-929f-f90452cd71ca": {"doc_hash": "f25be5633f4b76e848ca1aea16a422d2f65aaca69dff6a1b2cc06df965df24ed", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "9cd5886b-518b-42b6-80e5-8fcaf57365aa": {"doc_hash": "50183d8887ca479f7b7df616d1f45abcf5fa085a05e5407bdd642ee0e60bb713", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e": {"doc_hash": "d8f546d79d53b26b0a70a9fa40cc685ddcf81d353baba2b286bd9743f396b5fc", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "041dcbc3-b10c-426b-995b-38234f2d3a38": {"doc_hash": "b9b6ad707c4e7f88de0d16bb44b5b49c908c092f1307f0404dbe12ee1c1741ef", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "4de4eb18-c462-40ee-ba75-65aad1ab0a9e": {"doc_hash": "e98e1e824549e83e616cfc95e58c875cdc04f499d0280f0ddfc9e0cb1f4f1d4a", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "3090aed2-bf72-47d4-8ba2-f82f70cef882": {"doc_hash": "d5eaf6098000ae70a6dba69d2c16e30a9c5d28eca2879d5a5dba7bc0878a38be", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "143faccb-70a4-4f32-99ba-66824f74c219": {"doc_hash": "eb4a5ab4188ab9c4dbcebeb49c4986e253f10ab2fc6630295da5116c5432b7a0", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "bc72ac09-e3b2-4896-9bf4-18aedb7738da": {"doc_hash": "8599422c9409cc4072e3b185a7d1e67d8fc636f15d1a9f3bac7e764fbb665a0a", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd": {"doc_hash": "3b372141d4b682da0d8737bf4ab44d4103ec790060e1567400a032f2ae0754b7", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "26864623-e72d-4b91-b583-8d12ace4d8e1": {"doc_hash": "10b12d1d094a40ac0c80bb6fc028a4ac37187be513cda85db82892a4a650186a", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "436bed26-cd7a-4c41-ab9b-76184bb157fd": {"doc_hash": "0e8a474a3d46973012ac4a2864580d56b3de46e4d829be4807d361aafafe5ac7", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "2c4ac0a6-1a71-4196-9877-d2964b0f3744": {"doc_hash": "42d508e1b7709facde480a15ba1aac697509be6d77565c6e014281c8a336d6e9", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "22825eff-8ad1-4d03-823a-8e232176aa19": {"doc_hash": "e3f9b75305d5f087a9645b2afec2d55d7abb617233f5dda44e49836234015ff8", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf": {"doc_hash": "1f09a6ac8ec233becf8a12fbca6eb8fbe0fcfe9dcdc70f5bc385137c930dfdc3", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "4d9297b7-b236-40fe-8cef-bb5cfd5f196f": {"doc_hash": "aafcfe6354764feca698600dac24bb49bda23e2dde7f02ec7225fbc1a46ded47", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "c03adbde-2467-4504-9b19-72d0e86c8cd5": {"doc_hash": "fb61364b00aff4c9c027c4f50ce38a840db13bfab496e7c6d0f29cc097ed2d12", "ref_doc_id": "adb9d1bd-1928-401f-8716-0ea1b42f3e59"}, "2587bb96-4640-4265-9076-9e1d531a2381": {"doc_hash": "c9624a64ea319fe12f869d99bd95b5302416fec75b4ce99a030a561780c5fb14", "ref_doc_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a"}, "45966987-f98a-421f-9271-5c503d67344d": {"doc_hash": "36399d7b6fe6ae64909541524f3ab7365bcf86ad4150afecdef8b34bad5186a9", "ref_doc_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a"}, "08040b3d-2224-4c75-a034-cfb9aa9f9b61": {"doc_hash": "5852f269b2566e1180b58b25004c39aeca4262f84789db17c19e2df016c30952", "ref_doc_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a"}, "6e438b00-2436-4718-b18e-6835511b6962": {"doc_hash": "259333a9d2d190a72086e799d6bb15372ad42ebadfe2e8ea8706bae3a952dffe", "ref_doc_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a"}, "f7399abd-cb1b-48f9-886a-8746f5f1e173": {"doc_hash": "0f63f2d3865eb73bcbe7d9640eb81ce9aaf264d8fc768e2598f9366304350af2", "ref_doc_id": "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a"}, "cc987eab-006b-480a-8593-97ad22ce1f35": {"doc_hash": "128f0e3808ef5e514e5345290fa3abd0657d5309062d638c04f6b409040c8761", "ref_doc_id": "78b629a5-d948-4752-953c-92ec8f2abd64"}, "b1743ce2-002e-4688-b43d-d0294a9d9f23": {"doc_hash": "21439aa5ee864d0382f22223c70d0e37d0b47d78c8f9a454a4d78e71b47566fc", "ref_doc_id": "78b629a5-d948-4752-953c-92ec8f2abd64"}, "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1": {"doc_hash": "196049b70886934754afc01a48871b5b9451d7ab82ae8744f89ee21858cc03f3", "ref_doc_id": "78b629a5-d948-4752-953c-92ec8f2abd64"}, "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43": {"doc_hash": "930806eb9313cbe49dbca686aa88877d7bea083e7f404978deb90a17454724f9", "ref_doc_id": "78b629a5-d948-4752-953c-92ec8f2abd64"}, "4f5a5cde-c754-4888-a5ed-2a21ee98be71": {"doc_hash": "0cb925af162397fad1504f1a6f778ebaff0c22db95a3980d89546b4d103ab791", "ref_doc_id": "78b629a5-d948-4752-953c-92ec8f2abd64"}, "37fb7373-7684-4059-9b8a-f119e12a9483": {"doc_hash": "38a08bf331f5620c4440bc6b94dcccb5d448b73d4c8987fa54ab08425f0c399d", "ref_doc_id": "4961f468-f454-4ff5-8b6c-07d1984842c8"}, "240d64f4-947b-4a30-8a40-4c32f3f3fb31": {"doc_hash": "cb279c456317cea05c418dd20b789e1b85c793053d5119e81b4885c04cd09a27", "ref_doc_id": "4961f468-f454-4ff5-8b6c-07d1984842c8"}, "2415d406-648d-488e-adee-8bd0fb74a5f5": {"doc_hash": "1c020e4e8243cba65bafc8c77593a1802184f4f2becddbd9803253ce3033abb7", "ref_doc_id": "4961f468-f454-4ff5-8b6c-07d1984842c8"}, "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9": {"doc_hash": "64659e19162f7287b10a48272d4ca7af4136bc8ea5df2f1cc782345b00e22c50", "ref_doc_id": "4961f468-f454-4ff5-8b6c-07d1984842c8"}, "9d81e32e-a897-46af-85a9-02e278286cd0": {"doc_hash": "1c581ea8f27640b23541bcda4f611dd2bb777d2ed23f9a757478ea4264ea11a1", "ref_doc_id": "4961f468-f454-4ff5-8b6c-07d1984842c8"}, "59f340a8-342e-491c-94b2-43642ff312f4": {"doc_hash": "daef8abf50b896b0693f1588123ab03a57cb1e2d17482e78f0adc4977c122fb4", "ref_doc_id": "801c3840-c975-4417-9efd-ba90e44414fc"}, "70f446ab-c0c8-4500-a291-c9562ad75f60": {"doc_hash": "13494be665c5dc92c346141dc855676ac61196e66efa2c2620e8befa9df0e7bb", "ref_doc_id": "801c3840-c975-4417-9efd-ba90e44414fc"}, "b7706ee9-3add-42ac-8e9c-799cc02542a3": {"doc_hash": "36942cc215a938d9b892d7734e9284e9e7d360d005b2417a0b4fcb85c8aa4631", "ref_doc_id": "801c3840-c975-4417-9efd-ba90e44414fc"}, "331dde64-1f43-487b-a9f4-dd1be4acf4a7": {"doc_hash": "509e1d4f8a3bb7be28142d60ee19f463b054ef3ddde327c28ab8641c006fbe21", "ref_doc_id": "801c3840-c975-4417-9efd-ba90e44414fc"}, "d78ca6ea-0bd4-4a06-a4de-9956656051dd": {"doc_hash": "5e1f7e76883b83dfadeac9a72b929a9ede1ad8ef5be9ea2275c5e90a49093aa6", "ref_doc_id": "801c3840-c975-4417-9efd-ba90e44414fc"}, "9dc57153-6fd5-499b-a634-fc574002380e": {"doc_hash": "9cf104e432a932e61f3f43fc46e6acf944af59596186f448fffee9b135664663", "ref_doc_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7"}, "633f7b61-b5f8-4142-8666-0a0ba4b804a8": {"doc_hash": "9e5bea079f93e660059ed9c521a10adfb10a49817af9d571c87a3d9cec12dbd5", "ref_doc_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7"}, "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b": {"doc_hash": "233c6c5070d4db866a177f27b8610d9e787e4e565310a1a05f5d378d81f589d3", "ref_doc_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7"}, "fac41315-4d32-4359-b216-55c151e8b5e1": {"doc_hash": "53379a78a51872280f4e36baff4960e71900bbadf14a537f3290978a40fc086b", "ref_doc_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7"}, "638f46aa-98ea-47c6-9083-3d5c75cc98b4": {"doc_hash": "47aa8341805eb8373995fbe9e89f3cf4f7b160423e5f87be8c6e8eee1b2e8464", "ref_doc_id": "2c96feaa-3fa4-4003-80fa-069f0af17fb7"}, "980c530c-1005-4c72-8d3a-68472f1c9de1": {"doc_hash": "d9852330cb4e182e1f3ec2a6d9942f39c48e67690203de809847cc83749867de", "ref_doc_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef"}, "89183965-5983-4d40-83bb-bc6c1f2a2d8d": {"doc_hash": "a8ccef5cf1041d247f958d265efc4e5c700b93bccafa0ee6453bca9add3c0f84", "ref_doc_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef"}, "6d7bfce5-d142-44e2-9d76-7001709526dd": {"doc_hash": "815900b2f6c11e55277cdb8c989282b97b53e4e0bcddae2b934ff4241b1d639c", "ref_doc_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef"}, "e288a398-5ad7-4fe0-9e03-11b716299770": {"doc_hash": "71a809498db587c7ed916495a123a005157a453468ab0ba35670f5962d5e78d6", "ref_doc_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef"}, "b833d9b9-cff4-4e06-9dba-20efb87d02d1": {"doc_hash": "86acf41015531bda8785a43dc2b5ced0fd4a0692d6ac5e7721c155949d42e2d9", "ref_doc_id": "0eb84439-aaea-4a8a-b8d2-504a38ad5eef"}, "3307b03b-7942-4e2b-a3ea-a083b2c85f81": {"doc_hash": "ebebedf1153a365d7eee6d4c097ddef5684b8c7ec4eb968fb7783a0de4901529", "ref_doc_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f"}, "5665018f-a756-44be-adc2-ce77d463ad0e": {"doc_hash": "cae488b233b19144891a81f1f6fba43940b2bd801835e7d2f57010fe47bbe9fa", "ref_doc_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f"}, "c7471598-449f-4ce6-870e-1a9e120e6ccd": {"doc_hash": "ac01684134096f869b6fd43b7ccb044ed51af753d1d07a6c9495a1bafc5e08b6", "ref_doc_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f"}, "51582689-3bf2-4ba0-b051-10e732354bda": {"doc_hash": "fa8c0e0814acacbd20708463ccc6e0e2a52b87c484e5417ba9bf006cd12a7ec4", "ref_doc_id": "f170389f-7074-4cd1-a5c2-dd67b644a58f"}, "cdd04060-a8e4-474c-a338-f65f89616e95": {"doc_hash": "1324d99147085ae101a55c746fe46011db425da302bd5435a71261e13aa2b374", "ref_doc_id": "d2915bcf-d475-4bab-9409-6cf973c97c17"}, "69604801-7765-4976-81e7-231241d2b2b1": {"doc_hash": "5a8efd53dccd388e5f09b619bd11a419592b6f5df549abc25ada01235260a557", "ref_doc_id": "d2915bcf-d475-4bab-9409-6cf973c97c17"}, "54f4b631-38f8-41b7-98e8-f94f97d23d56": {"doc_hash": "66ad87855cea172f18e898ac2b9f97f2f24533c3ddba576c329f0cb87f948c93", "ref_doc_id": "d2915bcf-d475-4bab-9409-6cf973c97c17"}, "4a77345a-c539-4264-a387-adf9022090e0": {"doc_hash": "80630485497617b96fb7bc2568bcb6d3d17c9aa391da07ef7ed43d19326c24b2", "ref_doc_id": "d2915bcf-d475-4bab-9409-6cf973c97c17"}, "c4884c63-ab65-499b-a999-c681de0dfcc4": {"doc_hash": "46bd8a4fc3ed4d3290e7003f3407af8e4d2db97749eb64131085bf710c8ab18a", "ref_doc_id": "d2915bcf-d475-4bab-9409-6cf973c97c17"}, "b3acbe4b-37d3-4ad7-8129-81790271bbfc": {"doc_hash": "cc4a2a0cf7e207372f947b427420b957a3a25559cff86ee7cc3c1886c33847a1", "ref_doc_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca"}, "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2": {"doc_hash": "86138d329ecd604eb215a1ccb7b009f06ae25b456d561dfd2f7f1b00d01c19d8", "ref_doc_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca"}, "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2": {"doc_hash": "dd46c3035b9a6f8d9cf416a02cb4e1524618e8a5aafb23b2c28a48c471200667", "ref_doc_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca"}, "9a68a403-80ce-43dd-b2dc-b6a49addecdf": {"doc_hash": "c40e61ef7f337e35ad2f6a415cc15dfd12006bba5651aad821f06ee26f4b03a4", "ref_doc_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca"}, "ad848f9e-6e82-4e71-92f7-1a534a8a184b": {"doc_hash": "58b053e1b7137d45e355adb640ff6ae383be6bfb4865c6fe8d3ada623fe85b69", "ref_doc_id": "d55f2bc4-1980-41a0-929f-f90452cd71ca"}, "3cb750da-ee3f-4d5f-90eb-46f05cfb3546": {"doc_hash": "c2bbcca3023f0b5510b58f27f6f61d4e769d90ea2f3abd834f8dfc3882f37793", "ref_doc_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa"}, "bb5f8140-17ea-42a8-b530-a5aaee35caa9": {"doc_hash": "a05f8971c076e28fa0a53046862f06de5fd884ac8a13e5680b7f048fbfc212e7", "ref_doc_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa"}, "92c16379-d8bd-463f-9c60-ec9831a037e6": {"doc_hash": "72151c63699531fb14c7b331771722da757f870d9e41840605c43389963e6e60", "ref_doc_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa"}, "54d96d23-c6e9-45cb-960d-1aedfdd1b580": {"doc_hash": "8d0ae261279af66a3c1d71b101fe9077fbbaaab459a8cf084e5c5aaee4169cc9", "ref_doc_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa"}, "b7b4af96-3f92-422b-a14e-63fbead4b498": {"doc_hash": "c94d5758c800353256e28de210bee124682beb04df926d46fc0e7f952804b1ee", "ref_doc_id": "9cd5886b-518b-42b6-80e5-8fcaf57365aa"}, "d9ef127c-8f59-4316-a43a-a91acee1dc89": {"doc_hash": "321cc8237c1acca6440da567086664725d0f2a36fe1578bcac1fb709459d975d", "ref_doc_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e"}, "3263c986-bb7b-4f7f-8240-c0cd405dec60": {"doc_hash": "f2e5aa523c6a8211c09454249b9ab577c96cd6769c54bf239dabd610e3ad5b7c", "ref_doc_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e"}, "da514732-a3cf-4a31-a994-8bc6710f22ec": {"doc_hash": "49d4ee6f0a5e5729e614c1b4e9b6c361862bf1cddca0be81628d8896ca973912", "ref_doc_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e"}, "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f": {"doc_hash": "b37896e97fd1d564b6deff86f02e85076fd579d73428b4bc516da7eb2538bc28", "ref_doc_id": "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e"}, "20062614-22e0-4695-9dae-521fdff26f79": {"doc_hash": "197b7aa58ec9b541b2861b71ef1bea67af825e7b49d215f4d253bdb82e8afa2c", "ref_doc_id": "041dcbc3-b10c-426b-995b-38234f2d3a38"}, "f056fd99-2019-42e5-9b42-718302e68fe2": {"doc_hash": "e616aad17ab07ff99331083bad03563ffec619ab60778f0d0a0ae7799416b45c", "ref_doc_id": "041dcbc3-b10c-426b-995b-38234f2d3a38"}, "32e72675-2960-49e4-afd6-5052ce1b0802": {"doc_hash": "8817d27a9c8577e578c439e2029c93ed0bd9c7ed24047fd41e528656dd0c7458", "ref_doc_id": "041dcbc3-b10c-426b-995b-38234f2d3a38"}, "dc06d373-fc7c-4d14-b2cb-95d390096739": {"doc_hash": "3a3a43e6ea8deb241dd1fd8169584d9b98ad99e35f18bb3f24acb334e61290b9", "ref_doc_id": "041dcbc3-b10c-426b-995b-38234f2d3a38"}, "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca": {"doc_hash": "c60ae4ed8fdcfe51dafce5c4af7675cbd46ee8f0bb30e601660346acd0633957", "ref_doc_id": "041dcbc3-b10c-426b-995b-38234f2d3a38"}, "b646a538-109b-47c0-ad7a-ac2283530d47": {"doc_hash": "e0216ecafd4c63c3dca79dd61b1a7176fd101638ac0da31b8d74c979aba9f99a", "ref_doc_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e"}, "fc9ebb92-7688-4767-b8f6-7b76327405bc": {"doc_hash": "881772ede3ea77917e317847d3b2c245ea991f4292e8ec44b15b3aab427b19c5", "ref_doc_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e"}, "fc9fe157-ab09-4904-b323-e8019ece8204": {"doc_hash": "d1199ff30d4afd78751ecad77feec1de1eb91a29d9bce2428b8b9287d88bd811", "ref_doc_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e"}, "d4055d84-e057-42ea-901f-7ba1753df337": {"doc_hash": "47f1345d584d3081d2e316ee3436e1d29025fca8960f1554ac8dbe118d3a45e4", "ref_doc_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e"}, "c98e98ee-03b5-4bf9-a652-999317a0b671": {"doc_hash": "70828131a4e089a5fd70194bf07328bef70c911b67ea89eae6f21470b4185c0c", "ref_doc_id": "4de4eb18-c462-40ee-ba75-65aad1ab0a9e"}, "25335f95-389e-4dfd-a165-a8c49e4ad787": {"doc_hash": "a2feee6405705526f83331266111f63a667c2160fa6fd473f93820e25f87b98c", "ref_doc_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882"}, "ff14e27b-df47-4028-8029-4a6e19eb36aa": {"doc_hash": "6521384df8f736f127d0ec9f979456bd1aafd65a8ff6743a68d3c6546d890635", "ref_doc_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882"}, "46dc9b82-6c13-4332-a2b3-4aa89903801f": {"doc_hash": "93223451fc578324ba06ff4b4808e645d18bca30b21a9aa3c87798bab4481533", "ref_doc_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882"}, "668623f4-21a5-4b59-b6ac-e78759e6b403": {"doc_hash": "f62f7496fd97bfdcb6ac019d55674da9cb0ecc21d45b018e3a151c03e9eee300", "ref_doc_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882"}, "c26f6c18-927a-4e11-b542-6fa85387b872": {"doc_hash": "d4de0904dc60501c95e3fd2e514611431f3b100c9259692bc819ded096879298", "ref_doc_id": "3090aed2-bf72-47d4-8ba2-f82f70cef882"}, "ad7a59b1-43f1-4324-99d3-262cc9769b05": {"doc_hash": "b77cd9d53c663df3868921dbdb9e6116cd1c75003c1d6213db59444896a5e194", "ref_doc_id": "143faccb-70a4-4f32-99ba-66824f74c219"}, "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4": {"doc_hash": "88e6e4ed873d9d80b5b7c3aae17dfdef9042698df53cb886d9c7181708fa81a0", "ref_doc_id": "143faccb-70a4-4f32-99ba-66824f74c219"}, "ce7a1637-31c5-4726-9393-835a332f13f2": {"doc_hash": "c6ae75c24c8092e6fdd261b267ba738d0cab9e87cae93b1c4ae9c48f700d087f", "ref_doc_id": "143faccb-70a4-4f32-99ba-66824f74c219"}, "001f02ad-43a4-4105-9aab-1eca82b49991": {"doc_hash": "3677ac6c9adda857c923f32464811a1224889d49683bc90ede376143584e28cb", "ref_doc_id": "143faccb-70a4-4f32-99ba-66824f74c219"}, "45593217-9f9a-4741-a8d5-1545ac7e429e": {"doc_hash": "c97aee1c760250dd596a785c4de320feadd0c4f892ad3cd7d5518a1cd165e1d0", "ref_doc_id": "143faccb-70a4-4f32-99ba-66824f74c219"}, "19780472-c500-4db8-9fe0-a3572c15efe9": {"doc_hash": "618812db7e4b4ab90384624543c6519bdaa5aa07b95511be492cfb490deb4399", "ref_doc_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da"}, "d6190d24-8b42-4a04-b7d0-7020e7d7af23": {"doc_hash": "bc664d2b88d44069a0708620d6b41d823f3d806b05b73575504f219892bee6fe", "ref_doc_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da"}, "17e36111-0937-4893-91bf-173e818746db": {"doc_hash": "b2c096bfb7d55ee8b135e7cf5c06d901ca85454836ee6cf851cad5ece86c418f", "ref_doc_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da"}, "3b9d1983-3c1b-423b-b132-1100f72674af": {"doc_hash": "241cfded3dac686dbea7a3306f9e29ce4455e4f68099e4de1a2a9d7144bd459e", "ref_doc_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da"}, "e209caf1-ead6-467f-9555-1a2e27ae1c9f": {"doc_hash": "e5f3c064f008e09ee6687542919075f8b54044aab99ed4a359a26a7af4ea6f39", "ref_doc_id": "bc72ac09-e3b2-4896-9bf4-18aedb7738da"}, "b9a22696-df65-46d8-9ae7-bc9360e912fb": {"doc_hash": "626fd034dd697668fc86d169d8b1a0ed004f02b7c970385e447426f1d10466b3", "ref_doc_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd"}, "6fa08b5f-e917-4b42-9ebe-3e710e3c170e": {"doc_hash": "077a75140bfc7350ca0a2a26eebe49fc935fc6f656b2bb6b6609e183b179da3c", "ref_doc_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd"}, "381ee8e8-4d6a-4558-b143-621cfd55728c": {"doc_hash": "760e470424f9cd9508079a7bbe737b16710bbd1306557277cbef637c9f560150", "ref_doc_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd"}, "56d2b4a8-ff0a-4335-939d-9aeb172ee460": {"doc_hash": "ebbbf37d48afaff0a7f4a3503ec0c1bd864b182ed27a4ab40a07946e937d6c36", "ref_doc_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd"}, "4d4afe67-524b-4449-a53a-5b1d6ae6c70f": {"doc_hash": "8edc8e65283541f8c15f5ecce593f0b9e2b2228dce4d420f53679e94054d3b16", "ref_doc_id": "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd"}, "c8034349-37aa-434a-baba-a1780c471578": {"doc_hash": "e03fcd275b0258b3ba74ffde2a837a1a02bd094e33ad804eaea0924004505e6f", "ref_doc_id": "26864623-e72d-4b91-b583-8d12ace4d8e1"}, "c2effa91-65ac-4805-976d-d7b90f4eb09e": {"doc_hash": "c9cb80ba79a8b9f6ef09c26911c85a22e54431f61a928d08f76ce707ec0ba1fb", "ref_doc_id": "26864623-e72d-4b91-b583-8d12ace4d8e1"}, "64e5a052-01aa-40bb-aa0a-d181d256bf60": {"doc_hash": "84f0eec1ccc9d21b304bada81d16ab571226bd6da9165930e7ef1906c39de586", "ref_doc_id": "26864623-e72d-4b91-b583-8d12ace4d8e1"}, "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0": {"doc_hash": "e3576198f53bd55ffbf31c9e89ed75f75677138965c7689767c63eae0f3ca2a1", "ref_doc_id": "26864623-e72d-4b91-b583-8d12ace4d8e1"}, "be6792a4-da55-47a2-b289-7b1b5869811d": {"doc_hash": "f96435e0fc6aced74c8216eab27599c4dced159add03bf04dae32d2430393afc", "ref_doc_id": "26864623-e72d-4b91-b583-8d12ace4d8e1"}, "99895f25-3030-456c-a22c-ea588744446b": {"doc_hash": "85ffca11ea202aa0bf5fe231788218d5fdda2ae1c6d24521e3cdf2fe5dac7d9d", "ref_doc_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd"}, "5f1a8ce1-3de5-4339-af32-d75e11da34f5": {"doc_hash": "dfc5e22ad6d68142b9db3d41291eb7c73c4db13dc7552b41dc2c4325cb206086", "ref_doc_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd"}, "e2690e2d-178c-4921-a734-f9334530f2e6": {"doc_hash": "5df9d01b6475809c8ceafc031ccc3e8f365bda475e19d3d11dfb325239b0b5a5", "ref_doc_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd"}, "d88f6d7f-5944-408b-bd56-1536e37a36df": {"doc_hash": "79075f2f1140d0f3409c52ee40630a6b92a3fd88f1544318cc055dd0d51f1ae3", "ref_doc_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd"}, "3592c0a4-8fb2-4091-8a28-67a42ea67b94": {"doc_hash": "d363c37c0208a96b75cdf2c8c5083629ba0e987c7fb6517541f69fbf03d4867c", "ref_doc_id": "436bed26-cd7a-4c41-ab9b-76184bb157fd"}, "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d": {"doc_hash": "1b5155ebaabd13cf2b27052360ade82cf0143d08a86969278d8df93eab61648d", "ref_doc_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744"}, "cbdf320d-0210-4751-bc8e-420cdcb3c46a": {"doc_hash": "0eaee92d4c7bf5bcec79dd92838ea72767021a04e0254a8d766322d01a0d1ec5", "ref_doc_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744"}, "eb544f90-4fa9-4e51-9b79-d85553445142": {"doc_hash": "7c21768db76ad95e4b07512321416c8f2d6623be0600325f4a3d2ce06519d7af", "ref_doc_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744"}, "ed003dcb-d68a-41b8-88ef-93e05e14cee3": {"doc_hash": "86037bcbb8ee7be090312f8b8663a37e9857940675956bf66feebf3b902fdbbf", "ref_doc_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744"}, "87de77de-8c9a-4dc5-987b-0dc007f852a7": {"doc_hash": "1cea245af87ea4c2c0f46e0709808b42ed0bfb92ef38349f3939fecce04411e0", "ref_doc_id": "2c4ac0a6-1a71-4196-9877-d2964b0f3744"}, "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab": {"doc_hash": "d6de6781ed63c6dabe8c92fc19a0c5c5279e2a37ecbd36b775faa43f7ec6ab01", "ref_doc_id": "22825eff-8ad1-4d03-823a-8e232176aa19"}, "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086": {"doc_hash": "564868384fd71fe378970c95bca1bd40180d0df432e47a566f6d5db5a03efd8e", "ref_doc_id": "22825eff-8ad1-4d03-823a-8e232176aa19"}, "1ceb9831-1954-4b8d-a153-1a613db8408a": {"doc_hash": "b235c0e07a1292bc52aa3dbd82916b90e9373aa10f7f117cd2f732057c9e080a", "ref_doc_id": "22825eff-8ad1-4d03-823a-8e232176aa19"}, "9c62f556-ced5-4c6a-8278-780ade4f4643": {"doc_hash": "512999a0778fb3ad1316c6624fe49fb3d738cc8a4832eb39d3a930b18e480207", "ref_doc_id": "22825eff-8ad1-4d03-823a-8e232176aa19"}, "40459286-87b0-4707-b53e-47333e6c46d3": {"doc_hash": "8bed597820ba42a06898232adfd8e321cf8d0a68e6f9513133e106e1d46dbb57", "ref_doc_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf"}, "c2c1168f-777a-4fc0-965c-9ce04fe596de": {"doc_hash": "dd490142ab78641932781c472895261c485f2077944b221efc3cb11834e484dd", "ref_doc_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf"}, "2600ede9-5e4f-4920-99c6-a34d23a10d23": {"doc_hash": "ae068504ed4972c5cceedbaca258ea4744449e3a47c65caeffb8520a33ce4d88", "ref_doc_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf"}, "af068cb7-b5e2-45f7-819e-91abfb0c0dd8": {"doc_hash": "231e2a659f3d899b1596b4c06ab56984316067d5b24478e2ce453ecfede6bd66", "ref_doc_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf"}, "b2b79e9f-0df5-46f7-b817-a49c288a6411": {"doc_hash": "1c7c217317296557c05da625f77cc0c9529a58c4230c59a18a99f88ff08fe221", "ref_doc_id": "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf"}, "3b862e92-4358-47af-950d-ebbb46d47f6d": {"doc_hash": "448c0626e3ef773415bddf170b2f0f52ea0aa69d8c8fb5bad5ddf0b06ace2904", "ref_doc_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f"}, "453b70bb-0eba-49cb-b453-f5e535d0ef69": {"doc_hash": "013419a55e6d880dca120245b9ee738c9cf36cc40896e67fa5057bca833a93c8", "ref_doc_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f"}, "93e520eb-8d02-43c4-b427-0a8ba52aa2e8": {"doc_hash": "88c82b621e8fef6b4463e9cd147d5620d87884b028f2ce4819f5e8c0d927ac94", "ref_doc_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f"}, "db514a78-d261-471f-8efb-8514117e6f4e": {"doc_hash": "7e3d7d836868ea487228233df25d747e2912c8cb7a0ea7dc749b29bb57e37221", "ref_doc_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f"}, "9278baa9-0cba-447f-b50c-77148e46b013": {"doc_hash": "29be3e881568d726fbf2459b3f5c8063aaf55426eea9a659531cae58f1f57c27", "ref_doc_id": "4d9297b7-b236-40fe-8cef-bb5cfd5f196f"}, "3caf8b77-3b63-4094-aee3-964c13c6c0f8": {"doc_hash": "44876d386f120987a9c65d27ffe63ca72c65b6e8ce0ae6b20485d0b5cf71ceaf", "ref_doc_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5"}, "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03": {"doc_hash": "5f2ab0bf51f4adf9dc42e0db2bd527fe5e680c48a300f508b34d2b3b095370da", "ref_doc_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5"}, "ea3280e2-9061-4d36-86ab-2bc4d70fce08": {"doc_hash": "d84a4fa0aa5d5dfaf3467309165bb22c19101c2ed6dec85cb948cdc96c82bf34", "ref_doc_id": "c03adbde-2467-4504-9b19-72d0e86c8cd5"}, "af53148f-7cee-4843-ad16-bf3bb0321242": {"doc_hash": "866f7c66be8396524be9584d90ae3a7613dce39989d2655a236c128d947807bf", "ref_doc_id": "2587bb96-4640-4265-9076-9e1d531a2381"}, "8d19f54d-598e-4495-8f5b-08c18d8e9da1": {"doc_hash": "347331191612ed06eab5a9c310a913709ba4d9575417ddcf11d6b55441e9ac26", "ref_doc_id": "2587bb96-4640-4265-9076-9e1d531a2381"}, "e33ebd69-c0b6-48d0-a472-cc8f6eeba54b": {"doc_hash": "90a8a0c5263affd2d6674b130a9634e7f2b7716d1e334f4f70270efee4b263f6", "ref_doc_id": "2587bb96-4640-4265-9076-9e1d531a2381"}, "ea666fa4-20be-459b-881c-fb0a89ce9d54": {"doc_hash": "0f634c2ab09acf1f889da7c595adf8f12a4f7a1b247c26988828da1984de4d11", "ref_doc_id": "2587bb96-4640-4265-9076-9e1d531a2381"}, "e04f5524-c742-40cc-a345-c5fec7d32d50": {"doc_hash": "379cd72cb5a5165ce5f6728716016e1b26e865949f5fe62c125577aee89ab89a", "ref_doc_id": "2587bb96-4640-4265-9076-9e1d531a2381"}, "af6b7a8d-77d9-4a37-8955-6eb0c86e7669": {"doc_hash": "bf0ae4fab0c3be5444915814833542f3ed98f2b32f86b28f76ebf583d1e26c65", "ref_doc_id": "45966987-f98a-421f-9271-5c503d67344d"}, "dd22f499-9d78-4f2d-ad79-a53ad1f46361": {"doc_hash": "da3d7e3d3284975ddf529101a852570f368f18f6e85446545f1e3e7188dd6f02", "ref_doc_id": "45966987-f98a-421f-9271-5c503d67344d"}, "2fbcab7a-aeb1-4153-ab79-2b54badcec49": {"doc_hash": "cfc8a6aa8e58551d569f4405bd319a5080a341da2c636413b7818cc8228c0428", "ref_doc_id": "45966987-f98a-421f-9271-5c503d67344d"}, "33c2d5cf-ea46-493f-a3e7-c702f5cc33af": {"doc_hash": "08a1bde488e1f091cdc30800f1ed032404445623c5e324821944130ca8ebe2dd", "ref_doc_id": "45966987-f98a-421f-9271-5c503d67344d"}, "e89ba5c4-3f8d-45e0-b6cc-e385486d7eff": {"doc_hash": "e16c5fa766882a4cebdb6bd5466a6834e7a19828da5649095b05c17b1ebca04e", "ref_doc_id": "45966987-f98a-421f-9271-5c503d67344d"}, "2ac6f711-0a4c-4e61-907b-5adb6944be21": {"doc_hash": "3464011a2cb4059692c14d8527315f1dba17774d87c447fc33b773f28cd3c392", "ref_doc_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61"}, "5639ad72-ec31-424b-982e-a75e3975ca22": {"doc_hash": "b353c1d2f59611a400cebe1c774a7e9d5cb5339cae16b0b8a24d953a59d50008", "ref_doc_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61"}, "988d88de-c8fb-44a0-bfcc-d9d9ebb6f2ea": {"doc_hash": "a328839604cca5ee1243766b1505cfdc95b7fa143f41a7f80698ba25c8198164", "ref_doc_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61"}, "3db498c9-6714-4117-8544-f0cbc7c7ba24": {"doc_hash": "59d8d14f952e64cd64764abf93c5e33633a4ffb20c4b4dec0bdb45a6fb0d6491", "ref_doc_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61"}, "a9c019c1-3346-44e1-89aa-8b9ce920f21c": {"doc_hash": "be7e5b2089d0ff6c9291ceab1e3fd6412df95bea66674a6d6630e8d40af875ea", "ref_doc_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61"}, "2b2e91e1-6e16-4f33-9934-a2862d8270d6": {"doc_hash": "ddca79e02d5b90a77e6f8aa43a7fe26217b9450e1678ea7ec37299f667dc2219", "ref_doc_id": "08040b3d-2224-4c75-a034-cfb9aa9f9b61"}, "461b359e-1bb5-4912-bc7d-3a73a3c00b25": {"doc_hash": "8945a6cd35fd64a02d7491742402b0fd31272bc02f9c239e754cb33f9ad4f567", "ref_doc_id": "6e438b00-2436-4718-b18e-6835511b6962"}, "2fde5e6b-04c9-4ac1-bf71-09a1c76b804b": {"doc_hash": "4603ff28f1c0e33013749ce1b15466d652003b6b49ecf2cc15839cf8bd130de7", "ref_doc_id": "6e438b00-2436-4718-b18e-6835511b6962"}, "32a36560-4b02-4f0a-a52e-a1ee6003e14b": {"doc_hash": "a148dc182352bbfd29ea06de2d176c5da69665699191df60b55dddc94330b211", "ref_doc_id": "6e438b00-2436-4718-b18e-6835511b6962"}, "f3be1b9c-cdfc-424a-b473-b1897295dd57": {"doc_hash": "176a10f956a1fb498b8fc006f5f597d844c43bc9e2c56c9736c6b7352de7e847", "ref_doc_id": "6e438b00-2436-4718-b18e-6835511b6962"}, "30557630-9b86-470e-a715-5603ee609c82": {"doc_hash": "73f3d26a92a23a10a934a1ae6f3813f014cafffe9d0e40b91745e5fab450a52c", "ref_doc_id": "6e438b00-2436-4718-b18e-6835511b6962"}, "09fba7a0-ea1b-4ddf-991c-663c646563c6": {"doc_hash": "0f63f2d3865eb73bcbe7d9640eb81ce9aaf264d8fc768e2598f9366304350af2", "ref_doc_id": "f7399abd-cb1b-48f9-886a-8746f5f1e173"}, "9b298a12-d1e7-402e-ac06-abf397573cc9": {"doc_hash": "28500d7bb796994b26ac0e5962b36a6456ee7a2cfc38b53e38f9b53634161352", "ref_doc_id": "cc987eab-006b-480a-8593-97ad22ce1f35"}, "4b602e68-392e-4e7a-9923-1045a4070a95": {"doc_hash": "97823f7b067ccf61c9f5cdf75c41ff33bf257b5f41220aa95cbe18e4c2449ae5", "ref_doc_id": "cc987eab-006b-480a-8593-97ad22ce1f35"}, "11ccb175-7cb7-493c-9654-5ff59910f24d": {"doc_hash": "2fda599b9d1537e2d126793d666912a1dd6324a10d5df6c6c9ca1d2536546990", "ref_doc_id": "cc987eab-006b-480a-8593-97ad22ce1f35"}, "f750e47e-2259-45fc-a520-500bf6a0d0dc": {"doc_hash": "5e1886df3a599483dc5a2cce1e016a486444b6cae97ce671fe6d01f1fca65c92", "ref_doc_id": "cc987eab-006b-480a-8593-97ad22ce1f35"}, "8a702c1b-9836-4918-beb5-5562d850aa84": {"doc_hash": "748db795c9935cd715683a62c64256119c87f6332e6c9f65237935c1c9fd8f63", "ref_doc_id": "cc987eab-006b-480a-8593-97ad22ce1f35"}, "873991ab-7ccf-4af5-bb28-c4b73edd709a": {"doc_hash": "79deaa4bb2eb8ae20ce2a74ac4175e0643b01c33b404c18c04bf704ec62372fb", "ref_doc_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23"}, "65a5d11a-33fc-43e0-9aa2-49305de58cfd": {"doc_hash": "d7c01fceb9f999eb6749330aace0566051bb4dea21bfeb60262e964fb0671e7a", "ref_doc_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23"}, "566ee55e-c714-4b11-a525-4fca02f448fd": {"doc_hash": "cc6746ec2074e0f9c722d9fef2fa36b7a4e52d08bf176da8573157e39a5abdc6", "ref_doc_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23"}, "7ff83fd5-cee9-4712-8271-478f9ddb56cc": {"doc_hash": "ea67e182e6425e4c8675a79ab0a108a3d47f0c3f5a7c90b389a77768fa147b0b", "ref_doc_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23"}, "e2ad5fef-179a-446f-a02a-c82560506f03": {"doc_hash": "b215b851556d71c698c41fae930e384481cd27d2cdff11de5c1ed17f62fcc398", "ref_doc_id": "b1743ce2-002e-4688-b43d-d0294a9d9f23"}, "2f7fd9ca-0cf7-4fb7-9873-c9b592738bdc": {"doc_hash": "d09340f2882e335df5e06604099dbd05404f2ee214e76d4e22a4d1eda04b58d3", "ref_doc_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1"}, "843191dc-4cee-4491-b706-4214fcde7797": {"doc_hash": "15c631742fc154993f28ddd810e0d9c996fe80de582c88fa6d3bb3356d7ab941", "ref_doc_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1"}, "195f3a73-cdcf-4c70-a5b2-1c2cf4b85211": {"doc_hash": "ede1e45f85f765d5f9be10c8cabb9afc2a6cd703028f6fe71201a313e51a5516", "ref_doc_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1"}, "5596ba26-93d2-4d1f-8f45-2a37cacbbd60": {"doc_hash": "4a9c2e8b557cdb642dd6c1f722d256d6b893715893101fc743604efe255c3498", "ref_doc_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1"}, "e4d4f399-d234-4f87-a944-243f12a6b60b": {"doc_hash": "772cf8d71c580420436a1510accd393345c93d7c0ff6357695d358e5f2ebd70b", "ref_doc_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1"}, "b3b0c525-5be2-4ddd-a29f-c9a87792531b": {"doc_hash": "df8dbbb4c5dae6a952eaef0e3264696c3ecd2174061bd7bd1a48941b57f620bb", "ref_doc_id": "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1"}, "52c28400-6e09-4062-b489-6291b73575ec": {"doc_hash": "31463060176385aa624f33fc503b9991212c4cdc6288d626b9d44a6fc4861c11", "ref_doc_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43"}, "e9b29251-dbf2-403d-9baf-1f9dbcca521e": {"doc_hash": "27e00ad0fb86e64f8660b771e78be1a457d3d99e23257d8799ff20a08739a7f9", "ref_doc_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43"}, "56eb2784-6b7f-4a78-8646-50d89033c3fa": {"doc_hash": "6fda48f7eb64f054bc5f51e56354a7dc48d0caca7dbdb79b66f1634bb01ab283", "ref_doc_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43"}, "9395cc04-3804-4a99-b757-9923150dc466": {"doc_hash": "03d485375ae0b899ea6f66194472dfc66985b777ce69e233e348e5f88d9bba5e", "ref_doc_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43"}, "86fc9790-90b1-4c30-a828-0b1d798b9b7c": {"doc_hash": "59f1fa94b6a045018deef756912ca8cf75564b2112e2f85c8531952cfd102210", "ref_doc_id": "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43"}, "1c73d00c-a783-4a14-ae37-a8248bcc62cd": {"doc_hash": "f35556f35ab7cd771b0f8860bcb865c52d138dab0eb7290d1207f7ffcbb24221", "ref_doc_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71"}, "9147c0b4-957d-4bab-b394-3aec4a36c6f2": {"doc_hash": "7627320f42e1ff59feeb8974a33ce815410340af46f69417dff0f36a9c018276", "ref_doc_id": "4f5a5cde-c754-4888-a5ed-2a21ee98be71"}, "ab9b36f9-5822-4d55-8dc2-a0c2334f7256": {"doc_hash": "ef4fa561d6aed9c408bdc9c4bf8c8ceba6886bfb415a196a2df45845dca3dab3", "ref_doc_id": "37fb7373-7684-4059-9b8a-f119e12a9483"}, "4a547e43-731c-454a-9b43-aa6128e5d71c": {"doc_hash": "524dd1964c06f3d7675bb36972b3b339af91fcbb234d8c81408462b17be67142", "ref_doc_id": "37fb7373-7684-4059-9b8a-f119e12a9483"}, "02f8004b-01f4-49f6-a3a6-adc81e2de14b": {"doc_hash": "6f377e222014d759c6dfaea880d0140c54a26d7c0b72f4c74d8d732f221b70dd", "ref_doc_id": "37fb7373-7684-4059-9b8a-f119e12a9483"}, "d0a2ad27-0fd6-4daa-b109-1ccbbad1a3a4": {"doc_hash": "c2351f2fec996d1e208ac30d9a16876479790b03e069383a295f5ee867a243d0", "ref_doc_id": "37fb7373-7684-4059-9b8a-f119e12a9483"}, "b600cf7c-debb-4303-a317-d75ff0849894": {"doc_hash": "f2b60bcf8228604996d8bf4bfe72713723d26f70e7b57031a9577fd7042e851e", "ref_doc_id": "37fb7373-7684-4059-9b8a-f119e12a9483"}, "ee606a3f-6129-4795-9c3f-d7c743aaa714": {"doc_hash": "4ce7d5b1795c44c295eec329b71d2b2afc46556d4d31eea300ba77a808410574", "ref_doc_id": "37fb7373-7684-4059-9b8a-f119e12a9483"}, "edf2a2b8-ee8c-4db5-95c2-41153e5155e3": {"doc_hash": "0cb7d262d972da8edd2f256f796aff89e8e4002ab357ab90135fe7557797d35a", "ref_doc_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31"}, "ad84ebeb-7552-4a56-a95c-9bd66c84eb7b": {"doc_hash": "93cf02f3b171215472aa74e19b6dc943807c49aadfae97b599ccd02db18afe50", "ref_doc_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31"}, "7adf1354-3259-464f-bed2-aeeb85ded3a7": {"doc_hash": "d3bb99f609dfd449331b064089a29027712f890adf3fa4c6a87a49dfe2883a71", "ref_doc_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31"}, "d2228bbc-835c-4746-9b2a-fa52a330de44": {"doc_hash": "2f366efbd57daa81af8bf8b34a96ddba48f07feeb03c6bfe59214b00f36cd44b", "ref_doc_id": "240d64f4-947b-4a30-8a40-4c32f3f3fb31"}, "c5823942-5724-49f0-bdcc-35b8eea56ee0": {"doc_hash": "5750a8ed7dc03f2dbad8f4a9134ee3069242bbd4cf24f7d26c9daab8814f5cc2", "ref_doc_id": "2415d406-648d-488e-adee-8bd0fb74a5f5"}, "b66452e2-8ce6-4fa9-9050-d4e0f3728888": {"doc_hash": "b3f08b94deeb1151858ca99daf69914479f6932e2fdae49240db89a369ea3276", "ref_doc_id": "2415d406-648d-488e-adee-8bd0fb74a5f5"}, "d95c5819-7658-4df8-8296-49d7e5cf845c": {"doc_hash": "4be66e81cfcdb363195b3275e58c2cadb5e648ce4c0c640771753ff2d90a6f21", "ref_doc_id": "2415d406-648d-488e-adee-8bd0fb74a5f5"}, "0200ef30-c427-45b4-a9d9-64b6cbc20ca6": {"doc_hash": "867fd0d62c4e70740f0a74c5fccb102ffec38f067da618a9940a1d4cc4863a39", "ref_doc_id": "2415d406-648d-488e-adee-8bd0fb74a5f5"}, "98cbfd2c-a158-4fba-9f6c-1a9279934bae": {"doc_hash": "3eeb808bd67c8491ab0ed7e46319290a43e2dc8b98e9b68eea007eca637d02e7", "ref_doc_id": "2415d406-648d-488e-adee-8bd0fb74a5f5"}, "d96d7a26-cf40-465c-a85b-e3e272e7b8b2": {"doc_hash": "af39deba15a30236adc1be3bc1713da6e884d3929e95c0dfab3857df1c947d8e", "ref_doc_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9"}, "725f8aab-d383-442b-a039-3f59785fa12d": {"doc_hash": "c5f5eb951d0813ca20637583c09c4e3723f826e8e42418599ac200f5b64cea0b", "ref_doc_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9"}, "b270e70d-0842-46d4-aec3-b2325f853782": {"doc_hash": "9f82351a0c19c16dd19ba00171a7fae601df89158aa923f206da092993c290a7", "ref_doc_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9"}, "1fdd4dd2-7a43-4965-ae4c-ef91448aece7": {"doc_hash": "4c80eb3165b3c06c748289ad7aa84c5eb49fb4fa284c1cbb799b13f2a2938867", "ref_doc_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9"}, "0a35fa84-92c0-4504-bc57-83425e0b900b": {"doc_hash": "b34420caa937cadacf76dd62cad12261d129a4457c4940bf6df16e2ab3a62c6d", "ref_doc_id": "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9"}, "75700ca3-a4eb-4c65-a733-d621db10c1a3": {"doc_hash": "1c581ea8f27640b23541bcda4f611dd2bb777d2ed23f9a757478ea4264ea11a1", "ref_doc_id": "9d81e32e-a897-46af-85a9-02e278286cd0"}, "c450e73d-15e3-4bdb-a091-1ec7e6dca295": {"doc_hash": "e3acc618300ff01dbd232052cfe306eccb2854ebc332a79567274eef6e95a625", "ref_doc_id": "59f340a8-342e-491c-94b2-43642ff312f4"}, "07038048-4e61-4bd9-8db8-e5de3caa80bb": {"doc_hash": "eedaed2218a3cd897167bc1608e4df7dd1e3b2f72b1a07d7563cd1fa21a941e7", "ref_doc_id": "59f340a8-342e-491c-94b2-43642ff312f4"}, "70e6d4c2-9123-48b5-9496-517b95606d04": {"doc_hash": "38b4d78d9bc27300083d3bde2e5c1eb8b334564fc52d284947505999b74310f4", "ref_doc_id": "59f340a8-342e-491c-94b2-43642ff312f4"}, "fab5a902-7a88-4285-acd8-0558cd7a3974": {"doc_hash": "ba4ca6bbf960bacc69853c2d66ebc687e3f3421111d91b7e160683b0d651aeef", "ref_doc_id": "59f340a8-342e-491c-94b2-43642ff312f4"}, "647c2fe0-ab0c-45db-82b0-9af9eaf4feac": {"doc_hash": "4919cb79ee556e0ae4d86eefb39a90956910dbf1366a7b3d81de2c4e66d3cf27", "ref_doc_id": "59f340a8-342e-491c-94b2-43642ff312f4"}, "cfeff0d1-aadf-4d52-8c3b-0e986b4cd6ce": {"doc_hash": "bb303f85a73ee0e2a6e9b5a3f85514eed6fc574efb2e5fa7040e62656d90b57e", "ref_doc_id": "70f446ab-c0c8-4500-a291-c9562ad75f60"}, "20581bbc-4949-4403-b086-da6b3edb5099": {"doc_hash": "c49587207e9182413b4e17af9ab7d8e81b11c4569c388e439b0f0b728cd667b3", "ref_doc_id": "70f446ab-c0c8-4500-a291-c9562ad75f60"}, "9d6adc2c-4482-41e2-8db4-1d7c490f0a90": {"doc_hash": "258c9adf36230883a49a612423d7da47361fc5d8d8da64202310bb0d37713a5f", "ref_doc_id": "70f446ab-c0c8-4500-a291-c9562ad75f60"}, "b70cee44-ce5a-4788-8ff4-c52b0d7696f2": {"doc_hash": "f1548a6b1ad6a89c9ee9bab6feadf0fe3f2cec50d8a73e615c203f7111fd8ac3", "ref_doc_id": "70f446ab-c0c8-4500-a291-c9562ad75f60"}, "f447e72d-4b10-4ac6-92c8-2e00aed7fe48": {"doc_hash": "8ec63b70c55853ccdad77674647c54e4852946db45bfda073a76591a38bf13a2", "ref_doc_id": "70f446ab-c0c8-4500-a291-c9562ad75f60"}, "446508f1-2908-4bc4-a220-f7638b3b0714": {"doc_hash": "898c8a93a0eb4e100148740afe8e4f05259180e4e65e5e5066b93294abb89988", "ref_doc_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3"}, "c7bcbfc6-a287-44da-824a-07ea4bcb9eba": {"doc_hash": "d80e6780de80cc16cf05ef1dca2f8639b01b6bba7b5891ecab3041e1458e91a5", "ref_doc_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3"}, "bf28dde5-d99f-4207-a394-11b22ff17b04": {"doc_hash": "844ad0c835ce614b7d7395bc902ccb91c97ed1430419c0b6001a67aee95de240", "ref_doc_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3"}, "d2fae188-76b6-4255-862b-8c0d22114254": {"doc_hash": "fb60aed20fb772e19ba17d8488314d0091d5167c8a07165a5194ff1c51421afb", "ref_doc_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3"}, "5157b639-edab-4c47-a9c6-771b6464b8a3": {"doc_hash": "9ab7544abb879d39377f58d84bd644ea3306f7e8b15210b7c8c3fe89ef36d91f", "ref_doc_id": "b7706ee9-3add-42ac-8e9c-799cc02542a3"}, "e910ae6a-e737-4702-b02a-6eb60c5e82fa": {"doc_hash": "b248567956e50d66624a62467f52786021bb02342a7b0bb21da3b937bf6b11d8", "ref_doc_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7"}, "f4617e78-1587-42e9-baff-320cf53f0c30": {"doc_hash": "e77c39de403e8a735b83488b744ffa6721f4e9096ebf3a923eb6c6f4977b2b0b", "ref_doc_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7"}, "7182afb7-8ad9-4544-96da-88dd70391ad6": {"doc_hash": "443b17deeaab2e6bb7b31d5f23b69420e9579246d367a7d5c59baf4bb717b4ee", "ref_doc_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7"}, "d11a8593-e158-4491-a23f-387fa20a2949": {"doc_hash": "9ad9fe17ae4654d2a4442d753dde3e5e34e59a46712df8c5dbe9e4d1eb663936", "ref_doc_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7"}, "67354aca-f02d-489b-8076-768319861122": {"doc_hash": "944c0a0b19482c05747ab66de688411e786894d90558f161ef81ee4457836e2e", "ref_doc_id": "331dde64-1f43-487b-a9f4-dd1be4acf4a7"}, "521bc758-b6b8-4767-ba22-0b5c7695f7e6": {"doc_hash": "5e1f7e76883b83dfadeac9a72b929a9ede1ad8ef5be9ea2275c5e90a49093aa6", "ref_doc_id": "d78ca6ea-0bd4-4a06-a4de-9956656051dd"}, "0fce0132-01b6-44ae-a579-03dddea37351": {"doc_hash": "df5df7a3d7632afe85b819f047ab0cd93d28f96e1ffb607b3508128f16e7a23f", "ref_doc_id": "9dc57153-6fd5-499b-a634-fc574002380e"}, "ff9fbb87-c761-46f6-a6ab-81f552c06371": {"doc_hash": "9e297e1e6d0ad11041908374be148a28c4936d88e235a043f9a94e16b3095e60", "ref_doc_id": "9dc57153-6fd5-499b-a634-fc574002380e"}, "833e21b8-65c9-44c2-980d-9da78f2499ec": {"doc_hash": "c6a57722d21e1eb0004da56067ae071eebd079f844d4f65db34fed921c337800", "ref_doc_id": "9dc57153-6fd5-499b-a634-fc574002380e"}, "06fba012-1729-4a78-b242-4b7f23225683": {"doc_hash": "443c5d8a8fba2f3bb196aa65a1f98dcd476d2414ec911f1113b77f8cbf481034", "ref_doc_id": "9dc57153-6fd5-499b-a634-fc574002380e"}, "4947b6b4-0407-4482-a4c2-6ede5a612a61": {"doc_hash": "a83c5c00f76247cc6deb0a886bb04e96740a53be43fc33841cd3c613d01ce650", "ref_doc_id": "9dc57153-6fd5-499b-a634-fc574002380e"}, "93d495a1-fa6d-4e6b-b58e-d71d21fc6dcc": {"doc_hash": "ef24e4c88670f09703fc785f860654a130018a2cf3ff4ffb7db3596545470e27", "ref_doc_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8"}, "cf11fe80-d25f-479b-ad30-4a45fd1b4ee4": {"doc_hash": "7650665160ddcadabdaf6ed5f29669f2ad40fafd4b0aa01982e41d56c408ece2", "ref_doc_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8"}, "916c7dd8-b06b-4893-8ee9-4802ad8ba187": {"doc_hash": "bb48b4a67eb4de213d26ba81df501946bb33c5681a9a0994f4e038d7eba69bd2", "ref_doc_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8"}, "c3c637f9-57fc-4689-b0ab-401bf0f103cd": {"doc_hash": "fdcbf9f039f932d129a769237f162be470154d20738827ae577b560820dd7396", "ref_doc_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8"}, "b5c1bab0-af3f-4195-9a99-494366bdfaeb": {"doc_hash": "70d586b0b3b6757f09540c5ff6b3369e4cf4f456f9bc1d14e9bbdd4710dc19fb", "ref_doc_id": "633f7b61-b5f8-4142-8666-0a0ba4b804a8"}, "5291ac61-aa0c-4268-aa6c-4431e2115530": {"doc_hash": "00c625443826b23f67f5255c081d9ed10a712cc6dc61dc66af70dbdf3a9b45f5", "ref_doc_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b"}, "db893a9b-7adf-4d69-a247-e0187299a6a5": {"doc_hash": "09159869813772405435517b8005b4db6f8d3d3285d16ca1afa985eec6600bc5", "ref_doc_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b"}, "b6cb972f-9382-4be4-8251-374170f5deaf": {"doc_hash": "eac9829ece5ce364de5719e2f67a1c72c733e9ce951d5d3e858aabe6c1433caa", "ref_doc_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b"}, "e8bbd7aa-500f-4460-b15c-c8959ea97ed7": {"doc_hash": "db71fc1a77cb8df69ca9fd2ad78c71ecf08144091bcb67a8a1acd3e455bd856d", "ref_doc_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b"}, "bc6d7f07-411a-40b5-b47f-5332e2253a43": {"doc_hash": "db6346e86c2a29df736cb6f745d44a7b111b1a149dfe97522bcf7cab30133aab", "ref_doc_id": "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b"}, "236dd4eb-04a5-45f2-84ca-1d849255c593": {"doc_hash": "88b8e3bdcfc87b9878aa403bc13c7999170b44f9a32c7180d2a27852c8f06d99", "ref_doc_id": "fac41315-4d32-4359-b216-55c151e8b5e1"}, "c1887afa-cd93-4a27-b62f-d7f1d01e26cb": {"doc_hash": "c69585fb915b622cc5e4cb4273bc2131e5e30cc2a978be334bff95c847346264", "ref_doc_id": "fac41315-4d32-4359-b216-55c151e8b5e1"}, "037972d6-86da-41d6-85fd-6f03cf5d8c9a": {"doc_hash": "d830658810f3425ba1435f16d727e21d94d20d4957acd423a0b36ebc0b685672", "ref_doc_id": "fac41315-4d32-4359-b216-55c151e8b5e1"}, "071b41de-61a2-4b47-8be3-d14fd05f5538": {"doc_hash": "46ffbf322df316c22fec8e4897f85b727381ef659acb96dbb7dd4ab3824be140", "ref_doc_id": "fac41315-4d32-4359-b216-55c151e8b5e1"}, "e656c762-fee1-489b-8bbf-5f3172ddcef9": {"doc_hash": "701b642bf94593756a8b62edfddb7cbad92bd19637fc9973dbc83231747cc9b6", "ref_doc_id": "fac41315-4d32-4359-b216-55c151e8b5e1"}, "90b32c62-1f04-44ba-b29a-0ca24bb90c12": {"doc_hash": "47aa8341805eb8373995fbe9e89f3cf4f7b160423e5f87be8c6e8eee1b2e8464", "ref_doc_id": "638f46aa-98ea-47c6-9083-3d5c75cc98b4"}, "fec8fb2e-3c48-487b-b5b0-a4ab6a3a5aa6": {"doc_hash": "4ada32db1c609fff7297b58456fd595ec117bc332ab3278d018f99014580b905", "ref_doc_id": "980c530c-1005-4c72-8d3a-68472f1c9de1"}, "427f2851-d74b-4810-b773-a43b53845b08": {"doc_hash": "e59a88b2df7394ede064070f936b7d0b7a21745d3e6955219538a6f3a2e41770", "ref_doc_id": "980c530c-1005-4c72-8d3a-68472f1c9de1"}, "d005b6b4-fbc3-4538-b88d-ebcc6675bb46": {"doc_hash": "85f5b7efc4d50fde33857b29a94dba2885ea3b5e65a020e7b28e761a0c08a969", "ref_doc_id": "980c530c-1005-4c72-8d3a-68472f1c9de1"}, "6a816ebb-a509-478b-bd71-41524128c6bd": {"doc_hash": "80a70a850d81d0a81e6f5d559f93ba5e85b769d29b6148cae336b27f471e3bc0", "ref_doc_id": "980c530c-1005-4c72-8d3a-68472f1c9de1"}, "57928373-8fa0-4601-abae-1b8a20f7afc4": {"doc_hash": "3270723f80fd1e1d103461e9c648111a8b3d3ac144e889a08c1a7f2613b3fe02", "ref_doc_id": "980c530c-1005-4c72-8d3a-68472f1c9de1"}, "9b6bcd96-26f3-4e02-9ee0-d0b553dfbbbf": {"doc_hash": "2af18ff4c671d075245f18e5c9b1532583c60fcef2c5bc88a264ecec24b9dfe9", "ref_doc_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d"}, "521b5c8b-39fc-4b8e-bdc5-fc37e17b6a92": {"doc_hash": "c6ca82d92452988f1526b29e489164b5fe0b1ee107937d3690e8e326745b938e", "ref_doc_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d"}, "f400ddea-f8d1-45c4-b737-76842703e6f5": {"doc_hash": "a8aacd529316f6d96751924db0132e294491418431d06acd5796c913fa2b00ac", "ref_doc_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d"}, "19e0ec7f-0645-402b-a264-668c85a59adb": {"doc_hash": "ec5e1a59a5c0490d1d6132b3982d37746f29d33ba6199fd8e43079187254a3e8", "ref_doc_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d"}, "d7938e9c-de67-4c48-8b5b-475d32455dd5": {"doc_hash": "638b42097f590ff117dc31186b6ecd7b95790c779f92fd54ec42e6f887f3259c", "ref_doc_id": "89183965-5983-4d40-83bb-bc6c1f2a2d8d"}, "1458b347-0390-4066-8436-60dd46fa3e53": {"doc_hash": "6db050db3e164bae6937244b219837ed0bddb5fffa84978bae5f04ea687868a6", "ref_doc_id": "6d7bfce5-d142-44e2-9d76-7001709526dd"}, "ae376509-a845-4656-bc3b-84889c1c4046": {"doc_hash": "85e1ad273eb4ed542b68a830364a3b1906d1ed892b38cf58e1f72fbf4b58f6c6", "ref_doc_id": "6d7bfce5-d142-44e2-9d76-7001709526dd"}, "cb450b82-3b7e-48ac-b189-52e905a1d67b": {"doc_hash": "29d80a94794e082ad943c9d3c5bd83d27abae6d86a8c01a0a2c77b981dd7e14e", "ref_doc_id": "6d7bfce5-d142-44e2-9d76-7001709526dd"}, "a349c88d-5d16-4818-8c37-f9d948c59dd5": {"doc_hash": "25da6dbf51d8c0ce7397c9711bcb748d4c62ad9529a0e7886fd7f5e8228449f4", "ref_doc_id": "6d7bfce5-d142-44e2-9d76-7001709526dd"}, "7bbf6e82-0859-4e81-8ca0-06d8aee22e61": {"doc_hash": "c3d284cc380ace0060370c94024e17d7c7a9078913ecdfc006fcce3b9bdc8fca", "ref_doc_id": "6d7bfce5-d142-44e2-9d76-7001709526dd"}, "1fac2605-63aa-4450-9179-db26c337f758": {"doc_hash": "e84cd8e449a606f23a605b6e2e16f038bde927c870a7a8aa8d100b7796609186", "ref_doc_id": "e288a398-5ad7-4fe0-9e03-11b716299770"}, "ea7e8084-0269-4117-b8a3-23557924e586": {"doc_hash": "7b2173bf2212186a5c0d3803e5fd190e5b454559f43b736791cbb874ef3e0aa5", "ref_doc_id": "e288a398-5ad7-4fe0-9e03-11b716299770"}, "5fcedf03-b441-4c81-a2f1-d9291f04c901": {"doc_hash": "1cdca8bcce56f0f68666044fd67bbb27cc11ba1fb177979eb72b9c113644b107", "ref_doc_id": "e288a398-5ad7-4fe0-9e03-11b716299770"}, "84264c77-f9e8-43b3-ba41-6723f6b6c82f": {"doc_hash": "19d103e403eb6ad162415ea2b0c51181f6f6f938d193e686c8ae61b068e3a2d0", "ref_doc_id": "e288a398-5ad7-4fe0-9e03-11b716299770"}, "3f595dd6-042a-41e4-95d4-e25bb8d30bf6": {"doc_hash": "a455fcd397f5a60d073d0e2e58f8121bf9d3e66dd33e44cadcf5a58a5c46359d", "ref_doc_id": "e288a398-5ad7-4fe0-9e03-11b716299770"}, "58196cf7-31c3-45cb-b305-47780d51c9a2": {"doc_hash": "86acf41015531bda8785a43dc2b5ced0fd4a0692d6ac5e7721c155949d42e2d9", "ref_doc_id": "b833d9b9-cff4-4e06-9dba-20efb87d02d1"}, "fc1b0850-0803-4987-819c-9783a3f90423": {"doc_hash": "52dcc586282e53f750e2d52165b05aa09ebfdcc29130c111e9d13d6da7efed97", "ref_doc_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81"}, "dafdee97-421f-4159-ad03-fe6c19162347": {"doc_hash": "467ce2cfea16d32515fbb933ab698db768319784ad6728a65da72c4b035c3614", "ref_doc_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81"}, "0980800e-b33e-4cc1-8f7c-f0de094be475": {"doc_hash": "a39804cca309068ef58df145563ebb1e520161906152f46fd8c16f25e9b14dbc", "ref_doc_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81"}, "9732bb2b-0e9f-4259-9387-38c9d98380ce": {"doc_hash": "dd4eaeb8530d58c6052726e28c80215adfbfaf9adb87280195fee778be217929", "ref_doc_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81"}, "70523c54-2fb7-42c6-ba94-f375b1199c89": {"doc_hash": "6104f65f0b7e32c1e344b3671c76d399384544d7ca92905d6fde656828695bb6", "ref_doc_id": "3307b03b-7942-4e2b-a3ea-a083b2c85f81"}, "925a1d28-1f61-4d13-94eb-e10d7f7f9962": {"doc_hash": "d52b32725149599eed9c76892dd18d75f2eb6828ef8aaf04800546d3aef20934", "ref_doc_id": "5665018f-a756-44be-adc2-ce77d463ad0e"}, "b8e0d74c-77c8-4bd1-b552-ef3c563f3ed8": {"doc_hash": "d331139405b2b3aa94378e91d12e884de0c7bd59668c3b58f4c4037fd6e98ae0", "ref_doc_id": "5665018f-a756-44be-adc2-ce77d463ad0e"}, "ae23095a-71b8-4464-861a-106ac4195c21": {"doc_hash": "4046189438dd0b65201626d4c183cf413ec4c2243c69c03f6ff4db08913e77ae", "ref_doc_id": "5665018f-a756-44be-adc2-ce77d463ad0e"}, "23e2260e-43be-456f-912d-25e99d0379cb": {"doc_hash": "6c8ece06ce7d893fb8d466ca1e06e05e2885d9cce414d8d0a520a418c378c272", "ref_doc_id": "5665018f-a756-44be-adc2-ce77d463ad0e"}, "f14e060e-bf7d-4077-af80-b992fe7858d5": {"doc_hash": "eb8d287dde1768a41557294123f9b85b41ae13b7c91f2a091df7b33e9be9cb7c", "ref_doc_id": "5665018f-a756-44be-adc2-ce77d463ad0e"}, "33406e86-99a8-48de-89db-58f1a36281d2": {"doc_hash": "36d9c719a9e79b3429e76ec2a0d8446b95d959e22e7b613ba4c6f2234fba3f75", "ref_doc_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd"}, "a4e3f0d5-3d58-4b4a-aefe-64c3b61670ac": {"doc_hash": "20ffce0a9e3e563045a7d3dd94c07e8e0a3941022a3742d6cdaea30cd5709511", "ref_doc_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd"}, "3c090fe5-243a-41c0-8129-aae9e995378d": {"doc_hash": "95b544fb71173001db2cef997e59cd39d831c936b7e0e5c1bc86f4c93d491941", "ref_doc_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd"}, "3984af1d-e971-46af-8596-695647deb311": {"doc_hash": "813624b36a4b9f9d5ca140283c54afe415e12d6788b37cfd343417e9702315a5", "ref_doc_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd"}, "0abdbd95-fcbf-4913-89d8-f527efd2400e": {"doc_hash": "8a4500445f9cd24183393f9bc6371095490678d0e7fff6432a5052e2eb591e8e", "ref_doc_id": "c7471598-449f-4ce6-870e-1a9e120e6ccd"}, "6e6fced2-1c4c-44c4-8d7c-2f6fdb25a3a0": {"doc_hash": "1cd39309c519ca0bb144895b03e1a94d3ebd40904ed4fbf72e830e0d774387df", "ref_doc_id": "51582689-3bf2-4ba0-b051-10e732354bda"}, "f9f5e60a-8be2-4228-8bfa-5d35ca444603": {"doc_hash": "504dcbdb20a5665898226c9c99137af0d19f8890826960d9803bc9ba1363b173", "ref_doc_id": "51582689-3bf2-4ba0-b051-10e732354bda"}, "9f04f21a-584f-4e6c-9df3-42e9a2f38966": {"doc_hash": "c0793e2ea7ea8fbc6a54769f1fc5a656745c62a2f75d8610db2cf81d8d69d9e8", "ref_doc_id": "51582689-3bf2-4ba0-b051-10e732354bda"}, "f37a12ea-f7e2-4d3b-8761-660a17587794": {"doc_hash": "043521c99d65eafb211c8d0f1c6a14d092a9fe335c06bd269e35a2e14504f6e4", "ref_doc_id": "51582689-3bf2-4ba0-b051-10e732354bda"}, "99734d75-ed29-4007-a8a8-dc4f79d9e8b6": {"doc_hash": "ce7095d7b065f243de29d6e4b7a1ab794598f777056463ae1af47a9f761c9e97", "ref_doc_id": "51582689-3bf2-4ba0-b051-10e732354bda"}, "ab870226-7a14-42b6-9fee-e893c73c5cd4": {"doc_hash": "78c170cea536e40744c67d3b9109c2452b044dff94ef0b21a9def3ab450354a2", "ref_doc_id": "cdd04060-a8e4-474c-a338-f65f89616e95"}, "90c41dad-e5cd-4732-9250-448c021e4383": {"doc_hash": "b67465ae3f47b8bf0c6d2bca74caac653135d04ec00f72af68ba3ddb7e4c3336", "ref_doc_id": "cdd04060-a8e4-474c-a338-f65f89616e95"}, "15ad2d42-ba2f-4e70-9b47-27a0ddbd9339": {"doc_hash": "b55021b3e46f6715b5b90741fad6a2b7215d07eeb6ad9db40a49c0d2723be1a1", "ref_doc_id": "cdd04060-a8e4-474c-a338-f65f89616e95"}, "47cdba86-01b2-4134-8ce2-89a9a558fc6f": {"doc_hash": "2c759c4e1d4bbf1f31cba44d9dfb074ac0aea35af15e734dd637449c7d92d5aa", "ref_doc_id": "cdd04060-a8e4-474c-a338-f65f89616e95"}, "803a9f00-6ef8-488e-b428-cda0027f4d4f": {"doc_hash": "df59a9100bbecd3559fd871a13502bbc4f7ff2933247da740fa2d2e10942fedf", "ref_doc_id": "cdd04060-a8e4-474c-a338-f65f89616e95"}, "3a7c4090-0eb8-406d-97aa-727111851701": {"doc_hash": "b8282c2ad2503a6110ab799e5ca9f170c077d49ebd84e55d2f696c5eefad8916", "ref_doc_id": "cdd04060-a8e4-474c-a338-f65f89616e95"}, "2ae6e793-db56-4b3f-a68e-609335a5b269": {"doc_hash": "4cbbdb588e4bc63d20bacfa8b9f4b2e14b1c191853f00a6f282c33a9906b8050", "ref_doc_id": "cdd04060-a8e4-474c-a338-f65f89616e95"}, "df0e1575-9ac0-4490-b8af-d43a8fa606ef": {"doc_hash": "7716787fa54deb6c06c7d2a449a5b31aa8e5d0fbb5f83f9e61a9d25686773564", "ref_doc_id": "69604801-7765-4976-81e7-231241d2b2b1"}, "a1409477-4e3c-4538-96b0-ad47279e810e": {"doc_hash": "43254f2cf24047a828eaa2bb62bc3f66ac4bfe60859c81894dacc2d90e40195d", "ref_doc_id": "69604801-7765-4976-81e7-231241d2b2b1"}, "f3d3599d-8950-4295-8694-79a0ddfd1125": {"doc_hash": "163c8055f315b311ed6fc8826b9f1e630fd0e4ec87eef7dcbc2f13e736e84693", "ref_doc_id": "69604801-7765-4976-81e7-231241d2b2b1"}, "83ae85ec-a12a-4af1-8b75-888ba8a4b1a0": {"doc_hash": "339b3412669c8da9f7837d1486cb78c4dc2a3fef96f753fa62d8cfc9a67d09bf", "ref_doc_id": "69604801-7765-4976-81e7-231241d2b2b1"}, "180aee7f-e742-4903-bab5-cfce033587e9": {"doc_hash": "2a7534500517c93f62de8b2a9f286b9f1d1058af6de3e12d2684a169447aaf39", "ref_doc_id": "69604801-7765-4976-81e7-231241d2b2b1"}, "51619bcb-760b-4af8-8b26-ad2d6635661d": {"doc_hash": "15683b26837fdf389ee09e1d95478661de799df79ec01184a2e3e4a963084234", "ref_doc_id": "69604801-7765-4976-81e7-231241d2b2b1"}, "31f544b8-fc3e-4aab-aa56-68a1f20f7e14": {"doc_hash": "5ac33e121490e6ec757c4d927dd6662d4f814884a7df62c28d1831e088aae63b", "ref_doc_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56"}, "1047af7a-6186-49ad-a0f0-9b3504bc95d4": {"doc_hash": "03ac4bdba0fccb257111d37163e87ab7e6738774bd3d9d68ee4000f7b4c9293f", "ref_doc_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56"}, "fbfb8209-388f-4094-a987-e33a058a0252": {"doc_hash": "7580ff928a48ce4f189bfcede1f0d1ef844838f4366a06d5cc23f477931b7b9c", "ref_doc_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56"}, "16b516cf-f294-4811-84de-614fd3732235": {"doc_hash": "6e7007578125968a72a37271e7a6ceb5d3ce393a0f4f98dad3fffe1f2ab7e942", "ref_doc_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56"}, "a664023c-ea54-4d03-805e-474d4063dd8b": {"doc_hash": "4bed89afca62165b8da4bb71e49f0f07a808396c31d8b0276ca3a736e1312d87", "ref_doc_id": "54f4b631-38f8-41b7-98e8-f94f97d23d56"}, "418fc8e2-aaf2-40b9-9c67-7e349f169503": {"doc_hash": "8bd99ac6d990dab535934da58469b133cb5cba145e84af7c874cf83adcf7d119", "ref_doc_id": "4a77345a-c539-4264-a387-adf9022090e0"}, "de618738-3540-432b-9a94-c7d107d99047": {"doc_hash": "346cabd965f44a84879007ecb97d4e84c0ae3c6680bbc62578612e8dc42de8ea", "ref_doc_id": "4a77345a-c539-4264-a387-adf9022090e0"}, "9072dec7-70eb-4adf-a06d-8419b381e147": {"doc_hash": "c8e4004b6301fbcc0dde13bd4a98401cbece0a56d8f1f345bd9c28dacb19dd77", "ref_doc_id": "4a77345a-c539-4264-a387-adf9022090e0"}, "f4debacb-72ef-420b-80cb-f38869c8f88a": {"doc_hash": "abcdabb421f7aa40586529b8925a82e8e298ae6c056de319b7994e49ae8c6c07", "ref_doc_id": "4a77345a-c539-4264-a387-adf9022090e0"}, "8aa981c8-f72d-4180-b9f8-c1019163280e": {"doc_hash": "fc37b98df62da7972342b729efc3faec48860c91aac19ede52ac6c18a908c3d5", "ref_doc_id": "4a77345a-c539-4264-a387-adf9022090e0"}, "72b96e4e-d8af-49f6-b47b-1a08f90868d5": {"doc_hash": "bc936310148fb4e9f57248298e2722518f33232cd3df7d593ebfa3209a9a9006", "ref_doc_id": "c4884c63-ab65-499b-a999-c681de0dfcc4"}, "54b25020-b744-4a8e-b370-c1ebce85705e": {"doc_hash": "956ab1da27c305e031d2bf652135d4845fafe3fcc02ce354f743d93098361d67", "ref_doc_id": "c4884c63-ab65-499b-a999-c681de0dfcc4"}, "b9d7ac54-078c-45bd-b1d4-8dfa6dbf170a": {"doc_hash": "d9647231a697ca7bf65e464ed7daff9f0dd2cb2f8b7ba598fbac015a7d253747", "ref_doc_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc"}, "e1e5279b-e384-4417-b2cb-d9c34616ccca": {"doc_hash": "0aee4a40af52680ca585b1560c0397b012a3d8dbab10a215160f651bfc48b9ab", "ref_doc_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc"}, "2d1e4e84-60a7-432c-bca6-5189663bcf12": {"doc_hash": "62aa6443caf0f05b663198b701b18cfb9c192b06cf974c870856ec6d7ba4e841", "ref_doc_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc"}, "bbbda80d-54fa-48fe-a793-c1fb0f178cd6": {"doc_hash": "1312862de7181e050df710916f0cd6a8bb576b5fc5baa7affd058ef99a3bcef9", "ref_doc_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc"}, "d05fe151-9ada-49db-8ce4-4086284632a5": {"doc_hash": "afde5d25f136883ae4af9cb82655fc3ab85d84e0468f47fed884c00dac997db9", "ref_doc_id": "b3acbe4b-37d3-4ad7-8129-81790271bbfc"}, "8d2dd5cc-61b0-40b5-917f-d7371b591334": {"doc_hash": "2ce498d4d0e77d4733c9ea7455880dccf40e9cda6a38dc8b93680100a80a6b81", "ref_doc_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2"}, "e4876034-8e45-4364-b80d-3e066f5fe57f": {"doc_hash": "4356bb4631422f6bbb91bf37c5e6f44f5cc87bb6a1b05cdce5b76001a1cacb5d", "ref_doc_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2"}, "a6fba84e-9aff-44df-8957-f463106d4046": {"doc_hash": "0f0d29e362cc918b3699de19f12832cc74568e11534b19f3573a7b67dc56c467", "ref_doc_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2"}, "1cc459b3-d325-40e8-9cd2-2a80a7a7f523": {"doc_hash": "93c3fc9d2429ef0ab639ab0294b5355fc54830c4a7f05ac63c97601df9c9750c", "ref_doc_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2"}, "dfabe07a-608f-4828-813d-97c83b376819": {"doc_hash": "a7ef90141bb9fa66607f9921cd3cfc59efd5174dc8ca34beb3eb67045ac9b304", "ref_doc_id": "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2"}, "dbc3a6be-152f-4360-a086-e317b1979737": {"doc_hash": "6f3a016faa77dd4691e65c98393271ca6ba86b60449aa726a2fc9cf818696ef9", "ref_doc_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2"}, "5206c7a8-aeeb-4e49-b845-ef739b76e3ce": {"doc_hash": "86f0e70f43fcd760c4dea692b88c797580efe42389a0cd517a13b26214324fb5", "ref_doc_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2"}, "50e13e9b-86a6-4bb9-b538-3c314a67bc03": {"doc_hash": "60e8bc62a163e6eb837552784eafcfc5795233b38dd7113d28ab8c90aca2a87b", "ref_doc_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2"}, "3a7ebacc-2913-49d8-9dd6-45bcfebe24f8": {"doc_hash": "01dbf15cc102e66fec9160d9dc1168ca57d55e49273ce0fc4bbfa91bf7735186", "ref_doc_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2"}, "92637d5c-b17b-47a1-9264-bf112dbb6030": {"doc_hash": "1322a0b65c4691879655abbe54e364ee6e6059e1025e15c8a8db3bc14e1477a6", "ref_doc_id": "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2"}, "ed5469d4-5c93-486d-81d8-c7f56922eb1b": {"doc_hash": "010731eb2a2146cf0b40c785666f86bbc1ff81fb0504fd2b4e274d1ae3b32008", "ref_doc_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf"}, "206281fa-552c-4bf2-93dd-014d41c7ff14": {"doc_hash": "fc31ca62bd3062edad05cc9cc9db407f9164beea8f54fd3ce681065c4fde5f4a", "ref_doc_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf"}, "4a7f8864-6fd6-44db-bca1-9b388792455b": {"doc_hash": "85a30c5a8222e21feeaa0fa966a1109023f1f7ba6e04f0b74644aca72475ea68", "ref_doc_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf"}, "00ca55a6-99e0-4f3d-8c8d-cf2f350b020c": {"doc_hash": "a9b8c1396b5928d35ba73802312dd896c62fd1478d49dbaa2d59a80de63f8f0e", "ref_doc_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf"}, "69d976e7-b806-444e-ba71-9e7eae0820b8": {"doc_hash": "2e5e42d92dbe93f4e08cfc902be01307824abc995499fa48db3ea761b070ca5d", "ref_doc_id": "9a68a403-80ce-43dd-b2dc-b6a49addecdf"}, "f21507d8-c965-413c-939d-44f3e5f1c095": {"doc_hash": "58b053e1b7137d45e355adb640ff6ae383be6bfb4865c6fe8d3ada623fe85b69", "ref_doc_id": "ad848f9e-6e82-4e71-92f7-1a534a8a184b"}, "936de599-ea95-4d78-9c3f-1c249d80d80b": {"doc_hash": "f40523a4df87bcb25e7c99f3d66a52d6b6523337c56997506af070d93515e489", "ref_doc_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546"}, "f30055d2-7f02-460a-8448-94843c6757af": {"doc_hash": "06077b5569c4638c3e429be712f1652ae498b9452d4bc00e72c91449b702f363", "ref_doc_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546"}, "c99b4aba-fa2d-455d-9d84-77c3d20635f6": {"doc_hash": "b2edc0a803af4671e5d9360245a9965f3afe77298c5f40c04b5f5d7c2148ce57", "ref_doc_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546"}, "39fab97e-52f2-4e71-88f2-fe89f43d9e96": {"doc_hash": "b67bcf24635157a6caa9356e8f5d1f83a7f9ffdedeb93438a45bc398702179a1", "ref_doc_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546"}, "27497f07-deb3-4d74-bcb8-7865ae6f8fca": {"doc_hash": "a0bf98d1b8dde780e20ff462ae52525c4448603c69be2dbb296466b947aea8cb", "ref_doc_id": "3cb750da-ee3f-4d5f-90eb-46f05cfb3546"}, "398b9c48-d6a9-41cb-b4c5-d800a081bd55": {"doc_hash": "c731288a38ae45222d36eddc88a86c59e6a38454421934bf6351586eea72e742", "ref_doc_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9"}, "278e7c07-9191-417a-9fc5-452bfcb6b8ef": {"doc_hash": "f76ce2ced3a02f398c81c9ebc4e1796d9a0f80c824520b0a85560966303bf574", "ref_doc_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9"}, "692f0364-6df0-4243-bd64-3a5b65ea337c": {"doc_hash": "d213eabab635020e7eadaef204ece34e38863722e9ce62a6f1c154f31c0d7d21", "ref_doc_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9"}, "acbc0f4c-614d-4494-b95f-1759fde42b04": {"doc_hash": "4ede8bb7e31ea7496e82bf828088b393a3677a8e8cf97be1439a1b5030ab6dd1", "ref_doc_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9"}, "6847293d-9f0b-427d-b17d-187f2b533b44": {"doc_hash": "0fab70f128917d34f24978c7f836ffd9044fe784f8e6c736e4d706a8e14b57c6", "ref_doc_id": "bb5f8140-17ea-42a8-b530-a5aaee35caa9"}, "e5a0007e-aa29-4be4-8b2c-177b5707175b": {"doc_hash": "c8cfef6c2fbad2bddf7bd38fb415981a629d2fefca854437453d8c71ae9ddbda", "ref_doc_id": "92c16379-d8bd-463f-9c60-ec9831a037e6"}, "27d32337-66ea-48ac-bc2d-dad3f1ad9f00": {"doc_hash": "3658d0b51ed1b9b7906452aa6727f38756893740f883f3b140a52217bb48ae8f", "ref_doc_id": "92c16379-d8bd-463f-9c60-ec9831a037e6"}, "5a65a88d-bb2b-4c2d-9a66-95d2302cc0f2": {"doc_hash": "54fc47517fe02ff2c264738afb2321e73283adeae52bdaa206198ec276fb733d", "ref_doc_id": "92c16379-d8bd-463f-9c60-ec9831a037e6"}, "dedc7f2c-f064-44fd-84bd-3f4c428591ad": {"doc_hash": "3b565b9e1b23086a38ba8f4cd58d7a29796157c1d10e6b7d03f56d0e78f2c4f1", "ref_doc_id": "92c16379-d8bd-463f-9c60-ec9831a037e6"}, "3759a715-0879-495a-b493-c4b225e963b7": {"doc_hash": "ae651c280891e84d0034439418e38059bab9f5019aaa471f90185f615a0b7d99", "ref_doc_id": "92c16379-d8bd-463f-9c60-ec9831a037e6"}, "bd1a0ed5-3007-43f1-98ad-4201cb85a042": {"doc_hash": "083dcfee709cf6951da210ad73396a73a02c3db6c94f40be30735ae3b488fd2e", "ref_doc_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580"}, "133d75a8-9182-42a0-999e-0436128e7872": {"doc_hash": "f5f572420f155b551cd1fcd0753f4be4432c0ceed9ae756f9a35ae8f86c91d6f", "ref_doc_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580"}, "3bf2fb29-9f66-46f7-859a-6d90dfc99792": {"doc_hash": "0ba5dcc18772d6b6de4d405bd88bf7a16e923a6c80d83565dac6acb2773ebe8d", "ref_doc_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580"}, "e20b5f0d-7df5-4c8e-932f-7d9935f2f2c2": {"doc_hash": "9a7004724a9337596c37aed41ddada2ee10ce166a5ef528d77f8d9fe3001edcb", "ref_doc_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580"}, "c2fba76e-41e1-4e4e-ab13-1b470c8216da": {"doc_hash": "90a0ce2703139eda58f06d328f72f331e55a18123f827878b221fb0f76628050", "ref_doc_id": "54d96d23-c6e9-45cb-960d-1aedfdd1b580"}, "a33b256d-abe8-4813-9b29-7f34c07e22b2": {"doc_hash": "c94d5758c800353256e28de210bee124682beb04df926d46fc0e7f952804b1ee", "ref_doc_id": "b7b4af96-3f92-422b-a14e-63fbead4b498"}, "a726854f-f8a9-431e-829b-30e4742f7d46": {"doc_hash": "ea52ad687cb13a7ec54c3f74179630253570e15f8d148d494b7942356022b6e8", "ref_doc_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89"}, "450ef87a-0957-4826-83a2-5d9d8e4773c7": {"doc_hash": "da8cb6a8784d136d0aad2cc17c8d2d18221184da47dc36c66ed3499bdc2b14e4", "ref_doc_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89"}, "9b24883c-5a4c-4f3b-be38-9c962ce7bc1f": {"doc_hash": "42589916d045fdcf2f3694e659c92851f6f4addd375e78669898df82b6985dc5", "ref_doc_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89"}, "e862db03-86f6-4080-81bd-d15c046aa835": {"doc_hash": "a9044711eb44f3f5c95ae885b8f5e837f354b88e3c7db9ace3d673be80d928ba", "ref_doc_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89"}, "843259ee-797d-4210-a4b1-8ec05c4e6bc7": {"doc_hash": "f1909e97b67db94e20e1fb205ed79114bc0462660e7f60a32fbd13a416248f32", "ref_doc_id": "d9ef127c-8f59-4316-a43a-a91acee1dc89"}, "8f8e59fd-24fd-49fa-8626-d7723415d3a5": {"doc_hash": "594f9849e7e66a5d46a98c2863f93f4987dd2f10b40fb2c50aebc0ba97812114", "ref_doc_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60"}, "c144f42f-9652-4ab8-b893-b42cbdcb8a98": {"doc_hash": "76dab4b41a2359b22b37552b5b44de99aac495c844fe1eb99086fe3fe3d92cfe", "ref_doc_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60"}, "49acf9e1-1fa1-40e3-9801-a7f64e4a132d": {"doc_hash": "66f74b21899b27f7755248d31d26cc574bdc4de4507bb6722e71a35c77f51e25", "ref_doc_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60"}, "d641491f-61ef-4359-b9fe-9dfa8bd9cb8d": {"doc_hash": "0593dff6bbb31422e64f41c87800c8af17bfe5d1f729322ce6795d4c07a67083", "ref_doc_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60"}, "aff25138-c9f0-4611-bb45-175028a2ca57": {"doc_hash": "8a6480c1cff870f48f255d1523ae0c99c09e17426f89620ee48e2d9151705ec5", "ref_doc_id": "3263c986-bb7b-4f7f-8240-c0cd405dec60"}, "733f45d2-d38c-4127-a959-e19802493591": {"doc_hash": "93d44df141bc660c8b690aeec3deb44ffab8d3cade1a8ef6a39403ae1c02fffc", "ref_doc_id": "da514732-a3cf-4a31-a994-8bc6710f22ec"}, "173fb9b0-9e9f-46a0-9829-c989e9918d6e": {"doc_hash": "f64b62db6925c0ea9576063430929b8dcdafe839a94656d2de46b03f260fc54d", "ref_doc_id": "da514732-a3cf-4a31-a994-8bc6710f22ec"}, "914f3071-6fb5-478c-acf8-1ec4f4529e08": {"doc_hash": "e8cf591ff745cb74efc940c41662648433104d08571c7f7ce8ffc561cff7e778", "ref_doc_id": "da514732-a3cf-4a31-a994-8bc6710f22ec"}, "14912092-810f-441d-9cae-bb9025dfafb1": {"doc_hash": "d514d5f2a086f1c7d5faadc6d13396b5aa734ac5938f086116e9d04a5658c280", "ref_doc_id": "da514732-a3cf-4a31-a994-8bc6710f22ec"}, "3220a14f-bb5c-4a16-9a37-6c03ab5029db": {"doc_hash": "1989e998e491eae739ce7adc26758bcec8eb276fd825941ceda8b6bada38f3e6", "ref_doc_id": "da514732-a3cf-4a31-a994-8bc6710f22ec"}, "e3480fc6-2d39-40e2-be85-87c2c822cede": {"doc_hash": "0b251a06b01867da1a9cb13084131026715e08ea78acb957a5f1d094b244c6a2", "ref_doc_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f"}, "8995d339-b032-4fa5-8f3b-43650975ed6e": {"doc_hash": "27a3d5ac32af29ad3d440f795997c9704675c56bdf7d226facd161bb46a46185", "ref_doc_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f"}, "661a03b4-e664-4d71-88ee-a259b4da4183": {"doc_hash": "fddabeaa9b428e2ebaf58c18a03c64a2f68180cab1c991467f2439f9df3a12c3", "ref_doc_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f"}, "2988ad8e-3321-476a-a8f9-5a2bbc50646f": {"doc_hash": "ca8349ec39b705fcdc18dcf074d49bcb4e2dc1955b85fa60909ec2c4ef9f949b", "ref_doc_id": "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f"}, "3b83c4fe-27e8-4f03-bc18-81e067a18526": {"doc_hash": "b57fbc2ebf590b52b8cb0bbdd48c42d44502010f5dd7ec9f73b1b12e7e91f597", "ref_doc_id": "20062614-22e0-4695-9dae-521fdff26f79"}, "93b3e0da-3e52-4b92-8a34-ab241bfe5838": {"doc_hash": "551bee74a9305e1db91f96c09d76ce2a6f35623064d56a1b30e728dcd2e4f134", "ref_doc_id": "20062614-22e0-4695-9dae-521fdff26f79"}, "89fb7b72-777a-4cc7-837e-9f5b47dabb85": {"doc_hash": "b82674335f03284752135f4dc8b3acd2ee1cfe7a17372db8afc4a1d5f82cea1b", "ref_doc_id": "20062614-22e0-4695-9dae-521fdff26f79"}, "bcc25b7c-9372-44a8-94ca-19e7319a9be9": {"doc_hash": "9865d3e13b72010893102e1184658ada8baeba91690c655a84fea99758db9bc9", "ref_doc_id": "20062614-22e0-4695-9dae-521fdff26f79"}, "9ab130ed-3f3d-4917-b41e-406b1094c588": {"doc_hash": "049fe44b6242213c14f99937de30e224b60860bde52cdee87f6e47c834c60590", "ref_doc_id": "20062614-22e0-4695-9dae-521fdff26f79"}, "70f5ba7c-7a51-4706-a4cf-fd4b7aadcaa6": {"doc_hash": "015e5673068b5f833b52db799479cab5b46df495ef645d4d06e4de3f63ee94c9", "ref_doc_id": "f056fd99-2019-42e5-9b42-718302e68fe2"}, "47039706-e2d3-4000-ab1b-9d20e70132ee": {"doc_hash": "45bda953d0f366db5c776f2f66a8bc0463baf8344d27f63e79d9ed353c3dc27c", "ref_doc_id": "f056fd99-2019-42e5-9b42-718302e68fe2"}, "976714c0-e4d5-43a0-ba7e-0fab9a51bcff": {"doc_hash": "350ff98415c71c002b25af89b10d0688c29d77d932f3dd9d65ab07666bfb47d5", "ref_doc_id": "f056fd99-2019-42e5-9b42-718302e68fe2"}, "075a15af-6fe7-4aa3-ad2e-93674b13240c": {"doc_hash": "21533d9f05f52b65109bc408bdf8bbaf25b507920bd699dd0c55640e0c18bf4a", "ref_doc_id": "f056fd99-2019-42e5-9b42-718302e68fe2"}, "68a5291a-3462-4b62-a5ec-ebe9bde1e764": {"doc_hash": "06b14469e41df83541f8ec8aebc3be24ae5b01b90b4a6864858eb7358cdc148c", "ref_doc_id": "f056fd99-2019-42e5-9b42-718302e68fe2"}, "90051f7a-8d7b-40ad-87e1-292a767de344": {"doc_hash": "07aec4a3479cee064cd203ac5f764888cb18ebbc174ab3eddedcc410c956a70d", "ref_doc_id": "32e72675-2960-49e4-afd6-5052ce1b0802"}, "b489fbfc-5190-4afb-a1df-d181d1dfe24a": {"doc_hash": "70d6cbb5f0ddbc2c67b4bed7ab5feae2c924188236f7747ac329686d4f5404a1", "ref_doc_id": "32e72675-2960-49e4-afd6-5052ce1b0802"}, "5813aad4-2cc6-4ea7-8a51-b5b7c512d119": {"doc_hash": "dfe9511d7a7d0489935b1517df9db707251218badbe7a8893f6220eb9df43622", "ref_doc_id": "32e72675-2960-49e4-afd6-5052ce1b0802"}, "da33621c-3be9-444b-8b72-86b42cadccd9": {"doc_hash": "af55e4ebfc1a3ccfe3e179906cb05169564745cbbfd578a4e5bb6f3206fd4d72", "ref_doc_id": "32e72675-2960-49e4-afd6-5052ce1b0802"}, "8d88553b-27a7-4aec-9b20-866a54191319": {"doc_hash": "728721a1408e46e2475b6aade3b1e409f904a943cdae18e75c1b8a532aef53f2", "ref_doc_id": "32e72675-2960-49e4-afd6-5052ce1b0802"}, "ddd5a5e0-ea32-47c9-8629-7b251984223f": {"doc_hash": "649f3e3f6eae647dc2c7cccb9a5c26373cc7764f54537472af0fd7698596f9b2", "ref_doc_id": "dc06d373-fc7c-4d14-b2cb-95d390096739"}, "49fb7aa6-8e4c-40de-b2f3-cc2a83b89252": {"doc_hash": "2fa5ad0999650b12a5ee597b9f587280bafce0c57a515c53f7cb764b209cd64d", "ref_doc_id": "dc06d373-fc7c-4d14-b2cb-95d390096739"}, "44b33897-061e-42bb-aa09-288cb0daa2c4": {"doc_hash": "7aa4aafade0a30f65b28e4167949bd05a7901f2ee7b4ffb647e5204be6a73c39", "ref_doc_id": "dc06d373-fc7c-4d14-b2cb-95d390096739"}, "90470b79-c441-4938-a09b-0bed43975300": {"doc_hash": "d97cafae7b1bca2b682ff10bc186a04ba59ffd5d249a66097681d7788f5a55ea", "ref_doc_id": "dc06d373-fc7c-4d14-b2cb-95d390096739"}, "a06ca537-930e-469e-8b48-e7a30a705046": {"doc_hash": "92c1d81820d166fdd215c7feb82cb56d5d68ae6ba36a3bf5378615625ca190b9", "ref_doc_id": "dc06d373-fc7c-4d14-b2cb-95d390096739"}, "397e8007-44af-4d39-827a-d0c027215c52": {"doc_hash": "c60ae4ed8fdcfe51dafce5c4af7675cbd46ee8f0bb30e601660346acd0633957", "ref_doc_id": "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca"}, "f2b2e28f-36c4-44d8-8e1d-f200916d2b17": {"doc_hash": "2d87addbd47e1638b3ab677261388b22c8684d16547131fa64cdde00c05114f7", "ref_doc_id": "b646a538-109b-47c0-ad7a-ac2283530d47"}, "302545ac-ad76-44da-813d-2c45ea69dcb2": {"doc_hash": "bc383378ddced11c32265c187bc61c78aab6c5e9dd79b0cc874b0b212abcee0d", "ref_doc_id": "b646a538-109b-47c0-ad7a-ac2283530d47"}, "4f3cf145-b04e-4360-857c-a098a8f9d4e9": {"doc_hash": "12bd7949a060e9cfc6589be7b308f0cf831db18ea708fee3dc06c8fb35cba6dd", "ref_doc_id": "b646a538-109b-47c0-ad7a-ac2283530d47"}, "205bf3a5-3e9b-4f95-a13b-506a843bbb2e": {"doc_hash": "2f7bf8a234dca68d77c97b1c2d3bdcf225399f814949b7d74df778417ac25acf", "ref_doc_id": "b646a538-109b-47c0-ad7a-ac2283530d47"}, "fd6e7113-d963-43a0-a7f5-6ad2a59683af": {"doc_hash": "71f625ce05e2377efb7c4d573d190af8262c140bbe59ec678fe52230757616b3", "ref_doc_id": "b646a538-109b-47c0-ad7a-ac2283530d47"}, "7a227faa-e6f0-439d-86e9-94aa2b4ba4c7": {"doc_hash": "ffb23b90835b4e19169323e10e9bafa8967ace6f061bc963630ab0ac6974e036", "ref_doc_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc"}, "5216234a-e278-4358-b80c-1309c790e54e": {"doc_hash": "72b02ec6981491005b3f5c95435a9f1171570e1eda6e058df6221fe6d1d05170", "ref_doc_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc"}, "a7cc3307-ed1b-4e3c-93ac-40875c2d3796": {"doc_hash": "c860231e3bea18303a5cd2c93b08d2a6e30193260c92536f247ed7ed2a70a2d4", "ref_doc_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc"}, "b5e16604-7e38-4ab2-8e19-a62413f43be8": {"doc_hash": "5ee101198383d6ca2370f84ed48e4be0ffe6a2da74daf79a511608317e5893d5", "ref_doc_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc"}, "659bcfea-85b8-403c-bc6f-60b89fc4ffcd": {"doc_hash": "1c354d09d92bef3ad875a3d1d68361a7d38e4183407b042a1eafe7eb29ad208f", "ref_doc_id": "fc9ebb92-7688-4767-b8f6-7b76327405bc"}, "777cf26e-c814-4055-ad5e-732d45dba164": {"doc_hash": "acc09292f676e1ffea0794cb34400731fd86968bc22364173d15a53ecdd65676", "ref_doc_id": "fc9fe157-ab09-4904-b323-e8019ece8204"}, "ab6eb898-4f1a-4cb8-8ea8-e3b33547d7e3": {"doc_hash": "ddc6cfcc0005f3fb6ec0474ccb6d775fb1a6e0525e02508ce376f9727deea40c", "ref_doc_id": "fc9fe157-ab09-4904-b323-e8019ece8204"}, "fdcb691e-6311-4440-a5cd-ae83b3f5a07a": {"doc_hash": "a36b46800ce88f00acdc7d1391739a35a19452fd9aca6b16667c158dc2fb244a", "ref_doc_id": "fc9fe157-ab09-4904-b323-e8019ece8204"}, "67eb05b7-2887-4807-a2d6-8703d4e2af90": {"doc_hash": "90db2db81321c29a3625ac42af3f5ab5e5eece66320a591113f02c2ad815e3b2", "ref_doc_id": "fc9fe157-ab09-4904-b323-e8019ece8204"}, "d886886d-ff01-4653-bb1a-adaea32bfb9d": {"doc_hash": "1e6d2cd3af9d14b626de8fcfeae80e420e8d121517c3b3e6303acb294e0c57ac", "ref_doc_id": "fc9fe157-ab09-4904-b323-e8019ece8204"}, "9a5ec0e0-ea1e-478b-9d66-97f0fb1cfca3": {"doc_hash": "a8dc18a87cea9bdda30c0a9e0da96ab38cc14062c117652010447a571dbdb7dd", "ref_doc_id": "d4055d84-e057-42ea-901f-7ba1753df337"}, "14288937-76a6-415e-b127-6be38d179ae5": {"doc_hash": "f2e85e1f011008644bd6e77d8b4dbed46579f2a38148f980d78a15a853e342d5", "ref_doc_id": "d4055d84-e057-42ea-901f-7ba1753df337"}, "ed353fdf-d90b-4d26-9c0f-227c3236f79d": {"doc_hash": "62fdaf3c02618b87c732fe7e851354a9a4e77666b856cdb7f27174241fec6e8b", "ref_doc_id": "d4055d84-e057-42ea-901f-7ba1753df337"}, "8633af80-72a5-42dd-8ad4-362e972bfc14": {"doc_hash": "5020e1dd3eaf177794475d9b17e8cb34a6ff91027da1d83c4ee2afe0a500a62e", "ref_doc_id": "d4055d84-e057-42ea-901f-7ba1753df337"}, "c488c930-29d8-4119-800f-e2a195f83bee": {"doc_hash": "ed387b66755b48b0ded2e5c36528bfedebcb85ee626460ebe25911f1aa82c2c1", "ref_doc_id": "d4055d84-e057-42ea-901f-7ba1753df337"}, "b8a28e70-6de3-4ea3-b8bb-68c535a48178": {"doc_hash": "70828131a4e089a5fd70194bf07328bef70c911b67ea89eae6f21470b4185c0c", "ref_doc_id": "c98e98ee-03b5-4bf9-a652-999317a0b671"}, "92be1258-6cac-48f4-9fc8-c2e56846fb94": {"doc_hash": "8cd0f612e6794f2d275bc30e52ef6cffdd6bb76955978871e9a35d5489de5274", "ref_doc_id": "25335f95-389e-4dfd-a165-a8c49e4ad787"}, "bdc7067e-8cb5-4467-8e3e-90737e05f7f2": {"doc_hash": "713d2e9a5a11988662306a934bd30ed4098a19a154b88b70a1a20c14e14b3c1c", "ref_doc_id": "25335f95-389e-4dfd-a165-a8c49e4ad787"}, "04338204-b1ff-477a-b016-8e7de89b26d5": {"doc_hash": "74ae8f85b1714060d332c2f5818befb7fc9b486f415fb86caa66924ff5e6cf2d", "ref_doc_id": "25335f95-389e-4dfd-a165-a8c49e4ad787"}, "67cd8374-8b19-4e27-b88d-659eaf35c055": {"doc_hash": "f99d525051e1ed451734b0e9fd9683f4232e9362bf8a92182a8c84a4a1e843c9", "ref_doc_id": "25335f95-389e-4dfd-a165-a8c49e4ad787"}, "7aec402f-4bb8-4eb0-b627-ed7989264302": {"doc_hash": "92c6cc55abd2f8ec86af9cbbe0df8a64e1825894463c00afcc99ef6099a65437", "ref_doc_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa"}, "b429fff9-0da0-4633-af64-90ad7e215920": {"doc_hash": "6b10c3b746dfdafe48deb59742440fbfac0a9cf84231f58ca8932e71467db6a7", "ref_doc_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa"}, "13274157-7229-45b0-8997-6fdb2dbae214": {"doc_hash": "ea76a66ba11bdba8a29eb6070d2f0fe53c943a6e0d1574c027b5ec4549f496e9", "ref_doc_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa"}, "f567d5c9-2dc1-47a3-9193-897d26055ee0": {"doc_hash": "2bf5d5f4d1bbd8192e7da5f07ad81a58d462885c505647410b4bed13178563d1", "ref_doc_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa"}, "a396e262-669d-4c9d-9a1c-17da560b9118": {"doc_hash": "bbb63ca1a3b8ff529d8b39f3fa9fabcc5e69078985e1e96210a64788ba7c000e", "ref_doc_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa"}, "a1817d0b-a1d0-4e69-8562-0a506d10f3e7": {"doc_hash": "baf0d0e5f9874e48d3e8a0ae8d89ce07bc6512843bd06bcdbb077fa21e0cb509", "ref_doc_id": "ff14e27b-df47-4028-8029-4a6e19eb36aa"}, "6760dec7-64fb-4236-8b3a-0d596faa42fa": {"doc_hash": "565a1fc0ca9660b9e84cef013466a7b26ee224161c074daadb84e05b26ebaf59", "ref_doc_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f"}, "4d643c3b-afb5-4c96-be4e-f5d35e44f22b": {"doc_hash": "e259dae028f671e24a833c32bc349fd5bc0d5edb563ae13cb79f4be8228ad506", "ref_doc_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f"}, "139b6acd-1363-4fe5-9b28-dc397c0a70e1": {"doc_hash": "19b5312e829790b1abc9d8bcdcfde1dcf125d3ef387cc0ac56dd9bce31a90502", "ref_doc_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f"}, "79244573-93ef-474d-b526-9fad3bb61d58": {"doc_hash": "e8e5317db72c541c28e57aefa4f6752d9b642b49b1edbe4954300336dcc96209", "ref_doc_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f"}, "50351b61-7460-48e7-ab37-0b865bdee1f3": {"doc_hash": "31ae0c605b4b880a0307a579f48a4c43f2a409d9d941e4d47bb63e6070c2b4ca", "ref_doc_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f"}, "c2bb9ba0-e566-4971-82e0-7da09e2666fe": {"doc_hash": "1b1c45fae9e01893f0142260fb929b4e7d65dc64e53a288037414d4cd0e9e9f1", "ref_doc_id": "46dc9b82-6c13-4332-a2b3-4aa89903801f"}, "12baea84-1c2e-406e-9fe6-d6c5ae40afe7": {"doc_hash": "2aeede78d038ba29f40fcaaf9ce6e9204b6a52cd0f6c0d514fa4736fb60543a9", "ref_doc_id": "668623f4-21a5-4b59-b6ac-e78759e6b403"}, "a2b02493-0913-4aa1-a942-76a5693942e6": {"doc_hash": "19e1a97de4bd3b6cd38a3d4a6a06edae0fddbbfe16a0808a0c15ec04f3174ed6", "ref_doc_id": "668623f4-21a5-4b59-b6ac-e78759e6b403"}, "b2d1d2c9-03fc-4370-8b49-bf0a6482fc52": {"doc_hash": "31968708e17aa7e3e46b6f80cbc31b8100b1dfece13d693db0719b73693f8245", "ref_doc_id": "668623f4-21a5-4b59-b6ac-e78759e6b403"}, "20beb164-b808-4fab-bdb1-2cab9205485b": {"doc_hash": "84f799769391b9be09a02d6763ab27bae4d2948e81d675f40d5c82ffc66e2cf2", "ref_doc_id": "668623f4-21a5-4b59-b6ac-e78759e6b403"}, "45e7963b-3f3f-4503-beab-9d52b2e33541": {"doc_hash": "e09994106885dd8c04c785edf18c9728e3b00eb7174a10204da1d43b740edbc9", "ref_doc_id": "668623f4-21a5-4b59-b6ac-e78759e6b403"}, "0d99a087-25e4-4697-aa87-74b61e82a6ad": {"doc_hash": "d4de0904dc60501c95e3fd2e514611431f3b100c9259692bc819ded096879298", "ref_doc_id": "c26f6c18-927a-4e11-b542-6fa85387b872"}, "bb13df0d-57d8-48be-9f81-b00f3aa17fe0": {"doc_hash": "863a26731ac86bfa63a1b2efcc3bf8dcfd08e3d777e5926cf01add9e54be611f", "ref_doc_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05"}, "a77cb3bb-d25e-43d3-a9c0-d530dd22739b": {"doc_hash": "9050ce00c526f93dcc82f7a1cce2aab4283c09c0b54e26bb876ee24dbd113811", "ref_doc_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05"}, "3194219b-ebcd-4142-b47c-5fa24070e90f": {"doc_hash": "72a840308d31a0302150fddd5aaae9f145064ee2ea3630351c964b80e3584a86", "ref_doc_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05"}, "31fd32b6-a023-49c1-8fa4-a4c977281d19": {"doc_hash": "fb4e74c749db844bc1f37627d035eb64ddaa9c5f30e41156ea390d254a625259", "ref_doc_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05"}, "f967a8e1-0805-45a8-8ffd-8652afc40ca8": {"doc_hash": "d87ba2f9649be406707cf8943eeac7c5add458702613a4203be3394a750c290f", "ref_doc_id": "ad7a59b1-43f1-4324-99d3-262cc9769b05"}, "e34bb962-915a-4424-974d-2c772aee08b6": {"doc_hash": "53372c64133ba60545e97b15510a4af2a1d8d4a65762fde1e3683b0360d36d95", "ref_doc_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4"}, "fc7c7c5e-5606-49c5-abc4-f7ae526fab8f": {"doc_hash": "a6ab9ece106b1dc1605f98b3b224a29614a04b2f3bc5dce77d703282ee06d460", "ref_doc_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4"}, "e1613544-b38c-4ed0-9a78-59133c3529f4": {"doc_hash": "901e70e1c2f5f93fbab7ee3e5ec525e86ff1532cafc6771162a4d1d598a5e261", "ref_doc_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4"}, "1a866526-497b-49f3-a6da-5c3d580b7c85": {"doc_hash": "5c847c5b287e488f1ad3805947b76e9c6eca2d21dbb0b4c34e3f5fda46a44898", "ref_doc_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4"}, "217c9a59-07ca-468b-8329-6c3f7031478d": {"doc_hash": "1c7ceaa85c9ae8fbd26360435168711ac8984197cf93af74c0167f8e2e3a65ba", "ref_doc_id": "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4"}, "a8767cf3-fa04-46e0-85e9-f36df1a1f878": {"doc_hash": "c61aa576e78e57206b42dba50fa8fb45dfd10de9ffa4e9cbe50df835d0cca675", "ref_doc_id": "ce7a1637-31c5-4726-9393-835a332f13f2"}, "4c8c9fd2-3b8c-4a7a-ad1e-a1b74840dc1a": {"doc_hash": "a443d2d5b71c1e86d5d172e72934343995e0e01e41569ec475b45c0ebf15ee1b", "ref_doc_id": "ce7a1637-31c5-4726-9393-835a332f13f2"}, "00eafba3-f472-4835-ab19-ac519d4aa3cb": {"doc_hash": "3430023d3b35f1cc958713d1ccbfe291475be1a641b6e3db62775d6aadc033ea", "ref_doc_id": "ce7a1637-31c5-4726-9393-835a332f13f2"}, "2117da92-d682-475d-aba8-a71d376f683a": {"doc_hash": "064cedaf76dcf26370c23f963fe33543c5c057a71bd040a92e67a796cc05af52", "ref_doc_id": "ce7a1637-31c5-4726-9393-835a332f13f2"}, "7b311141-51f6-48e8-85f4-19dcb6789636": {"doc_hash": "baca1b715cc6c66d857813ffd6ba3f8808089c4ec5417ca5b7d2e9c711a56bfc", "ref_doc_id": "001f02ad-43a4-4105-9aab-1eca82b49991"}, "f2b30c75-d798-43b7-a3dc-3876e3820cef": {"doc_hash": "265a59a04791bb7a2f0fa9ed01900df3453d0573b5a2babd96baeb2cbcb62c4f", "ref_doc_id": "001f02ad-43a4-4105-9aab-1eca82b49991"}, "0906a26a-d932-4a45-b2fd-6b5d80b92412": {"doc_hash": "80bfe8682c1596fcb512c2f3c981f3bede057b1967012b8cfc59bd46143485e8", "ref_doc_id": "001f02ad-43a4-4105-9aab-1eca82b49991"}, "dcd8e83f-99ae-4dab-adea-b18713a1e027": {"doc_hash": "0f9ad2e506a7c97760c83bc3fec86edebf45e91e9c5957b9ec270b479b915bce", "ref_doc_id": "001f02ad-43a4-4105-9aab-1eca82b49991"}, "5093a13c-9d13-46ed-9d22-057236d90bde": {"doc_hash": "7ba01220e68a02efb03eacc795a92bc17df1b0fb16d45fa9d45e2cb3906f3d58", "ref_doc_id": "001f02ad-43a4-4105-9aab-1eca82b49991"}, "479df2ba-adbd-48c0-912c-45351d9583ca": {"doc_hash": "c97aee1c760250dd596a785c4de320feadd0c4f892ad3cd7d5518a1cd165e1d0", "ref_doc_id": "45593217-9f9a-4741-a8d5-1545ac7e429e"}, "30edca9e-602b-4751-8f1e-8ceea3fa7144": {"doc_hash": "1aa5d5997c67fcf1db567ade1d71c118c3ef1d05ad54d88093e4dae616699bb7", "ref_doc_id": "19780472-c500-4db8-9fe0-a3572c15efe9"}, "e9ac09f7-abbd-4291-8e1b-59826aa8e9fb": {"doc_hash": "ec7bb8e20dd6cf16abe9a86cd42b285ccf6d240cdbcf8c8c396ddb7ffa225bc3", "ref_doc_id": "19780472-c500-4db8-9fe0-a3572c15efe9"}, "9f025190-0e23-42a3-8fa2-bad898a7321e": {"doc_hash": "5d60c2d112691791a8f931895499dfecafe42faabb05fa5597eff5922bb5e13b", "ref_doc_id": "19780472-c500-4db8-9fe0-a3572c15efe9"}, "d6e7f471-d1a0-48aa-93cb-990c54902330": {"doc_hash": "4c006ac775ec69130023d6533d2961725059ebc8f6c4cdee2be43a96acfc8e87", "ref_doc_id": "19780472-c500-4db8-9fe0-a3572c15efe9"}, "a7f52bb1-c615-4d48-a1fa-281c34570180": {"doc_hash": "1f7f77ee4a3d8dd691b67d237409073e092f2232018e33d4ecfc2966fc161df8", "ref_doc_id": "19780472-c500-4db8-9fe0-a3572c15efe9"}, "d643aa0b-a69d-41b5-8731-a0b676b53177": {"doc_hash": "ce2d048fa36bdbb619690152be1d24e33df0e2dfa0b71eb5ab2906c0f4066e95", "ref_doc_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23"}, "52d9581f-cd10-4219-850b-7cfa60631540": {"doc_hash": "6305023dd1bb84b1de043dc2641644b37cbaf17b5a10b8c2c8593776e5f5f0a2", "ref_doc_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23"}, "51acae86-bf53-4591-8137-987b5f3a0c6c": {"doc_hash": "504194cd9029ef71a80737bf125a2b63236236f7291b8f4812148675703bbb52", "ref_doc_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23"}, "f75ff35e-1ac0-488e-829c-f62aa23907e3": {"doc_hash": "83dda942aed08acfee5b36e535c3e1384fc635ad9355b22311195cc2ed168277", "ref_doc_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23"}, "a91d0b66-8351-440d-b6f8-5bce59073802": {"doc_hash": "aeffa4332e0d98db2bc2c5bcf56cf6c9f9c6a959a6bf4db76a13f9c99d5cddd2", "ref_doc_id": "d6190d24-8b42-4a04-b7d0-7020e7d7af23"}, "b5bc3b1a-4bf1-4261-a969-6e7883ff5c4a": {"doc_hash": "289b69fbd9473307f8b274e9d118151bcc73fe65a2a2ba8ef626f4d27b4ad996", "ref_doc_id": "17e36111-0937-4893-91bf-173e818746db"}, "a23e8c3e-903b-41c7-a0a0-652cbcebb4a1": {"doc_hash": "fa6387ddcae8c4dd5120c6c3930dacd954fa1745791c842498ef46e449e066b9", "ref_doc_id": "17e36111-0937-4893-91bf-173e818746db"}, "a4081b4f-d08e-416d-bde6-65a05eccbc1d": {"doc_hash": "75bac725f262aae6b18ffaef66cdee7b0acc1d743322dbb112bd29e25d335f0d", "ref_doc_id": "17e36111-0937-4893-91bf-173e818746db"}, "451e2b54-d25f-4063-948f-01bd7cebaab7": {"doc_hash": "f7fb578fd52e3cb144aae359abe069f89aa3dbfe4a7fbb3bcf9c013fdd5c7129", "ref_doc_id": "17e36111-0937-4893-91bf-173e818746db"}, "138049a2-5244-4799-b421-bab9d1feada8": {"doc_hash": "1480e5cf29e0125b1ed50a523a6671aa7cca4f2b09a5cce7e82742e9137b15dc", "ref_doc_id": "17e36111-0937-4893-91bf-173e818746db"}, "1686d5fe-b11e-433d-b2ae-ae0d0ac166d6": {"doc_hash": "49ed764b19c7b75918ac9854e42c9af38bf8f5a91de6c84936998c0c8edd6510", "ref_doc_id": "3b9d1983-3c1b-423b-b132-1100f72674af"}, "ed2d2138-7296-4372-9bbf-171bee1866c2": {"doc_hash": "5a7f0b35a6e18fdd8d16de1e6ae8ab7dc202871d3bf5e21bf87d34520c59d4ac", "ref_doc_id": "3b9d1983-3c1b-423b-b132-1100f72674af"}, "a2b13420-35ca-40d2-9df7-87435b80f27f": {"doc_hash": "74ef3c302d5b60735e4860d4a66b8605a663f1e378f1ff254f411e9ae9076276", "ref_doc_id": "3b9d1983-3c1b-423b-b132-1100f72674af"}, "d05a8f34-087a-44e4-ad06-8a128b226335": {"doc_hash": "c898d8cbf75ec59da38a8d33d028a1cdfc531750ed9e011deee04be3aa8c0f3b", "ref_doc_id": "3b9d1983-3c1b-423b-b132-1100f72674af"}, "4452e0c4-d1eb-470e-8439-f7cdfd816f13": {"doc_hash": "0f7be290d0887dc9fb9a71e161ab04749b91195a6a6db670d2a470a7dd087fa3", "ref_doc_id": "3b9d1983-3c1b-423b-b132-1100f72674af"}, "e02bfa91-da6a-4d9d-9f4e-2347f1e94a0f": {"doc_hash": "e5f3c064f008e09ee6687542919075f8b54044aab99ed4a359a26a7af4ea6f39", "ref_doc_id": "e209caf1-ead6-467f-9555-1a2e27ae1c9f"}, "e6a2f908-211f-415d-81d3-09169bfb6813": {"doc_hash": "eb97d5aaae1c9e28e4d03c9bb640716c99b6bf98136f2e020dfc0c8a8023ab8f", "ref_doc_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb"}, "4e243190-6938-47b3-8006-d15ada0261cc": {"doc_hash": "c30a7ec0dd443d67dc1637e359654d89346b4ef7f16e37b8beb995cbb5a53abe", "ref_doc_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb"}, "c514d792-de01-46d1-87ca-abf33eda5d0e": {"doc_hash": "d90631eb36dc84c8523b1e5398103b24dde746c1a8f2755ec083c5b6f35afbe8", "ref_doc_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb"}, "e0992dd4-0c0d-498e-a846-4f7f7b68f59f": {"doc_hash": "753cd5a5e55d964c4bcef72ce1ae5071592ae9a5dae07cef279d37843b465b8f", "ref_doc_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb"}, "e5c3898d-8380-4d44-bfa2-269be17f7145": {"doc_hash": "dc1ccf44a6c6fe428aa18abb96ebc067dabf366a6b268661ffe61a5da184f600", "ref_doc_id": "b9a22696-df65-46d8-9ae7-bc9360e912fb"}, "bff354a3-9750-4889-a0dd-11699b472edd": {"doc_hash": "a090b4b62ad158c3d1eff259bd59dc8e13e83c4660da5ec4ceb8349a9d702ac4", "ref_doc_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e"}, "e895353f-be88-42b4-934b-393a825705db": {"doc_hash": "99a6434b29246bcfbef8dd3840bd8bbaf891b077d8466cb6e17e1bee332b9f6d", "ref_doc_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e"}, "3dba9662-1f9c-4b25-8f7a-cd783c2ae84c": {"doc_hash": "33efce2539999b18ca35a5ab149bb5b9a5874cad5595a96a87c250c2c760379e", "ref_doc_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e"}, "9c2e66f1-7761-4419-a5df-cd1f3ea06672": {"doc_hash": "2826c44898df37a4742b621df8f67423537a661ad09aaa96ccdc91d0f5a0f669", "ref_doc_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e"}, "7089157f-0053-48fd-a59b-f1648da859b3": {"doc_hash": "867d8cd5a95f027f7c856bf49abab2ac13e43610e40f1bb579ad7a67a1f5372a", "ref_doc_id": "6fa08b5f-e917-4b42-9ebe-3e710e3c170e"}, "745f7e7c-d8e7-4889-9eb5-618fcad78057": {"doc_hash": "ecea6b6fcd3f012d1896fabc7c08adc74e40a9fcf5ad790037736e49a09d34ad", "ref_doc_id": "381ee8e8-4d6a-4558-b143-621cfd55728c"}, "a1e8f74d-fa3f-413c-8b62-dd170422eaff": {"doc_hash": "e956286d16e8242e1a80157a7936c287286453d76c2cd5f424873c3060d9189f", "ref_doc_id": "381ee8e8-4d6a-4558-b143-621cfd55728c"}, "1800ede4-c7b2-4de9-b55d-2b34066e14ae": {"doc_hash": "625e626d11766e2f788af3cc6c6fc2cbff06939a111f51df981f8e0afae50807", "ref_doc_id": "381ee8e8-4d6a-4558-b143-621cfd55728c"}, "abde9574-d6b3-4840-bcc0-e33de5bd3070": {"doc_hash": "8e0eaaa16361aebfbf01015d519265747ac47f753454a3641f90fe706c402a06", "ref_doc_id": "381ee8e8-4d6a-4558-b143-621cfd55728c"}, "c9c2b5a6-8b09-40ba-ac0f-b2a6873f5f23": {"doc_hash": "3be0b42916ed68b31f592385e2c5b9aaf74b99ab37a70e501f53c53b16664b1f", "ref_doc_id": "381ee8e8-4d6a-4558-b143-621cfd55728c"}, "e58987c4-bd55-4bf0-bb09-1698c797a463": {"doc_hash": "d3b2f988a6ede2eb558eab53553d2703ee3e43096808515c01d041e841a36687", "ref_doc_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460"}, "0e90a33d-f58e-4365-a8a9-4ffa41e09c95": {"doc_hash": "c284589570f68a43f745abde60a0ebdac3cfbb5bf6b619af0d3f4f8b64928d6a", "ref_doc_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460"}, "e5bbbc6e-ba7f-4ffd-a27f-c7606dca596e": {"doc_hash": "b40023c0fe1a3fd176e7b8872af8036701a46b6d7294d7d2ee00c9cc1be41bbb", "ref_doc_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460"}, "77e00ea8-8fb5-4044-91fd-7b108537e696": {"doc_hash": "bf3777df857681a33c3915572d2ba0608a557201169a931fa5351735775bc0b2", "ref_doc_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460"}, "afeebcea-17f8-4375-bf26-5a290876fe4b": {"doc_hash": "e48d2e31b94aaf985dde26b9365c57ea82b6f086ca79bf9968b94305633f7803", "ref_doc_id": "56d2b4a8-ff0a-4335-939d-9aeb172ee460"}, "cbc3690b-efdf-4a99-943f-660533e7b5f9": {"doc_hash": "8edc8e65283541f8c15f5ecce593f0b9e2b2228dce4d420f53679e94054d3b16", "ref_doc_id": "4d4afe67-524b-4449-a53a-5b1d6ae6c70f"}, "32985451-0f54-4e07-80c7-552d7245cb21": {"doc_hash": "6e1a5a210dc7d842d97c68742a1cdfb9ee154be44ce2d8b8d5d9f15e7e2f84b0", "ref_doc_id": "c8034349-37aa-434a-baba-a1780c471578"}, "82a40930-4bf1-484d-8e80-27b16b533ce9": {"doc_hash": "c3fab7c99fcaa1e6924d82710f438cb1a5b46b08f11ddd7c8353261eafaa9e7c", "ref_doc_id": "c8034349-37aa-434a-baba-a1780c471578"}, "cbb1b4f2-8ae6-42b3-b5a5-32c0fd1a45bc": {"doc_hash": "4c977108c3535d2913c7e9f1c7b448f2d8043b89fc8b530eac08f82f82ba2c71", "ref_doc_id": "c8034349-37aa-434a-baba-a1780c471578"}, "401146da-c25d-4ba5-ad60-1e09269f229f": {"doc_hash": "ad0a048b31dfa982b05f70d4998785d5e24708a8bedb3a284200f1244c6b2c1b", "ref_doc_id": "c8034349-37aa-434a-baba-a1780c471578"}, "cf048c23-77df-48f8-b313-a9669d8d1d1c": {"doc_hash": "a8320f9c6c05fb94f30817a491c529d5fc38ddcea3f49ffca8a3bd6fc1e5c2d2", "ref_doc_id": "c8034349-37aa-434a-baba-a1780c471578"}, "1691a577-07ca-4fb1-ae78-7496bf7c34f2": {"doc_hash": "5152f74bb3173eb01ee428cb6017b0d6fd1cdc9e2995df6a3d7df4421d5a47a5", "ref_doc_id": "c8034349-37aa-434a-baba-a1780c471578"}, "0ec3eea0-5965-4197-86c4-c6de0f49eb59": {"doc_hash": "016fa8a704ebeaa45b595a36bbe1b9d94b0ffe29f004eafa485e71a61313cd8c", "ref_doc_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e"}, "b4d029c3-4be1-41fb-b582-9b251b13ce44": {"doc_hash": "bb607afb8ab9e4c79a6f9644d555f42617a7b6173500270a2801506da8dc44be", "ref_doc_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e"}, "3ed89d36-44f5-49b6-b641-ec69f55c4c45": {"doc_hash": "a2912157e1bc157e274d9de5f00b6f9f1bb535d826099043061d1adce90e4b0a", "ref_doc_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e"}, "d3afa8dc-ece3-4538-859d-2395de671669": {"doc_hash": "fb7bb6e405e2dac3f190c757accbfe4e8de75e9d4fc5b225c5e1fe7d625e20ab", "ref_doc_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e"}, "284e6c20-913d-45df-943d-2415e097f426": {"doc_hash": "61c247edd1304705944f140830d9a1d9e9bea189fff7831e14ce337d565ef1fa", "ref_doc_id": "c2effa91-65ac-4805-976d-d7b90f4eb09e"}, "fb895d59-7721-489d-8f8f-a2433ddff395": {"doc_hash": "3a87a652a6f86075d377ce9afc2ccce3b28bdfcaffbe872dce1c347d018b12f6", "ref_doc_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60"}, "e905510b-f520-4148-ac32-cfde6e07b550": {"doc_hash": "6671f6f4a810ac84da61d0269bb1e30d5229215a50a58043608b430e970fc719", "ref_doc_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60"}, "e7fdee8d-fb8b-4b70-88d7-28387deff320": {"doc_hash": "4d7e1a39e009e85483a05336a17d42c29f88e56145e0203d53721033c59a77b3", "ref_doc_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60"}, "d97b4cae-b4b0-4f06-a7be-1069f141ac10": {"doc_hash": "a1b1aaf1104de8f0bfa389a154d097aec46b0cfa48570c87403bc96b16c6acf2", "ref_doc_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60"}, "dec47915-f50b-44a2-be1d-c858cfbae020": {"doc_hash": "c3171852d81b620af7e61e3eeb8c6fa8ddafc17b0f9dcfbee952350af953a8f5", "ref_doc_id": "64e5a052-01aa-40bb-aa0a-d181d256bf60"}, "476c0ba3-36ff-4975-a779-221211e47d3b": {"doc_hash": "7fc2dfa101bea461f9d42ba6b5081e56374181aa78e977c8627aaf7190dab339", "ref_doc_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0"}, "6448425b-2559-4546-8595-c3b65dd0e3e4": {"doc_hash": "0286e9e3d50cd3d87b69996feff4938ccd8fdfcdff7d4511d1eb04f082addb5c", "ref_doc_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0"}, "7a35ffa7-9401-4a09-98af-479d127abace": {"doc_hash": "3ddd875a82ae87b6dacee134dfac0046dc0fc56803b10cda69076fb4a54f5115", "ref_doc_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0"}, "c1814bc3-fec5-4b96-82b3-676e2b6b0347": {"doc_hash": "af2299a56891f2f862fccb94236f7866a0f5c59ef49ba60e0f5f0b1b84490e4b", "ref_doc_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0"}, "c7191792-a0cf-4120-a368-bbd2841d09a2": {"doc_hash": "83ca15596edab9d4a4e4fa43e7bcc948adb4d49d2e4f199086788cd0f6913299", "ref_doc_id": "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0"}, "0f91f59a-b476-412b-9245-f889cbc210fd": {"doc_hash": "f96435e0fc6aced74c8216eab27599c4dced159add03bf04dae32d2430393afc", "ref_doc_id": "be6792a4-da55-47a2-b289-7b1b5869811d"}, "60d49ff3-1b7b-4a96-8e1f-afabc2cee86c": {"doc_hash": "242fa1918ffb0da90b5b82992a5e4878ed0839be50c891420bee4863cbe09834", "ref_doc_id": "99895f25-3030-456c-a22c-ea588744446b"}, "02c2a2a6-761e-4211-bfd5-86c926d63d22": {"doc_hash": "dbdd689ba75bfdd2280cc0d59256b3918b8eb1e8240b75a88b9ea05f218205b7", "ref_doc_id": "99895f25-3030-456c-a22c-ea588744446b"}, "749657c9-194f-478d-b477-5fa7311abb91": {"doc_hash": "8edefe3efa2b413f716f0df12a5a3daa56a1c34812b2309e6bf3e96a2178e828", "ref_doc_id": "99895f25-3030-456c-a22c-ea588744446b"}, "02ca04f3-521e-43e5-8af5-2acd94eed1c6": {"doc_hash": "09815c57fa0b93b03b3d2afa08a681a76e740d5cbbd73ac4a269f79f02a8fea3", "ref_doc_id": "99895f25-3030-456c-a22c-ea588744446b"}, "0c4d1346-cb20-48bd-8b99-723773cb2c04": {"doc_hash": "464640c70193231b16ee89b72e5fbd7f8e746a1f99bc02e0b9ec8c04a71dde62", "ref_doc_id": "99895f25-3030-456c-a22c-ea588744446b"}, "280e9ac2-9291-48a8-9bf0-2cbb0074a5b2": {"doc_hash": "3b39c5e6ce412a9abca8fc16f6cb3b40899ca3563cd916833c688215bc969554", "ref_doc_id": "99895f25-3030-456c-a22c-ea588744446b"}, "9f60617e-1a2a-40e2-a93c-1dc9b6ab3bb4": {"doc_hash": "e4be48d349161dfcfb7e9ddb57c73f23fbe67cdf8afc366a4df412f0d7e81b65", "ref_doc_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5"}, "3dc9760a-cda7-4939-8472-c521a7f46e66": {"doc_hash": "fcd46a50fcef1ddbfdd3e2b562f84c6a8692fbfea8bac39ea1ebfdf67970e8db", "ref_doc_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5"}, "894a1219-36f1-4b31-986c-03bc089e1262": {"doc_hash": "de1d4b99c10db9aa551edd6112c6ea4ef5cd7a3bd8089bf226f53f9f7bdfdeb8", "ref_doc_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5"}, "33afe81c-7321-4870-b761-ee193aa81ad2": {"doc_hash": "2b43f7fd4bbc49c5c11bce548c27d32b4e777aac016348e4a783bccf50e0bc91", "ref_doc_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5"}, "6d1760c6-4e1b-40df-9be3-f2f77ecd10fd": {"doc_hash": "50e827384787f4f73008f65caef654179164d6ecb748091579c0581712b234af", "ref_doc_id": "5f1a8ce1-3de5-4339-af32-d75e11da34f5"}, "d9b944ae-7d4c-4001-9ede-131a7bae71ed": {"doc_hash": "e14a8fc720d36aaf7ebaf31e04a2330924feb61e5ec30599b093090550d214a1", "ref_doc_id": "e2690e2d-178c-4921-a734-f9334530f2e6"}, "e6ec21f9-4ef7-4c2f-9981-ebb4ea136d25": {"doc_hash": "c87f7538e91a66d29f4e65c5c400e389214e34bcc169fb018b5051b089013b82", "ref_doc_id": "e2690e2d-178c-4921-a734-f9334530f2e6"}, "e300f2d2-8346-4926-9e45-958579e50006": {"doc_hash": "6a7c02ced3fd4ac1205faed566ef63c127c203b47ba1cec9c0dc3a954355a87c", "ref_doc_id": "e2690e2d-178c-4921-a734-f9334530f2e6"}, "61eeeea0-f16d-4f99-861b-ab4e26d8f559": {"doc_hash": "e2e7fcd8947c9df3c2ea51aab17d9f5d0bd7ce6b71d0d262a20b1ea65e57c59a", "ref_doc_id": "e2690e2d-178c-4921-a734-f9334530f2e6"}, "cf9e4349-a22c-4b71-b1f4-43964496d78d": {"doc_hash": "ea193ec17f467678656fd450d8273954166a37c7dff016e229440b011ec23f24", "ref_doc_id": "e2690e2d-178c-4921-a734-f9334530f2e6"}, "92eb3717-2b38-491d-baf3-6c0ff9b342e7": {"doc_hash": "b8e0c472b81098d79b399af1a212347f5ae96ad64c2652b5dee776368aeec2c2", "ref_doc_id": "d88f6d7f-5944-408b-bd56-1536e37a36df"}, "3d25af56-930f-403f-8f20-f02c3a197bc7": {"doc_hash": "eef2f65a892389cd1e9b956738993356488fc8ad644dd1fc6ea75dce8ef8ff24", "ref_doc_id": "d88f6d7f-5944-408b-bd56-1536e37a36df"}, "5414bd03-37a1-473a-858d-a56a90cd023b": {"doc_hash": "379e713ebe54fdbd426cd7c7ee6304a56209e04313ba11b9c925945aa7c8ca07", "ref_doc_id": "d88f6d7f-5944-408b-bd56-1536e37a36df"}, "8e7379bb-e063-4df2-8867-a18129fcda65": {"doc_hash": "be9119a02e9d2969b0135681db778caa41f5b3a7f4deb0e4e4e11118aaee5598", "ref_doc_id": "d88f6d7f-5944-408b-bd56-1536e37a36df"}, "abf97515-a241-4826-aae1-39007c84f619": {"doc_hash": "756e1017767c48066deec1d3e8e016d8f66895d088a09215e924c76dba96e7f6", "ref_doc_id": "d88f6d7f-5944-408b-bd56-1536e37a36df"}, "4acb61f4-1b83-4081-9d70-8affb4aa5e0c": {"doc_hash": "d363c37c0208a96b75cdf2c8c5083629ba0e987c7fb6517541f69fbf03d4867c", "ref_doc_id": "3592c0a4-8fb2-4091-8a28-67a42ea67b94"}, "4530412d-0461-4aec-8c2e-94451c75012d": {"doc_hash": "ae2cb352b8a9a5b6a37bb9a45fdbd374d533da31d0634b37b84c984be158a440", "ref_doc_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d"}, "0e1b6ac2-7a00-4b97-981b-5fc244c31c5e": {"doc_hash": "796cc5569f7b79f17e863cb81c44dd080744263907b9b900251d4c9fff9c50c2", "ref_doc_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d"}, "045dbdf7-80fc-4477-9cdd-fe8ba3dffe30": {"doc_hash": "db6d4dbab74a08a5e05a2caf0c64a41251f0051f7446415202216221934bce42", "ref_doc_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d"}, "f9735bdd-5304-4a97-9c17-ebaf054f6327": {"doc_hash": "4a6ccc1997816d156334f654f25e0bd57f793ede736e3f22d03a2d5844eb3b0e", "ref_doc_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d"}, "19526a39-41ed-4d66-bb7a-142aa63cc92d": {"doc_hash": "040830d10fababd13698456acb44eb487bca10f3b354605bd93448d378206a39", "ref_doc_id": "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d"}, "7b9fbb45-d937-4b73-a820-43f0c42e1d9d": {"doc_hash": "abe78ff677d5d2c4973ee48b761aa71102ea35ec9a4edb501adcd551f115bc81", "ref_doc_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a"}, "e5ab0743-bfe3-4e21-9e1b-3113313cb509": {"doc_hash": "7ddafac6cdd173978a7f489d4d9bd4dc8ca2c4955cc3746a23219de709851f9e", "ref_doc_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a"}, "4041d27a-1bdd-45ff-a06e-af9f1e2f8b98": {"doc_hash": "d8cbe2d91a7c82a65608b245827df267ceb113705fb9d98880f941550e9a03c8", "ref_doc_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a"}, "cefee77b-cebd-48e7-a40a-9c1f7c770a2b": {"doc_hash": "995f0a9f1316a9dc1420e1b4e77e03e8c616a3ad556862778514c819d2edadc0", "ref_doc_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a"}, "50ce162f-34f2-4eda-811c-936d27b9fb31": {"doc_hash": "b3168d27d763c71b013c14ba72d78326d639ad460b2f2afc750b798b102db7bc", "ref_doc_id": "cbdf320d-0210-4751-bc8e-420cdcb3c46a"}, "1a369f9b-0d6f-4bbe-9c0b-fc461b2629a8": {"doc_hash": "2241eb8ba9fbe7c1fc140f0461669da8bd0095626d26d246ccb691a6df0e5244", "ref_doc_id": "eb544f90-4fa9-4e51-9b79-d85553445142"}, "22e7c4c1-23b8-4bb4-ae60-589ba5c3d7a3": {"doc_hash": "7db6672cf96271b26bad2242b171ec987ed22abb86de121d54cac6d6b82f4e23", "ref_doc_id": "eb544f90-4fa9-4e51-9b79-d85553445142"}, "691ba182-53b9-4313-9afc-5c68f3bfb72a": {"doc_hash": "b81ddc7e1d9920321aaf31e9b029618e53314c01d589f49270cf601f0cb98cd3", "ref_doc_id": "eb544f90-4fa9-4e51-9b79-d85553445142"}, "0567f877-e780-4e95-a4a9-238de74ab8fa": {"doc_hash": "2ab0ba8a9e45669097f5dbaec7306fcbdf937cfca7513d37cdddbff8d8b45e89", "ref_doc_id": "eb544f90-4fa9-4e51-9b79-d85553445142"}, "fe3b96dc-a5d8-4db6-9f81-0c2bab67df04": {"doc_hash": "e4fc90625dc3578202c399fca6ea56894a2d8d85bf0fa1c8ab71f41e30d5250a", "ref_doc_id": "eb544f90-4fa9-4e51-9b79-d85553445142"}, "8dbfd7da-3238-4854-8260-ae26e9f8190d": {"doc_hash": "6cdf8def1341f4a14a5bfef8a35884e5690195447b399c386f528728919c0864", "ref_doc_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3"}, "83b74235-3739-4666-9e38-f63ed2c59679": {"doc_hash": "fc75bc8f6cbd4323e33271d315807750b35481ba08a9c0eb577694f585121312", "ref_doc_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3"}, "e688435b-aaaf-4141-87e8-f1e22fecb5a6": {"doc_hash": "d20b7023b89841ef24f0bec946084949fa971a3bd849d37250dbceb014a45811", "ref_doc_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3"}, "ac07f681-2700-4b32-af6a-a4597a472e52": {"doc_hash": "5e839c61ab73b9c5265881ee99630c477ca003ccbf54646d98f82db2fa0dd7ce", "ref_doc_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3"}, "9e12552e-e877-4bb0-84ae-6c51230f6ad8": {"doc_hash": "921fd551544d041ad3c293aea7109b6ae8f1c7ef39bc70ab8e194cbe80fe0271", "ref_doc_id": "ed003dcb-d68a-41b8-88ef-93e05e14cee3"}, "0de97f25-2e9e-4277-9c7a-25200581dd02": {"doc_hash": "1cea245af87ea4c2c0f46e0709808b42ed0bfb92ef38349f3939fecce04411e0", "ref_doc_id": "87de77de-8c9a-4dc5-987b-0dc007f852a7"}, "710a03ff-d5ec-48fc-bb24-240df35a4ac8": {"doc_hash": "8682a7d99b5b1ed95823e92a26c87cdb76b67db2ede71a6adb89122de519e5b1", "ref_doc_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab"}, "25839326-3a8d-4ccc-bfd8-f565f47279db": {"doc_hash": "a36bbdda813e718fbde1f001e6b5bfd3f5ab2e5d1ccc70c92aa998800e5c543e", "ref_doc_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab"}, "3551135e-b305-4d1f-ad3f-740b6e77e337": {"doc_hash": "7bf987c4b8fb0a7d480d24cf0c53f4fc31d506fe7713a833ca31712cd59d305c", "ref_doc_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab"}, "ca24b290-c3e9-4f9d-912a-914981fc368f": {"doc_hash": "c128cbfca05c7ca28b713879b0a9b437579fd2b75867d0ffabd4973579c544e2", "ref_doc_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab"}, "c091822f-cf84-43c4-912a-f283e9175500": {"doc_hash": "b6c6136f6c1d3b5b265e2c1e0ea846eb144283e68f089251b111209ff0ec99f9", "ref_doc_id": "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab"}, "978a5215-1623-4a61-8feb-9255f630d968": {"doc_hash": "128ace82b6ae2563c7a381efeb4fbb6624ccacb97311a365bce02e073c1f601f", "ref_doc_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086"}, "f126c652-bc73-4eae-90a8-9e0fdf8e0aa7": {"doc_hash": "c3839d871f54de06c2a76bc8a3eb9cc4526d9b321cce19a8654890a5f4a60ced", "ref_doc_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086"}, "7d4e9a1a-35c4-4795-96f8-97258f6ae872": {"doc_hash": "c383037de9ecff9b72bb6cddc18dd274e506c81fcd081b5919146741709e6520", "ref_doc_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086"}, "0932b200-84c6-4488-bfc5-2396cd5c9b95": {"doc_hash": "cf9c85b69747bc4b53da10ddab70e413839c435a1ab4dd5df6f68768c47472b3", "ref_doc_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086"}, "759f6c8c-8c8c-4cad-a6b3-a5984bcc99d5": {"doc_hash": "2136eb08e48e73e21fe2062daf9e91b8273cc994dbc7f57b7759fada6378125e", "ref_doc_id": "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086"}, "a043749e-7ef5-4b9e-aeb1-6890d7db8c8d": {"doc_hash": "fbbeb9a212c3f4f49364acb3a9eba0c5bbd72e63a8fb61d3324a6916da91bb0a", "ref_doc_id": "1ceb9831-1954-4b8d-a153-1a613db8408a"}, "fb6f45f0-2d5b-41ac-9bfd-8950cde6e539": {"doc_hash": "86e4f89610c7e3535b341ebae1ad370be8bf7ce6a888f135c323c73ea208fd29", "ref_doc_id": "1ceb9831-1954-4b8d-a153-1a613db8408a"}, "3077a316-a55b-413f-9eb6-138dbe78451a": {"doc_hash": "2c71fd75f5b6d894bbe32eab16472a1741facda3bc0bc5e60a16c91c6d83c10d", "ref_doc_id": "1ceb9831-1954-4b8d-a153-1a613db8408a"}, "03f038cc-c3c7-4d93-91da-42d3daa5a276": {"doc_hash": "06dd0c1be81a8bfbab95f733151d92c790302e982e3bab37cb514ae9bea4098e", "ref_doc_id": "1ceb9831-1954-4b8d-a153-1a613db8408a"}, "6575adcf-4350-4942-834d-e5fa50c56dd1": {"doc_hash": "5a6039f86ea79bced90d965f8685dc525aad4dd7a9c2eedeaafd5f97495220f8", "ref_doc_id": "1ceb9831-1954-4b8d-a153-1a613db8408a"}, "0fbf824b-c772-4db5-b5fa-ef25a6ab47fa": {"doc_hash": "af3a2449cf55f4b962a86b0e5675b18594cef35a79afe06b94acc77f291806bd", "ref_doc_id": "9c62f556-ced5-4c6a-8278-780ade4f4643"}, "cec0d187-467b-4b39-9224-7ca575532cbb": {"doc_hash": "9a9344b675485242253c35109b798d6986e8a79712271ea270e2495f86921f06", "ref_doc_id": "9c62f556-ced5-4c6a-8278-780ade4f4643"}, "b2f6ade5-eaca-44b1-af47-fb893e56fa1d": {"doc_hash": "225257a78780b4879ed5be2d5ddec36375a986deff3558cd3c9e5afe06806331", "ref_doc_id": "9c62f556-ced5-4c6a-8278-780ade4f4643"}, "6ea88205-e0b2-44cf-9548-75623465ac25": {"doc_hash": "888afc592a16b6180226fa6e42d716f2662fe7d039489c45f3becc4e656666e9", "ref_doc_id": "9c62f556-ced5-4c6a-8278-780ade4f4643"}, "113fc76d-16b2-4b81-ba17-6abb5a94f511": {"doc_hash": "257e8d2008a1e29564a58175d585c69ab6279e4f4cfce81ba8f838a13e99344f", "ref_doc_id": "9c62f556-ced5-4c6a-8278-780ade4f4643"}, "e687371a-384d-4866-bbed-8676f1b0a469": {"doc_hash": "03c166355cfcabed5a64a146b3e7ef2187f51a7bb239fe34b196ef1cdfcf55e0", "ref_doc_id": "40459286-87b0-4707-b53e-47333e6c46d3"}, "dd6520ae-784e-4cca-9dbf-a7a4daa3831b": {"doc_hash": "87af8aef80571944f8d4745d45cbb599ca7c0d86c1a2433dbaa098d509a8d94a", "ref_doc_id": "40459286-87b0-4707-b53e-47333e6c46d3"}, "7ced47e8-c50b-41d1-9e66-05758ec081c7": {"doc_hash": "82d340f379882ea34926b5b83ac223777c323ab548df5bb41f3d93e9dae525d7", "ref_doc_id": "40459286-87b0-4707-b53e-47333e6c46d3"}, "5478d272-b834-4aea-b564-51d14663a9e3": {"doc_hash": "91899be3da3e19910bbc91cadb966b3d5486e4f05c54dc2f536e2efcfd2994be", "ref_doc_id": "40459286-87b0-4707-b53e-47333e6c46d3"}, "2e51a6da-ad2d-453a-bfe9-5c2c4fc099da": {"doc_hash": "1593eff9db65962ae3c58ff20c4d01235f1b4d597bb3fea27990036c3ccfceda", "ref_doc_id": "40459286-87b0-4707-b53e-47333e6c46d3"}, "68e6cf96-535c-4e35-ac82-1e87ef0e7c77": {"doc_hash": "895087b36c2e9880905458ebe4e6847e198c7f48851cbcdec837dd8cb700cec9", "ref_doc_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de"}, "04b6beaf-8804-4a39-9022-0b734e37e3a8": {"doc_hash": "5cefee2f7197dc42816f47d743c8cebb3b40797cf4ca13fe97833f58feb6cd8b", "ref_doc_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de"}, "fedcb3ca-1430-4371-ad76-fc95ec58436f": {"doc_hash": "10d2e443e04e8b7f2b7380c20f3121ebb8cedd466df183dd8693f52b6a8db9e8", "ref_doc_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de"}, "6f374abd-ef51-4f8f-a1da-f098442c5477": {"doc_hash": "9e92bd3b9337b987ffa729df8aef5a0b7d38d316e2bf222405e8f24e8ed41b11", "ref_doc_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de"}, "981cb88b-4244-44d6-a938-3aa5646afe1d": {"doc_hash": "b7098b5232963024be15a4d4b1a4e59e97678966cad9ef2227bca4a44454177e", "ref_doc_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de"}, "dfd31fdd-e9a9-4dab-a171-abb51b1b01e3": {"doc_hash": "dc8aaa2901a56043184176caa0df80ec6aa48c90174ce09e8c13da928d1b34c0", "ref_doc_id": "c2c1168f-777a-4fc0-965c-9ce04fe596de"}, "f0b5557e-d64e-44ba-a140-95adb1f5dfbd": {"doc_hash": "2f0d5bb64538c3dc0d9d82db5eb56041b9af6b28ff96ea525a9193f102e0ec8f", "ref_doc_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23"}, "528e2c6f-bd10-4799-8f30-9da620bfa474": {"doc_hash": "a39560e3eb4e7d1e0c3e78ce1542cb9290915fef8e27df389806183b9ff531ca", "ref_doc_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23"}, "8901e7d9-2c64-4f66-b5e7-84bee0d8cbab": {"doc_hash": "72b8ca19f09ee16ba22b64ec55eae35eec56c9d2cef23c5318923e6b6b83b06e", "ref_doc_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23"}, "8c245c70-d293-4329-bf96-74de97202104": {"doc_hash": "fffb6554c2c675722446ebc493afebfde76e754f47831d2786ca3d97665692e2", "ref_doc_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23"}, "488f77e7-8395-4fc0-b72f-91c002ec5a4f": {"doc_hash": "cc7d985edcd477bcdbb0ebd21f1d2800106095479c22a6c6833961e52b24f030", "ref_doc_id": "2600ede9-5e4f-4920-99c6-a34d23a10d23"}, "1e703c2b-dba4-46cf-8ded-6734778d1d5a": {"doc_hash": "d844c544b26f303eed76930fbe0f0d359d0469eede8e1f3fb98e92e4109569f9", "ref_doc_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8"}, "3ac57fb1-b698-4d77-b606-aa7ec2dde7bc": {"doc_hash": "55cb36b23880a53e722c7e524a501507fab326f9e42e4fa40dde0b2c89ed33da", "ref_doc_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8"}, "5221a53c-2c0c-44db-8829-db95e656070e": {"doc_hash": "b080948b3ede0ac5e0e92dabee796077f7c390758f9948c28fd0a3dce7dbf31c", "ref_doc_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8"}, "cc43bea9-8e24-4e6d-9241-e795f49eb960": {"doc_hash": "f3832e65463c8ed6b6d493856bf09835dd6fa7e6f864ece74cba14172708f6d1", "ref_doc_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8"}, "e6947143-c9bc-4abe-9c9d-79a5fe1e7f8b": {"doc_hash": "f03ead6bf70d24c0db8c9a736e0b53c2b0276168cb84342e26ab62593d4db33d", "ref_doc_id": "af068cb7-b5e2-45f7-819e-91abfb0c0dd8"}, "a7dec7ad-180d-4584-aba7-1440a17e4926": {"doc_hash": "1c7c217317296557c05da625f77cc0c9529a58c4230c59a18a99f88ff08fe221", "ref_doc_id": "b2b79e9f-0df5-46f7-b817-a49c288a6411"}, "1f3a2651-8e67-4f13-b519-4fb05133d539": {"doc_hash": "df56259fc0d40d1b77dc1eac26c524023327b96d67363bf07004bb33e60d14a2", "ref_doc_id": "3b862e92-4358-47af-950d-ebbb46d47f6d"}, "7ab974a1-c2d5-4f5b-ba7f-7cb8895494fb": {"doc_hash": "2521929a6f40b4bdb1fe61250b278fac3f35e880df83ab5e0f092eeb0c504945", "ref_doc_id": "3b862e92-4358-47af-950d-ebbb46d47f6d"}, "3daf5567-08fc-4895-9fc4-a5ddd2814134": {"doc_hash": "da05fb640e2428ef73a5f2dcf8e208b19e24f5f5f39778e6010d087e1cd8fd91", "ref_doc_id": "3b862e92-4358-47af-950d-ebbb46d47f6d"}, "515b74fb-2324-44c3-bd65-4fb244f6e74d": {"doc_hash": "75f2d308eac87f9e29d3d3a24a6aa8be868a72e30cf3b1a30cadf415e7f25997", "ref_doc_id": "3b862e92-4358-47af-950d-ebbb46d47f6d"}, "a4af6706-eb29-45ef-b74d-d7de98e6c324": {"doc_hash": "f5356c305bd9ede44480cbf292b58f3594d28989d3823c65fe9b8a26a1c9f0a1", "ref_doc_id": "3b862e92-4358-47af-950d-ebbb46d47f6d"}, "2f4221be-6049-4901-8efb-e694d6bf02d3": {"doc_hash": "5ec09b093f3b6b6ab1d21c44c2cc76e7e59ee729cdeb4d1e8e52c5eb351196a9", "ref_doc_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69"}, "ce5eeb5c-6acf-4e7b-930a-a01bbad620b5": {"doc_hash": "3648978ac2cd3e3d1f759bcfa6ced7d418797432a9bbdc3b867201c3a78e4da4", "ref_doc_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69"}, "cb6869a9-dce1-4e01-b5f5-ab824ac0c734": {"doc_hash": "0c3eedac0486cfabe18c592fef3f7b353a44feec5286614ab7a0b58824261e93", "ref_doc_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69"}, "d4eed92e-1087-49a4-a506-5e1c481ca07c": {"doc_hash": "938e50e1d287bff1ee90f2f42c7099ba96aecbf80240efaaff4af1d05db3ba20", "ref_doc_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69"}, "d6078340-927d-4d92-b7c9-fd480fb21ce4": {"doc_hash": "4b195bc9bf2fd8e04dfb0e3b5f7ceddf9fbb40deffcbf83af2b677f720e01433", "ref_doc_id": "453b70bb-0eba-49cb-b453-f5e535d0ef69"}, "d5d3ba2f-8b66-4e08-8aaf-5ece6ba7cf19": {"doc_hash": "8b714012ffb895a7b06bb6e0345c6c61df4fe702a3c7a24b89209d1b957abf7b", "ref_doc_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8"}, "f4317992-1e40-424e-8538-d9ad7e83c557": {"doc_hash": "f5b2d4b4be2c9562849c991a94f84bcb1c3fca254e320b22e3526cb8a2ad023a", "ref_doc_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8"}, "e7ea3364-fb19-4d7e-baa9-ea9b311062e0": {"doc_hash": "b97b335fef6dd815fbff2f8cbd6bf8b4d73a2fb1c65116096af0d3012d06e40e", "ref_doc_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8"}, "0a07698f-dd77-4a84-a079-4546508d342a": {"doc_hash": "2dda67a7b890fa906707af3141ecb84314bb10db8a59b929b909f5a7f0be1851", "ref_doc_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8"}, "848f1cde-102a-4379-ada5-a556f70b4b2b": {"doc_hash": "6d1114011a1106eceded9c900877c93a6a1d3cc3a1797fa194a358940c91b055", "ref_doc_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8"}, "a1157e1f-ad11-4ca4-9184-987e74252bae": {"doc_hash": "ee4e1b792d4171e086d0235b7b2ffc7ea240936beb813429c737c7f2ffcc6cf6", "ref_doc_id": "93e520eb-8d02-43c4-b427-0a8ba52aa2e8"}, "317486b6-b619-44da-b962-cbee9a56f424": {"doc_hash": "76590c98284586dee98cf2ad0f5a22ade960808cf462ab763c3de0211a6b5a7d", "ref_doc_id": "db514a78-d261-471f-8efb-8514117e6f4e"}, "b2b58adf-a680-4275-ae91-7be5c85e20a7": {"doc_hash": "b25ea2969b66a8fd75bb1ceed26c4f1cce59f24f62a1e89e7d15ffe28ac7a2f8", "ref_doc_id": "db514a78-d261-471f-8efb-8514117e6f4e"}, "236a59e2-54bc-4212-a555-5ed4d2bce5c0": {"doc_hash": "1183292c66a8eb663f72dddc27f2a0b9f766082cfcd86b3ccb422b6ee8daf1d5", "ref_doc_id": "db514a78-d261-471f-8efb-8514117e6f4e"}, "787786c1-3692-41ff-aaba-cf5b6b1c3e57": {"doc_hash": "327c353a260d80583864df84f0830ed5bfa4b5d7a76e8e616b49b5f657ba6535", "ref_doc_id": "db514a78-d261-471f-8efb-8514117e6f4e"}, "089dcf6d-5b10-43cf-9ea5-1dea702c7438": {"doc_hash": "f2a6a931615fc6906432bbe4c85bbdee305a3ea8aef2b7471bce942f95f4518a", "ref_doc_id": "db514a78-d261-471f-8efb-8514117e6f4e"}, "94d116e3-498d-4027-89f8-a5254567289c": {"doc_hash": "29be3e881568d726fbf2459b3f5c8063aaf55426eea9a659531cae58f1f57c27", "ref_doc_id": "9278baa9-0cba-447f-b50c-77148e46b013"}, "78085d7d-4ea8-45e2-9aee-16f04562ec6e": {"doc_hash": "f765b393ec1faf2e6cf4a0fa1053401bde53029b79c42ea1efff2b6c3bbaee73", "ref_doc_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8"}, "4b46f14d-21db-4664-98ba-8af0ed85d3ee": {"doc_hash": "0e3b0c278f9b142b890269cb5fd2d3e6d2d522d1387c3b904022f52fb0e2e3db", "ref_doc_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8"}, "4930c5a3-29a2-47bb-bd40-8bae48133f38": {"doc_hash": "87e54c77c26f8671e26ce5feab1e894a573ee11e436db9488a51e71f8d5f7700", "ref_doc_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8"}, "1034634c-de88-4479-b5e0-5222c05e3d8b": {"doc_hash": "ac0162af9ac43490214d70974394e0fd821b517f64ed5a95ec81fd40c47cba1f", "ref_doc_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8"}, "8a76ebce-b70f-40d6-ab38-599e67907623": {"doc_hash": "5ef82369c213bb59ca1ce528702c523e591cdf7564fe7dc68076942b7010ef1f", "ref_doc_id": "3caf8b77-3b63-4094-aee3-964c13c6c0f8"}, "be0e2e87-03db-4fa1-8995-760715058d4c": {"doc_hash": "068dc30bc98aa568bf15ee519f219abb956711d691e7c3f8ddb632a39405530e", "ref_doc_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03"}, "83de8f90-ecb7-4af1-889c-436add074d8c": {"doc_hash": "2d62cf3862b30951c57ac1d83c0e2b8371a5de387c8f8fcd17baa5b6f27c9e99", "ref_doc_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03"}, "2b8bb825-14cb-4e09-83a1-e561d198f0db": {"doc_hash": "b0964e6394cad3acb35ab2e1c357ddaefeafb92228ea5e0f9c110039ee57b831", "ref_doc_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03"}, "eea9192d-fb03-4210-8a8b-4f4422ced3b3": {"doc_hash": "ef934910724b5ea4ca55e31e62ff250182d6b12bc5eb6e635a319767c6f46b2d", "ref_doc_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03"}, "d9c00693-e096-43b0-9234-78e0bb1e380e": {"doc_hash": "001ae1d1b6916cf1d6b95233f50099b663812c53fac4d7b7a594c640f7083fbb", "ref_doc_id": "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03"}, "6e31af5c-b9e7-4a58-a655-31047102064b": {"doc_hash": "d84a4fa0aa5d5dfaf3467309165bb22c19101c2ed6dec85cb948cdc96c82bf34", "ref_doc_id": "ea3280e2-9061-4d36-86ab-2bc4d70fce08"}}, "docstore/ref_doc_info": {"adb9d1bd-1928-401f-8716-0ea1b42f3e59": {"node_ids": ["a4213a0c-8fd1-46eb-aafa-d22b7215bb4a", "78b629a5-d948-4752-953c-92ec8f2abd64", "4961f468-f454-4ff5-8b6c-07d1984842c8", "801c3840-c975-4417-9efd-ba90e44414fc", "2c96feaa-3fa4-4003-80fa-069f0af17fb7", "0eb84439-aaea-4a8a-b8d2-504a38ad5eef", "f170389f-7074-4cd1-a5c2-dd67b644a58f", "d2915bcf-d475-4bab-9409-6cf973c97c17", "d55f2bc4-1980-41a0-929f-f90452cd71ca", "9cd5886b-518b-42b6-80e5-8fcaf57365aa", "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e", "041dcbc3-b10c-426b-995b-38234f2d3a38", "4de4eb18-c462-40ee-ba75-65aad1ab0a9e", "3090aed2-bf72-47d4-8ba2-f82f70cef882", "143faccb-70a4-4f32-99ba-66824f74c219", "bc72ac09-e3b2-4896-9bf4-18aedb7738da", "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd", "26864623-e72d-4b91-b583-8d12ace4d8e1", "436bed26-cd7a-4c41-ab9b-76184bb157fd", "2c4ac0a6-1a71-4196-9877-d2964b0f3744", "22825eff-8ad1-4d03-823a-8e232176aa19", "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf", "4d9297b7-b236-40fe-8cef-bb5cfd5f196f", "c03adbde-2467-4504-9b19-72d0e86c8cd5"], "metadata": {}}, "a4213a0c-8fd1-46eb-aafa-d22b7215bb4a": {"node_ids": ["2587bb96-4640-4265-9076-9e1d531a2381", "45966987-f98a-421f-9271-5c503d67344d", "08040b3d-2224-4c75-a034-cfb9aa9f9b61", "6e438b00-2436-4718-b18e-6835511b6962", "f7399abd-cb1b-48f9-886a-8746f5f1e173"], "metadata": {}}, "78b629a5-d948-4752-953c-92ec8f2abd64": {"node_ids": ["cc987eab-006b-480a-8593-97ad22ce1f35", "b1743ce2-002e-4688-b43d-d0294a9d9f23", "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1", "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43", "4f5a5cde-c754-4888-a5ed-2a21ee98be71"], "metadata": {}}, "4961f468-f454-4ff5-8b6c-07d1984842c8": {"node_ids": ["37fb7373-7684-4059-9b8a-f119e12a9483", "240d64f4-947b-4a30-8a40-4c32f3f3fb31", "2415d406-648d-488e-adee-8bd0fb74a5f5", "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9", "9d81e32e-a897-46af-85a9-02e278286cd0"], "metadata": {}}, "801c3840-c975-4417-9efd-ba90e44414fc": {"node_ids": ["59f340a8-342e-491c-94b2-43642ff312f4", "70f446ab-c0c8-4500-a291-c9562ad75f60", "b7706ee9-3add-42ac-8e9c-799cc02542a3", "331dde64-1f43-487b-a9f4-dd1be4acf4a7", "d78ca6ea-0bd4-4a06-a4de-9956656051dd"], "metadata": {}}, "2c96feaa-3fa4-4003-80fa-069f0af17fb7": {"node_ids": ["9dc57153-6fd5-499b-a634-fc574002380e", "633f7b61-b5f8-4142-8666-0a0ba4b804a8", "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b", "fac41315-4d32-4359-b216-55c151e8b5e1", "638f46aa-98ea-47c6-9083-3d5c75cc98b4"], "metadata": {}}, "0eb84439-aaea-4a8a-b8d2-504a38ad5eef": {"node_ids": ["980c530c-1005-4c72-8d3a-68472f1c9de1", "89183965-5983-4d40-83bb-bc6c1f2a2d8d", "6d7bfce5-d142-44e2-9d76-7001709526dd", "e288a398-5ad7-4fe0-9e03-11b716299770", "b833d9b9-cff4-4e06-9dba-20efb87d02d1"], "metadata": {}}, "f170389f-7074-4cd1-a5c2-dd67b644a58f": {"node_ids": ["3307b03b-7942-4e2b-a3ea-a083b2c85f81", "5665018f-a756-44be-adc2-ce77d463ad0e", "c7471598-449f-4ce6-870e-1a9e120e6ccd", "51582689-3bf2-4ba0-b051-10e732354bda"], "metadata": {}}, "d2915bcf-d475-4bab-9409-6cf973c97c17": {"node_ids": ["cdd04060-a8e4-474c-a338-f65f89616e95", "69604801-7765-4976-81e7-231241d2b2b1", "54f4b631-38f8-41b7-98e8-f94f97d23d56", "4a77345a-c539-4264-a387-adf9022090e0", "c4884c63-ab65-499b-a999-c681de0dfcc4"], "metadata": {}}, "d55f2bc4-1980-41a0-929f-f90452cd71ca": {"node_ids": ["b3acbe4b-37d3-4ad7-8129-81790271bbfc", "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2", "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2", "9a68a403-80ce-43dd-b2dc-b6a49addecdf", "ad848f9e-6e82-4e71-92f7-1a534a8a184b"], "metadata": {}}, "9cd5886b-518b-42b6-80e5-8fcaf57365aa": {"node_ids": ["3cb750da-ee3f-4d5f-90eb-46f05cfb3546", "bb5f8140-17ea-42a8-b530-a5aaee35caa9", "92c16379-d8bd-463f-9c60-ec9831a037e6", "54d96d23-c6e9-45cb-960d-1aedfdd1b580", "b7b4af96-3f92-422b-a14e-63fbead4b498"], "metadata": {}}, "ddd39d80-0bfe-4998-9e6e-3262b0f8c87e": {"node_ids": ["d9ef127c-8f59-4316-a43a-a91acee1dc89", "3263c986-bb7b-4f7f-8240-c0cd405dec60", "da514732-a3cf-4a31-a994-8bc6710f22ec", "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f"], "metadata": {}}, "041dcbc3-b10c-426b-995b-38234f2d3a38": {"node_ids": ["20062614-22e0-4695-9dae-521fdff26f79", "f056fd99-2019-42e5-9b42-718302e68fe2", "32e72675-2960-49e4-afd6-5052ce1b0802", "dc06d373-fc7c-4d14-b2cb-95d390096739", "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca"], "metadata": {}}, "4de4eb18-c462-40ee-ba75-65aad1ab0a9e": {"node_ids": ["b646a538-109b-47c0-ad7a-ac2283530d47", "fc9ebb92-7688-4767-b8f6-7b76327405bc", "fc9fe157-ab09-4904-b323-e8019ece8204", "d4055d84-e057-42ea-901f-7ba1753df337", "c98e98ee-03b5-4bf9-a652-999317a0b671"], "metadata": {}}, "3090aed2-bf72-47d4-8ba2-f82f70cef882": {"node_ids": ["25335f95-389e-4dfd-a165-a8c49e4ad787", "ff14e27b-df47-4028-8029-4a6e19eb36aa", "46dc9b82-6c13-4332-a2b3-4aa89903801f", "668623f4-21a5-4b59-b6ac-e78759e6b403", "c26f6c18-927a-4e11-b542-6fa85387b872"], "metadata": {}}, "143faccb-70a4-4f32-99ba-66824f74c219": {"node_ids": ["ad7a59b1-43f1-4324-99d3-262cc9769b05", "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4", "ce7a1637-31c5-4726-9393-835a332f13f2", "001f02ad-43a4-4105-9aab-1eca82b49991", "45593217-9f9a-4741-a8d5-1545ac7e429e"], "metadata": {}}, "bc72ac09-e3b2-4896-9bf4-18aedb7738da": {"node_ids": ["19780472-c500-4db8-9fe0-a3572c15efe9", "d6190d24-8b42-4a04-b7d0-7020e7d7af23", "17e36111-0937-4893-91bf-173e818746db", "3b9d1983-3c1b-423b-b132-1100f72674af", "e209caf1-ead6-467f-9555-1a2e27ae1c9f"], "metadata": {}}, "11fcdedd-ffe1-447c-b629-8c8e1d2df2cd": {"node_ids": ["b9a22696-df65-46d8-9ae7-bc9360e912fb", "6fa08b5f-e917-4b42-9ebe-3e710e3c170e", "381ee8e8-4d6a-4558-b143-621cfd55728c", "56d2b4a8-ff0a-4335-939d-9aeb172ee460", "4d4afe67-524b-4449-a53a-5b1d6ae6c70f"], "metadata": {}}, "26864623-e72d-4b91-b583-8d12ace4d8e1": {"node_ids": ["c8034349-37aa-434a-baba-a1780c471578", "c2effa91-65ac-4805-976d-d7b90f4eb09e", "64e5a052-01aa-40bb-aa0a-d181d256bf60", "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0", "be6792a4-da55-47a2-b289-7b1b5869811d"], "metadata": {}}, "436bed26-cd7a-4c41-ab9b-76184bb157fd": {"node_ids": ["99895f25-3030-456c-a22c-ea588744446b", "5f1a8ce1-3de5-4339-af32-d75e11da34f5", "e2690e2d-178c-4921-a734-f9334530f2e6", "d88f6d7f-5944-408b-bd56-1536e37a36df", "3592c0a4-8fb2-4091-8a28-67a42ea67b94"], "metadata": {}}, "2c4ac0a6-1a71-4196-9877-d2964b0f3744": {"node_ids": ["7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d", "cbdf320d-0210-4751-bc8e-420cdcb3c46a", "eb544f90-4fa9-4e51-9b79-d85553445142", "ed003dcb-d68a-41b8-88ef-93e05e14cee3", "87de77de-8c9a-4dc5-987b-0dc007f852a7"], "metadata": {}}, "22825eff-8ad1-4d03-823a-8e232176aa19": {"node_ids": ["7e38e9f9-1c19-4994-9ebf-7a7f025e98ab", "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086", "1ceb9831-1954-4b8d-a153-1a613db8408a", "9c62f556-ced5-4c6a-8278-780ade4f4643"], "metadata": {}}, "6a9a87fc-ee75-42b7-9306-89b5ad8d63bf": {"node_ids": ["40459286-87b0-4707-b53e-47333e6c46d3", "c2c1168f-777a-4fc0-965c-9ce04fe596de", "2600ede9-5e4f-4920-99c6-a34d23a10d23", "af068cb7-b5e2-45f7-819e-91abfb0c0dd8", "b2b79e9f-0df5-46f7-b817-a49c288a6411"], "metadata": {}}, "4d9297b7-b236-40fe-8cef-bb5cfd5f196f": {"node_ids": ["3b862e92-4358-47af-950d-ebbb46d47f6d", "453b70bb-0eba-49cb-b453-f5e535d0ef69", "93e520eb-8d02-43c4-b427-0a8ba52aa2e8", "db514a78-d261-471f-8efb-8514117e6f4e", "9278baa9-0cba-447f-b50c-77148e46b013"], "metadata": {}}, "c03adbde-2467-4504-9b19-72d0e86c8cd5": {"node_ids": ["3caf8b77-3b63-4094-aee3-964c13c6c0f8", "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03", "ea3280e2-9061-4d36-86ab-2bc4d70fce08"], "metadata": {}}, "2587bb96-4640-4265-9076-9e1d531a2381": {"node_ids": ["af53148f-7cee-4843-ad16-bf3bb0321242", "8d19f54d-598e-4495-8f5b-08c18d8e9da1", "e33ebd69-c0b6-48d0-a472-cc8f6eeba54b", "ea666fa4-20be-459b-881c-fb0a89ce9d54", "e04f5524-c742-40cc-a345-c5fec7d32d50"], "metadata": {}}, "45966987-f98a-421f-9271-5c503d67344d": {"node_ids": ["af6b7a8d-77d9-4a37-8955-6eb0c86e7669", "dd22f499-9d78-4f2d-ad79-a53ad1f46361", "2fbcab7a-aeb1-4153-ab79-2b54badcec49", "33c2d5cf-ea46-493f-a3e7-c702f5cc33af", "e89ba5c4-3f8d-45e0-b6cc-e385486d7eff"], "metadata": {}}, "08040b3d-2224-4c75-a034-cfb9aa9f9b61": {"node_ids": ["2ac6f711-0a4c-4e61-907b-5adb6944be21", "5639ad72-ec31-424b-982e-a75e3975ca22", "988d88de-c8fb-44a0-bfcc-d9d9ebb6f2ea", "3db498c9-6714-4117-8544-f0cbc7c7ba24", "a9c019c1-3346-44e1-89aa-8b9ce920f21c", "2b2e91e1-6e16-4f33-9934-a2862d8270d6"], "metadata": {}}, "6e438b00-2436-4718-b18e-6835511b6962": {"node_ids": ["461b359e-1bb5-4912-bc7d-3a73a3c00b25", "2fde5e6b-04c9-4ac1-bf71-09a1c76b804b", "32a36560-4b02-4f0a-a52e-a1ee6003e14b", "f3be1b9c-cdfc-424a-b473-b1897295dd57", "30557630-9b86-470e-a715-5603ee609c82"], "metadata": {}}, "f7399abd-cb1b-48f9-886a-8746f5f1e173": {"node_ids": ["09fba7a0-ea1b-4ddf-991c-663c646563c6"], "metadata": {}}, "cc987eab-006b-480a-8593-97ad22ce1f35": {"node_ids": ["9b298a12-d1e7-402e-ac06-abf397573cc9", "4b602e68-392e-4e7a-9923-1045a4070a95", "11ccb175-7cb7-493c-9654-5ff59910f24d", "f750e47e-2259-45fc-a520-500bf6a0d0dc", "8a702c1b-9836-4918-beb5-5562d850aa84"], "metadata": {}}, "b1743ce2-002e-4688-b43d-d0294a9d9f23": {"node_ids": ["873991ab-7ccf-4af5-bb28-c4b73edd709a", "65a5d11a-33fc-43e0-9aa2-49305de58cfd", "566ee55e-c714-4b11-a525-4fca02f448fd", "7ff83fd5-cee9-4712-8271-478f9ddb56cc", "e2ad5fef-179a-446f-a02a-c82560506f03"], "metadata": {}}, "9206241a-3f2f-4c0f-aca0-79ecdcffb2e1": {"node_ids": ["2f7fd9ca-0cf7-4fb7-9873-c9b592738bdc", "843191dc-4cee-4491-b706-4214fcde7797", "195f3a73-cdcf-4c70-a5b2-1c2cf4b85211", "5596ba26-93d2-4d1f-8f45-2a37cacbbd60", "e4d4f399-d234-4f87-a944-243f12a6b60b", "b3b0c525-5be2-4ddd-a29f-c9a87792531b"], "metadata": {}}, "4c7f3b6e-ad2f-4923-aa6b-dd8a148d8a43": {"node_ids": ["52c28400-6e09-4062-b489-6291b73575ec", "e9b29251-dbf2-403d-9baf-1f9dbcca521e", "56eb2784-6b7f-4a78-8646-50d89033c3fa", "9395cc04-3804-4a99-b757-9923150dc466", "86fc9790-90b1-4c30-a828-0b1d798b9b7c"], "metadata": {}}, "4f5a5cde-c754-4888-a5ed-2a21ee98be71": {"node_ids": ["1c73d00c-a783-4a14-ae37-a8248bcc62cd", "9147c0b4-957d-4bab-b394-3aec4a36c6f2"], "metadata": {}}, "37fb7373-7684-4059-9b8a-f119e12a9483": {"node_ids": ["ab9b36f9-5822-4d55-8dc2-a0c2334f7256", "4a547e43-731c-454a-9b43-aa6128e5d71c", "02f8004b-01f4-49f6-a3a6-adc81e2de14b", "d0a2ad27-0fd6-4daa-b109-1ccbbad1a3a4", "b600cf7c-debb-4303-a317-d75ff0849894", "ee606a3f-6129-4795-9c3f-d7c743aaa714"], "metadata": {}}, "240d64f4-947b-4a30-8a40-4c32f3f3fb31": {"node_ids": ["edf2a2b8-ee8c-4db5-95c2-41153e5155e3", "ad84ebeb-7552-4a56-a95c-9bd66c84eb7b", "7adf1354-3259-464f-bed2-aeeb85ded3a7", "d2228bbc-835c-4746-9b2a-fa52a330de44"], "metadata": {}}, "2415d406-648d-488e-adee-8bd0fb74a5f5": {"node_ids": ["c5823942-5724-49f0-bdcc-35b8eea56ee0", "b66452e2-8ce6-4fa9-9050-d4e0f3728888", "d95c5819-7658-4df8-8296-49d7e5cf845c", "0200ef30-c427-45b4-a9d9-64b6cbc20ca6", "98cbfd2c-a158-4fba-9f6c-1a9279934bae"], "metadata": {}}, "f0f37bde-b5b8-4bb7-b246-f204d5ad51f9": {"node_ids": ["d96d7a26-cf40-465c-a85b-e3e272e7b8b2", "725f8aab-d383-442b-a039-3f59785fa12d", "b270e70d-0842-46d4-aec3-b2325f853782", "1fdd4dd2-7a43-4965-ae4c-ef91448aece7", "0a35fa84-92c0-4504-bc57-83425e0b900b"], "metadata": {}}, "9d81e32e-a897-46af-85a9-02e278286cd0": {"node_ids": ["75700ca3-a4eb-4c65-a733-d621db10c1a3"], "metadata": {}}, "59f340a8-342e-491c-94b2-43642ff312f4": {"node_ids": ["c450e73d-15e3-4bdb-a091-1ec7e6dca295", "07038048-4e61-4bd9-8db8-e5de3caa80bb", "70e6d4c2-9123-48b5-9496-517b95606d04", "fab5a902-7a88-4285-acd8-0558cd7a3974", "647c2fe0-ab0c-45db-82b0-9af9eaf4feac"], "metadata": {}}, "70f446ab-c0c8-4500-a291-c9562ad75f60": {"node_ids": ["cfeff0d1-aadf-4d52-8c3b-0e986b4cd6ce", "20581bbc-4949-4403-b086-da6b3edb5099", "9d6adc2c-4482-41e2-8db4-1d7c490f0a90", "b70cee44-ce5a-4788-8ff4-c52b0d7696f2", "f447e72d-4b10-4ac6-92c8-2e00aed7fe48"], "metadata": {}}, "b7706ee9-3add-42ac-8e9c-799cc02542a3": {"node_ids": ["446508f1-2908-4bc4-a220-f7638b3b0714", "c7bcbfc6-a287-44da-824a-07ea4bcb9eba", "bf28dde5-d99f-4207-a394-11b22ff17b04", "d2fae188-76b6-4255-862b-8c0d22114254", "5157b639-edab-4c47-a9c6-771b6464b8a3"], "metadata": {}}, "331dde64-1f43-487b-a9f4-dd1be4acf4a7": {"node_ids": ["e910ae6a-e737-4702-b02a-6eb60c5e82fa", "f4617e78-1587-42e9-baff-320cf53f0c30", "7182afb7-8ad9-4544-96da-88dd70391ad6", "d11a8593-e158-4491-a23f-387fa20a2949", "67354aca-f02d-489b-8076-768319861122"], "metadata": {}}, "d78ca6ea-0bd4-4a06-a4de-9956656051dd": {"node_ids": ["521bc758-b6b8-4767-ba22-0b5c7695f7e6"], "metadata": {}}, "9dc57153-6fd5-499b-a634-fc574002380e": {"node_ids": ["0fce0132-01b6-44ae-a579-03dddea37351", "ff9fbb87-c761-46f6-a6ab-81f552c06371", "833e21b8-65c9-44c2-980d-9da78f2499ec", "06fba012-1729-4a78-b242-4b7f23225683", "4947b6b4-0407-4482-a4c2-6ede5a612a61"], "metadata": {}}, "633f7b61-b5f8-4142-8666-0a0ba4b804a8": {"node_ids": ["93d495a1-fa6d-4e6b-b58e-d71d21fc6dcc", "cf11fe80-d25f-479b-ad30-4a45fd1b4ee4", "916c7dd8-b06b-4893-8ee9-4802ad8ba187", "c3c637f9-57fc-4689-b0ab-401bf0f103cd", "b5c1bab0-af3f-4195-9a99-494366bdfaeb"], "metadata": {}}, "285f1ba7-5827-4ae3-8ca2-6042f0e41a4b": {"node_ids": ["5291ac61-aa0c-4268-aa6c-4431e2115530", "db893a9b-7adf-4d69-a247-e0187299a6a5", "b6cb972f-9382-4be4-8251-374170f5deaf", "e8bbd7aa-500f-4460-b15c-c8959ea97ed7", "bc6d7f07-411a-40b5-b47f-5332e2253a43"], "metadata": {}}, "fac41315-4d32-4359-b216-55c151e8b5e1": {"node_ids": ["236dd4eb-04a5-45f2-84ca-1d849255c593", "c1887afa-cd93-4a27-b62f-d7f1d01e26cb", "037972d6-86da-41d6-85fd-6f03cf5d8c9a", "071b41de-61a2-4b47-8be3-d14fd05f5538", "e656c762-fee1-489b-8bbf-5f3172ddcef9"], "metadata": {}}, "638f46aa-98ea-47c6-9083-3d5c75cc98b4": {"node_ids": ["90b32c62-1f04-44ba-b29a-0ca24bb90c12"], "metadata": {}}, "980c530c-1005-4c72-8d3a-68472f1c9de1": {"node_ids": ["fec8fb2e-3c48-487b-b5b0-a4ab6a3a5aa6", "427f2851-d74b-4810-b773-a43b53845b08", "d005b6b4-fbc3-4538-b88d-ebcc6675bb46", "6a816ebb-a509-478b-bd71-41524128c6bd", "57928373-8fa0-4601-abae-1b8a20f7afc4"], "metadata": {}}, "89183965-5983-4d40-83bb-bc6c1f2a2d8d": {"node_ids": ["9b6bcd96-26f3-4e02-9ee0-d0b553dfbbbf", "521b5c8b-39fc-4b8e-bdc5-fc37e17b6a92", "f400ddea-f8d1-45c4-b737-76842703e6f5", "19e0ec7f-0645-402b-a264-668c85a59adb", "d7938e9c-de67-4c48-8b5b-475d32455dd5"], "metadata": {}}, "6d7bfce5-d142-44e2-9d76-7001709526dd": {"node_ids": ["1458b347-0390-4066-8436-60dd46fa3e53", "ae376509-a845-4656-bc3b-84889c1c4046", "cb450b82-3b7e-48ac-b189-52e905a1d67b", "a349c88d-5d16-4818-8c37-f9d948c59dd5", "7bbf6e82-0859-4e81-8ca0-06d8aee22e61"], "metadata": {}}, "e288a398-5ad7-4fe0-9e03-11b716299770": {"node_ids": ["1fac2605-63aa-4450-9179-db26c337f758", "ea7e8084-0269-4117-b8a3-23557924e586", "5fcedf03-b441-4c81-a2f1-d9291f04c901", "84264c77-f9e8-43b3-ba41-6723f6b6c82f", "3f595dd6-042a-41e4-95d4-e25bb8d30bf6"], "metadata": {}}, "b833d9b9-cff4-4e06-9dba-20efb87d02d1": {"node_ids": ["58196cf7-31c3-45cb-b305-47780d51c9a2"], "metadata": {}}, "3307b03b-7942-4e2b-a3ea-a083b2c85f81": {"node_ids": ["fc1b0850-0803-4987-819c-9783a3f90423", "dafdee97-421f-4159-ad03-fe6c19162347", "0980800e-b33e-4cc1-8f7c-f0de094be475", "9732bb2b-0e9f-4259-9387-38c9d98380ce", "70523c54-2fb7-42c6-ba94-f375b1199c89"], "metadata": {}}, "5665018f-a756-44be-adc2-ce77d463ad0e": {"node_ids": ["925a1d28-1f61-4d13-94eb-e10d7f7f9962", "b8e0d74c-77c8-4bd1-b552-ef3c563f3ed8", "ae23095a-71b8-4464-861a-106ac4195c21", "23e2260e-43be-456f-912d-25e99d0379cb", "f14e060e-bf7d-4077-af80-b992fe7858d5"], "metadata": {}}, "c7471598-449f-4ce6-870e-1a9e120e6ccd": {"node_ids": ["33406e86-99a8-48de-89db-58f1a36281d2", "a4e3f0d5-3d58-4b4a-aefe-64c3b61670ac", "3c090fe5-243a-41c0-8129-aae9e995378d", "3984af1d-e971-46af-8596-695647deb311", "0abdbd95-fcbf-4913-89d8-f527efd2400e"], "metadata": {}}, "51582689-3bf2-4ba0-b051-10e732354bda": {"node_ids": ["6e6fced2-1c4c-44c4-8d7c-2f6fdb25a3a0", "f9f5e60a-8be2-4228-8bfa-5d35ca444603", "9f04f21a-584f-4e6c-9df3-42e9a2f38966", "f37a12ea-f7e2-4d3b-8761-660a17587794", "99734d75-ed29-4007-a8a8-dc4f79d9e8b6"], "metadata": {}}, "cdd04060-a8e4-474c-a338-f65f89616e95": {"node_ids": ["ab870226-7a14-42b6-9fee-e893c73c5cd4", "90c41dad-e5cd-4732-9250-448c021e4383", "15ad2d42-ba2f-4e70-9b47-27a0ddbd9339", "47cdba86-01b2-4134-8ce2-89a9a558fc6f", "803a9f00-6ef8-488e-b428-cda0027f4d4f", "3a7c4090-0eb8-406d-97aa-727111851701", "2ae6e793-db56-4b3f-a68e-609335a5b269"], "metadata": {}}, "69604801-7765-4976-81e7-231241d2b2b1": {"node_ids": ["df0e1575-9ac0-4490-b8af-d43a8fa606ef", "a1409477-4e3c-4538-96b0-ad47279e810e", "f3d3599d-8950-4295-8694-79a0ddfd1125", "83ae85ec-a12a-4af1-8b75-888ba8a4b1a0", "180aee7f-e742-4903-bab5-cfce033587e9", "51619bcb-760b-4af8-8b26-ad2d6635661d"], "metadata": {}}, "54f4b631-38f8-41b7-98e8-f94f97d23d56": {"node_ids": ["31f544b8-fc3e-4aab-aa56-68a1f20f7e14", "1047af7a-6186-49ad-a0f0-9b3504bc95d4", "fbfb8209-388f-4094-a987-e33a058a0252", "16b516cf-f294-4811-84de-614fd3732235", "a664023c-ea54-4d03-805e-474d4063dd8b"], "metadata": {}}, "4a77345a-c539-4264-a387-adf9022090e0": {"node_ids": ["418fc8e2-aaf2-40b9-9c67-7e349f169503", "de618738-3540-432b-9a94-c7d107d99047", "9072dec7-70eb-4adf-a06d-8419b381e147", "f4debacb-72ef-420b-80cb-f38869c8f88a", "8aa981c8-f72d-4180-b9f8-c1019163280e"], "metadata": {}}, "c4884c63-ab65-499b-a999-c681de0dfcc4": {"node_ids": ["72b96e4e-d8af-49f6-b47b-1a08f90868d5", "54b25020-b744-4a8e-b370-c1ebce85705e"], "metadata": {}}, "b3acbe4b-37d3-4ad7-8129-81790271bbfc": {"node_ids": ["b9d7ac54-078c-45bd-b1d4-8dfa6dbf170a", "e1e5279b-e384-4417-b2cb-d9c34616ccca", "2d1e4e84-60a7-432c-bca6-5189663bcf12", "bbbda80d-54fa-48fe-a793-c1fb0f178cd6", "d05fe151-9ada-49db-8ce4-4086284632a5"], "metadata": {}}, "d9613bd5-dcbd-40ba-aab7-dfc66cb635c2": {"node_ids": ["8d2dd5cc-61b0-40b5-917f-d7371b591334", "e4876034-8e45-4364-b80d-3e066f5fe57f", "a6fba84e-9aff-44df-8957-f463106d4046", "1cc459b3-d325-40e8-9cd2-2a80a7a7f523", "dfabe07a-608f-4828-813d-97c83b376819"], "metadata": {}}, "8dedc0da-4961-4c9d-8c9a-70c34c94d4a2": {"node_ids": ["dbc3a6be-152f-4360-a086-e317b1979737", "5206c7a8-aeeb-4e49-b845-ef739b76e3ce", "50e13e9b-86a6-4bb9-b538-3c314a67bc03", "3a7ebacc-2913-49d8-9dd6-45bcfebe24f8", "92637d5c-b17b-47a1-9264-bf112dbb6030"], "metadata": {}}, "9a68a403-80ce-43dd-b2dc-b6a49addecdf": {"node_ids": ["ed5469d4-5c93-486d-81d8-c7f56922eb1b", "206281fa-552c-4bf2-93dd-014d41c7ff14", "4a7f8864-6fd6-44db-bca1-9b388792455b", "00ca55a6-99e0-4f3d-8c8d-cf2f350b020c", "69d976e7-b806-444e-ba71-9e7eae0820b8"], "metadata": {}}, "ad848f9e-6e82-4e71-92f7-1a534a8a184b": {"node_ids": ["f21507d8-c965-413c-939d-44f3e5f1c095"], "metadata": {}}, "3cb750da-ee3f-4d5f-90eb-46f05cfb3546": {"node_ids": ["936de599-ea95-4d78-9c3f-1c249d80d80b", "f30055d2-7f02-460a-8448-94843c6757af", "c99b4aba-fa2d-455d-9d84-77c3d20635f6", "39fab97e-52f2-4e71-88f2-fe89f43d9e96", "27497f07-deb3-4d74-bcb8-7865ae6f8fca"], "metadata": {}}, "bb5f8140-17ea-42a8-b530-a5aaee35caa9": {"node_ids": ["398b9c48-d6a9-41cb-b4c5-d800a081bd55", "278e7c07-9191-417a-9fc5-452bfcb6b8ef", "692f0364-6df0-4243-bd64-3a5b65ea337c", "acbc0f4c-614d-4494-b95f-1759fde42b04", "6847293d-9f0b-427d-b17d-187f2b533b44"], "metadata": {}}, "92c16379-d8bd-463f-9c60-ec9831a037e6": {"node_ids": ["e5a0007e-aa29-4be4-8b2c-177b5707175b", "27d32337-66ea-48ac-bc2d-dad3f1ad9f00", "5a65a88d-bb2b-4c2d-9a66-95d2302cc0f2", "dedc7f2c-f064-44fd-84bd-3f4c428591ad", "3759a715-0879-495a-b493-c4b225e963b7"], "metadata": {}}, "54d96d23-c6e9-45cb-960d-1aedfdd1b580": {"node_ids": ["bd1a0ed5-3007-43f1-98ad-4201cb85a042", "133d75a8-9182-42a0-999e-0436128e7872", "3bf2fb29-9f66-46f7-859a-6d90dfc99792", "e20b5f0d-7df5-4c8e-932f-7d9935f2f2c2", "c2fba76e-41e1-4e4e-ab13-1b470c8216da"], "metadata": {}}, "b7b4af96-3f92-422b-a14e-63fbead4b498": {"node_ids": ["a33b256d-abe8-4813-9b29-7f34c07e22b2"], "metadata": {}}, "d9ef127c-8f59-4316-a43a-a91acee1dc89": {"node_ids": ["a726854f-f8a9-431e-829b-30e4742f7d46", "450ef87a-0957-4826-83a2-5d9d8e4773c7", "9b24883c-5a4c-4f3b-be38-9c962ce7bc1f", "e862db03-86f6-4080-81bd-d15c046aa835", "843259ee-797d-4210-a4b1-8ec05c4e6bc7"], "metadata": {}}, "3263c986-bb7b-4f7f-8240-c0cd405dec60": {"node_ids": ["8f8e59fd-24fd-49fa-8626-d7723415d3a5", "c144f42f-9652-4ab8-b893-b42cbdcb8a98", "49acf9e1-1fa1-40e3-9801-a7f64e4a132d", "d641491f-61ef-4359-b9fe-9dfa8bd9cb8d", "aff25138-c9f0-4611-bb45-175028a2ca57"], "metadata": {}}, "da514732-a3cf-4a31-a994-8bc6710f22ec": {"node_ids": ["733f45d2-d38c-4127-a959-e19802493591", "173fb9b0-9e9f-46a0-9829-c989e9918d6e", "914f3071-6fb5-478c-acf8-1ec4f4529e08", "14912092-810f-441d-9cae-bb9025dfafb1", "3220a14f-bb5c-4a16-9a37-6c03ab5029db"], "metadata": {}}, "dbd89bcf-3a28-4eb6-bf90-3c4743a74f5f": {"node_ids": ["e3480fc6-2d39-40e2-be85-87c2c822cede", "8995d339-b032-4fa5-8f3b-43650975ed6e", "661a03b4-e664-4d71-88ee-a259b4da4183", "2988ad8e-3321-476a-a8f9-5a2bbc50646f"], "metadata": {}}, "20062614-22e0-4695-9dae-521fdff26f79": {"node_ids": ["3b83c4fe-27e8-4f03-bc18-81e067a18526", "93b3e0da-3e52-4b92-8a34-ab241bfe5838", "89fb7b72-777a-4cc7-837e-9f5b47dabb85", "bcc25b7c-9372-44a8-94ca-19e7319a9be9", "9ab130ed-3f3d-4917-b41e-406b1094c588"], "metadata": {}}, "f056fd99-2019-42e5-9b42-718302e68fe2": {"node_ids": ["70f5ba7c-7a51-4706-a4cf-fd4b7aadcaa6", "47039706-e2d3-4000-ab1b-9d20e70132ee", "976714c0-e4d5-43a0-ba7e-0fab9a51bcff", "075a15af-6fe7-4aa3-ad2e-93674b13240c", "68a5291a-3462-4b62-a5ec-ebe9bde1e764"], "metadata": {}}, "32e72675-2960-49e4-afd6-5052ce1b0802": {"node_ids": ["90051f7a-8d7b-40ad-87e1-292a767de344", "b489fbfc-5190-4afb-a1df-d181d1dfe24a", "5813aad4-2cc6-4ea7-8a51-b5b7c512d119", "da33621c-3be9-444b-8b72-86b42cadccd9", "8d88553b-27a7-4aec-9b20-866a54191319"], "metadata": {}}, "dc06d373-fc7c-4d14-b2cb-95d390096739": {"node_ids": ["ddd5a5e0-ea32-47c9-8629-7b251984223f", "49fb7aa6-8e4c-40de-b2f3-cc2a83b89252", "44b33897-061e-42bb-aa09-288cb0daa2c4", "90470b79-c441-4938-a09b-0bed43975300", "a06ca537-930e-469e-8b48-e7a30a705046"], "metadata": {}}, "4f5603bf-2fbc-40e9-80dc-8cbecc07b1ca": {"node_ids": ["397e8007-44af-4d39-827a-d0c027215c52"], "metadata": {}}, "b646a538-109b-47c0-ad7a-ac2283530d47": {"node_ids": ["f2b2e28f-36c4-44d8-8e1d-f200916d2b17", "302545ac-ad76-44da-813d-2c45ea69dcb2", "4f3cf145-b04e-4360-857c-a098a8f9d4e9", "205bf3a5-3e9b-4f95-a13b-506a843bbb2e", "fd6e7113-d963-43a0-a7f5-6ad2a59683af"], "metadata": {}}, "fc9ebb92-7688-4767-b8f6-7b76327405bc": {"node_ids": ["7a227faa-e6f0-439d-86e9-94aa2b4ba4c7", "5216234a-e278-4358-b80c-1309c790e54e", "a7cc3307-ed1b-4e3c-93ac-40875c2d3796", "b5e16604-7e38-4ab2-8e19-a62413f43be8", "659bcfea-85b8-403c-bc6f-60b89fc4ffcd"], "metadata": {}}, "fc9fe157-ab09-4904-b323-e8019ece8204": {"node_ids": ["777cf26e-c814-4055-ad5e-732d45dba164", "ab6eb898-4f1a-4cb8-8ea8-e3b33547d7e3", "fdcb691e-6311-4440-a5cd-ae83b3f5a07a", "67eb05b7-2887-4807-a2d6-8703d4e2af90", "d886886d-ff01-4653-bb1a-adaea32bfb9d"], "metadata": {}}, "d4055d84-e057-42ea-901f-7ba1753df337": {"node_ids": ["9a5ec0e0-ea1e-478b-9d66-97f0fb1cfca3", "14288937-76a6-415e-b127-6be38d179ae5", "ed353fdf-d90b-4d26-9c0f-227c3236f79d", "8633af80-72a5-42dd-8ad4-362e972bfc14", "c488c930-29d8-4119-800f-e2a195f83bee"], "metadata": {}}, "c98e98ee-03b5-4bf9-a652-999317a0b671": {"node_ids": ["b8a28e70-6de3-4ea3-b8bb-68c535a48178"], "metadata": {}}, "25335f95-389e-4dfd-a165-a8c49e4ad787": {"node_ids": ["92be1258-6cac-48f4-9fc8-c2e56846fb94", "bdc7067e-8cb5-4467-8e3e-90737e05f7f2", "04338204-b1ff-477a-b016-8e7de89b26d5", "67cd8374-8b19-4e27-b88d-659eaf35c055"], "metadata": {}}, "ff14e27b-df47-4028-8029-4a6e19eb36aa": {"node_ids": ["7aec402f-4bb8-4eb0-b627-ed7989264302", "b429fff9-0da0-4633-af64-90ad7e215920", "13274157-7229-45b0-8997-6fdb2dbae214", "f567d5c9-2dc1-47a3-9193-897d26055ee0", "a396e262-669d-4c9d-9a1c-17da560b9118", "a1817d0b-a1d0-4e69-8562-0a506d10f3e7"], "metadata": {}}, "46dc9b82-6c13-4332-a2b3-4aa89903801f": {"node_ids": ["6760dec7-64fb-4236-8b3a-0d596faa42fa", "4d643c3b-afb5-4c96-be4e-f5d35e44f22b", "139b6acd-1363-4fe5-9b28-dc397c0a70e1", "79244573-93ef-474d-b526-9fad3bb61d58", "50351b61-7460-48e7-ab37-0b865bdee1f3", "c2bb9ba0-e566-4971-82e0-7da09e2666fe"], "metadata": {}}, "668623f4-21a5-4b59-b6ac-e78759e6b403": {"node_ids": ["12baea84-1c2e-406e-9fe6-d6c5ae40afe7", "a2b02493-0913-4aa1-a942-76a5693942e6", "b2d1d2c9-03fc-4370-8b49-bf0a6482fc52", "20beb164-b808-4fab-bdb1-2cab9205485b", "45e7963b-3f3f-4503-beab-9d52b2e33541"], "metadata": {}}, "c26f6c18-927a-4e11-b542-6fa85387b872": {"node_ids": ["0d99a087-25e4-4697-aa87-74b61e82a6ad"], "metadata": {}}, "ad7a59b1-43f1-4324-99d3-262cc9769b05": {"node_ids": ["bb13df0d-57d8-48be-9f81-b00f3aa17fe0", "a77cb3bb-d25e-43d3-a9c0-d530dd22739b", "3194219b-ebcd-4142-b47c-5fa24070e90f", "31fd32b6-a023-49c1-8fa4-a4c977281d19", "f967a8e1-0805-45a8-8ffd-8652afc40ca8"], "metadata": {}}, "82f4d5b4-fd51-4bf1-8bfd-5e8280010bb4": {"node_ids": ["e34bb962-915a-4424-974d-2c772aee08b6", "fc7c7c5e-5606-49c5-abc4-f7ae526fab8f", "e1613544-b38c-4ed0-9a78-59133c3529f4", "1a866526-497b-49f3-a6da-5c3d580b7c85", "217c9a59-07ca-468b-8329-6c3f7031478d"], "metadata": {}}, "ce7a1637-31c5-4726-9393-835a332f13f2": {"node_ids": ["a8767cf3-fa04-46e0-85e9-f36df1a1f878", "4c8c9fd2-3b8c-4a7a-ad1e-a1b74840dc1a", "00eafba3-f472-4835-ab19-ac519d4aa3cb", "2117da92-d682-475d-aba8-a71d376f683a"], "metadata": {}}, "001f02ad-43a4-4105-9aab-1eca82b49991": {"node_ids": ["7b311141-51f6-48e8-85f4-19dcb6789636", "f2b30c75-d798-43b7-a3dc-3876e3820cef", "0906a26a-d932-4a45-b2fd-6b5d80b92412", "dcd8e83f-99ae-4dab-adea-b18713a1e027", "5093a13c-9d13-46ed-9d22-057236d90bde"], "metadata": {}}, "45593217-9f9a-4741-a8d5-1545ac7e429e": {"node_ids": ["479df2ba-adbd-48c0-912c-45351d9583ca"], "metadata": {}}, "19780472-c500-4db8-9fe0-a3572c15efe9": {"node_ids": ["30edca9e-602b-4751-8f1e-8ceea3fa7144", "e9ac09f7-abbd-4291-8e1b-59826aa8e9fb", "9f025190-0e23-42a3-8fa2-bad898a7321e", "d6e7f471-d1a0-48aa-93cb-990c54902330", "a7f52bb1-c615-4d48-a1fa-281c34570180"], "metadata": {}}, "d6190d24-8b42-4a04-b7d0-7020e7d7af23": {"node_ids": ["d643aa0b-a69d-41b5-8731-a0b676b53177", "52d9581f-cd10-4219-850b-7cfa60631540", "51acae86-bf53-4591-8137-987b5f3a0c6c", "f75ff35e-1ac0-488e-829c-f62aa23907e3", "a91d0b66-8351-440d-b6f8-5bce59073802"], "metadata": {}}, "17e36111-0937-4893-91bf-173e818746db": {"node_ids": ["b5bc3b1a-4bf1-4261-a969-6e7883ff5c4a", "a23e8c3e-903b-41c7-a0a0-652cbcebb4a1", "a4081b4f-d08e-416d-bde6-65a05eccbc1d", "451e2b54-d25f-4063-948f-01bd7cebaab7", "138049a2-5244-4799-b421-bab9d1feada8"], "metadata": {}}, "3b9d1983-3c1b-423b-b132-1100f72674af": {"node_ids": ["1686d5fe-b11e-433d-b2ae-ae0d0ac166d6", "ed2d2138-7296-4372-9bbf-171bee1866c2", "a2b13420-35ca-40d2-9df7-87435b80f27f", "d05a8f34-087a-44e4-ad06-8a128b226335", "4452e0c4-d1eb-470e-8439-f7cdfd816f13"], "metadata": {}}, "e209caf1-ead6-467f-9555-1a2e27ae1c9f": {"node_ids": ["e02bfa91-da6a-4d9d-9f4e-2347f1e94a0f"], "metadata": {}}, "b9a22696-df65-46d8-9ae7-bc9360e912fb": {"node_ids": ["e6a2f908-211f-415d-81d3-09169bfb6813", "4e243190-6938-47b3-8006-d15ada0261cc", "c514d792-de01-46d1-87ca-abf33eda5d0e", "e0992dd4-0c0d-498e-a846-4f7f7b68f59f", "e5c3898d-8380-4d44-bfa2-269be17f7145"], "metadata": {}}, "6fa08b5f-e917-4b42-9ebe-3e710e3c170e": {"node_ids": ["bff354a3-9750-4889-a0dd-11699b472edd", "e895353f-be88-42b4-934b-393a825705db", "3dba9662-1f9c-4b25-8f7a-cd783c2ae84c", "9c2e66f1-7761-4419-a5df-cd1f3ea06672", "7089157f-0053-48fd-a59b-f1648da859b3"], "metadata": {}}, "381ee8e8-4d6a-4558-b143-621cfd55728c": {"node_ids": ["745f7e7c-d8e7-4889-9eb5-618fcad78057", "a1e8f74d-fa3f-413c-8b62-dd170422eaff", "1800ede4-c7b2-4de9-b55d-2b34066e14ae", "abde9574-d6b3-4840-bcc0-e33de5bd3070", "c9c2b5a6-8b09-40ba-ac0f-b2a6873f5f23"], "metadata": {}}, "56d2b4a8-ff0a-4335-939d-9aeb172ee460": {"node_ids": ["e58987c4-bd55-4bf0-bb09-1698c797a463", "0e90a33d-f58e-4365-a8a9-4ffa41e09c95", "e5bbbc6e-ba7f-4ffd-a27f-c7606dca596e", "77e00ea8-8fb5-4044-91fd-7b108537e696", "afeebcea-17f8-4375-bf26-5a290876fe4b"], "metadata": {}}, "4d4afe67-524b-4449-a53a-5b1d6ae6c70f": {"node_ids": ["cbc3690b-efdf-4a99-943f-660533e7b5f9"], "metadata": {}}, "c8034349-37aa-434a-baba-a1780c471578": {"node_ids": ["32985451-0f54-4e07-80c7-552d7245cb21", "82a40930-4bf1-484d-8e80-27b16b533ce9", "cbb1b4f2-8ae6-42b3-b5a5-32c0fd1a45bc", "401146da-c25d-4ba5-ad60-1e09269f229f", "cf048c23-77df-48f8-b313-a9669d8d1d1c", "1691a577-07ca-4fb1-ae78-7496bf7c34f2"], "metadata": {}}, "c2effa91-65ac-4805-976d-d7b90f4eb09e": {"node_ids": ["0ec3eea0-5965-4197-86c4-c6de0f49eb59", "b4d029c3-4be1-41fb-b582-9b251b13ce44", "3ed89d36-44f5-49b6-b641-ec69f55c4c45", "d3afa8dc-ece3-4538-859d-2395de671669", "284e6c20-913d-45df-943d-2415e097f426"], "metadata": {}}, "64e5a052-01aa-40bb-aa0a-d181d256bf60": {"node_ids": ["fb895d59-7721-489d-8f8f-a2433ddff395", "e905510b-f520-4148-ac32-cfde6e07b550", "e7fdee8d-fb8b-4b70-88d7-28387deff320", "d97b4cae-b4b0-4f06-a7be-1069f141ac10", "dec47915-f50b-44a2-be1d-c858cfbae020"], "metadata": {}}, "6ca71bcc-f9a1-44ab-bf63-d8e8933276c0": {"node_ids": ["476c0ba3-36ff-4975-a779-221211e47d3b", "6448425b-2559-4546-8595-c3b65dd0e3e4", "7a35ffa7-9401-4a09-98af-479d127abace", "c1814bc3-fec5-4b96-82b3-676e2b6b0347", "c7191792-a0cf-4120-a368-bbd2841d09a2"], "metadata": {}}, "be6792a4-da55-47a2-b289-7b1b5869811d": {"node_ids": ["0f91f59a-b476-412b-9245-f889cbc210fd"], "metadata": {}}, "99895f25-3030-456c-a22c-ea588744446b": {"node_ids": ["60d49ff3-1b7b-4a96-8e1f-afabc2cee86c", "02c2a2a6-761e-4211-bfd5-86c926d63d22", "749657c9-194f-478d-b477-5fa7311abb91", "02ca04f3-521e-43e5-8af5-2acd94eed1c6", "0c4d1346-cb20-48bd-8b99-723773cb2c04", "280e9ac2-9291-48a8-9bf0-2cbb0074a5b2"], "metadata": {}}, "5f1a8ce1-3de5-4339-af32-d75e11da34f5": {"node_ids": ["9f60617e-1a2a-40e2-a93c-1dc9b6ab3bb4", "3dc9760a-cda7-4939-8472-c521a7f46e66", "894a1219-36f1-4b31-986c-03bc089e1262", "33afe81c-7321-4870-b761-ee193aa81ad2", "6d1760c6-4e1b-40df-9be3-f2f77ecd10fd"], "metadata": {}}, "e2690e2d-178c-4921-a734-f9334530f2e6": {"node_ids": ["d9b944ae-7d4c-4001-9ede-131a7bae71ed", "e6ec21f9-4ef7-4c2f-9981-ebb4ea136d25", "e300f2d2-8346-4926-9e45-958579e50006", "61eeeea0-f16d-4f99-861b-ab4e26d8f559", "cf9e4349-a22c-4b71-b1f4-43964496d78d"], "metadata": {}}, "d88f6d7f-5944-408b-bd56-1536e37a36df": {"node_ids": ["92eb3717-2b38-491d-baf3-6c0ff9b342e7", "3d25af56-930f-403f-8f20-f02c3a197bc7", "5414bd03-37a1-473a-858d-a56a90cd023b", "8e7379bb-e063-4df2-8867-a18129fcda65", "abf97515-a241-4826-aae1-39007c84f619"], "metadata": {}}, "3592c0a4-8fb2-4091-8a28-67a42ea67b94": {"node_ids": ["4acb61f4-1b83-4081-9d70-8affb4aa5e0c"], "metadata": {}}, "7696f2c1-fe39-4dc0-ae6a-71a2bc3db02d": {"node_ids": ["4530412d-0461-4aec-8c2e-94451c75012d", "0e1b6ac2-7a00-4b97-981b-5fc244c31c5e", "045dbdf7-80fc-4477-9cdd-fe8ba3dffe30", "f9735bdd-5304-4a97-9c17-ebaf054f6327", "19526a39-41ed-4d66-bb7a-142aa63cc92d"], "metadata": {}}, "cbdf320d-0210-4751-bc8e-420cdcb3c46a": {"node_ids": ["7b9fbb45-d937-4b73-a820-43f0c42e1d9d", "e5ab0743-bfe3-4e21-9e1b-3113313cb509", "4041d27a-1bdd-45ff-a06e-af9f1e2f8b98", "cefee77b-cebd-48e7-a40a-9c1f7c770a2b", "50ce162f-34f2-4eda-811c-936d27b9fb31"], "metadata": {}}, "eb544f90-4fa9-4e51-9b79-d85553445142": {"node_ids": ["1a369f9b-0d6f-4bbe-9c0b-fc461b2629a8", "22e7c4c1-23b8-4bb4-ae60-589ba5c3d7a3", "691ba182-53b9-4313-9afc-5c68f3bfb72a", "0567f877-e780-4e95-a4a9-238de74ab8fa", "fe3b96dc-a5d8-4db6-9f81-0c2bab67df04"], "metadata": {}}, "ed003dcb-d68a-41b8-88ef-93e05e14cee3": {"node_ids": ["8dbfd7da-3238-4854-8260-ae26e9f8190d", "83b74235-3739-4666-9e38-f63ed2c59679", "e688435b-aaaf-4141-87e8-f1e22fecb5a6", "ac07f681-2700-4b32-af6a-a4597a472e52", "9e12552e-e877-4bb0-84ae-6c51230f6ad8"], "metadata": {}}, "87de77de-8c9a-4dc5-987b-0dc007f852a7": {"node_ids": ["0de97f25-2e9e-4277-9c7a-25200581dd02"], "metadata": {}}, "7e38e9f9-1c19-4994-9ebf-7a7f025e98ab": {"node_ids": ["710a03ff-d5ec-48fc-bb24-240df35a4ac8", "25839326-3a8d-4ccc-bfd8-f565f47279db", "3551135e-b305-4d1f-ad3f-740b6e77e337", "ca24b290-c3e9-4f9d-912a-914981fc368f", "c091822f-cf84-43c4-912a-f283e9175500"], "metadata": {}}, "410e3c43-05c1-4ae2-b7ce-b4ce82ac3086": {"node_ids": ["978a5215-1623-4a61-8feb-9255f630d968", "f126c652-bc73-4eae-90a8-9e0fdf8e0aa7", "7d4e9a1a-35c4-4795-96f8-97258f6ae872", "0932b200-84c6-4488-bfc5-2396cd5c9b95", "759f6c8c-8c8c-4cad-a6b3-a5984bcc99d5"], "metadata": {}}, "1ceb9831-1954-4b8d-a153-1a613db8408a": {"node_ids": ["a043749e-7ef5-4b9e-aeb1-6890d7db8c8d", "fb6f45f0-2d5b-41ac-9bfd-8950cde6e539", "3077a316-a55b-413f-9eb6-138dbe78451a", "03f038cc-c3c7-4d93-91da-42d3daa5a276", "6575adcf-4350-4942-834d-e5fa50c56dd1"], "metadata": {}}, "9c62f556-ced5-4c6a-8278-780ade4f4643": {"node_ids": ["0fbf824b-c772-4db5-b5fa-ef25a6ab47fa", "cec0d187-467b-4b39-9224-7ca575532cbb", "b2f6ade5-eaca-44b1-af47-fb893e56fa1d", "6ea88205-e0b2-44cf-9548-75623465ac25", "113fc76d-16b2-4b81-ba17-6abb5a94f511"], "metadata": {}}, "40459286-87b0-4707-b53e-47333e6c46d3": {"node_ids": ["e687371a-384d-4866-bbed-8676f1b0a469", "dd6520ae-784e-4cca-9dbf-a7a4daa3831b", "7ced47e8-c50b-41d1-9e66-05758ec081c7", "5478d272-b834-4aea-b564-51d14663a9e3", "2e51a6da-ad2d-453a-bfe9-5c2c4fc099da"], "metadata": {}}, "c2c1168f-777a-4fc0-965c-9ce04fe596de": {"node_ids": ["68e6cf96-535c-4e35-ac82-1e87ef0e7c77", "04b6beaf-8804-4a39-9022-0b734e37e3a8", "fedcb3ca-1430-4371-ad76-fc95ec58436f", "6f374abd-ef51-4f8f-a1da-f098442c5477", "981cb88b-4244-44d6-a938-3aa5646afe1d", "dfd31fdd-e9a9-4dab-a171-abb51b1b01e3"], "metadata": {}}, "2600ede9-5e4f-4920-99c6-a34d23a10d23": {"node_ids": ["f0b5557e-d64e-44ba-a140-95adb1f5dfbd", "528e2c6f-bd10-4799-8f30-9da620bfa474", "8901e7d9-2c64-4f66-b5e7-84bee0d8cbab", "8c245c70-d293-4329-bf96-74de97202104", "488f77e7-8395-4fc0-b72f-91c002ec5a4f"], "metadata": {}}, "af068cb7-b5e2-45f7-819e-91abfb0c0dd8": {"node_ids": ["1e703c2b-dba4-46cf-8ded-6734778d1d5a", "3ac57fb1-b698-4d77-b606-aa7ec2dde7bc", "5221a53c-2c0c-44db-8829-db95e656070e", "cc43bea9-8e24-4e6d-9241-e795f49eb960", "e6947143-c9bc-4abe-9c9d-79a5fe1e7f8b"], "metadata": {}}, "b2b79e9f-0df5-46f7-b817-a49c288a6411": {"node_ids": ["a7dec7ad-180d-4584-aba7-1440a17e4926"], "metadata": {}}, "3b862e92-4358-47af-950d-ebbb46d47f6d": {"node_ids": ["1f3a2651-8e67-4f13-b519-4fb05133d539", "7ab974a1-c2d5-4f5b-ba7f-7cb8895494fb", "3daf5567-08fc-4895-9fc4-a5ddd2814134", "515b74fb-2324-44c3-bd65-4fb244f6e74d", "a4af6706-eb29-45ef-b74d-d7de98e6c324"], "metadata": {}}, "453b70bb-0eba-49cb-b453-f5e535d0ef69": {"node_ids": ["2f4221be-6049-4901-8efb-e694d6bf02d3", "ce5eeb5c-6acf-4e7b-930a-a01bbad620b5", "cb6869a9-dce1-4e01-b5f5-ab824ac0c734", "d4eed92e-1087-49a4-a506-5e1c481ca07c", "d6078340-927d-4d92-b7c9-fd480fb21ce4"], "metadata": {}}, "93e520eb-8d02-43c4-b427-0a8ba52aa2e8": {"node_ids": ["d5d3ba2f-8b66-4e08-8aaf-5ece6ba7cf19", "f4317992-1e40-424e-8538-d9ad7e83c557", "e7ea3364-fb19-4d7e-baa9-ea9b311062e0", "0a07698f-dd77-4a84-a079-4546508d342a", "848f1cde-102a-4379-ada5-a556f70b4b2b", "a1157e1f-ad11-4ca4-9184-987e74252bae"], "metadata": {}}, "db514a78-d261-471f-8efb-8514117e6f4e": {"node_ids": ["317486b6-b619-44da-b962-cbee9a56f424", "b2b58adf-a680-4275-ae91-7be5c85e20a7", "236a59e2-54bc-4212-a555-5ed4d2bce5c0", "787786c1-3692-41ff-aaba-cf5b6b1c3e57", "089dcf6d-5b10-43cf-9ea5-1dea702c7438"], "metadata": {}}, "9278baa9-0cba-447f-b50c-77148e46b013": {"node_ids": ["94d116e3-498d-4027-89f8-a5254567289c"], "metadata": {}}, "3caf8b77-3b63-4094-aee3-964c13c6c0f8": {"node_ids": ["78085d7d-4ea8-45e2-9aee-16f04562ec6e", "4b46f14d-21db-4664-98ba-8af0ed85d3ee", "4930c5a3-29a2-47bb-bd40-8bae48133f38", "1034634c-de88-4479-b5e0-5222c05e3d8b", "8a76ebce-b70f-40d6-ab38-599e67907623"], "metadata": {}}, "d66fb9bc-c8b7-47cd-aa6a-95e397b0fc03": {"node_ids": ["be0e2e87-03db-4fa1-8995-760715058d4c", "83de8f90-ecb7-4af1-889c-436add074d8c", "2b8bb825-14cb-4e09-83a1-e561d198f0db", "eea9192d-fb03-4210-8a8b-4f4422ced3b3", "d9c00693-e096-43b0-9234-78e0bb1e380e"], "metadata": {}}, "ea3280e2-9061-4d36-86ab-2bc4d70fce08": {"node_ids": ["6e31af5c-b9e7-4a58-a655-31047102064b"], "metadata": {}}}}